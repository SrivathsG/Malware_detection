Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Language),Custom field (Language),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Spark UI throws 500 error when StructuredStreaming query filter is selected,SPARK-45714,13555911,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,stym06,stym06,28/Oct/23 05:45,28/Oct/23 05:55,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Java API,,,,,0,,,,,"h2.  

When I'm trying to do the following inside the listener class, and go to the UI to check the streaming query tab and click on Sort by Latest Batch, I get the below error.

 
{code:java}
class MyListener extends StreamingQueryListener {

  private val kafkaAdminClient = try {
    Some(Admin.create(getKafkaProp()))
  } catch {
    case _: Throwable => None
  } 
  private val registeredConsumerGroups = kafkaAdminClient.get.listConsumerGroups().all().get()

  override def onQueryProgress(event: StreamingQueryListener.QueryProgressEvent): Unit = {
    val eventName = event.progress.name
    commitConsumerOffset(event.progress.sources.head.endOffset, eventName)
  }

  private def getKafkaProp(eventName: String = ""dummy-admin""): Properties = {
    val props: Properties = new Properties()
    props.put(""bootstrap.servers"", Configuration.configurationMap(""kafka.broker""))
    props.put(""group.id"", eventName)
    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ""org.apache.kafka.common.serialization.StringDeserializer"")
    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ""org.apache.kafka.common.serialization.StringDeserializer"")
    props.put(""security.protocol"", Configuration.configurationMap(""kafka.security.protocol""))
    props.put(""sasl.mechanism"", Configuration.configurationMap(""kafka.sasl.mechanism""))
    props.put(""sasl.jaas.config"", Configuration.configurationMap(""kafka.sasl.jaas.config""))
    props
  }

  private def getTopicPartitionMap(topic: String, jsonOffsets: Map[String, Map[String, Long]]) = {
    val offsets = jsonOffsets.head._2
    val topicPartitionMap = new util.HashMap[TopicPartition, OffsetAndMetadata]()
    offsets.keys.foreach(partition => {
      val tp = new TopicPartition(topic, partition.toInt)
      val oam = new OffsetAndMetadata(offsets(partition).asInstanceOf[Number].longValue())
      topicPartitionMap.put(tp, oam)
    })
    topicPartitionMap
  }


  private def commitConsumerOffset(endOffset: String, eventName: String) = {
    val jsonOffsets = mapper.readValue(endOffset, classOf[Map[String, Map[String, Long]]])
    val topicPartitionMap = getTopicPartitionMap(eventName, jsonOffsets)
    if (!registeredConsumerGroups.contains(eventName)) {
      print(""topic consumer not created! creating"")
      val kafkaConsumer = new KafkaConsumer[String, String](getKafkaProp(eventName))
      kafkaConsumer.commitSync(topicPartitionMap)
      kafkaConsumer.close()
    } else {
      print(""committing cg offsets using admin"")
      kafkaAdminClient.get.alterConsumerGroupOffsets(eventName, topicPartitionMap).all().get()
    }
  }
}{code}
 

 
h2. HTTP ERROR 500 java.lang.NullPointerException
||URI:|/StreamingQuery/active|
||STATUS:|500|
||MESSAGE:|java.lang.NullPointerException|
||SERVLET:|org.apache.spark.ui.JettyUtils$$anon$1-522a82dd|
||CAUSED BY:|java.lang.NullPointerException|
h3. Caused by:

 
{code:java}
java.lang.NullPointerException at org.apache.spark.sql.streaming.ui.StreamingQueryDataSource.$anonfun$ordering$9(StreamingQueryPage.scala:258) at org.apache.spark.sql.streaming.ui.StreamingQueryDataSource.$anonfun$ordering$9$adapted(StreamingQueryPage.scala:258) at scala.math.Ordering$$anon$5.compare(Ordering.scala:253) at java.base/java.util.TimSort.binarySort(TimSort.java:296) at java.base/java.util.TimSort.sort(TimSort.java:239) at java.base/java.util.Arrays.sort(Arrays.java:1441) at scala.collection.SeqLike.sorted(SeqLike.scala:659) at scala.collection.SeqLike.sorted$(SeqLike.scala:647) at scala.collection.AbstractSeq.sorted(Seq.scala:45) at org.apache.spark.sql.streaming.ui.StreamingQueryDataSource.<init>(StreamingQueryPage.scala:223) at org.apache.spark.sql.streaming.ui.StreamingQueryPagedTable.dataSource(StreamingQueryPage.scala:149) at org.apache.spark.ui.PagedTable.table(PagedTable.scala:101) at org.apache.spark.ui.PagedTable.table$(PagedTable.scala:100) at org.apache.spark.sql.streaming.ui.StreamingQueryPagedTable.table(StreamingQueryPage.scala:114) at org.apache.spark.sql.streaming.ui.StreamingQueryPage.queryTable(StreamingQueryPage.scala:101) at org.apache.spark.sql.streaming.ui.StreamingQueryPage.generateStreamingQueryTable(StreamingQueryPage.scala:60) at org.apache.spark.sql.streaming.ui.StreamingQueryPage.render(StreamingQueryPage.scala:38) at org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90) at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81) at javax.servlet.http.HttpServlet.service(HttpServlet.java:503) at javax.servlet.http.HttpServlet.service(HttpServlet.java:590) at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:799) at org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626) at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95) at org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193) at org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601) at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:185) at org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193) at org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601) at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548) at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233) at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434) at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188) at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501) at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186) at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349) at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763) at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234) at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127) at org.sparkproject.jetty.server.Server.handle(Server.java:516) at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388) at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633) at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380) at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277) at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311) at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105) at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131) at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:386) at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883) at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034) at java.base/java.lang.Thread.run(Thread.java:829)
 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-28 05:45:34.0,,,,,,,,,,"0|z1l974:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish all snapshots or none,SPARK-45709,13555826,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,EnricoMi,EnricoMi,27/Oct/23 12:01,27/Oct/23 12:05,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Build,,,,,0,pull-request-available,,,,"We have seen failures during publishing snapshots due to build failures (SPARK-45651). As a consequence, some packages have been published and others haven't. This leads to an inconsistency across the latest Spark snapshots, which can cause {{ClassNotFoundException}}s like in https://github.com/apache/spark/pull/43102#issuecomment-1759541718.

Ideally, packages are published once *all* packages have been built, not once individual packages have been built.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-27 12:01:12.0,,,,,,,,,,"0|z1l8o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retry mvn deploy failures,SPARK-45708,13555825,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,EnricoMi,EnricoMi,27/Oct/23 11:52,27/Oct/23 11:53,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Build,,,,,0,pull-request-available,,,,"It is common to see {{408 Request Timeout}} and {{502 Proxy Error}} when deploying artifacts to Apache snapshot repository: https://github.com/apache/spark/actions/runs/6437635317

{quote}
2023-10-07T01:09:51.1719360Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:3.0.0-M1:deploy (default-deploy) on project spark-streaming_2.13: ArtifactDeployerException: Failed to deploy artifacts: Could not transfer artifact org.apache.spark:spark-streaming_2.13:jar:tests:3.3.4-20231007.005815-57 from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): transfer failed for https://repository.apache.org/content/repositories/snapshots/org/apache/spark/spark-streaming_2.13/3.3.4-SNAPSHOT/spark-streaming_2.13-3.3.4-20231007.005815-57-tests.jar, status: 502 Proxy Error -> [Help 1]
{quote}

{quote}
2023-10-07T01:11:48.5651501Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:3.0.0:deploy (default-deploy) on project spark-connect-common_2.12: Failed to deploy artifacts: Could not transfer artifact org.apache.spark:spark-connect-common_2.12:xml:cyclonedx:3.4.2-20231007.001102-103 from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): transfer failed for https://repository.apache.org/content/repositories/snapshots/org/apache/spark/spark-connect-common_2.12/3.4.2-SNAPSHOT/spark-connect-common_2.12-3.4.2-20231007.001102-103-cyclonedx.xml, status: 408 Request Timeout -> [Help 1]
{quote}

Such errors should be retried by `mvn deploy`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-27 11:52:28.0,,,,,,,,,,"0|z1l8o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cover BufferReleasingInputStream.available under tryOrFetchFailedException,SPARK-45678,13555735,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,26/Oct/23 21:00,28/Oct/23 04:15,30/Oct/23 17:26,28/Oct/23 02:21,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,,3.4.2,3.5.1,4.0.0,,Spark Core,,,,,0,pull-request-available,,,,"We have encountered shuffle data corruption issue:

```
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:112)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:504)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:543)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:450)
	at org.xerial.snappy.SnappyInputStream.available(SnappyInputStream.java:497)
	at org.apache.spark.storage.BufferReleasingInputStream.available(ShuffleBlockFetcherIterator.scala:1356)
 ```

Spark shuffle has capacity to detect corruption for a few stream op like `read` and `skip`, such `IOException` in the stack trace will be rethrown as `FetchFailedException` that will re-try the failed shuffle task. But in the stack trace it is `available` that is not covered by the mechanism. So no-retry has been happened and the Spark application just failed.

As the `available` op will also involve data decompression, we should be able to check it like `read` and `skip` do.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Oct 28 02:21:18 UTC 2023,,,,,,,,,,"0|z1l840:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/23 02:21;csun;Issue resolved by pull request 43543
[https://github.com/apache/spark/pull/43543];;;",,,,,,,,,,,,,,
Upgrade to PySpark 3.5.0 gives Class org.apache.hadoop.fs.s3a.S3AFileSystem not found,SPARK-45676,13555623,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,milesgranger,milesgranger,26/Oct/23 08:50,26/Oct/23 08:50,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"Using PySpark 3.4.1 w/ the following dependencies works fine for reading S3 files:

hadoop-client:3.3.4
hadoop-common:3.3.4
hadoop-aws:3.3.4
aws-java-sdk-bundle:1.12.262

Doing a simple upgrade to PySpark 3.5.0 (which is still using hadoop 3.3.4 AFAIK) results in failing to read the same S3 files:

```
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.parquet.hadoop.util.HadoopInputFile.fromStatus(HadoopInputFile.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
	... 14 more
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-26 08:50:27.0,,,,,,,,,,"0|z1l7f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSubmit does not support --total-executor-cores when deploying on K8s,SPARK-45670,13555583,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,26/Oct/23 04:09,27/Oct/23 06:26,30/Oct/23 17:26,27/Oct/23 06:24,3.3.3,3.4.1,3.5.0,,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.1,,Spark Submit,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 27 06:24:50 UTC 2023,,,,,,,,,,"0|z1l76g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/23 06:24;gurwls223;Issue resolved by pull request 43548
[https://github.com/apache/spark/pull/43548];;;",,,,,,,,,,,,,,
spark-sql-kafka-0-10_2.11 - Custom Configuration's for Partitioner not set.,SPARK-45666,13555529,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dinesh028,dinesh028,25/Oct/23 17:32,25/Oct/23 17:42,30/Oct/23 17:26,,2.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"We had a requirement to write Custom org.apache.kafka.clients.producer.Partitioner to use with Kafka Data Source available with package ""{{{}spark-sql-kafka-0-10_2.11{}}} ""

Ideally, properties set as part of Producer are available to  Partitioner method -

[configure|https://kafka.apache.org/24/javadoc/org/apache/kafka/common/Configurable.html#configure-java.util.Map-]([Map|https://docs.oracle.com/javase/8/docs/api/java/util/Map.html?is-external=true]<[String|https://docs.oracle.com/javase/8/docs/api/java/lang/String.html?is-external=true],?> configs)

 
 
But, we realized that Custom properties passed as options to Kafka format DataFrameWriter are not available to Partitioner whether we append that property with literal ""kafka."" or not.

Only, Configs listed on - [https://kafka.apache.org/documentation/#producerconfigs] were passed to Partitioner. But, in some cases it is required to pass custom properties for initialization of Partitioner. 

Thus, there should be provision to set custom properties as options with Kafka Data Source not just producer configs. Otherwise, custom partitioner can't be initialized and implemented as per need.

 

For example - 
_df.write.format(""kafka""){color:#57d9a3}.option(""Kafka.customproperty1"", ""value1"").option(""kafka.partitioner.class"", ""com.mycustom.ipartitioner""){color}_

..
...
.....
_package com.mycustom;_
_import org.apache.kafka.clients.producer.Partitioner;_
public class _ipartitioner implemets Partitioner{_

_@override_
_public void configure(Map<String,?> configs){_

{color:#FF0000}_system.out.println(configs) //_ _customproperty1 is missing here which should be available._{color}

_}_
_..._
_..._

_}_

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-25 17:32:19.0,,,,,,,,,,"0|z1l6ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyspark  union method throw 'pyspark.sql.utils.IllegalArgumentException: requirement failed',SPARK-45662,13555438,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,angerszhuuuu,angerszhuuuu,25/Oct/23 07:01,25/Oct/23 07:57,30/Oct/23 17:26,,3.2.1,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"!image-2023-10-25-15-01-28-896.png|width=1795,height=194!

 

 

The code is 
{code:java}
    output_df = spark.createDataFrame([], output_schema())
    output_df.printSchema()
    print(output_df.count()) {code}
I have checked that the schema is same, `final` is from a sql query.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/23 07:01;angerszhuuuu;image-2023-10-25-15-01-28-896.png;https://issues.apache.org/jira/secure/attachment/13063847/image-2023-10-25-15-01-28-896.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 25 07:57:47 UTC 2023,,,,,,,,,,"0|z1l6a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/23 07:05;angerszhuuuu;ping [~gurwls223] Could you take a look? I have checked that the env is correct and python is 3.6;;;","25/Oct/23 07:57;gurwls223;Can you provide a reproducer please;;;",,,,,,,,,,,,,
Canonicalization of DynamicPruningSubquery is broken,SPARK-45658,13555430,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ashahid7,ashahid7,25/Oct/23 04:59,26/Oct/23 01:15,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"The canonicalization of (buildKeys: Seq[Expression]) in the class DynamicPruningSubquery is broken, as the buildKeys are canonicalized just by calling 
buildKeys.map(_.canonicalized)
The  above would result in incorrect canonicalization as it would not be normalizing the exprIds relative to buildQuery output
The fix is to use the buildQuery : LogicalPlan's output to normalize the buildKeys expression
as given below, using the standard approach.

buildKeys.map(QueryPlan.normalizeExpressions(_, buildQuery.output)),

Will be filing a PR and bug test for the same.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-25 04:59:40.0,,,,,,,,,,"0|z1l68g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caching SQL UNION of different column data types does not work inside Dataset.union,SPARK-45657,13555412,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jzhuge,jzhuge,25/Oct/23 00:20,25/Oct/23 05:34,30/Oct/23 17:26,25/Oct/23 05:34,3.3.2,3.4.0,3.4.1,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,," 

Cache SQL UNION of 2 sides with different column data types
{code:java}
scala> spark.sql(""select 1 id union select 's2' id"").cache()  {code}
Dataset.union does not leverage the cache
{code:java}
scala> spark.sql(""select 1 id union select 's2' id"").union(spark.sql(""select 's3'"")).queryExecution.optimizedPlan
res15: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Union false, false
:- Aggregate [id#109], [id#109]
:  +- Union false, false
:     :- Project [1 AS id#109]
:     :  +- OneRowRelation
:     +- Project [s2 AS id#108]
:        +- OneRowRelation
+- Project [s3 AS s3#111]
   +- OneRowRelation {code}
SQL UNION of the cached SQL UNION does use the cache! Please note `InMemoryRelation` used.
{code:java}
scala> spark.sql(""(select 1 id union select 's2' id) union select 's3'"").queryExecution.optimizedPlan
res16: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Aggregate [id#117], [id#117]
+- Union false, false
   :- InMemoryRelation [id#117], StorageLevel(disk, memory, deserialized, 1 replicas)
   :     +- *(4) HashAggregate(keys=[id#100], functions=[], output=[id#100])
   :        +- Exchange hashpartitioning(id#100, 500), ENSURE_REQUIREMENTS, [plan_id=241]
   :           +- *(3) HashAggregate(keys=[id#100], functions=[], output=[id#100])
   :              +- Union
   :                 :- *(1) Project [1 AS id#100]
   :                 :  +- *(1) Scan OneRowRelation[]
   :                 +- *(2) Project [s2 AS id#99]
   :                    +- *(2) Scan OneRowRelation[]
   +- Project [s3 AS s3#116]
      +- OneRowRelation {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 25 05:34:44 UTC 2023,,,,,,,,,,"0|z1l64g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/23 00:35;jzhuge;Root cause:
 # SQL UNION of 2 sides with different data types produce a Project of Project on 1 side to cast the type. When this is cached, the Project of Project is preserved.
{noformat}
Distinct
+- Union false, false
   :- Project [cast(id#153 as string) AS id#155]
   :  +- Project [1 AS id#153]
   :     +- OneRowRelation
   +- Project [s2 AS id#154]
      +- OneRowRelation{noformat}

 # Dataset.union applies `CombineUnions` which applies to all unions in the tree. CombineUnions collapses the 2 Projects into 1, thus Dataset.union of the above plan with any plan will not be able to find a matching cached plan.
{code:java}
object CombineUnions extends Rule[LogicalPlan] {
...
  private def flattenUnion(union: Union, flattenDistinct: Boolean):
...
    case p1 @ Project(_, p2: Project)
      if canCollapseExpressions(p1.projectList, p2.projectList, alwaysInline = false) &&
        !p1.projectList.exists(SubqueryExpression.hasCorrelatedSubquery) &&
        !p2.projectList.exists(SubqueryExpression.hasCorrelatedSubquery) =>
      val newProjectList = buildCleanedProjectList(p1.projectList, p2.projectList)
      stack.pushAll(Seq(p2.copy(projectList = newProjectList))){code}

 ;;;","25/Oct/23 00:46;jzhuge;Checking whether this is still an issue in main branch.;;;","25/Oct/23 00:50;jzhuge;Interesting, there is warning in Dataset.union
{code:java}
def union(other: Dataset[T]): Dataset[T] = withSetOperator {
  // This breaks caching, but it's usually ok because it addresses a very specific use case:
  // using union to union many files or partitions.
  CombineUnions(Union(logicalPlan, other.logicalPlan))
} {code};;;","25/Oct/23 04:32;jzhuge;It is fixed in main branch
{code:java}
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 4.0.0-SNAPSHOT
      /_/Using Scala version 2.13.12 (OpenJDK 64-Bit Server VM, Java 17.0.7)
Type in expressions to have them evaluated.
Type :help for more information.
23/10/24 21:30:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://192.168.86.29:4040
Spark context available as 'sc' (master = local[*], app id = local-1698208231783).
Spark session available as 'spark'.scala> spark.sql(""select 1 id union select 's2' id"").cache()
val res0: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string]scala> spark.sql(""select 1 id union select 's2' id"").union(spark.sql(""select 's3'"")).queryExecution.optimizedPlan
val res1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Union false, false
:- InMemoryRelation [id#11], StorageLevel(disk, memory, deserialized, 1 replicas)
:     +- AdaptiveSparkPlan isFinalPlan=false
:        +- HashAggregate(keys=[id#2], functions=[], output=[id#2])
:           +- Exchange hashpartitioning(id#2, 200), ENSURE_REQUIREMENTS, [plan_id=30]
:              +- HashAggregate(keys=[id#2], functions=[], output=[id#2])
:                 +- Union
:                    :- Project [1 AS id#2]
:                    :  +- Scan OneRowRelation[]
:                    +- Project [s2 AS id#1]
:                       +- Scan OneRowRelation[]
+- Project [s3 AS s3#13]
   +- OneRowRelation {code};;;","25/Oct/23 05:34;jzhuge;The issue is fixed in 3.5.0;;;",,,,,,,,,,
Fix observation when named observations with the same name on different datasets.,SPARK-45656,13555411,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,25/Oct/23 00:18,25/Oct/23 07:59,30/Oct/23 17:26,25/Oct/23 07:59,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 25 07:59:35 UTC 2023,,,,,,,,,,"0|z1l648:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/23 07:59;gurwls223;Issue resolved by pull request 43519
[https://github.com/apache/spark/pull/43519];;;",,,,,,,,,,,,,,
current_date() not supported in Streaming Query Observed metrics,SPARK-45655,13555397,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,bhuwan.sahni,bhuwan.sahni,24/Oct/23 20:48,30/Oct/23 07:20,30/Oct/23 17:26,,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,,,,,Structured Streaming,,,,,0,pull-request-available,,,,"Streaming queries do not support current_date() inside CollectMetrics. The primary reason is that current_date() (resolves to CurrentBatchTimestamp) is marked as non-deterministic. However, {{current_date}} and {{current_timestamp}} are both deterministic today, and {{current_batch_timestamp}} should be the same.

 

As an example, the query below fails due to observe call on the DataFrame.

 
{quote}val inputData = MemoryStream[Timestamp]

inputData.toDF()
      .filter(""value < current_date()"")
      .observe(""metrics"", count(expr(""value >= current_date()"")).alias(""dropped""))
      .writeStream
      .queryName(""ts_metrics_test"")
      .format(""memory"")
      .outputMode(""append"")
      .start()
{quote}
 ",,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 24 22:21:14 UTC 2023,,,,,,,,,,"0|z1l614:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/23 20:48;bhuwan.sahni;I am working on a fix for this issue, and will submit a PR soon.;;;","24/Oct/23 22:21;bhuwan.sahni;PR link https://github.com/apache/spark/pull/43517;;;",,,,,,,,,,,,,
Snapshots of some packages are not published any more,SPARK-45651,13555371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EnricoMi,EnricoMi,EnricoMi,24/Oct/23 15:52,27/Oct/23 22:22,30/Oct/23 17:26,27/Oct/23 09:43,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Build,,,,,0,pull-request-available,,,,"Snapshots of some packages are not been published anymore, e.g. spark-sql_2.13-4.0.0 has not been published since Sep, 13th: https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-sql_2.13/4.0.0-SNAPSHOT/

There have been some attempts to fix CI: SPARK-45535 SPARK-45536

Assumption is that memory consumption during build exceeds the available memory of the Github host.

The following could be attempted:
- enable manual trigger of the {{publish_snapshots.yml}} workflow
- enable some memory use logging to proof that exceeded memory is the root cause
- attempt to reduce memory footprint and see impact in above logging
- revert memory use logging",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 27 09:43:19 UTC 2023,,,,,,,,,,"0|z1l5vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/23 23:58;gurwls223;Issue resolved by pull request 43512
[https://github.com/apache/spark/pull/43512];;;","27/Oct/23 00:48;gurwls223;Reverted in https://github.com/apache/spark/commit/df0262f29969fe40f53dee070a150f2bfe98484c and https://github.com/apache/spark/commit/0d665fe8c87b037516f21162d2f5545580776af3;;;","27/Oct/23 09:43;LuciferYang;Issue resolved by pull request 43555
[https://github.com/apache/spark/pull/43555];;;",,,,,,,,,,,,
"After upgrading to Spark 3.4.1 and 3.5.0 we receive RuntimeException ""scala.Some is not a valid external type for schema of array<string>""",SPARK-45644,13555277,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,adiwehrli,adiwehrli,24/Oct/23 04:33,25/Oct/23 06:37,30/Oct/23 17:26,,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,,,,,Spark Core,SQL,,,,0,,,,,"I do not really know if this is a bug, but I am at the end with my knowledge.

A Spark job ran successfully with Spark 3.2.x and 3.3.x. 

But after upgrading to 3.4.1 (as well as with 3.5.0) running the same job with the same data the following always occurs now:
{code}
scala.Some is not a valid external type for schema of array<string>
{code}

The corresponding stacktrace is:
{code}
2023-10-24T06:28:50.932 level=ERROR logger=org.apache.spark.executor.Executor msg=""Exception in task 0.0 in stage 0.0 (TID 0)"" thread=""Executor task launch worker for task 0.0 in stage 0.0 (TID 0)""
java.lang.RuntimeException: scala.Some is not a valid external type for schema of array<string>
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.MapObjects_10$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.ExternalMapToCatalyst_1$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.createNamedStruct_14_3$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.If_12$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.ObjectOperator$.$anonfun$serializeObjectToRow$1(objects.scala:165) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.sql.execution.AppendColumnsWithObjectExec.$anonfun$doExecute$15(objects.scala:380) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:169) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.0.jar:3.5.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:834) [?:?]
2023-10-24T06:28:50.932 level=ERROR logger=org.apache.spark.executor.Executor msg=""Exception in task 1.0 in stage 0.0 (TID 1)"" thread=""Executor task launch worker for task 1.0 in stage 0.0 (TID 1)""
java.lang.RuntimeException: scala.Some is not a valid external type for schema of array<string>
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.MapObjects_10$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.ExternalMapToCatalyst_1$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.createNamedStruct_14_3$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.If_12$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.ObjectOperator$.$anonfun$serializeObjectToRow$1(objects.scala:165) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.sql.execution.AppendColumnsWithObjectExec.$anonfun$doExecute$15(objects.scala:380) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:169) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.0.jar:3.5.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.0.jar:3.5.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:834) [?:?]
{code}

As the error occurs in generated code we cannot debug what was really the cause. We patched the {{ValidateExternalType}} case class (in trait {{org.apache.spark.sql.catalyst.expressions.InvokeLike}}) adding some sysout statements but we could still not get any answer which data structure was causing this.

And we did not find anything in the upgrade guides about such a behaviour or how to change some property to re-gain the former behaviour.

What could be the cause for this? In Spark 3.3.3 {{ScalaReflection}} was used in {{InvokeLite}}, Spark 3.4.x and 3.5.0 now use {{EncoderUtils}} instead. 

The same also occurs if we use Scala 2.12.18.

h4. Some dependencies information:

h5. Spark 3.3.3
* Avro {{1.11.0}}
* SnakeYAML {{1.31}}
* FasterXML Jackson {{2.13.4}}
* Json4s {{3.7.0-M11}}
* scala-collection-compat_2.12 {{2.3.0}}
* Kafka {{3.4.1}}
* kafka-avro-serializer {{7.4.1}}

h5. Spark 3.5.0
* Avro {{1.11.2}}
* SnakeYAML {{2.0}}
* FasterXML Jackson {{2.15.2}}
* Json4s {{3.7.0-M11}}
* scala-collection-compat_2.12 {{2.3.0}}
* Kafka {{3.5.1}}
* kafka-avro-serializer {{7.5.1}}

BTW: I tried Spark 3.5.0 with the same dependcies as listed above for Spark 3.3.3 but the error still occurred.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-24 04:33:45.0,,,,,,,,,,"0|z1l5ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Time window aggregation in separate streams followed by stream-stream join not returning results,SPARK-45637,13555222,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,azera,azera,23/Oct/23 16:29,27/Oct/23 06:00,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Structured Streaming,,,,,0,,,,,"According to documentation update (SPARK-42591) resulting from SPARK-42376, Spark 3.5.0 should support time-window aggregations in two separate streams followed by stream-stream window join:

[https://github.com/apache/spark/blob/261b281e6e57be32eb28bf4e50bea24ed22a9f21/docs/structured-streaming-programming-guide.md?plain=1#L1939-L1995]

However, I failed to reproduce this example and the query I built doesn't return any results:
{code:java}
from pyspark.sql.functions import rand
from pyspark.sql.functions import expr, window, window_time

spark.conf.set(""spark.sql.shuffle.partitions"", ""1"")

impressions = (
    spark    
    .readStream.format(""rate"").option(""rowsPerSecond"", ""5"").option(""numPartitions"", ""1"").load()    
    .selectExpr(""value AS adId"", ""timestamp AS impressionTime"")
)

impressionsWithWatermark = impressions \
    .selectExpr(""adId AS impressionAdId"", ""impressionTime"") \
    .withWatermark(""impressionTime"", ""10 seconds"")

clicks = (  
    spark  
    .readStream.format(""rate"").option(""rowsPerSecond"", ""5"").option(""numPartitions"", ""1"").load()  
    .where((rand() * 100).cast(""integer"") < 10)  # 10 out of every 100 impressions result in a click  
    .selectExpr(""(value - 10) AS adId "", ""timestamp AS clickTime"")  # -10 so that a click with same id as impression is generated later (i.e. delayed data).
    .where(""adId > 0"")  
) 

clicksWithWatermark = clicks \
    .selectExpr(""adId AS clickAdId"", ""clickTime"") \
    .withWatermark(""clickTime"", ""10 seconds"")

clicksWindow = clicksWithWatermark.groupBy(      
    window(clicksWithWatermark.clickTime, ""1 minute"")
).count()

impressionsWindow = impressionsWithWatermark.groupBy(
    window(impressionsWithWatermark.impressionTime, ""1 minute"")
).count()

clicksAndImpressions = clicksWindow.join(impressionsWindow, ""window"", ""inner"")

clicksAndImpressions.writeStream \
    .format(""memory"") \
    .queryName(""clicksAndImpressions"") \
    .outputMode(""append"") \
    .start() {code}
 

My intuition is that I'm getting no results because to output results of the first stateful operator (time window aggregation), a watermark needs to pass the end timestamp of the window. And once the watermark is after the end timestamp of the window, this window is ignored at the second stateful operator (stream-stream) join because it's behind the watermark. Indeed, a small hack done to event time column (adding one minute) between two stateful operators makes it possible to get results:
{code:java}
clicksWindow2 = clicksWithWatermark.groupBy( 
    window(clicksWithWatermark.clickTime, ""1 minute"")
).count().withColumn(""window_time"", window_time(""window"") + expr('INTERVAL 1 MINUTE')).drop(""window"")

impressionsWindow2 = impressionsWithWatermark.groupBy(
    window(impressionsWithWatermark.impressionTime, ""1 minute"")
).count().withColumn(""window_time"", window_time(""window"") + expr('INTERVAL 1 MINUTE')).drop(""window"")

clicksAndImpressions2 = clicksWindow2.join(impressionsWindow2, ""window_time"", ""inner"")

clicksAndImpressions2.writeStream \
    .format(""memory"") \
    .queryName(""clicksAndImpressions2"") \
    .outputMode(""append"") \
    .start()  {code}
 ",I'm using Spark 3.5.0 on Databricks Runtime 14.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-23 16:29:11.0,,,,,,,,,,"0|z1l4y8:",9223372036854775807,,,,,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup unused import for PySpark testing,SPARK-45635,13555185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,23/Oct/23 10:58,26/Oct/23 02:17,30/Oct/23 17:26,26/Oct/23 02:17,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,PySpark,Tests,,,,0,pull-request-available,,,,Cleanup unused import for PySpark testing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Oct 26 02:17:31 UTC 2023,,,,,,,,,,"0|z1l4q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/23 02:17;gurwls223;Issue resolved by pull request 43489
[https://github.com/apache/spark/pull/43489];;;",,,,,,,,,,,,,,
Remove `get_dtype_counts` from Pandas API on Spark,SPARK-45634,13555180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,23/Oct/23 10:34,26/Oct/23 02:17,30/Oct/23 17:26,26/Oct/23 02:16,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Pandas API on Spark,,,,,0,pull-request-available,,,,The internal API get_dtype_counts is no longer used from Pandas API on Spark.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Oct 26 02:16:49 UTC 2023,,,,,,,,,,"0|z1l4ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/23 02:16;gurwls223;Issue resolved by pull request 43488
[https://github.com/apache/spark/pull/43488];;;",,,,,,,,,,,,,,
Broken backward compatibility in PySpark: StreamingQueryListener due to the addition of onQueryIdle,SPARK-45631,13555142,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kabhwan,kabhwan,kabhwan,23/Oct/23 06:13,23/Oct/23 12:25,30/Oct/23 17:26,23/Oct/23 12:25,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Structured Streaming,,,,,0,pull-request-available,,,,"We observed the issue that existing StreamingQueryListener implementations in PySpark start to fail after upgrading to 3.5.0. We identified that the root cause is https://issues.apache.org/jira/browse/SPARK-43183 since onQueryIdle was defined as `@abstractmethod`, requiring all exiting implementations to implement the method.

It seems like abstractmethod annotation should be removed to retain backward compatibility.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Oct 23 12:25:15 UTC 2023,,,,,,,,,,"0|z1l4gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/23 06:13;kabhwan;Will submit a PR shortly.;;;","23/Oct/23 12:25;kabhwan;Issue resolved by pull request 43483
[https://github.com/apache/spark/pull/43483];;;",,,,,,,,,,,,,
Convert _LEGACY_ERROR_TEMP_1055 to REQUIRES_SINGLE_PART_NAMESPACE,SPARK-45626,13555111,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,22/Oct/23 13:18,24/Oct/23 07:47,30/Oct/23 17:26,24/Oct/23 07:47,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 24 07:47:56 UTC 2023,,,,,,,,,,"0|z1l49k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/23 07:47;maxgekk;Issue resolved by pull request 43479
[https://github.com/apache/spark/pull/43479];;;",,,,,,,,,,,,,,
Usages of ParVector are unsafe because it does not propagate ThreadLocals or SparkSession,SPARK-45616,13554986,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ankurd,ankurd,ankurd,20/Oct/23 17:41,23/Oct/23 02:48,30/Oct/23 17:26,23/Oct/23 02:48,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Spark Core,SQL,Tests,,,0,pull-request-available,,,,"CastSuiteBase and ExpressionInfoSuite use ParVector.foreach() to run Spark SQL queries in parallel. They incorrectly assume that each parallel operation will inherit the main thread’s active SparkSession. This is only true when these parallel operations run in freshly-created threads. However, when other code has already run some parallel operations before Spark was started, then there may be existing threads that do not have an active SparkSession. In that case, these tests fail with NullPointerExceptions when creating SparkPlans or running SQL queries.

The fix is to use the existing method ThreadUtils.parmap(). This method creates fresh threads that inherit the current active SparkSession, and it propagates the Spark ThreadLocals.

We should also add a scalastyle warning against use of ParVector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Oct 23 02:48:29 UTC 2023,,,,,,,,,,"0|z1l3hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/23 02:48;cloud_fan;Issue resolved by pull request 43466
[https://github.com/apache/spark/pull/43466];;;",,,,,,,,,,,,,,
spark.python.pyspark.sql.functions Typo at date_format Function,SPARK-45611,13554825,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,metecanakar,metecanakar,metecanakar,19/Oct/23 17:42,20/Oct/23 08:37,30/Oct/23 17:26,20/Oct/23 00:12,3.5.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,PySpark,,,,,0,pull-request-available,,,,"In the spark.python.pyspark.sql.functions module, at the {{date_format}} method's doctest, there is a typo in the year format.

Instead of '{{{}MM/dd/yyy'{}}}, it should be {{'MM/dd/yyyy'}} as the expected output{{{}[Row(date='04/08/2015')]{}}} indicates the following format {{""MM/dd/yyyy"".}}

 

From the official documentation:

!image-2023-10-19-19-46-22-918.png|width=633,height=365!
{code:java}
df = spark.createDataFrame([('2015-04-08',)], ['dt'])
df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()
[Row(date='04/08/2015')]
{code}
 

As a solution, I proposed the PR [https://github.com/apache/spark/pull/43442].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/23 17:46;metecanakar;image-2023-10-19-19-46-22-918.png;https://issues.apache.org/jira/secure/attachment/13063715/image-2023-10-19-19-46-22-918.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 20 00:12:07 UTC 2023,,,,,,,,,,"0|z1l2i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/23 00:12;gurwls223;Issue resolved by pull request 43442
[https://github.com/apache/spark/pull/43442];;;",,,,,,,,,,,,,,
Converting timestamp_ntz to array<timestamp_ntz> can cause NPE or SEGFAULT on parquet vectorized reader,SPARK-45604,13554711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,majdyz,majdyz,majdyz,19/Oct/23 07:47,22/Oct/23 05:54,30/Oct/23 17:26,22/Oct/23 05:54,3.5.0,,,,,,,,,,,,,,,,,,,3.4.2,3.5.1,4.0.0,,Spark Core,,,,,0,pull-request-available,,,,"Repro:

```

val path = ""/tmp/sample_parquet_file""

spark.sql(""SELECT CAST('2019-01-01' AS TIMESTAMP_NTZ) AS field"").write.parquet(path)
spark.read.schema(""field ARRAY<TIMESTAMP_NTZ>"").parquet(path).collect()

```

Depending on the memory mode, it will throw an NPE on OnHeap mode and SEGFAULT on OffHeap mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Oct 22 05:54:12 UTC 2023,,,,,,,,,,"0|z1l1so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/23 05:54;maxgekk;Issue resolved by pull request 43451
[https://github.com/apache/spark/pull/43451];;;",,,,,,,,,,,,,,
merge_spark_pr shall notice us about GITHUB_OAUTH_KEY expiry,SPARK-45603,13554689,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,19/Oct/23 04:54,20/Oct/23 01:03,30/Oct/23 17:26,20/Oct/23 01:02,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Project Infra,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 20 01:02:52 UTC 2023,,,,,,,,,,"0|z1l1ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/23 01:02;podongfeng;Issue resolved by pull request 43447
[https://github.com/apache/spark/pull/43447];;;",,,,,,,,,,,,,,
stackoverflow when executing rule ExtractWindowExpressions,SPARK-45601,13554681,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Resolved,,JacobZheng,JacobZheng,19/Oct/23 03:43,20/Oct/23 02:27,30/Oct/23 17:26,20/Oct/23 02:27,3.2.3,,,,,,,,,,,,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"I am encountering stackoverflow errors while executing the following test case. I looked at the source code and it is ExtractWindowExpressions not extracting the window correctly and encountering a dead loop at resolveOperatorsDownWithPruning that is causing it.

{code:scala}
 test(""agg filter contains window"") {
    val src = Seq((1, ""b"", ""c"")).toDF(""col1"", ""col2"", ""col3"")
      .withColumn(""test"",
        expr(""count(col1) filter (where min(col1) over(partition by col2 order by col3)>1)""))
    src.show()
  }
{code}

Now my question is this kind of in agg filter (window) is the correct usage? Or should I add a check like spark sql and throw an error ""It is not allowed to use window functions inside WHERE clause""?
",,,,,,,,,,,,,,,,SPARK-38666,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 20 02:26:18 UTC 2023,,,,,,,,,,"0|z1l1m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/23 14:54;bersprockets;Possibly SPARK-38666;;;","20/Oct/23 02:26;JacobZheng;Got it, Thanks [~bersprockets];;;",,,,,,,,,,,,,
Percentile can produce a wrong answer if -0.0 and 0.0 are mixed in the dataset,SPARK-45599,13554652,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,,,revans2,revans2,18/Oct/23 21:33,18/Oct/23 21:35,30/Oct/23 17:26,,3.2.3,3.3.0,3.5.0,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,data-corruption,,,,"I think this actually impacts all versions that have ever supported percentile and it may impact other things because the bug is in OpenHashMap.

 

I am really surprised that we caught this bug because everything has to hit just wrong to make it happen. in python/pyspark if you run

 
{code:python}
from math import *
from pyspark.sql.types import *

data = [(1.779652973678931e+173,), (9.247723870123388e-295,), (5.891823952773268e+98,), (inf,), (1.9042708096454302e+195,), (-3.085825028509117e+74,), (-1.9569489404314425e+128,), (2.0738138203216883e+201,), (inf,), (2.5212410617263588e-282,), (-2.646144697462316e-35,), (-3.468683249247593e-196,), (nan,), (None,), (nan,), (1.822129180806602e-245,), (5.211702553315461e-259,), (-1.0,), (-5.682293414619055e+46,), (-4.585039307326895e+166,), (-5.936844510098297e-82,), (-5234708055733.116,), (4920675036.053339,), (None,), (4.4501477170144023e-308,), (2.176024662699802e-210,), (-5.046677974902737e+132,), (-5.490780063080251e-09,), (1.703824427218836e-55,), (-1.1961155424160076e+102,), (1.4403274475565667e+41,), (None,), (5.4470705929955455e-86,), (5.120795466142678e-215,), (-9.01991342808203e+282,), (4.051866849943636e-254,), (-3588518231990.927,), (-1.8891559842111865e+63,), (3.4543959813437507e-304,), (-7.590734560275502e-63,), (9.376528689861087e+117,), (-2.1696969883753554e-292,), (7.227411393136537e+206,), (-2.428999624265911e-293,), (5.741383583382542e-14,), (-1.4882040107841963e+286,), (2.1973064836362255e-159,), (0.028096279323357867,), (8.475809563703283e-64,), (3.002803065141241e-139,), (-1.1041009815645263e+203,), (1.8461539468514548e-225,), (-5.620339412794757e-251,), (3.5103766991437114e-60,), (2.4925669515657655e+165,), (3.217759099462207e+108,), (-8.796717685143486e+203,), (2.037360925124577e+292,), (-6.542279108216022e+206,), (-7.951172614280046e-74,), (6.226527569272003e+152,), (-5.673977270111637e-84,), (-1.0186016078084965e-281,), (1.7976931348623157e+308,), (4.205809391029644e+137,), (-9.871721037428167e+119,), (None,), (-1.6663254121185628e-256,), (1.0075153091760986e-236,), (-0.0,), (0.0,), (1.7976931348623157e+308,), (4.3214483342777574e-117,), (-7.973642629411105e-89,), (-1.1028137694801181e-297,), (2.9000325280299273e-39,), (-1.077534929323113e-264,), (-1.1847952892216515e+137,), (nan,), (7.849390806334983e+226,), (-1.831402251805194e+65,), (-2.664533698035492e+203,), (-2.2385155698231885e+285,), (-2.3016388448634844e-155,), (-9.607772864590422e+217,), (3.437191836077251e+209,), (1.9846569552093057e-137,), (-3.010452936419635e-233,), (1.4309793775440402e-87,), (-2.9383643865423363e-103,), (-4.696878567317712e-162,), (8.391630779050713e-135,), (nan,), (-3.3885098786542755e-128,), (-4.5154178008513483e-122,), (nan,), (nan,), (2.187766760184779e+306,), (7.679268835670585e+223,), (6.3131466321042515e+153,), (1.779652973678931e+173,), (9.247723870123388e-295,), (5.891823952773268e+98,), (inf,), (1.9042708096454302e+195,), (-3.085825028509117e+74,), (-1.9569489404314425e+128,), (2.0738138203216883e+201,), (inf,), (2.5212410617263588e-282,), (-2.646144697462316e-35,), (-3.468683249247593e-196,), (nan,), (None,), (nan,), (1.822129180806602e-245,), (5.211702553315461e-259,), (-1.0,), (-5.682293414619055e+46,), (-4.585039307326895e+166,), (-5.936844510098297e-82,), (-5234708055733.116,), (4920675036.053339,), (None,), (4.4501477170144023e-308,), (2.176024662699802e-210,), (-5.046677974902737e+132,), (-5.490780063080251e-09,), (1.703824427218836e-55,), (-1.1961155424160076e+102,), (1.4403274475565667e+41,), (None,), (5.4470705929955455e-86,), (5.120795466142678e-215,), (-9.01991342808203e+282,), (4.051866849943636e-254,), (-3588518231990.927,), (-1.8891559842111865e+63,), (3.4543959813437507e-304,), (-7.590734560275502e-63,), (9.376528689861087e+117,), (-2.1696969883753554e-292,), (7.227411393136537e+206,), (-2.428999624265911e-293,), (5.741383583382542e-14,), (-1.4882040107841963e+286,), (2.1973064836362255e-159,), (0.028096279323357867,), (8.475809563703283e-64,), (3.002803065141241e-139,), (-1.1041009815645263e+203,), (1.8461539468514548e-225,), (-5.620339412794757e-251,), (3.5103766991437114e-60,), (2.4925669515657655e+165,), (3.217759099462207e+108,), (-8.796717685143486e+203,), (2.037360925124577e+292,), (-6.542279108216022e+206,), (-7.951172614280046e-74,), (6.226527569272003e+152,), (-5.673977270111637e-84,), (-1.0186016078084965e-281,), (1.7976931348623157e+308,), (4.205809391029644e+137,), (-9.871721037428167e+119,), (None,), (-1.6663254121185628e-256,), (1.0075153091760986e-236,), (-0.0,), (0.0,), (1.7976931348623157e+308,), (4.3214483342777574e-117,), (-7.973642629411105e-89,), (-1.1028137694801181e-297,), (2.9000325280299273e-39,), (-1.077534929323113e-264,), (-1.1847952892216515e+137,), (nan,), (7.849390806334983e+226,), (-1.831402251805194e+65,), (-2.664533698035492e+203,), (-2.2385155698231885e+285,), (-2.3016388448634844e-155,), (-9.607772864590422e+217,), (3.437191836077251e+209,), (1.9846569552093057e-137,), (-3.010452936419635e-233,), (1.4309793775440402e-87,), (-2.9383643865423363e-103,), (-4.696878567317712e-162,), (8.391630779050713e-135,), (nan,), (-3.3885098786542755e-128,), (-4.5154178008513483e-122,), (nan,), (nan,), (2.187766760184779e+306,), (7.679268835670585e+223,), (6.3131466321042515e+153,), (1.779652973678931e+173,), (9.247723870123388e-295,), (5.891823952773268e+98,), (inf,), (1.9042708096454302e+195,), (-3.085825028509117e+74,), (-1.9569489404314425e+128,), (2.0738138203216883e+201,), (inf,), (2.5212410617263588e-282,), (-2.646144697462316e-35,), (-3.468683249247593e-196,), (nan,), (None,), (nan,), (1.822129180806602e-245,), (5.211702553315461e-259,), (-1.0,), (-5.682293414619055e+46,), (-4.585039307326895e+166,), (-5.936844510098297e-82,), (-5234708055733.116,), (4920675036.053339,), (None,), (4.4501477170144023e-308,), (2.176024662699802e-210,), (-5.046677974902737e+132,), (-5.490780063080251e-09,), (1.703824427218836e-55,), (-1.1961155424160076e+102,), (1.4403274475565667e+41,), (None,), (5.4470705929955455e-86,), (5.120795466142678e-215,), (-9.01991342808203e+282,), (4.051866849943636e-254,), (-3588518231990.927,), (-1.8891559842111865e+63,), (3.4543959813437507e-304,), (-7.590734560275502e-63,), (9.376528689861087e+117,), (-2.1696969883753554e-292,), (7.227411393136537e+206,), (-2.428999624265911e-293,), (5.741383583382542e-14,), (-1.4882040107841963e+286,), (2.1973064836362255e-159,), (0.028096279323357867,), (8.475809563703283e-64,), (3.002803065141241e-139,), (-1.1041009815645263e+203,), (1.8461539468514548e-225,), (-5.620339412794757e-251,), (3.5103766991437114e-60,), (2.4925669515657655e+165,), (3.217759099462207e+108,), (-8.796717685143486e+203,), (2.037360925124577e+292,), (-6.542279108216022e+206,), (-7.951172614280046e-74,), (6.226527569272003e+152,), (-5.673977270111637e-84,), (-1.0186016078084965e-281,), (1.7976931348623157e+308,), (4.205809391029644e+137,), (-9.871721037428167e+119,), (None,), (-1.6663254121185628e-256,), (1.0075153091760986e-236,), (-0.0,), (0.0,), (1.7976931348623157e+308,), (4.3214483342777574e-117,), (-7.973642629411105e-89,), (-1.1028137694801181e-297,), (2.9000325280299273e-39,), (-1.077534929323113e-264,), (-1.1847952892216515e+137,), (nan,), (7.849390806334983e+226,), (-1.831402251805194e+65,), (-2.664533698035492e+203,), (-2.2385155698231885e+285,), (-2.3016388448634844e-155,), (-9.607772864590422e+217,), (3.437191836077251e+209,), (1.9846569552093057e-137,), (-3.010452936419635e-233,), (1.4309793775440402e-87,), (-2.9383643865423363e-103,), (-4.696878567317712e-162,), (8.391630779050713e-135,), (nan,), (-3.3885098786542755e-128,), (-4.5154178008513483e-122,), (nan,), (nan,), (2.187766760184779e+306,), (7.679268835670585e+223,), (6.3131466321042515e+153,), (1.779652973678931e+173,), (9.247723870123388e-295,), (5.891823952773268e+98,), (inf,), (1.9042708096454302e+195,), (-3.085825028509117e+74,), (-1.9569489404314425e+128,), (2.0738138203216883e+201,), (inf,), (2.5212410617263588e-282,), (-2.646144697462316e-35,), (-3.468683249247593e-196,), (nan,), (None,), (nan,), (1.822129180806602e-245,), (5.211702553315461e-259,), (-1.0,), (-5.682293414619055e+46,), (-4.585039307326895e+166,), (-5.936844510098297e-82,), (-5234708055733.116,), (4920675036.053339,), (None,), (4.4501477170144023e-308,), (2.176024662699802e-210,), (-5.046677974902737e+132,), (-5.490780063080251e-09,), (1.703824427218836e-55,), (-1.1961155424160076e+102,), (1.4403274475565667e+41,), (None,), (5.4470705929955455e-86,), (5.120795466142678e-215,), (-9.01991342808203e+282,), (4.051866849943636e-254,), (-3588518231990.927,), (-1.8891559842111865e+63,), (3.4543959813437507e-304,), (-7.590734560275502e-63,), (9.376528689861087e+117,), (-2.1696969883753554e-292,), (7.227411393136537e+206,), (-2.428999624265911e-293,), (5.741383583382542e-14,), (-1.4882040107841963e+286,), (2.1973064836362255e-159,), (0.028096279323357867,), (8.475809563703283e-64,), (3.002803065141241e-139,), (-1.1041009815645263e+203,), (1.8461539468514548e-225,), (-5.620339412794757e-251,), (3.5103766991437114e-60,), (2.4925669515657655e+165,), (3.217759099462207e+108,), (-8.796717685143486e+203,), (2.037360925124577e+292,), (-6.542279108216022e+206,), (-7.951172614280046e-74,), (6.226527569272003e+152,), (-5.673977270111637e-84,), (-1.0186016078084965e-281,), (1.7976931348623157e+308,), (4.205809391029644e+137,), (-9.871721037428167e+119,), (None,), (-1.6663254121185628e-256,), (1.0075153091760986e-236,), (-0.0,), (0.0,), (1.7976931348623157e+308,), (4.3214483342777574e-117,)]

df = spark.createDataFrame(SparkContext.getOrCreate().parallelize(data, numSlices=4), StructType([StructField('val',DoubleType(),True)]))

df.selectExpr('percentile(val, 0.1)').show(truncate=False){code}
You will get back a result of {{-5.924228780007003E136}} but the correct answer is {{-4.739483957565084E136}} which can be verified by changing the number of slices used to import the data into spark.

 

What is happening is that we are getting super unlucky. In the 4th input partition the data is read in from 7.849390806334983E226 to the end. This works fine and we get an OpenHashMap with an entry for both 0.0 and -0.0

 
{code:java}
 OpenHashMap((1.0075153091760986E-236,1), (0.0,1), (-2.646144697462316E-35,1), (-7.951172614280046E-74,1), (-3.468683249247593E-196,1), (5.741383583382542E-14,1), (-2.664533698035492E203,1), (7.227411393136537E206,1), (-3.588518231990927E12,1), (1.9042708096454302E195,1), (-1.9569489404314425E128,1), (7.849390806334983E226,1), (2.187766760184779E306,1), (2.4925669515657655E165,1), (-5.620339412794757E-251,1), (-0.0,1), (-1.1041009815645263E203,1), (-2.3016388448634844E-155,1), (2.1973064836362255E-159,1), (-1.831402251805194E65,1), (1.4403274475565667E41,1), (-3.085825028509117E74,1), (-6.542279108216022E206,1), (-9.871721037428167E119,1), (8.475809563703283E-64,1), (-5.673977270111637E-84,1), (5.120795466142678E-215,1), (-5.046677974902737E132,1), (-4.5154178008513483E-122,1), (1.9846569552093057E-137,1), (-3.3885098786542755E-128,1), (7.679268835670585E223,1), (4.920675036053339E9,1), (-1.0,1), (-4.585039307326895E166,1), (-9.01991342808203E282,1), (5.4470705929955455E-86,1), (9.247723870123388E-295,1), (-1.8891559842111865E63,1), (-4.696878567317712E-162,1), (-1.4882040107841963E286,1), (-5.936844510098297E-82,1), (6.226527569272003E152,1), (-1.1961155424160076E102,1), (-1.6663254121185628E-256,1), (4.4501477170144023E-308,1), (-9.607772864590422E217,1), (-3.010452936419635E-233,1), (4.051866849943636E-254,1), (1.4309793775440402E-87,1), (2.5212410617263588E-282,1), (3.4543959813437507E-304,1), (0.028096279323357867,1), (-7.590734560275502E-63,1), (5.211702553315461E-259,1), (-1.0186016078084965E-281,1), (3.437191836077251E209,1), (NaN,1), (NaN,1), (NaN,1), (8.391630779050713E-135,1), (-5.490780063080251E-9,1), (-2.9383643865423363E-103,1), (2.0738138203216883E201,1), (1.8461539468514548E-225,1), (1.822129180806602E-245,1), (NaN,1), (4.205809391029644E137,1), (2.037360925124577E292,1), (-2.428999624265911E-293,1), (3.002803065141241E-139,1), (6.3131466321042515E153,1), (3.5103766991437114E-60,1), (3.217759099462207E108,1), (-8.796717685143486E203,1), (-2.2385155698231885E285,1), (2.176024662699802E-210,1), (1.703824427218836E-55,1), (9.376528689861087E117,1), (1.7976931348623157E308,2), (NaN,1), (5.891823952773268E98,1), (-2.1696969883753554E-292,1), (4.3214483342777574E-117,1), (Infinity,2), (1.779652973678931E173,1), (-5.234708055733116E12,1), (-5.682293414619055E46,1))
{code}
But when we go to deserialize the map after the shuffle we get a different result out with the entry for -0.0 gone. That is because the deserialize code uses update that will overwrite the count if the keys are the same. But -0.0 and 0.0 are not the same. Unless they happen to hash to the same position when they are being added in. In that case they end up being equal to each other and the count for 0.0 is replaced with the count for -0.0 and we lose one row in the data.

 

Because the keys are stored in OpenHashSet that is where the bug actually is. [https://github.com/apache/spark/blob/7e82e1bc43e0297c3036d802b3a151d2b93db2f6/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala#L139-L159]

 

I see a few ways to fix this.
 # Update OpenHashSet/OpenHashMap to do the right thing for floats and doubles around -0.0 and 0.0
 # normalize nans and zeros before doing percentiles
 # reinterpret the bits for float/double as an int/long before putting them into the map and do the reverse when we pull them out.  That would also have the advantage of making NaN == NaN which would reduce the size of the map in those cases for percentile.
 # Update the deserialize code for percentile to not call update, but instead to call {{counts.changeValue(key, count, _ + count)}}

I am not sure if something similar can happen in other places, but I know for hash aggregate/etc we normalize the floating point values because of things like this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-18 21:33:03.0,,,,,,,,,,"0|z1l1fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delta table 3.0.0 not working with Spark Connect 3.5.0,SPARK-45598,13554646,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,haldefaiz,haldefaiz,18/Oct/23 20:39,29/Oct/23 13:29,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Connect,,,,,0,,,,,"Spark version 3.5.0

Spark Connect version 3.5.0

Delta table 3.0.0

Spark connect server was started using

*{{./sbin/start-connect-server.sh --master spark://localhost:7077 --packages org.apache.spark:spark-connect_2.12:3.5.0,io.delta:delta-spark_2.12:3.0.0}}* --{*}{{conf ""spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"" --conf ""spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"" --conf 'spark.jars.repositories=[https://oss.sonatype.org/content/repositories/iodelta-1120']}}{*}

{{Connect client depends on}}
*libraryDependencies += ""io.delta"" %% ""delta-spark"" % ""3.0.0""*
*and the connect libraries*
 

When trying to run a simple job that writes to a delta table

{{val spark = SparkSession.builder().remote(""sc://localhost"").getOrCreate()}}
{{val data = spark.read.json(""profiles.json"")}}
{{data.write.format(""delta"").save(""/tmp/delta"")}}

 

{{Error log in connect client}}

{{Exception in thread ""main"" org.apache.spark.SparkException: io.grpc.StatusRuntimeException: INTERNAL: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4) (172.23.128.15 executor 0): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.catalyst.expressions.ScalaUDF.f of type scala.Function1 in instance of org.apache.spark.sql.catalyst.expressions.ScalaUDF}}
{{    at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)}}
{{    at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2437)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{...}}
{{    at org.apache.spark.sql.connect.client.GrpcExceptionConverter$.toThrowable(GrpcExceptionConverter.scala:110)}}
{{    at org.apache.spark.sql.connect.client.GrpcExceptionConverter$.convert(GrpcExceptionConverter.scala:41)}}
{{    at org.apache.spark.sql.connect.client.GrpcExceptionConverter$$anon$1.hasNext(GrpcExceptionConverter.scala:49)}}
{{    at scala.collection.Iterator.foreach(Iterator.scala:943)}}
{{    at scala.collection.Iterator.foreach$(Iterator.scala:943)}}
{{    at org.apache.spark.sql.connect.client.GrpcExceptionConverter$$anon$1.foreach(GrpcExceptionConverter.scala:46)}}
{{    at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)}}
{{    at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)}}
{{    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)}}
{{    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)}}
{{    at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)}}
{{    at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)}}
{{    at org.apache.spark.sql.connect.client.GrpcExceptionConverter$$anon$1.to(GrpcExceptionConverter.scala:46)}}
{{    at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)}}
{{    at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)}}
{{    at org.apache.spark.sql.connect.client.GrpcExceptionConverter$$anon$1.toBuffer(GrpcExceptionConverter.scala:46)}}
{{    at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:554)}}
{{    at org.apache.spark.sql.DataFrameWriter.executeWriteOperation(DataFrameWriter.scala:257)}}
{{    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:221)}}
{{    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:210)}}
{{    at Main$.main(Main.scala:11)}}
{{    at Main.main(Main.scala)}}

 

{{Error log in spark connect server}}

{{23/10/13 12:26:32 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (172.23.128.15 executor 0): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.catalyst.expressions.ScalaUDF.f of type scala.Function1 in instance of org.apache.spark.sql.catalyst.expressions.ScalaUDF}}
{{    at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)}}
{{    at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2437)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2311)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)}}
{{    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)}}
{{    at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:527)}}
{{    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)}}
{{    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)}}
{{    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)}}
{{    at java.lang.reflect.Method.invoke(Method.java:498)}}
{{    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2322)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)}}
{{    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)}}
{{    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)}}
{{    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)}}
{{    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)}}
{{    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)}}
{{    at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)}}
{{    at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)}}
{{    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)}}
{{    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)}}
{{    at org.apache.spark.scheduler.Task.run(Task.scala:141)}}
{{    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)}}
{{    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)}}
{{    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)}}
{{    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)}}
{{    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)}}
{{    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}
{{    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}
{{    at java.lang.Thread.run(Thread.java:750)}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Oct 26 11:07:24 UTC 2023,,,,,,,,,,"0|z1l1e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/23 10:05;sdaberdaku;Hello [~haldefaiz], you need to use the latest delta-spark version 3.0.0 which came out just yesterday. It now supports delta with Spark 3.5.0.
[https://github.com/delta-io/delta/releases/tag/v3.0.0];;;","19/Oct/23 12:31;haldefaiz;Hi [~sdaberdaku] , corrected the title. I tested it with 3.0.0 delta. What I meant was, delta table does not work with {*}spark connect{*}. It does work with vanilla spark 3.5.0 otherwise;;;","26/Oct/23 11:07;haldefaiz;Hi, do we have any updates here? Happy to help;;;",,,,,,,,,,,,
AQE and InMemoryTableScanExec correctness bug,SPARK-45592,13554580,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,,apachespark,eejbyfeldt,eejbyfeldt,18/Oct/23 13:34,30/Oct/23 16:10,30/Oct/23 17:08,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,correctness,pull-request-available,,,"The following query should return 1000000
{code:java}
import org.apache.spark.storage.StorageLevelval

df = spark.range(0, 1000000, 1, 5).map(l => (l, l))
val ee = df.select($""_1"".as(""src""), $""_2"".as(""dst""))
  .persist(StorageLevel.MEMORY_AND_DISK)

ee.count()
val minNbrs1 = ee
  .groupBy(""src"").agg(min(col(""dst"")).as(""min_number""))
  .persist(StorageLevel.MEMORY_AND_DISK)
val join = ee.join(minNbrs1, ""src"")
join.count(){code}
but on spark 3.5.0 there is a correctness bug causing it to return `104800` or some other smaller value.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Oct 30 05:48:09 UTC 2023,,,,,,,,,,"0|z1l0zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/23 05:48;dongjoon;Thank you, [~eejbyfeldt]. I added a label, `correctness`, and raised the priority to `Blocker`.;;;",,,,,,,,,,,,,,
Execution fails when there are subqueries in TakeOrderedAndProjectExec,SPARK-45584,13554493,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,18/Oct/23 03:49,20/Oct/23 00:39,30/Oct/23 17:26,20/Oct/23 00:39,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,SQL,,,,,0,pull-request-available,,,,"When there are subqueries in TakeOrderedAndProjectExec, the query can throw this exception:

 java.lang.IllegalArgumentException: requirement failed: Subquery subquery#242, [id=#109|#109] has not finished 

This is because TakeOrderedAndProjectExec does not wait for subquery execution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 20 00:39:01 UTC 2023,,,,,,,,,,"0|z1l0g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/23 00:39;cloud_fan;Issue resolved by pull request 43419
[https://github.com/apache/spark/pull/43419];;;",,,,,,,,,,,,,,
Spark SQL returning incorrect values for full outer join on keys with the same name.,SPARK-45583,13554492,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,hcampbell,hcampbell,18/Oct/23 03:38,20/Oct/23 15:03,30/Oct/23 17:26,20/Oct/23 15:03,3.4.1,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"{{The following query gives the wrong results.}}

 

{{WITH people as (}}
{{  SELECT * FROM (VALUES }}
{{    (1, 'Peter'), }}
{{    (2, 'Homer'), }}
{{    (3, 'Ned'),}}
{{    (3, 'Jenny')}}
{{  ) AS Idiots(id, FirstName)}}
{{{}){}}}{{{}, location as ({}}}
{{  SELECT * FROM (VALUES}}
{{    (1, 'sample0'),}}
{{    (1, 'sample1'),}}
{{    (2, 'sample2')  }}
{{  ) as Locations(id, address)}}
{{{}){}}}{{{}SELECT{}}}
{{  *}}
{{FROM}}
{{  people}}
{{FULL OUTER JOIN}}
{{  location}}
{{ON}}
{{  people.id = location.id}}

{{We find the following table:}}
||id: integer||FirstName: string||id: integer||address: string||
|2|Homer|2|sample2|
|null|Ned|null|null|
|null|Jenny|null|null|
|1|Peter|1|sample0|
|1|Peter|1|sample1|

{{But clearly the first `id` column is wrong, the nulls should be 3.}}

If we rename the id column in (only) the person table to pid we get the correct results:
||pid: integer||FirstName: string||id: integer||address: string||
|2|Homer|2|sample2|
|3|Ned|null|null|
|3|Jenny|null|null|
|1|Peter|1|sample0|
|1|Peter|1|sample1|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Oct 19 23:28:53 UTC 2023,,,,,,,,,,"0|z1l0g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/23 16:30;bersprockets;Strangely, I cannot reproduce. Is some setting required?
{noformat}
sql(""select version()"").show(false)
+----------------------------------------------+
|version()                                     |
+----------------------------------------------+
|3.5.0 ce5ddad990373636e94071e7cef2f31021add07b|
+----------------------------------------------+

scala> sql(""""""WITH people as (
  SELECT * FROM (VALUES 
    (1, 'Peter'), 
    (2, 'Homer'), 
    (3, 'Ned'),
    (3, 'Jenny')
  ) AS Idiots(id, FirstName)
), location as (
  SELECT * FROM (VALUES
    (1, 'sample0'),
    (1, 'sample1'),
    (2, 'sample2')  
  ) as Locations(id, address)
)SELECT
  *
FROM
  people
FULL OUTER JOIN
  location
ON
  people.id = location.id"""""").show(false)
     |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      | 
+---+---------+----+-------+
|id |FirstName|id  |address|
+---+---------+----+-------+
|1  |Peter    |1   |sample0|
|1  |Peter    |1   |sample1|
|2  |Homer    |2   |sample2|
|3  |Ned      |NULL|NULL   |
|3  |Jenny    |NULL|NULL   |
+---+---------+----+-------+

scala> 
{noformat};;;","19/Oct/23 23:28;hcampbell;Ahh, apologies, it looks like I was running 3.4.1 when I found this issue.

Testing in 3.5 it does appear to be resolved.;;;",,,,,,,,,,,,,
Subquery changes the output schema of the outer query,SPARK-45580,13554466,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,bersprockets,bersprockets,17/Oct/23 21:13,21/Oct/23 16:57,30/Oct/23 17:26,,3.3.3,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"A query can have an incorrect output schema because of a subquery.

Assume this data:
{noformat}
create or replace temp view t1(a) as values (1), (2), (3), (7);
create or replace temp view t2(c1) as values (1), (2), (3);
create or replace temp view t3(col1) as values (3), (9);
cache table t1;
cache table t2;
cache table t3;
{noformat}
When run in {{spark-sql}}, the following query has a superfluous boolean column:
{noformat}
select *
from t1
where exists (
  select c1
  from t2
  where a = c1
  or a in (select col1 from t3)
);

1	false
2	false
3	true
{noformat}
The result should be:
{noformat}
1
2
3
{noformat}
When executed via the {{Dataset}} API, you don't see the incorrect result, because the Dataset API truncates the right-side of the rows based on the analyzed plan's schema (it's the optimized plan's schema that goes wrong).

However, even with the {{Dataset}} API, this query goes wrong:
{noformat}
select (
  select *
  from t1
  where exists (
    select c1
    from t2
    where a = c1
    or a in (select col1 from t3)
  )
  limit 1
)
from range(1);

java.lang.AssertionError: assertion failed: Expects 1 field, but got 2; something went wrong in analysis
	at scala.Predef$.assert(Predef.scala:279)
	at org.apache.spark.sql.execution.ScalarSubquery.updateResult(subquery.scala:88)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1(SparkPlan.scala:276)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1$adapted(SparkPlan.scala:275)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:933)
        ...
{noformat}
Other queries that have the wrong schema:
{noformat}
select *
from t1
where a in (
  select c1
  from t2
  where a in (select col1 from t3)
);
{noformat}
and
{noformat}
select *
from t1
where not exists (
  select c1
  from t2
  where a = c1
  or a in (select col1 from t3)
);
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 17 21:38:56 UTC 2023,,,,,,,,,,"0|z1l0a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/23 21:38;bersprockets;I'll make a PR in the coming days.;;;",,,,,,,,,,,,,,
Executor hangs indefinitely due to decommissioner errors,SPARK-45579,13554461,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ukby1234,ukby1234,17/Oct/23 20:18,25/Oct/23 12:33,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,pull-request-available,,,,"During Spark executor decommission, the fallback storage uploads can fail due to some race conditions even though we check the actual file exists: 

java.io.FileNotFoundException: No file: /var/data/spark-ab14b716-630d-435e-a92a-1403f6206dd8/blockmgr-7f9ab4d7-1340-4b39-9558-fde994a82090/0b/shuffle_175_66754_0.index
	at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.checkSource(CopyFromLocalOperation.java:314)
	at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.execute(CopyFromLocalOperation.java:167)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$copyFromLocalFile$26(S3AFileSystem.java:3854)
	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)
	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)
	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.copyFromLocalFile(S3AFileSystem.java:3847)
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2558)
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2520)
	at org.apache.spark.storage.FallbackStorage.copy(FallbackStorage.scala:67)
	at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.$anonfun$run$12(BlockManagerDecommissioner.scala:146)
	at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.$anonfun$run$12$adapted(BlockManagerDecommissioner.scala:146)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.run(BlockManagerDecommissioner.scala:146)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

This will block the executor from exiting properly because the decommissioner doesn't think shuffle migration is complete. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-17 20:18:31.0,,,,,,,,,,"0|z1l094:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix UserDefinedPythonTableFunctionAnalyzeRunner to pass folded values from named arguments,SPARK-45577,13554459,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,17/Oct/23 19:47,17/Oct/23 22:54,30/Oct/23 17:26,17/Oct/23 22:54,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,PySpark,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 17 22:54:16 UTC 2023,,,,,,,,,,"0|z1l08o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/23 22:54;ueshin;Issue resolved by pull request 43407
https://github.com/apache/spark/pull/43407;;;",,,,,,,,,,,,,,
Remove unnecessary debug logs in ReloadingX509TrustManagerSuite,SPARK-45576,13554441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hasnain-db,hasnain-db,hasnain-db,17/Oct/23 16:35,18/Oct/23 16:17,30/Oct/23 17:26,18/Oct/23 04:48,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Spark Core,,,,,0,pull-request-available,,,,These were added accidentally.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-44937,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 18 04:48:29 UTC 2023,,,,,,,,,,"0|z1l04o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/23 04:48;mridulm80;Issue resolved by pull request 43404
[https://github.com/apache/spark/pull/43404];;;",,,,,,,,,,,,,,
Spark connect executor ignore jars in classpath,SPARK-45571,13554400,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,lirany,lirany,17/Oct/23 11:21,29/Oct/23 19:06,30/Oct/23 17:26,29/Oct/23 19:06,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Connect,,,,,0,,,,,"I am starting a spark connect server in k8s.

While trying to access S3 I'm getting the following error from the executor
{code:java}
Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (10.1.0.174 executor 1): java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
... {code}
The driver is able to access S3 without trouble.

I tried adding the hadoop-aws jar in multiple ways: --packcages, --jars, 
SPARK_EXTRA_CLASSPATH env and adding to the spark Jars folder in my dockerfile.
 
When looking in my executor pod, I'm seeing the classpath is set up properly and should have access to the jar located in multiple places.

 

Only by using the addArtifact API I can add the missing jar.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Oct 29 19:06:07 UTC 2023,,,,,,,,,,"0|z1kzvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/23 19:06;lirany;I had wrong permissions on the files in the jars directory;;;",,,,,,,,,,,,,,
Spark job hangs due to task launch thread failed to create,SPARK-45570,13554373,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,lifulong,lifulong,17/Oct/23 08:33,18/Oct/23 10:22,30/Oct/23 17:26,,3.1.2,3.5.0,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"spark job hangs while web ui show there is one task in running stage keep running for multi hours, while other tasks finished in a few minutes 

executor will never report task launch failed info to driver

 

Below is spark task execute thread launch log:

23/10/17 04:45:42 ERROR Inbox: An error happened while processing message in the inbox for Executor
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:717)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:957)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1378)
        at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
        at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1.applyOrElse(CoarseGrainedExecutorBackend.scala:173)
        at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
        at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
        at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
        at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
        at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)","spark.speculation is use default value false

spark version 3.1.2
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/23 10:18;lifulong;image-2023-10-18-18-18-36-132.png;https://issues.apache.org/jira/secure/attachment/13063635/image-2023-10-18-18-18-36-132.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 18 10:22:53 UTC 2023,,,,,,,,,,"0|z1kzpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/23 10:22;lifulong;!image-2023-10-18-18-18-36-132.png!
catch thread create exception from line ""threadPool.execute(tr)"", and do
execBackend.statusUpdate(taskDescription.taskId, TaskState.FAILED, EMPTY_BYTE_BUFFER)
after get exception can fix this problem in theory
is this solution ok?;;;",,,,,,,,,,,,,,
Convert TINYINT catalyst properly in MySQL Dialect,SPARK-45561,13554308,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,m.zhang,m.zhang,m.zhang,16/Oct/23 22:06,24/Oct/23 09:52,30/Oct/23 17:26,24/Oct/23 09:52,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Spark Core,,,,,0,pull-request-available,,,,"MySQL dialect currently incorrectly converts catalyst types `TINYINT` to BYTE. However, both MySQL doesn't have a byte type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 24 09:52:22 UTC 2023,,,,,,,,,,"0|z1kzb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/23 09:52;maxgekk;Issue resolved by pull request 43390
[https://github.com/apache/spark/pull/43390];;;",,,,,,,,,,,,,,
Spark Connect can not be started because of missing user home dir in Docker container,SPARK-45557,13554279,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,zrlpar,zrlpar,16/Oct/23 17:21,16/Oct/23 17:36,30/Oct/23 17:26,,3.4.0,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,,,,Spark Docker,,,,,0,,,,,"I was trying to start Spark Connect within a container using the Spark Docker container images and ran into an issue where Ivy could not pull the Spark Connect JAR since the user home /home/spark does not exist.

Steps to reproduce:

1. Start the Spark container with `/bin/bash` as the command:
{code:java}
docker run -it --rm apache/spark:3.5.0 /bin/bash {code}
2. Try to start Spark Connect within the container:

 
{code:java}
/opt/spark/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.5.0 {code}
which lead to this output:

 

 
{code:java}
starting org.apache.spark.sql.connect.service.SparkConnectServer, logging to /opt/spark/logs/spark--org.apache.spark.sql.connect.service.SparkConnectServer-1-d8470a71dbd7.out
failed to launch: nice -n 0 bash /opt/spark/bin/spark-submit --class org.apache.spark.sql.connect.service.SparkConnectServer --name Spark Connect server --packages org.apache.spark:spark-connect_2.12:3.5.0
  	at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1535)
  	at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)
  	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)
  	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)
  	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
  	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
  	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
  	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
  	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
  	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
full log in /opt/spark/logs/spark--org.apache.spark.sql.connect.service.SparkConnectServer-1-d8470a71dbd7.out {code}
where then the full log file looks like this:
{code:java}
Spark Command: /opt/java/openjdk/bin/java -cp /opt/spark/conf:/opt/spark/jars/* -Xmx1g -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.connect.service.SparkConnectServer --name Spark Connect server --packages org.apache.spark:spark-connect_2.12:3.5.0 spark-internal
========================================
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/spark/.ivy2/cache
The jars for the packages stored in: /home/spark/.ivy2/jars
org.apache.spark#spark-connect_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-f8a04936-e8af-4f37-bdb0-e4026a8a3be5;1.0
	confs: [default]
Exception in thread ""main"" java.io.FileNotFoundException: /home/spark/.ivy2/cache/resolved-org.apache.spark-spark-submit-parent-f8a04936-e8af-4f37-bdb0-e4026a8a3be5-1.0.xml (No such file or directory)
	at java.base/java.io.FileOutputStream.open0(Native Method)
	at java.base/java.io.FileOutputStream.open(Unknown Source)
	at java.base/java.io.FileOutputStream.<init>(Unknown Source)
	at java.base/java.io.FileOutputStream.<init>(Unknown Source)
	at org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorWriter.write(XmlModuleDescriptorWriter.java:71)
	at org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorWriter.write(XmlModuleDescriptorWriter.java:63)
	at org.apache.ivy.core.module.descriptor.DefaultModuleDescriptor.toIvyFile(DefaultModuleDescriptor.java:553)
	at org.apache.ivy.core.cache.DefaultResolutionCacheManager.saveResolvedModuleDescriptor(DefaultResolutionCacheManager.java:184)
	at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:259)
	at org.apache.ivy.Ivy.resolve(Ivy.java:522)
	at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1535)
	at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) {code}
 

The issue is that the user home /home/spark directory does not exist.
{code:java}
$ ls -l /home
total 0 
$
{code}
It seems there is an easy fix: simply switching from useradd to adduser in the Dockerfile should get the user home directory created.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-16 17:21:20.0,,,,,,,,,,"0|z1kz4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent status code between web page and REST API when exception is thrown,SPARK-45556,13554257,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,wekoms,wekoms,16/Oct/23 14:26,16/Oct/23 14:26,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Web UI,,,,,0,,,,,"Spark history server provides [AppHistoryServerPlugin|https://github.com/kuwii/spark/blob/dev/status-code/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala] to add extra REST API and web pages. However there's an issue when exceptions are thrown, causing incnosistent status code between web page and REST API.

For REST API, if the thrown exception is an instance of WebApplicationException, then the status code will be set as the one defined within the exception.

However for web page, all exceptions are wrapped within a 500 response.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-16 14:26:25.0,,,,,,,,,,"0|z1kyzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated APIs from Pandas API on Spark,SPARK-45550,13554224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,16/Oct/23 10:53,17/Oct/23 11:41,30/Oct/23 17:26,17/Oct/23 11:40,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,PySpark,,,,,0,pull-request-available,,,,"There are some APIs that marked as deprecated from Spark 3.x, so we should remove them from Spark 4.x.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 17 11:40:47 UTC 2023,,,,,,,,,,"0|z1kysg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/23 11:40;podongfeng;Issue resolved by pull request 43384
[https://github.com/apache/spark/pull/43384];;;",,,,,,,,,,,,,,
Failure to generate pyspark documents due to capitalization in the MacOS environment,SPARK-45548,13554204,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,panbingkun,panbingkun,16/Oct/23 09:19,16/Oct/23 09:23,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Oct 16 09:23:55 UTC 2023,,,,,,,,,,"0|z1kyo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/23 09:23;panbingkun;When I want to solve the `Failure to generate pyspark documents due to capitalization in the MacOS environment` issue by upgrading the sphinx version,
I encountered another problem, eg:
ModuleNotFoundError: No module named 'DataFrame'

Currently, I have submitted a PR in Sphinx to address this issue
Wait for sphinx to confirm before continuing to complete this function;;;",,,,,,,,,,,,,,
InferWindowGroupLimit causes bug if the other window functions haven't the same window frame as the rank-like functions,SPARK-45543,13554149,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,beliefer,ronserruya,ronserruya,15/Oct/23 16:22,19/Oct/23 12:19,30/Oct/23 17:26,19/Oct/23 12:18,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Optimizer,Spark Core,SQL,,,0,correctness,data-loss,pull-request-available,,"First, it's my first bug, so I'm hoping I'm doing it right, also, as I'm not very knowledgeable about spark internals, I hope I diagnosed the problem correctly

I found the degradation in spark version 3.5.0:

When using multiple windows that share the same partition and ordering (but with different ""frame boundaries"", where one window is a ranking function, ""WindowGroupLimit"" is added to the plan causing wrong values to be created from the other windows.

*This behavior didn't exist in versions 3.3 and 3.4.*

Example:

 
{code:python}
import pysparkfrom pyspark.sql import functions as F, Window  

df = spark.createDataFrame([
    {'row_id': 1, 'name': 'Dave', 'score': 1, 'year': 2020},
    {'row_id': 1, 'name': 'Dave', 'score': 2, 'year': 2022},
    {'row_id': 1, 'name': 'Dave', 'score': 3, 'year': 2023},
    {'row_id': 2, 'name': 'Amy', 'score': 6, 'year': 2021},
])

# Create first window for row number
window_spec = Window.partitionBy('row_id', 'name').orderBy(F.desc('year'))

# Create additional window from the first window with unbounded frame
unbound_spec = window_spec.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)

# Try to keep the first row by year, and also collect all scores into a list
df2 = df.withColumn(
    'rn', 
    F.row_number().over(window_spec)
).withColumn(
    'all_scores', 
    F.collect_list('score').over(unbound_spec)
){code}
So far everything works, and if we display df2:

 
{noformat}
+----+------+-----+----+---+----------+
|name|row_id|score|year|rn |all_scores|
+----+------+-----+----+---+----------+
|Dave|1     |3    |2023|1  |[3, 2, 1] |
|Dave|1     |2    |2022|2  |[3, 2, 1] |
|Dave|1     |1    |2020|3  |[3, 2, 1] |
|Amy |2     |6    |2021|1  |[6]       |
+----+------+-----+----+---+----------+{noformat}
 

However, once we filter to keep only the first row number:

 
{noformat}
df2.filter(""rn=1"").show(truncate=False)
+----+------+-----+----+---+----------+
|name|row_id|score|year|rn |all_scores|
+----+------+-----+----+---+----------+
|Dave|1     |3    |2023|1  |[3]       |
|Amy |2     |6    |2021|1  |[6]       |
+----+------+-----+----+---+----------+{noformat}
As you can see just filtering changed the ""all_scores"" array for Dave.

(This example uses `collect_list`, however, the same result happens with other functions, such as max, min, count, etc)

 

Now, if instead of using the two windows we used, I will use the first window and a window with different ordering, or create a completely new window with same partition but no ordering, it will work fine:
{code:python}
new_window = Window.partitionBy('row_id', 'name').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)

df3 = df.withColumn(
    'rn',
    F.row_number().over(window_spec)
).withColumn(
    'all_scores',
    F.collect_list('score').over(new_window)
)
df3.filter(""rn=1"").show(truncate=False){code}
{noformat}
+----+------+-----+----+---+----------+
|name|row_id|score|year|rn |all_scores|
+----+------+-----+----+---+----------+
|Dave|1     |3    |2023|1  |[3, 2, 1] |
|Amy |2     |6    |2021|1  |[6]       |
+----+------+-----+----+---+----------+
{noformat}
In addition, if we use all 3 windows to create 3 different columns, it will also work ok. So it seems the issue happens only when all the windows used share the same partition and ordering.

Here is the final plan for the faulty dataframe:
{noformat}
df2.filter(""rn=1"").explain()
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Filter (rn#9 = 1)
   +- Window [row_number() windowspecdefinition(row_id#1L, name#0, year#3L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#9, collect_list(score#2L, 0, 0) windowspecdefinition(row_id#1L, name#0, year#3L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS all_scores#16], [row_id#1L, name#0], [year#3L DESC NULLS LAST]
      +- WindowGroupLimit [row_id#1L, name#0], [year#3L DESC NULLS LAST], row_number(), 1, Final
         +- Sort [row_id#1L ASC NULLS FIRST, name#0 ASC NULLS FIRST, year#3L DESC NULLS LAST], false, 0
            +- Exchange hashpartitioning(row_id#1L, name#0, 200), ENSURE_REQUIREMENTS, [plan_id=425]
               +- WindowGroupLimit [row_id#1L, name#0], [year#3L DESC NULLS LAST], row_number(), 1, Partial
                  +- Sort [row_id#1L ASC NULLS FIRST, name#0 ASC NULLS FIRST, year#3L DESC NULLS LAST], false, 0
                     +- Scan ExistingRDD[name#0,row_id#1L,score#2L,year#3L]{noformat}
I suspect the issue is caused due to the ""WindowGroupLimit"" clause in the plan.

This clause doesn't appear in the dataframes that work fine, and before filtering the rn.

So I assume that since the optimizer detects that we want to only keep the first row of the ranking function, it first removes all other rows from the following computations, which causes the incorrect result or loss of data.

I think the bug comes from this change (and the attached PRs):

https://issues.apache.org/jira/browse/SPARK-44340

It was added in spark 3.5.0, and in addition, I noticed that it was included in databricks release 13.3, which included spark 3.4.0, but also this fix in their release note. And evidently, this bug happens on databricks13 spark3.4, but not on my local spark 3.4

tagging user [~beliefer] as I believe you would know most about this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Oct 19 12:18:23 UTC 2023,,,,,,,,,,"0|z1kybs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/23 07:20;beliefer;[~ronserruya]I see.;;;","17/Oct/23 01:30;beliefer;[~ronserruya] Thank you.;;;","19/Oct/23 12:18;beliefer;Issue resolved by pull request 43385
[https://github.com/apache/spark/pull/43385];;;",,,,,,,,,,,,
The last Task may get stuck in multi resource-profile,SPARK-45537,13553983,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,liangyongyuan,liangyongyuan,13/Oct/23 08:16,17/Oct/23 14:25,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,pull-request-available,,,,"Description:
In scenarios involving multiple resource profiles (e.g., prof1 and prof2), when a taskset (prof1) has only one remaining task (task0) awaiting scheduling, and there are executors (executor0 with prof0 and executor1 with prof1), if executor1 fails to run task0, executor1 gets blacklisted. Consequently, task0 becomes unschedulable, leading to a blockage in the task scheduling process.

Example Code:


{code:java}
val rprof = new ResourceProfileBuilder()
val ereqs = new ExecutorResourceRequests()
ereqs.memory(""4g"")
ereqs.memoryOverhead(""2g"").offHeapMemory(""1g"")
val resourceProfile = rprof.require(ereqs).build()
val rdd = sc.parallelize(1 to 10, 1).withResources(resourceProfile)
rdd.map(num => {
  if (TaskContext.get().attemptNumber() == 0) {
    throw new RuntimeException(""First attempt encounters an error"")
  } else {
    num / 2
  }
}).collect()
{code}

Issue:
The issue arises when the taskSet becomes unschedulable. The logic attempts to find a task that cannot be scheduled on any executor across all profiles. However, when determining whether an executor can schedule the task, there is no distinction made based on the resource profile. This leads to an incorrect assumption that executor0 (prof0) can schedule the task, which is not the case.

Relevant Code:

{code:java}
if (!launchedAnyTask) {
  taskSet.getCompletelyExcludedTaskIfAny(hostToExecutors) ........
}

def getCompletelyExcludedTaskIfAny(
      hostToExecutors: HashMap[String, HashSet[String]]): Option[Int] = {
........
  pendingTask.find { indexInTaskSet =>
    // try to find some executor this task can run on.
    // It's possible that some *other* task isn't schedulable anywhere,
    // but we will discover that in some later call, when that unschedulable task is the last task remaining.
    hostToExecutors.forall { case (host, execsOnHost) =>
      // Check if the task can run on the node
      val nodeExcluded =
        appHealthTracker.isNodeExcluded(host) ||
          taskSetExcludelist.isNodeExcludedForTaskSet(host) ||
          taskSetExcludelist.isNodeExcludedForTask(host, indexInTaskSet)
      if (nodeExcluded) {
        true
      } else {
        // Check if the task can run on any of the executors
        execsOnHost.forall { exec =>
          appHealthTracker.isExecutorExcluded(exec) ||
            taskSetExcludelist.isExecutorExcludedForTaskSet(exec) ||
            taskSetExcludelist.isExecutorExcludedForTask(exec, indexInTaskSet)
        }
      }
    }
  }
}
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-13 08:16:48.0,,,,,,,,,,"0|z1kxaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task fraction resource request is not expected,SPARK-45527,13553942,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,Ngone51,Ngone51,13/Oct/23 01:56,30/Oct/23 12:25,30/Oct/23 17:26,,3.2.1,3.3.3,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,pull-request-available,,,," 
{code:java}
test(""SPARK-XXX"") {
  import org.apache.spark.resource.{ResourceProfileBuilder, TaskResourceRequests}

  withTempDir { dir =>
    val scriptPath = createTempScriptWithExpectedOutput(dir, ""gpuDiscoveryScript"",
      """"""{""name"": ""gpu"",""addresses"":[""0""]}"""""")

    val conf = new SparkConf()
      .setAppName(""test"")
      .setMaster(""local-cluster[1, 12, 1024]"")
      .set(""spark.executor.cores"", ""12"")
    conf.set(TASK_GPU_ID.amountConf, ""0.08"")
    conf.set(WORKER_GPU_ID.amountConf, ""1"")
    conf.set(WORKER_GPU_ID.discoveryScriptConf, scriptPath)
    conf.set(EXECUTOR_GPU_ID.amountConf, ""1"")
    sc = new SparkContext(conf)
    val rdd = sc.range(0, 100, 1, 4)
    var rdd1 = rdd.repartition(3)
    val treqs = new TaskResourceRequests().cpus(1).resource(""gpu"", 1.0)
    val rp = new ResourceProfileBuilder().require(treqs).build
    rdd1 = rdd1.withResources(rp)
    assert(rdd1.collect().size === 100)
  }
} {code}
In the above test, the 3 tasks generated by rdd1 are expected to be executed in sequence as we expect ""new TaskResourceRequests().cpus(1).resource(""gpu"", 1.0)"" should override ""conf.set(TASK_GPU_ID.amountConf, ""0.08"")"". However, those 3 tasks are run in parallel in fact.

The root cause is that ExecutorData#ExecutorResourceInfo#numParts is static. In this case, the ""gpu.numParts"" is initialized with 12 (1/0.08) and won't change even if there's a new task resource request (e.g., resource(""gpu"", 1.0) in this case). Thus, those 3 tasks are able to be executed in parallel.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Oct 16 23:16:50 UTC 2023,,,,,,,,,,"0|z1kx1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/23 01:58;Ngone51;cc [~wbo4958]   [~tgraves] ;;;","13/Oct/23 15:30;tgraves;thanks for filing and digging into this. I assume this is only with the TaskResourceRequests and using the default ExecutorResourceRequests.  seems a bug since that functionality was added.  Either way when we fix should add tests similar if we can.;;;","16/Oct/23 23:16;wbo4958;Working on a PR to fix it.;;;",,,,,,,,,,,,
cleanSource problem on FileStreamSource for Windows env,SPARK-45519,13553867,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,yemregurses,yemregurses,12/Oct/23 12:52,12/Oct/23 12:52,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,Structured Streaming,,,,,0,,,,,"We are using Spark with Scala in Windows environment. While streaming using Spark, I give the *{{cleanSource}}* option as ""archive"" and the *{{sourceArchiveDir}}* option as ""archived"" as in the code below.
{code:java}
spark.readStream
  .option(""cleanSource"", ""archive"")
  .option(""sourceArchiveDir"", ""archived""){code}
When I tried this in a Linux environment, I realized that the problem was with the paths. Because when I set archive mode to ""delete"", it works on both Linux and Windows. But for the archive mode, it does not work on Windows. 

The problem is related to appending paths in Windows. There is a method

 
{code:java}
override protected def cleanTask(entry: FileEntry): Unit{code}
in the FileStreamSource.scala file in the org.apache.spark.sql.execution.streaming package. On line 569, the !fileSystem.rename(curPath, newPath) code supposed to move source file to archive folder. However, when I debugged, I noticed that the curPath and newPath values were as follows in windows:

 
{code:java}
curPath: file:/C:/dev/be/data-integration-suite/test-data/streaming-folder/patients/patients-success.csv{code}
{code:java}
newPath: file:/C:/dev/be/data-integration-suite/archived/C:/dev/be/data-integration-suite/test-data/streaming-folder/patients/patients-success.csv{code}
It seems that absolute path of csv file were appended when creating newPath because there are two *C:/dev/be/data-integration-suite* in the newPath. This is the reason probably spark archiving does not work. Instead, newPath should be: file:/C:/dev/be/data-integration-suite/archived/test-data/streaming-folder/patients/patients-success.csv",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-12 12:52:16.0,,,,,,,,,,"0|z1kwl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add ""--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED"" so Platform can access cleaner on Java 9+",SPARK-45508,13553769,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,12/Oct/23 00:59,13/Oct/23 17:44,30/Oct/23 17:26,13/Oct/23 05:30,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.1,4.0.0,Spark Core,,,,,0,pull-request-available,,,,We need to add `--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED` to our JVM options so that the code in `org.apache.spark.unsafe.Platform` can access the JDK internal cleaner classes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 13 05:30:00 UTC 2023,,,,,,,,,,"0|z1kvzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/23 05:30;LuciferYang;Issue resolved by pull request 43344
[https://github.com/apache/spark/pull/43344];;;",,,,,,,,,,,,,,
Correctness bug in correlated scalar subqueries with COUNT aggregates,SPARK-45507,13553768,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andyylam,andyylam,andyylam,12/Oct/23 00:41,19/Oct/23 02:37,30/Oct/23 17:26,19/Oct/23 02:36,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,"{code:java}
 
create view if not exists t1(a1, a2) as values (0, 1), (1, 2);
create view if not exists t2(b1, b2) as values (0, 2), (0, 3);
create view if not exists t3(c1, c2) as values (0, 2), (0, 3);

-- Example 1
select (
  select SUM(l.cnt + r.cnt)
  from (select count(*) cnt from t2 where t1.a1 = t2.b1 having cnt = 0) l
  join (select count(*) cnt from t3 where t1.a1 = t3.c1 having cnt = 0) r
  on l.cnt = r.cnt
) from t1

-- Correct answer: (null, 0)
+----------------------+
|scalarsubquery(c1, c1)|
+----------------------+
|null                  |
|null                  |
+----------------------+

-- Example 2
select ( select sum(cnt) from (select count(*) cnt from t2 where t1.c1 = t2.c1) ) from t1

-- Correct answer: (2, 0)
+------------------+
|scalarsubquery(c1)|
+------------------+
|2                 |
|null              |
+------------------+

-- Example 3
select ( select count(*) from (select count(*) cnt from t2 where t1.c1 = t2.c1) ) from t1

-- Correct answer: (1, 1)
+------------------+
|scalarsubquery(c1)|
+------------------+
|1                 |
|0                 |
+------------------+ {code}
 

 

DB fiddle for correctness check:[https://www.db-fiddle.com/f/4jyoMCicNSZpjMt4jFYoz5/10403#]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Oct 19 02:36:57 UTC 2023,,,,,,,,,,"0|z1kvz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/23 02:36;cloud_fan;Issue resolved by pull request 43341
[https://github.com/apache/spark/pull/43341];;;",,,,,,,,,,,,,,
Followup: Ignore task completion from old stage after retrying indeterminate stages,SPARK-45498,13553626,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mayurb31,mayurb31,mayurb31,11/Oct/23 06:30,13/Oct/23 02:19,30/Oct/23 17:26,13/Oct/23 02:19,3.5.1,4.0.0,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Spark Core,,,,,0,pull-request-available,,,,"With SPARK-45182, we added a fix for not letting laggard tasks of the older attempts of the indeterminate stage from marking the partition has completed in map output tracker.

When a task completes, DAG scheduler also notifies all the tasksets of the stage about that partition being completed. Tasksets would not schedule such task if they are not already scheduled. This is not correct for indeterminate stage, since we want to re-run all the tasks on re-attempt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 13 02:19:07 UTC 2023,,,,,,,,,,"0|z1kv3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/23 02:19;cloud_fan;Issue resolved by pull request 43326
[https://github.com/apache/spark/pull/43326];;;",,,,,,,,,,,,,,
Fix the bug that uses incorrect parquet compression codec lz4raw,SPARK-45484,13553530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,10/Oct/23 12:32,23/Oct/23 02:54,30/Oct/23 17:26,17/Oct/23 01:52,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,3.5.1,,,,SQL,,,,,0,pull-request-available,,,,"lz4raw is not a correct parquet compression codec name.
We should use lz4_raw as its name.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 17 01:52:09 UTC 2023,,,,,,,,,,"0|z1kui8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/23 01:52;beliefer;Issue resolved by pull request 43330
[https://github.com/apache/spark/pull/43330];;;",,,,,,,,,,,,,,
Raise exception directly instead of calling `resolveColumnsByPosition`,SPARK-45476,13553459,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,10/Oct/23 05:31,16/Oct/23 00:35,30/Oct/23 17:26,16/Oct/23 00:35,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,"We can directly throw error when resolving output columns if there is any error, instead of calling {{resolveColumnsByPosition}} again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Oct 16 00:35:12 UTC 2023,,,,,,,,,,"0|z1ku2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/23 00:35;gurwls223;Issue resolved by pull request 42762
[https://github.com/apache/spark/pull/42762];;;",,,,,,,,,,,,,,
Incorrect error message for RoundBase,SPARK-45473,13553442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,09/Oct/23 22:16,11/Oct/23 02:51,30/Oct/23 17:26,11/Oct/23 02:50,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,3.4.2,3.5.1,4.0.0,,SQL,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 11 02:50:31 UTC 2023,,,,,,,,,,"0|z1ktyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/23 02:50;dongjoon;Issue resolved by pull request 43316
[https://github.com/apache/spark/pull/43316];;;",,,,,,,,,,,,,,
[CORE] Fix yarn distribution build,SPARK-45464,13553307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hasnain-db,hasnain-db,hasnain-db,08/Oct/23 20:27,11/Oct/23 03:31,30/Oct/23 17:26,11/Oct/23 02:20,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Spark Core,YARN,,,,0,pull-request-available,,,,"[https://github.com/apache/spark/pull/43164] introduced a regression in:

 

```

./dev/make-distribution.sh --tgz -Phive -Phive-thriftserver -Pyarn

```

 

this needs to be fixed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-44937,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 11 02:20:05 UTC 2023,,,,,,,,,,"0|z1kt4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/23 02:20;LuciferYang;Issue resolved by pull request 43289
[https://github.com/apache/spark/pull/43289];;;",,,,,,,,,,,,,,
Cache Invalidation Issue with JDBC Table,SPARK-45449,13553213,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liangyongyuan,liangyongyuan,liangyongyuan,07/Oct/23 03:36,10/Oct/23 06:43,30/Oct/23 17:26,10/Oct/23 06:42,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,SQL,,,,,0,pull-request-available,,,,"We have identified a cache invalidation issue when caching JDBC tables in Spark SQL. The cached table is unexpectedly invalidated when queried, leading to a re-read from the JDBC table instead of retrieving data from the cache.
Example SQL:

{code:java}
CACHE TABLE cache_t SELECT * FROM mysql.test.test1;
SELECT * FROM cache_t;
{code}

Expected Behavior:
The expectation is that querying the cached table (cache_t) should retrieve the result from the cache without re-evaluating the execution plan.

Actual Behavior:
However, the cache is invalidated, and the content is re-read from the JDBC table.

Root Cause:
The issue lies in the 'CacheData' class, where the comparison involves 'JDBCTable.' The 'JDBCTable' is a case class:
{code:java}
case class JDBCTable(ident: Identifier, schema: StructType, jdbcOptions: JDBCOptions)
{code}

The comparison of non-case class components, such as 'jdbcOptions,' involves pointer comparison. This leads to unnecessary cache invalidation.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 10 06:42:03 UTC 2023,,,,,,,,,,"0|z1ksk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/23 06:42;cloud_fan;Issue resolved by pull request 43258
[https://github.com/apache/spark/pull/43258];;;",,,,,,,,,,,,,,
Revisit TableCacheQueryStage to avoid replicated InMemoryRelation materialization,SPARK-45443,13553196,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,erenavsarogullari,erenavsarogullari,06/Oct/23 22:00,10/Oct/23 02:17,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"TableCacheQueryStage is created per InMemoryTableScanExec by AdaptiveSparkPlanExec and it materializes InMemoryTableScanExec output (cached RDD) to provide runtime stats in order to apply AQE  optimizations into remaining physical plan stages. TableCacheQueryStage materializes InMemoryTableScanExec eagerly by submitting job per TableCacheQueryStage instance. For example, if there are 2 TableCacheQueryStage instances referencing same IMR instance (cached RDD) and first InMemoryTableScanExec' s materialization takes longer, following logic will return false (inMemoryTableScan.isMaterialized => false) and this may cause replicated IMR materialization. This behavior can be more visible when cached RDD size is high.
[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/QueryStageExec.scala#L281]

Would like to get community feedback. Thanks in advance.
cc [~ulysses] [~cloud_fan]

*Sample Query to simulate the problem:*
// Both join legs uses same IMR instance
{code:java}
import spark.implicits._

val arr = (1 to 12).map { i => {
    val index = i % 5
    (index, s""Employee_$index"", s""Department_$index"")
  }
}
val df = arr.toDF(""id"", ""name"", ""department"")
  .filter('id >= 0)
  .sort(""id"")
  .groupBy('id, 'name, 'department)
  .count().as(""count"")
df.persist()

val df2 = df.sort(""count"").filter('count <= 2)
val df3 = df.sort(""count"").filter('count >= 3)
val df4 = df2.join(df3, Seq(""id"", ""name"", ""department""), ""fullouter"")

df4.show() {code}
*Physical Plan:*
{code:java}
== Physical Plan ==
AdaptiveSparkPlan (31)
+- == Final Plan ==
   CollectLimit (21)
   +- * Project (20)
      +- * SortMergeJoin FullOuter (19)
         :- * Sort (10)
         :  +- * Filter (9)
         :     +- TableCacheQueryStage (8), Statistics(sizeInBytes=210.0 B, rowCount=5)
         :        +- InMemoryTableScan (1)
         :              +- InMemoryRelation (2)
         :                    +- AdaptiveSparkPlan (7)
         :                       +- HashAggregate (6)
         :                          +- Exchange (5)
         :                             +- HashAggregate (4)
         :                                +- LocalTableScan (3)
         +- * Sort (18)
            +- * Filter (17)
               +- TableCacheQueryStage (16), Statistics(sizeInBytes=210.0 B, rowCount=5)
                  +- InMemoryTableScan (11)
                        +- InMemoryRelation (12)
                              +- AdaptiveSparkPlan (15)
                                 +- HashAggregate (14)
                                    +- Exchange (13)
                                       +- HashAggregate (4)
                                          +- LocalTableScan (3) {code}
*Stages DAGs materializing the same IMR instance:*
!IMR Materialization - Stage 2.png|width=303,height=134!
!IMR Materialization - Stage 3.png|width=303,height=134!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/23 22:05;erenavsarogullari;IMR Materialization - Stage 2.png;https://issues.apache.org/jira/secure/attachment/13063393/IMR+Materialization+-+Stage+2.png","06/Oct/23 22:05;erenavsarogullari;IMR Materialization - Stage 3.png;https://issues.apache.org/jira/secure/attachment/13063392/IMR+Materialization+-+Stage+3.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 10 02:17:21 UTC 2023,,,,,,,,,,"0|z1ksg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/23 01:51;ulysses;hi [~erenavsarogullari] , it seems that, it depends on the behavior of rdd cache. Say, what happens if we materialize a cached rdd twice at the same time ? There are some race condition in block manager per rdd partition so it makes things slow. BTW, what's the behavior before we have TableCacheQueryStage ? Does not it have this issue ?;;;","08/Oct/23 20:23;erenavsarogullari;Hi [~ulysses], 

Firstly, thanks for reply.

For above sample query, if TableCacheQueryStage flow is disabled, IMR materialization will be triggered by ShuffleQueryStage (introduced by Sort' s Exchange node). Both ShuffleQueryStage nodes will also need to materialize same IMR instance in this case so i believe same issue may also occur in previous flow. TableCacheQueryStage materializes IMR eagerly as different from previous flow. Can this increase probability of concurrent IMR materialization for same IMR instance?

I think this behavior is not visible when IMR cached data size is low. However, replicated IMR materialization can be expensive and can introduce potential regression when IMR cached data size is high (e.g: observing this behavior when IMR needs to read high shuffle data size). Also, the queries can have multiple IMR instances by referencing multiple replicated IMR instances, this can also increase probability of concurrent IMR materialization for same IMR instance.

Thinking on potential solution options (if makes sense):
For queries using AQE, can introducing TableCacheQueryStage into physical plan once per unique IMR instance help? IMR instances can be compared if they are equivalent before its TableCacheQueryStage instance is created by AdaptiveSparkPlanExec and TableCacheQueryStage can materialize unique IMR instance once.;;;","09/Oct/23 01:52;ulysses;> Can this increase probability of concurrent IMR materialization for same IMR instance?

I think they are same, The TableCacheQueryStage is more like a barrier and report some metrics to AQE framework. The gap of `eagerly` is very small.

> For queries using AQE, can introducing TableCacheQueryStage into physical plan once per unique IMR instance help

I did not see the difference. I think one idea is, we can introduce something like `ReusedTableCacheQueryStage`. The `ReusedTableCacheQueryStage` only holds an empty future which wait for the first TableCacheQueryStage materialization, so that we can make sure the cached RDD only be executed once. But this idea only work for one query, say, if there are multi-queries which reference the same caced RDD (e.g., in thiftserver), the issue is still existed.
;;;","09/Oct/23 21:10;erenavsarogullari;> But this idea only work for one query
Could you please provide more info on this? For example, same IMR instance can be used by multiple queries. Lets say, there are 2 queries as Q0 & Q1 and both of them use same IMR instance. Q0 will be materializing IMR instance by TableCacheQueryStage and IMR materialization has to be done before Q0 is completed. Q1 can still introduce TableCacheQueryStage instance to physical plan for same IMR instance, however, this TableCacheQueryStage instance will not submit IMR materialization job due to IMR already materialized at that level, right? Can we have any other use-cases not covered?
[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/QueryStageExec.scala#L281]

Also, thinking on following use-cases such as 
*1- Queries using AQE under IMR feature (a.k.a spark.sql.optimizer.canChangeCachedPlanOutputPartitioning):* 
TableCacheQueryStage materializes IMR by submitting Spark job per TableCacheQueryStage/InMemoryTableScanExec instance. 

*2- Queries not using AQE under IMR feature:* 
IMR will be materialized by InMemoryTableScanExec.doExecute/doExecuteColumnar() flow. Can InMemoryTableScanExec based solution (to avoid replicated InMemoryRelation materialization) be more inclusive by covering all use-cases?
spark.sql.optimizer.canChangeCachedPlanOutputPartitioning is enabled for Spark 3.5 as default but for queries using < Spark 3.5 or if the feature may need to be disabled in >= Spark 3.5 for some reason.;;;","10/Oct/23 02:17;ulysses;> But this idea only work for one query

Please see the following `say, if there are multi-queries which reference the same caced RDD (e.g., in thiftserver)`. There are some race condition if multi-queries reference and materialize the same cached rdd. They are in different query execution and different thread.

> spark.sql.optimizer.canChangeCachedPlanOutputPartitioning

It is totally irrelevant with the TableCacheQueryStage. This config is used to make AQE work for the cached plan.;;;",,,,,,,,,,
Incorrect summary counts from a CSV file,SPARK-45440,13553184,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,evanv,evanv,06/Oct/23 20:51,07/Oct/23 00:02,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Input/Output,,,,,0,aggregation,bug,pyspark,,"I am using pip-installed Pyspark version 3.5.0 inside the context of an IPython shell. The task is straightforward: take [this CSV file|https://gist.githubusercontent.com/evanvolgas/e5cb082673ec947239658291f2251de4/raw/a9c5e9866ac662a816f9f3828a2d184032f604f0/AAPL.csv] of AAPL stock prices and compute the minimum and maximum volume weighted average price for the entire file. 

My code is [here. |https://gist.github.com/evanvolgas/e4aa75fec4179bb7075a5283867f127c]I've also performed the same computation in DuckDB because I noticed that the results of the Spark code are wrong. 

Literally, the exact same SQL in DuckDB and in Spark yield different results, and Spark's are wrong. 

I have never seen this behavior in a Spark release before. I'm very confused by it, and curious if anyone else can replicate this behavior. ",Pyspark version 3.5.0 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Oct 07 00:02:46 UTC 2023,,,,,,,,,,"0|z1ksdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/23 00:02;bersprockets;I added {{inferSchema=true}} as a datasource option in your example and I got the expected answer. Otherwise it's doing a max and min on a string (not a number).;;;",,,,,,,,,,,,,,
Decimal precision exceeds max precision error when using unary minus on min Decimal values on Scala 2.13 Spark,SPARK-45438,13553173,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,navkumar,navkumar,06/Oct/23 17:46,06/Oct/23 17:48,30/Oct/23 17:26,,3.2.0,3.2.1,3.2.2,3.2.3,3.2.4,3.3.0,3.3.1,3.3.2,3.3.3,3.4.0,3.4.1,3.5.0,,,,,,,,,,,,SQL,,,,,0,scala,,,,"When submitting an application to Spark built with Scala 2.13, there are issues with Decimal overflow that show up when using unary minus (and also {{abs()}} which uses unary minus under the hood.

Here is an example PySpark reproduce use case:

{code}
from decimal import Decimal

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, DecimalType

spark = SparkSession.builder \
      .master(""local[*]"") \
      .appName(""decimal_precision"") \
      .config(""spark.rapids.sql.explain"", ""ALL"") \
      .config(""spark.sql.ansi.enabled"", ""true"") \
      .config(""spark.sql.legacy.allowNegativeScaleOfDecimal"", 'true') \
      .getOrCreate()  

precision = 38
scale = 0
DECIMAL_MIN = Decimal('-' + ('9' * precision) + 'e' + str(-scale))

data = [[DECIMAL_MIN]]

schema = StructType([
    StructField(""a"", DecimalType(precision, scale), True)])
df = spark.createDataFrame(data=data, schema=schema)

df.selectExpr(""a"", ""-a"").show()
{code}

This particular example will run successfully on Spark built with Scala 2.12, but throw a java.math.ArithmeticException on Spark built with Scala 2.13. 

If you change the value of {{DECIMAL_MIN}} in the previous code to something just ahead of the original DECIMAL_MIN, you will not get an exception thrown, but instead you will get an incorrect answer (possibly due to overflow):

{code}
...
DECIMAL_MIN = Decimal('-8' + ('9' * (precision-1)) + 'e' + str(-scale))
...
{code} 

Output:
{code}
+--------------------+--------------------+
|                   a|               (- a)|
+--------------------+--------------------+
|-8999999999999999...|90000000000000000...|
+--------------------+--------------------+
{code}

It looks like the code in {{Decimal.scala}} uses {{scala.math.BigDecimal}}. See https://github.com/scala/bug/issues/11590 with updates on how Scala 2.13 handles BigDecimal. It looks like there is {{java.math.MathContext}} missing when performing these operations. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-06 17:46:16.0,,,,,,,,,,"0|z1ksb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV/JSON schema inference when timestamps do not match specified timestampFormat with only one row on each partition report error,SPARK-45433,13553074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,06/Oct/23 04:34,12/Oct/23 12:10,30/Oct/23 17:26,11/Oct/23 16:34,3.3.0,3.4.0,3.5.0,,,,,,,,,,,,,,,,,3.4.2,3.5.1,4.0.0,,SQL,,,,,0,pull-request-available,,,,"CSV/JSON schema inference when timestamps do not match specified timestampFormat with `only one row on each partition` report error.
{code:java}
//eg
val csv = spark.read.option(""timestampFormat"", ""yyyy-MM-dd'T'HH:mm:ss"")
  .option(""inferSchema"", true).csv(Seq(""2884-06-24T02:45:51.138"").toDS())
csv.show() {code}
{code:java}
//error
Caused by: java.time.format.DateTimeParseException: Text '2884-06-24T02:45:51.138' could not be parsed, unparsed text found at index 19 {code}
This bug affect 3.3/3.4/3.5. Unlike https://issues.apache.org/jira/browse/SPARK-45424 , this is a different bug but has the same error message",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 11 16:34:18 UTC 2023,,,,,,,,,,"0|z1krp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/23 16:34;maxgekk;Issue resolved by pull request 43243
[https://github.com/apache/spark/pull/43243];;;",,,,,,,,,,,,,,
FramelessOffsetWindowFunctionFrame fails when ignore nulls and offset > # of rows ,SPARK-45430,13553070,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vli-databricks,vli-databricks,vli-databricks,06/Oct/23 01:06,24/Oct/23 07:18,30/Oct/23 17:26,24/Oct/23 07:17,3.5.0,,,,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.1,4.0.0,SQL,,,,,0,pull-request-available,,,,"Failure when function that utilized `FramelessOffsetWindowFunctionFrame` is used with `ignoreNulls = true` and `offset > rowCount`.

e.g. 

```
select x, lead(x, 5) IGNORE NULLS over (order by x) from (select explode(sequence(1, 3)) x)
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 24 07:17:40 UTC 2023,,,,,,,,,,"0|z1kro8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/23 07:17;cloud_fan;Issue resolved by pull request 43236
[https://github.com/apache/spark/pull/43236];;;",,,,,,,,,,,,,,
Regression in CSV schema inference when timestamps do not match specified timestampFormat,SPARK-45424,13553041,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,andygrove,andygrove,05/Oct/23 17:25,09/Oct/23 09:30,30/Oct/23 17:26,09/Oct/23 09:30,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,SQL,,,,,0,pull-request-available,,,,"There is a regression in Spark 3.5.0 when inferring the schema of CSV files containing timestamps, where a column will be inferred as a timestamp even if the contents do not match the specified timestampFormat.

*Test Data*

I have the following CSV file:
{code:java}
2884-06-24T02:45:51.138
2884-06-24T02:45:51.138
2884-06-24T02:45:51.138
{code}
*Spark 3.4.0 Behavior (correct)*

In Spark 3.4.0, if I specify the correct timestamp format, then the schema is inferred as timestamp:
{code:java}
scala> val df = spark.read.option(""timestampFormat"", ""yyyy-MM-dd'T'HH:mm:ss.SSS"").option(""inferSchema"", true).csv(""/tmp/timestamps.csv"")
df: org.apache.spark.sql.DataFrame = [_c0: timestamp]
{code}
If I specify an incompatible timestampFormat, then the schema is inferred as string:
{code:java}
scala> val df = spark.read.option(""timestampFormat"", ""yyyy-MM-dd'T'HH:mm:ss"").option(""inferSchema"", true).csv(""/tmp/timestamps.csv"")
df: org.apache.spark.sql.DataFrame = [_c0: string]
{code}
*Spark 3.5.0*

In Spark 3.5.0, the column will be inferred as timestamp even if the data does not match the specified timestampFormat.
{code:java}
scala> val df = spark.read.option(""timestampFormat"", ""yyyy-MM-dd'T'HH:mm:ss"").option(""inferSchema"", true).csv(""/tmp/timestamps.csv"")
df: org.apache.spark.sql.DataFrame = [_c0: timestamp]
{code}
Reading the DataFrame then results in an error:
{code:java}
Caused by: java.time.format.DateTimeParseException: Text '2884-06-24T02:45:51.138' could not be parsed, unparsed text found at index 19
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Oct 09 09:30:52 UTC 2023,,,,,,,,,,"0|z1krhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/23 20:07;andygrove;The regression seems to have been introduced in https://issues.apache.org/jira/browse/SPARK-39280 and/or https://issues.apache.org/jira/browse/SPARK-39281

Commits:

[https://github.com/apache/spark/commit/b1c0d599ba32a4562ae1697e3f488264f1d03c76]

[https://github.com/apache/spark/commit/3192bbd29585607d43d0819c6c2d3ac00180261a]

 

[~fanjia] Do you understand why this behavior has changed?;;;","06/Oct/23 05:22;fanjia;Thanks [~andygrove] , I found the reason of the bug. Let me create a PR for this.;;;","09/Oct/23 09:30;maxgekk;Issue resolved by pull request 43245
[https://github.com/apache/spark/pull/43245];;;",,,,,,,,,,,,
Change CURRENT_SCHEMA() column alias to match function name,SPARK-45418,13552935,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,m.zhang,m.zhang,m.zhang,04/Oct/23 20:35,13/Oct/23 00:21,30/Oct/23 17:26,13/Oct/23 00:21,3.5.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Documentation,SQL,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 13 00:21:20 UTC 2023,,,,,,,,,,"0|z1kqu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/23 20:39;m.zhang;I am currently working on this;;;","13/Oct/23 00:21;cloud_fan;Issue resolved by pull request 43235
[https://github.com/apache/spark/pull/43235];;;",,,,,,,,,,,,,
Make InheritableThread inherit the active session,SPARK-45417,13552920,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,lee@chungmin.dev,lee@chungmin.dev,04/Oct/23 17:21,10/Oct/23 03:08,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,pull-request-available,,,,"Repro:

{code:java}
# repro.py
from multiprocessing.pool import ThreadPool
from pyspark import inheritable_thread_target
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(""Test"").getOrCreate()
spark.sparkContext.setLogLevel(""ERROR"")

def f(i, spark):
    print(f""{i} spark = {spark}"")
    print(f""{i} active session = {SparkSession.getActiveSession()}"")
    print(f""{i} local property foo = {spark.sparkContext.getLocalProperty('foo')}"")
    spark = SparkSession.builder.appName(""Test"").getOrCreate()
    print(f""{i} spark = {spark}"")
    print(f""{i} active session = {SparkSession.getActiveSession()}"")

pool = ThreadPool(4)
spark.sparkContext.setLocalProperty(""foo"", ""bar"")
pool.starmap(inheritable_thread_target(f), [(i, spark) for i in range(4)]){code}

Run as: {{./bin/spark-submit repro.py}}

{{getOrCreate()}} doesn't set the active session either. The only way is calling the Java function directly: {{spark._jsparkSession.setActiveSession(spark._jsparkSession)}}.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-04 17:21:40.0,,,,,,,,,,"0|z1kqqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB consumes excessive disk space when many concurrent streaming queries are using dedup,SPARK-45415,13552893,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,schenksj,schenksj,schenksj,04/Oct/23 13:44,11/Oct/23 23:46,30/Oct/23 17:26,11/Oct/23 23:46,3.3.2,,,,,,,,,,,,,,,,,,,4.0.0,,,,Structured Streaming,,,,,0,pull-request-available,,,,"Our spark environment features a number of parallel structured streaming jobs, many of which use state store. Most use state store for dropDuplicates and work with a tiny amount of information, but a few have a substantially large state store requiring use of RocksDB. In such a configuration, spark allocates a minimum of {{spark.sql.shuffle.partitions * queryCount}} partitions, each of which pre-allocate about 74mb (observed on EMR/Hadoop) disk storage for RocksDB. This allocation is due to pre-allocation of log files space using [fallocate|https://github.com/facebook/rocksdb/blob/main/include/rocksdb/options.h#L871-L880], requiring users to either unnaturally reduce shuffle partitions, split running spark instances, or allocate a large amount of wasted storage.","Apache spark on AWS EMR, local spark on a laptop on Linux.  Does not impact MacOS due to missing support in RocksDB for pre-allocation (MacOS does not support the fallocate system call).",14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 11 23:46:04 UTC 2023,,,,,,,,,,"0|z1kqkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/23 23:46;kabhwan;Issue resolved by pull request 43202
[https://github.com/apache/spark/pull/43202];;;",,,,,,,,,,,,,,
spark-xml misplaces string tag content,SPARK-45414,13552891,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,gcera21,gcera21,04/Oct/23 13:31,04/Oct/23 16:02,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,Spark Core,,,,0,,,,,"h1. Intro

Hi all! Please expect some degree of incompleteness in this issue as this is the very first one I post, and feel free to edit it as you like - I welcome your feedback.

My goal is to provide you with as many details and indications as I can on this issue that I am currently facing with a Client of mine on its Production environment (we use Azure Databricks DBR 11.3 LTS).

I was told by Sean Owen [[srowen (Sean Owen) (github.com)|https://github.com/srowen]], who maintains the spark-xml maven repository on GitHub [[https://github.com/srowen/spark-xml]] to post an issue here because ""This code has been ported to Apache Spark now anyway so won't be updated here"" (refer to his comment [here|#issuecomment-1744792958]).
h1. Issue

When I write a DataFrame into xml format via the spark-xml library either (1) I get an error if empty string columns are in between non-string nested ones or (2) if I put all string columns at the end then I get a wrong xml where the content of string tags are misplaced into the following ones.
h1. Code to reproduce the issue

Please find below the end-to-end code snippet that results into the error
h2. CASE (1): ERROR

When empty strings are in between non-string nested ones, the write fails with the following error.

_Caused by: java.lang.IllegalArgumentException: Failed to convert value MyDescription (class of class java.lang.String) in type ArrayType(StructType(StructField(_ID,StringType,true),StructField(_Level,StringType,true)),true) to XML._

Please find attached the full trace of the error.
{code:python}
fake_file_df = spark \
    .sql(
        """"""SELECT
            CAST(STRUCT('ItemId' AS `_Type`, '123' AS `_VALUE`) AS STRUCT<_Type: STRING, _VALUE: STRING>) AS ItemID,
            CAST(STRUCT('UPC' AS `_Type`, '123' AS `_VALUE`) AS STRUCT<_Type: STRING, _VALUE: STRING>) AS UPC,
            CAST('' AS STRING) AS _SerialNumberFlag,
            CAST('MyDescription' AS STRING) AS Description,
            CAST(ARRAY(STRUCT(NULL AS `_ID`, NULL AS `_Level`)) AS ARRAY<STRUCT<_ID: STRING, _Level: STRING>>) AS MerchandiseHierarchy,
            CAST(ARRAY(STRUCT(NULL AS `_ValueTypeCode`, NULL AS `_VALUE`)) AS ARRAY<STRUCT<_ValueTypeCode: STRING, _Value: STRING>>) AS ItemPrice,
            CAST('' AS STRING) AS Color,
            CAST('' AS STRING) AS IntendedIndustry,
            CAST(STRUCT(NULL AS `Name`) AS STRUCT<Name: STRING>) AS Manufacturer,
            CAST(STRUCT(NULL AS `Season`) AS STRUCT<Season: STRING>) AS Marketing,
            CAST(STRUCT(NULL AS `_Name`) AS STRUCT<_Name: STRING>) AS BrandOwner,
            CAST(ARRAY(STRUCT('Attribute1' AS `_Name`, 'Value1' AS `_VALUE`)) AS ARRAY<STRUCT<_Name: STRING, AttributeValue: STRING>>) AS ItemAttribute_culinary,
            CAST(ARRAY(STRUCT(NULL AS `_Name`, ARRAY(ARRAY(STRUCT(NULL AS `AttributeCode`, NULL AS `AttributeValue`))) AS `_VALUE`)) AS ARRAY<STRUCT<_Name: STRING, _VALUE: ARRAY<ARRAY<STRUCT<AttributeCode: STRING, AttributeValue: STRING>>>>>) AS ItemAttribute_noculinary,
            CAST(STRUCT(STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Depth`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Height`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Width`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Diameter`) AS STRUCT<Depth: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Height: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Width: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Diameter: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>>) AS ItemMeasurements,
            CAST(STRUCT('GroupA' AS `TaxGroupID`, 'CodeA' AS `TaxExemptCode`, '1' AS `TaxAmount`) AS STRUCT<TaxGroupID: STRING, TaxExemptCode: STRING, TaxAmount: STRING>) AS TaxInformation,
            CAST('' AS STRING) AS ItemImageUrl,
            CAST(ARRAY(ARRAY(STRUCT(NULL AS `_action`, NULL AS `_franchiseeId`, NULL AS `_franchiseeName`))) AS ARRAY<ARRAY<STRUCT<_action: STRING, _franchiseeId: STRING, _franchiseeName: STRING>>>) AS ItemFranchisees,
            CAST('Add' AS STRING) AS _Action
        ;""""""
    )

# fake_file_df.display()
fake_file_df \
    .coalesce(1) \
    .write \
    .format('com.databricks.spark.xml') \
    .option('declaration', 'version=""1.0"" encoding=""UTF-8""') \
    .option(""nullValue"", """") \
    .option('rootTag', ""root_tag"") \
    .option('rowTag', ""row_tag"") \
    .mode('overwrite') \
    .save(xml_folder_path) {code}
I noticed that it works if I try to write all columns up to ""Color"" (excluded), namely:
{code:python}
fake_file_df \
    .select(
        ""ItemID"",
        ""UPC"",
        ""_SerialNumberFlag"",
        ""Description"",
        ""MerchandiseHierarchy"",
        ""ItemPrice""
    ) \
    .coalesce(1) \
    .write \
    .format('com.databricks.spark.xml') \
    .option('declaration', 'version=""1.0"" encoding=""UTF-8""') \
    .option(""nullValue"", """") \
    .option('rootTag', ""root_tag"") \
    .option('rowTag', ""row_tag"") \
    .mode('overwrite') \
    .save(xml_folder_path){code}
h2. CASE (2): MISPLACED XML

When I put all string columns at the end of the 1-row DataFrame it mistakenly writes the content of one column into the tag right after it.
{code:python}
fake_file_df = spark \
    .sql(
        """"""SELECT
            CAST(STRUCT('ItemId' AS `_Type`, '123' AS `_VALUE`) AS STRUCT<_Type: STRING, _VALUE: STRING>) AS ItemID,
            CAST(STRUCT('UPC' AS `_Type`, '123' AS `_VALUE`) AS STRUCT<_Type: STRING, _VALUE: STRING>) AS UPC,
            CAST(ARRAY(STRUCT(NULL AS `_ID`, NULL AS `_Level`)) AS ARRAY<STRUCT<_ID: STRING, _Level: STRING>>) AS MerchandiseHierarchy,
            CAST(ARRAY(STRUCT(NULL AS `_ValueTypeCode`, NULL AS `_VALUE`)) AS ARRAY<STRUCT<_ValueTypeCode: STRING, _Value: STRING>>) AS ItemPrice,
            CAST(STRUCT(NULL AS `Name`) AS STRUCT<Name: STRING>) AS Manufacturer,
            CAST(STRUCT(NULL AS `Season`) AS STRUCT<Season: STRING>) AS Marketing,
            CAST(STRUCT(NULL AS `_Name`) AS STRUCT<_Name: STRING>) AS BrandOwner,
            CAST(ARRAY(STRUCT('Attribute1' AS `_Name`, 'Value1' AS `_VALUE`)) AS ARRAY<STRUCT<_Name: STRING, AttributeValue: STRING>>) AS ItemAttribute_culinary,
            CAST(ARRAY(STRUCT(NULL AS `_Name`, ARRAY(ARRAY(STRUCT(NULL AS `AttributeCode`, NULL AS `AttributeValue`))) AS `_VALUE`)) AS ARRAY<STRUCT<_Name: STRING, _VALUE: ARRAY<ARRAY<STRUCT<AttributeCode: STRING, AttributeValue: STRING>>>>>) AS ItemAttribute_noculinary,
            CAST(STRUCT(STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Depth`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Height`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Width`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Diameter`) AS STRUCT<Depth: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Height: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Width: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Diameter: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>>) AS ItemMeasurements,
            CAST(STRUCT('GroupA' AS `TaxGroupID`, 'CodeA' AS `TaxExemptCode`, '1' AS `TaxAmount`) AS STRUCT<TaxGroupID: STRING, TaxExemptCode: STRING, TaxAmount: STRING>) AS TaxInformation,
            CAST(ARRAY(ARRAY(STRUCT(NULL AS `_action`, NULL AS `_franchiseeId`, NULL AS `_franchiseeName`))) AS ARRAY<ARRAY<STRUCT<_action: STRING, _franchiseeId: STRING, _franchiseeName: STRING>>>) AS ItemFranchisees,
            CAST('' AS STRING) AS _SerialNumberFlag,
            CAST('MyDescription' AS STRING) AS Description,
            CAST('' AS STRING) AS Color,
            CAST('' AS STRING) AS IntendedIndustry,
            CAST('' AS STRING) AS ItemImageUrl,
            CAST('Add' AS STRING) AS _Action
        ;""""""
    )

fake_file_df \
    .coalesce(1) \
    .write \
    .format('com.databricks.spark.xml') \
    .option('declaration', 'version=""1.0"" encoding=""UTF-8""') \
    .option(""nullValue"", """") \
    .option('rootTag', ""root_tag"") \
    .option('rowTag', ""row_tag"") \
    .mode('overwrite') \
    .save(xml_folder_path) {code}
The output is a wrong xml where ""MyDescription"" is written inside the ""Color"" tag instead of the ""Description"" tag (but if you display the ""fake_file_df"" DataFrame it looks good as ""MyDescription"" is under the ""Description"" column).
{code:xml}
<?xml version=""1.0"" encoding=""UTF-8""?>
<root_tag>
    <row_tag SerialNumberFlag="""" Action=""Add"">
        <ItemID Type=""ItemId"">123</ItemID>
        <UPC Type=""UPC"">123</UPC>
        <MerchandiseHierarchy ID="""" Level=""""/>
        <ItemPrice ValueTypeCode="""" Value=""""/>
        <Manufacturer>
            <Name></Name>
        </Manufacturer>
        <Marketing>
            <Season></Season>
        </Marketing>
        <BrandOwner Name=""""/>
        <ItemAttribute_culinary Name=""Attribute1"">
            <AttributeValue>Value1</AttributeValue>
        </ItemAttribute_culinary>
        <ItemAttribute_noculinary Name="""">
            <item>
                <AttributeCode></AttributeCode>
                <AttributeValue></AttributeValue>
            </item>
        </ItemAttribute_noculinary>
        <ItemMeasurements>
            <Depth UnitOfMeasure=""""></Depth>
            <Height UnitOfMeasure=""""></Height>
            <Width UnitOfMeasure=""""></Width>
            <Diameter UnitOfMeasure=""""></Diameter>
        </ItemMeasurements>
        <TaxInformation>
            <TaxGroupID>GroupA</TaxGroupID>
            <TaxExemptCode>CodeA</TaxExemptCode>
            <TaxAmount>1</TaxAmount>
        </TaxInformation>
        <ItemFranchisees>
            <item action="""" franchiseeId="""" franchiseeName=""""/>
        </ItemFranchisees>
        <Description></Description>
        <Color>MyDescription</Color>
        <IntendedIndustry></IntendedIndustry>
        <ItemImageUrl></ItemImageUrl>
    </row_tag>
</root_tag> {code}
h1. Current workaround I put into Production

As it looks like spark-xml is having a hard time when non-empty and empty string columns are separated by non-string ones (e.g., a nested struct or array column) I programmatically move all string columns at the end of the DataFrame right before the write command executes.

Not only that, I add a ""fake"" string column before each and every string column (""Col1 AS FAKE_Col1"") as it also looks like spark-xml is misplacing ahead of 1 tag the content of string columns when writing the xml. And, of course, I have to read back the xml file and get rid of all these ""fake"" tags before I can feed it into the downward process.

 

Thanks!

~Giuseppe Ceravolo

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/23 13:32;gcera21;IllegalArgumentException.txt;https://issues.apache.org/jira/secure/attachment/13063356/IllegalArgumentException.txt",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-04 13:31:30.0,,,,,,,,,,"0|z1kqkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL returns table column names as literal data values for Hive tables,SPARK-45403,13552803,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,reece.robinson,reece.robinson,03/Oct/23 23:01,03/Oct/23 23:03,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"When using Spark SQL and Hive JDBC driver to access a Hive table the resulting row data is replaced with the literal column name in the resulting dataframe result.

When I run this:

jdbcDF = spark.read \
  .format(""jdbc"") \
  .options(driver=""org.apache.hive.jdbc.HiveDriver"",
           url=""jdbc:hive2://10.20.174.171:10009"",
           user=""10009"",
           password=""123"",
           query=""select * from demo.hospitals limit 10""
           ) \
  .load()

 

I get:

+```+

+------------+---+----+-------+----+-----+---+-----------+---+---+-----+------------------+--------+---------+----+---+-----------+---------+---------------+------+ |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| +------------+---+----+-------+----+-----+---+-----------+---+---+-----+------------------+--------+---------+----+---+-----------+---------+---------------+------+ |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| |provider_num|npi|name|address|city|state|zip|fips_county|lat|lon|phone|provider_type_code|category|emergency|upin|pin|region_code|bed_count|clia_lab_number|HIP_PK| +------------+---+----+-------+----+-----+---+-----------+---+---+-----+------------------+--------+---------+----+---+-----------+---------+---------------+------+

+```+

I should see:

```

+--------------------+--------------------+--------+-------------+-------------+-------+--------------------+--------------------+--------------------+------+-------+----------+---------+---------+ | person_pk| race_value|sex_code|poverty_value|veteran_value|ppr_pro| patient_pk| di_dk| pov_pk|vet_pk|veteran|total_paid|num_drugs|immunized| +--------------------+--------------------+--------+-------------+-------------+-------+--------------------+--------------------+--------------------+------+-------+----------+---------+---------+ |001252a7-a1e7-428...|01 - American Ind...| F| 37.0| null| 2|65007233-424e-4c2...|9d66f5b7-ab10-47f...|1f3d76c8-d039-483...| |unknown| null| null| true| |002673d4-579a-4d1...|01 - American Ind...| M| 64.0| null| 2|a3c89a7f-d57d-4be...|2f6ffa09-e5b3-419...|7dbfc730-64bc-4a9...| |unknown| null| null| true| |00267822-8192-44f...|01 - American Ind...| F| 0.0| null| 2|cd318b72-35d4-422...|44646492-60ef-44e...|d5f462ef-cd4c-497...| |unknown| null| null| true| |0028fece-59ec-4db...|01 - American Ind...| F| 0.0| null| 2|ee9e09aa-67bc-47e...|3be068de-7fe3-44d...|63a04010-c381-4aa...| |unknown| null| null| true| |003470e7-b548-444...|06 - American Ind...| M| 171.0| null| 2|7ed5b0f9-02b3-459...|1b778c9f-71ab-45a...|84ecc23a-6c39-44d...| |unknown| null| null| false| |0044a493-e226-409...|01 - American Ind...| F| 0.0| null| 2|c821f5b2-d0af-428...|26144dac-81f0-44e...|f7355eeb-89a3-4f0...| |unknown| null| null| true| |004d44d0-fdf7-403...|01 - American Ind...| F| 37.0| null| 2|cb6c8e5c-71ab-409...|88eaf3c4-5f00-4e9...|78679644-f4e7-450...| |unknown| null| null| true| |0059c1bf-5263-42a...|03 - Black or Afr...| M| 0.0| null| 2|da9247d1-96fb-44d...|6831544a-faf9-426...|3534f3a8-a367-41e...| |unknown| null| null| true| |007b82b6-ae2e-49e...|01 - American Ind...| M| 43.0| null| 2|3e6fcc8c-c484-465...|90e2a03f-f0a4-48f...|5c9c71e1-901b-481...| |unknown| null| null| true| |00917cf5-d879-43f...|01 - American Ind...| F| 0.0| null| 2|b4fa8b2f-7452-4f5...|779eaa9e-7961-4f0...|2d19ad6b-de35-4bc...| |unknown| null| null| true| +--------------------+--------------------+--------+-------------+-------------+-------+--------------------+--------------------+--------------------+------+-------+----------+---------+---------+

```

(This is de-identified data)",I am using Spark 3.4.0 however this has been an issue for years.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/23 23:02;reece.robinson;Screenshot 2023-10-04 at 11.11.28 AM.png;https://issues.apache.org/jira/secure/attachment/13063343/Screenshot+2023-10-04+at+11.11.28%E2%80%AFAM.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-10-03 23:01:54.0,,,,,,,,,,"0|z1kq1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-connect-repl is not working on macOS,SPARK-45391,13552564,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tanvu,tanvu,01/Oct/23 08:39,04/Oct/23 17:05,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Connect Contrib,,,,,0,,,,,"I followed [https://spark.apache.org/docs/latest/spark-connect-overview.html#use-spark-connect-for-interactive-analysis] to try spark-connect-repl on my local PC but got the following error:

 

---------------------------

spark-connect-repl
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/sparkproject/connect/client/com/google/common/io/BaseEncoding
    at org.sparkproject.connect.client.io.grpc.Metadata.<clinit>(Metadata.java:114)
    at org.apache.spark.sql.connect.client.SparkConnectClient$.<init>(SparkConnectClient.scala:329)
    at org.apache.spark.sql.connect.client.SparkConnectClient$.<clinit>(SparkConnectClient.scala)
    at org.apache.spark.sql.application.ConnectRepl$.doMain(ConnectRepl.scala:61)
    at org.apache.spark.sql.application.ConnectRepl$.main(ConnectRepl.scala:50)
    at org.apache.spark.sql.application.ConnectRepl.main(ConnectRepl.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at coursier.bootstrap.launcher.a.a(Unknown Source)
    at coursier.bootstrap.launcher.Launcher.main(Unknown Source)
Caused by: java.lang.ClassNotFoundException: org.sparkproject.connect.client.com.google.common.io.BaseEncoding
    at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    ... 12 more

---------------------------
 
Do you have any idea why this is happening and how to solve it? 
Thank you.
 ","MacBook M2

cs version
2.1.7

scala -version
Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 04 17:05:11 UTC 2023,,,,,,,,,,"0|z1kokw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/23 17:05;mtbaker;I'm getting this same error after installing spark-connect-repl in an arm64 ubuntu:focal Docker container.;;;",,,,,,,,,,,,,,
Correct MetaException matching rule on getting partition metadata,SPARK-45389,13552551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,30/Sep/23 19:41,10/Oct/23 06:45,30/Oct/23 17:26,02/Oct/23 13:27,3.3.3,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,SQL,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Oct 07 22:26:58 UTC 2023,,,,,,,,,,"0|z1koi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/23 13:27;srowen;https://github.com/apache/spark/pull/43191;;;","07/Oct/23 22:26;dongjoon;This landed at branch-3.5 via [https://github.com/apache/spark/pull/43260];;;",,,,,,,,,,,,,
Missing case for RelationTimeTravel in CheckAnalysis,SPARK-45383,13552486,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,ryan.johnson@databricks.com,ryan.johnson@databricks.com,29/Sep/23 19:40,09/Oct/23 19:16,30/Oct/23 17:26,09/Oct/23 19:16,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Spark Core,,,,,0,pull-request-available,,,,"{{CheckAnalysis.checkAnalysis0}} lacks a case for {{{}RelationTimeTravel{}}}, and since the latter is (intentionally) an {{UnresolvedLeafNode}} rather than a {{{}UnaryNode{}}}, the existing checks do not traverse it.

Result: Attempting time travel over a non-existing table produces a spark internal error from the [default case|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala#L818], rather than the expected {{{}AnalysisException{}}}:
{code:java}
[info]   Cause: org.apache.spark.SparkException: [INTERNAL_ERROR] Found the unresolved operator: 'RelationTimeTravel 'UnresolvedRelation [not_exists], [], false, 0
[info]   at org.apache.spark.SparkException$.internalError(SparkException.scala:77)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$54(CheckAnalysis.scala:753) {code}
Fix should be simple enough:
{code:java}
case tt: RelationTimeTravel =>
  checkAnalysis0(tt.table) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Oct 09 19:16:54 UTC 2023,,,,,,,,,,"0|z1ko3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/23 19:16;maxgekk;Issue resolved by pull request 43298
[https://github.com/apache/spark/pull/43298];;;",,,,,,,,,,,,,,
Incorrect COUNT bug handling in scalar subqueries,SPARK-45381,13552464,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,gubichev,gubichev,29/Sep/23 16:04,29/Sep/23 16:04,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"This query has incorrect results:
create temp view l (a, b)
as values
(1, 2.0),
(1, 2.0),
(2, 1.0),
(2, 1.0),
(3, 3.0),
(null, null),
(null, 5.0),
(6, null);

create temp view r (c, d)
as values
(2, 3.0),
(2, 3.0),
(3, 2.0),
(4, 1.0),
(null, null),
(null, 5.0),
(6, null);

select (
select sum(cnt)
from (select count ( * ) cnt from r where l.a = r.c)
) from l;
 
 
It returns 

– !query output
1
1
2
2
NULL
NULL
NULL
NULL

NULLs in the output should be zeros.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-29 16:04:34.0,,,,,,,,,,"0|z1knyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FIx shading problem in Spark Connect,SPARK-45371,13552341,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,hvanhovell,hvanhovell,hvanhovell,28/Sep/23 18:06,19/Oct/23 05:06,30/Oct/23 17:26,02/Oct/23 17:04,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Connect,,,,,0,pull-request-available,,,,See: https://stackoverflow.com/questions/77151840/spark-connect-client-failing-with-java-lang-noclassdeffounderror,,,,,,,,,,,,,,,,,,SPARK-45255,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 29 12:05:15 UTC 2023,,,,,,,,,,"0|z1kn7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/23 12:05;boltonidze;We have the same issue here
https://issues.apache.org/jira/browse/SPARK-45255;;;",,,,,,,,,,,,,,
Maven test `SparkConnectProtoSuite` failed,SPARK-45357,13552176,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,27/Sep/23 12:31,06/Oct/23 09:23,30/Oct/23 17:26,06/Oct/23 06:10,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Connect,,,,,0,pull-request-available,,,," 

build/mvn clean install -pl connector/connect/server -am -DskipTests

mvn test -pl connector/connect/server 

 
{code:java}
- Test observe *** FAILED ***
  == FAIL: Plans do not match ===
  !CollectMetrics my_metric, [min(id#0) AS min_val#0, max(id#0) AS max_val#0, sum(id#0) AS sum(id)#0L], 0   CollectMetrics my_metric, [min(id#0) AS min_val#0, max(id#0) AS max_val#0, sum(id#0) AS sum(id)#0L], 53
   +- LocalRelation <empty>, [id#0, name#0]                                                                 +- LocalRelation <empty>, [id#0, name#0] (PlanTest.scala:179) {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 06 06:10:06 UTC 2023,,,,,,,,,,"0|z1km6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/23 06:10;podongfeng;Issue resolved by pull request 43155
[https://github.com/apache/spark/pull/43155];;;",,,,,,,,,,,,,,
Parquet schema inference should respect case sensitive flag when merging schema,SPARK-45346,13552068,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,26/Sep/23 16:56,27/Sep/23 08:01,30/Oct/23 17:26,27/Sep/23 08:01,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,SQL,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 27 08:01:33 UTC 2023,,,,,,,,,,"0|z1klio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/23 08:01;cloud_fan;Issue resolved by pull request 43134
[https://github.com/apache/spark/pull/43134];;;",,,,,,,,,,,,,,
The driver did not receive the StatusUpdate sent by the executor,SPARK-45326,13551956,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,Echo Lee,Echo Lee,26/Sep/23 01:05,26/Sep/23 01:13,30/Oct/23 17:26,,2.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"I have a spark task that occasionally runs for a long time and does not finished. Usually it is normal and I did not find any error logs. The deployment mode of the task is standalone
But I found from the driver and executor logs that the executor showed that some tasks had been completed, but the driver only received one StatusUpdate event.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/23 01:07;Echo Lee;driver&executor log.jpg;https://issues.apache.org/jira/secure/attachment/13063159/driver%26executor+log.jpg","26/Sep/23 01:06;Echo Lee;driver-submit.log;https://issues.apache.org/jira/secure/attachment/13063157/driver-submit.log","26/Sep/23 01:09;Echo Lee;driver.stack;https://issues.apache.org/jira/secure/attachment/13063161/driver.stack","26/Sep/23 01:07;Echo Lee;executor log.png;https://issues.apache.org/jira/secure/attachment/13063160/executor+log.png","26/Sep/23 01:09;Echo Lee;executor1.stack;https://issues.apache.org/jira/secure/attachment/13063162/executor1.stack","26/Sep/23 01:09;Echo Lee;executor2.stack;https://issues.apache.org/jira/secure/attachment/13063163/executor2.stack",,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-26 01:05:58.0,,,,,,,,,,"0|z1kkts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark 3.5.0 missing in pypi,SPARK-45324,13551943,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,raphaelauv,raphaelauv,25/Sep/23 20:18,25/Sep/23 22:27,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Build,,,,,1,,,,,"the pyspark 3.5.0 is not present in pypi -> [https://pypi.org/project/pyspark/#history]
the version is 3.4.1 is currently the latest",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 25 22:27:15 UTC 2023,,,,,,,,,,"0|z1kkqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/23 22:02;husseinawala;I was going to report the same issue, the published doc and the release note mention PySpark 3.5.0 features, but its package was not published to pypi.;;;","25/Sep/23 22:14;husseinawala;I haven't found any CI jobs calling `release-build.sh finalize`, it looks like the push script is executed manually by one of the PMC members, and they just missed this step.;;;","25/Sep/23 22:27;husseinawala;I found this in the announcement email:

> (Please note: the PyPi upload is pending due to a size limit request; we're actively following up here <[https://github.com/pypi/support/issues/3175]> with the PyPi organization)

 

The issue was resolved 3 hours ago, so the package should be pushed in the next 24 hours.;;;",,,,,,,,,,,,
SHOW COLUMNS namespace is not catalog-aware ,SPARK-45319,13551914,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ryan.johnson@databricks.com,ryan.johnson@databricks.com,25/Sep/23 15:08,25/Sep/23 15:13,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Today, the namespace argument of {{SHOW COLUMNS}} command is only partly catalog-aware. [AstBuilder.scala|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala#L4693] correctly pulls in and applies namespaces with more than one element, but [ResolveSessionCatalog.scala|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala#L316] does not correctly validate a multi-element namespace against the qualified table name:
{code:java}
    case ShowColumns(ResolvedV1TableOrViewIdentifier(ident), ns, output) =>
      val v1TableName = ident
      val resolver = conf.resolver
      val db = ns match {
        case Some(db) if v1TableName.database.exists(!resolver(_, db.head)) =>
          throw QueryCompilationErrors.showColumnsWithConflictDatabasesError(db, v1TableName)
        case _ => ns.map(_.head)
      } {code}
By always/only checking {{db.head}} against {{{}v1TableName.database{}}}, it will not correctly handle e.g.
{code:java}
SHOW COLUMNS FOR a.b.table IN a.b -- incorrectly fails
{code}
There is also some ambiguity in the semantics of the check:
{code:java}
SHOW COLUMNS FOR a.b.table IN a -- fails today (should it succeed?) 
SHOW COLUMNS FOR a.b.table IN b -- succeeds today (should it fail?) {code}
It might work better to use an actual {{Identifier}} (instead of {{{}TableIdentifier{}}}), and compare its {{namespace}} against the user-provided namespace?

Tangentially: It's a arguably a bit strange for the validity check to reside in {{ResolveSessionCatalog}} rule, given that it doesn't actually have anything to do with session catalogs? This seems more an artifact of an implementation that searches specifically for V1 tables, and {{ResolveSessionCatalog}} providing matchers that make that job easier?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-25 15:08:31.0,,,,,,,,,,"0|z1kkkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle null filename in stack traces of exceptions,SPARK-45317,13551896,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,heyihong,heyihong,heyihong,25/Sep/23 13:19,26/Sep/23 02:04,30/Oct/23 17:26,26/Sep/23 02:04,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Connect,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 26 02:04:24 UTC 2023,,,,,,,,,,"0|z1kkgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/23 02:04;gurwls223;Issue resolved by pull request 43103
[https://github.com/apache/spark/pull/43103];;;",,,,,,,,,,,,,,
Support toggle display/hide plan svg on execution page,SPARK-45312,13551862,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,25/Sep/23 09:06,26/Sep/23 10:33,30/Oct/23 17:26,26/Sep/23 03:00,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,UI,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 26 03:00:06 UTC 2023,,,,,,,,,,"0|z1kk8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/23 03:00;dongjoon;Issue resolved by pull request 43099
[https://github.com/apache/spark/pull/43099];;;",,,,,,,,,,,,,,
"Encoder fails on many ""NoSuchElementException: None.get"" since 3.4.x, search for an encoder for a generic type, and since 3.5.x isn't ""an expression encoder""",SPARK-45311,13551860,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,mlebihan,mlebihan,25/Sep/23 08:46,25/Sep/23 14:40,30/Oct/23 17:26,,3.4.0,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"If you find it convenient, you might clone the [https://gitlab.com/territoirevif/minimal-tests-spark-issue] project (that does many operations around cities, local authorities and accounting with open data) where I've extracted from my work what's necessary to make a set of 35 tests that run correctly with Spark 3.3.x, and show the troubles encountered with 3.4.x and 3.5.x.

 

It is working well with Spark 3.2.x, 3.3.x. But as soon as I selec{*}t Spark 3.4.x{*}, where the encoder seems to have deeply changed, the encoder fails with two problems:

 

*1)* It throws *java.util.NoSuchElementException: None.get* messages everywhere.

Asking over the Internet, I wasn't alone facing this problem. Reading it, you'll see that I've attempted a debug but my Scala skills are low.

[https://stackoverflow.com/questions/76036349/encoders-bean-doesnt-work-anymore-on-a-java-pojo-with-spark-3-4-0]

{color:#172b4d}by the way, if possible, the encoder and decoder functions should forward a parameter as soon as the name of the field being handled is known, and then all the long of their process, so that when the encoder is at any point where it has to throw an exception, it knows the field it is handling in its specific call and can send a message like:{color}
{color:#00875a}_java.util.NoSuchElementException: None.get when encoding [the method or field it was targeting]_{color}

 

*2)* *Not found an encoder of the type RS to Spark SQL internal representation.* Consider to change the input type to one of supported at (...)
Or : Not found an encoder of the type *OMI_ID* to Spark SQL internal representation (...)

 
where *RS* and *OMI_ID* are generic types.
This is strange.
[https://stackoverflow.com/questions/76045255/encoders-bean-attempts-to-check-the-validity-of-a-return-type-considering-its-ge]

 

*3)* When I switch to the *Spark 3.5.0* version, the same problems remain, but another add itself to the list:
""{*}Only expression encoders are supported for now{*}"" on what was accepted and working before.
 ","Debian 12

Java 17

Underlying Spring-Boot 2.7.14",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,,,2023-09-25 08:46:50.0,,,,,,,,,,"0|z1kk8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Zulu JDK in `benchmark` GitHub Action and Java 21,SPARK-45307,13551836,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,25/Sep/23 06:03,25/Sep/23 16:11,30/Oct/23 17:26,25/Sep/23 14:39,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Project Infra,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 25 14:39:54 UTC 2023,,,,,,,,,,"0|z1kk34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/23 14:39;LuciferYang;Issue resolved by pull request 43094
[https://github.com/apache/spark/pull/43094];;;",,,,,,,,,,,,,,
Make `InMemoryColumnarBenchmark` use AQE-aware utils to collect plans,SPARK-45306,13551834,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,25/Sep/23 05:40,25/Sep/23 14:16,30/Oct/23 17:26,25/Sep/23 11:06,3.5.1,4.0.0,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,SQL,Tests,,,,0,pull-request-available,,,,"After SPARK-42768, the default value of `spark.sql.optimizer.canChangeCachedPlanOutputPartitioning` has changed from false to true, so we should use AQE-aware utils to collect plans.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 25 11:06:45 UTC 2023,,,,,,,,,,"0|z1kk2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/23 11:06;cloud_fan;Issue resolved by pull request 43093
[https://github.com/apache/spark/pull/43093];;;",,,,,,,,,,,,,,
Use unknown query execution id instead of no such app when id is invalid,SPARK-45291,13551784,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,24/Sep/23 15:32,25/Sep/23 06:27,30/Oct/23 17:26,25/Sep/23 06:26,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,SQL,UI,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 25 06:27:23 UTC 2023,,,,,,,,,,"0|z1kjrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/23 06:27;yao; 

Issue resolved by [GitHub Pull Request #43073|https://github.com/apache/spark/pull/43073];;;",,,,,,,,,,,,,,
The return status is incorrect in standalone mode,SPARK-45290,13551761,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,changhu,changhu,24/Sep/23 01:53,06/Oct/23 03:13,30/Oct/23 17:26,,3.4.0,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,,,,Block Manager,,,,,0,pull-request-available,,,," When a job is submitted in standalone mode using spark launch, sparkLancher returns a successful execution and then a failed execution. Examples are as follows

log: Spark App Id [app-20230922160022-0006] State Changed.  State [FINISHED]
23/09/22 16:01:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/09/22 16:01:10 INFO MemoryStore: MemoryStore cleared
23/09/22 16:01:10 INFO BlockManager: BlockManager stopped
23/09/22 16:01:10 INFO BlockManagerMaster: BlockManagerMaster stopped
23/09/22 16:01:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/09/22 16:01:10 INFO SparkContext: Successfully stopped SparkContext
23/09/22 16:01:10 INFO ShutdownHookManager: Shutdown hook called
23/09/22 16:01:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-e015933f-a220-45a8-9d73-650b8bd8a337
23/09/22 16:01:10 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-8879f1d8-1f21-4b52-bca1-d3c66af6f754
23/09/22 16:01:11 INFO log: Spark App Id [app-20230922160022-0006] State Changed.  State [FAILED]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-24 01:53:21.0,,,,,,,,,,"0|z1kjmg:",9223372036854775807,,,,,,,,,,,,,3.4.0,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,,,,,
ClassCastException when reading Delta table on AWS S3,SPARK-45289,13551743,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tanawatpan,tanawatpan,23/Sep/23 14:55,20/Oct/23 05:08,30/Oct/23 17:26,20/Oct/23 05:08,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,SQL,,,,0,,,,,"When attempting to read a Delta table from S3 using version 3.5.0, a _*{{ClassCastException}}*_ occurs involving {{_*org.apache.hadoop.fs.s3a.S3AFileStatus*_}} and {_}*{{org.apache.spark.sql.execution.datasources.FileStatusWithMetadata}}*{_}. The error appears to be related to the new feature SPARK-43039.

_*Steps to Reproduce:*_
{code:java}
export AWS_ACCESS_KEY_ID='<ACCESS_KEY>'
export AWS_SECRET_ACCESS_KEY='<SECRET_KEY>'
export AWS_REGION='<REGION>'

docker run --rm -it apache/spark:3.5.0-scala2.12-java11-ubuntu /opt/spark/bin/spark-shell \
--packages 'org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-core_2.12:2.4.0' \
--conf ""spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"" \
--conf ""spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"" \
--conf ""spark.hadoop.aws.region=$AWS_REGION"" \
--conf ""spark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID"" \
--conf ""spark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY"" \
--conf ""spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"" \
--conf ""spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"" \
--conf ""spark.hadoop.fs.s3a.path.style.access=true"" \
--conf ""spark.hadoop.fs.s3a.connection.ssl.enabled=true"" \
--conf ""spark.jars.ivy=/tmp/ivy/cache""{code}
{code:java}
scala> spark.read.format(""delta"").load(""s3://<bucket_name>/<delta_table_name>/"").show() {code}
*Logs:*
{code:java}
java.lang.ClassCastException: class org.apache.hadoop.fs.s3a.S3AFileStatus cannot be cast to class org.apache.spark.sql.execution.datasources.FileStatusWithMetadata (org.apache.hadoop.fs.s3a.S3AFileStatus is in unnamed module of loader scala.reflect.internal.util.ScalaClassLoader$URLClassLoader @4552f905; org.apache.spark.sql.execution.datasources.FileStatusWithMetadata is in unnamed module of loader 'app')
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
  at scala.collection.TraversableLike.map(TraversableLike.scala:286)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2(DataSourceScanExec.scala:466)
  at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2$adapted(DataSourceScanExec.scala:466)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.sql.execution.FileSourceScanLike.setFilesNumAndSizeMetric(DataSourceScanExec.scala:466)
  at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions(DataSourceScanExec.scala:257)
  at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions$(DataSourceScanExec.scala:251)
  at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions$lzycompute(DataSourceScanExec.scala:506)
  at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions(DataSourceScanExec.scala:506)
  at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions(DataSourceScanExec.scala:286)
  at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions$(DataSourceScanExec.scala:267)
  at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions$lzycompute(DataSourceScanExec.scala:506)
  at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions(DataSourceScanExec.scala:506)
  at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:553)
  at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
  at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
  at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
  at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
  at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
  at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)
  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)
  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:445)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3585)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:3585)
  at org.apache.spark.sql.delta.Snapshot.protocolAndMetadataReconstruction(Snapshot.scala:215)
  at org.apache.spark.sql.delta.Snapshot.x$1$lzycompute(Snapshot.scala:134)
  at org.apache.spark.sql.delta.Snapshot.x$1(Snapshot.scala:129)
  at org.apache.spark.sql.delta.Snapshot._metadata$lzycompute(Snapshot.scala:129)
  at org.apache.spark.sql.delta.Snapshot._metadata(Snapshot.scala:129)
  at org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:197)
  at org.apache.spark.sql.delta.Snapshot.toString(Snapshot.scala:390)
  at java.base/java.lang.String.valueOf(Unknown Source)
  at java.base/java.lang.StringBuilder.append(Unknown Source)
  at org.apache.spark.sql.delta.Snapshot.$anonfun$new$1(Snapshot.scala:393)
  at org.apache.spark.sql.delta.Snapshot.$anonfun$logInfo$1(Snapshot.scala:370)
  at org.apache.spark.internal.Logging.logInfo(Logging.scala:60)
  at org.apache.spark.internal.Logging.logInfo$(Logging.scala:59)
  at org.apache.spark.sql.delta.Snapshot.logInfo(Snapshot.scala:370)
  at org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:393)
  at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$4(SnapshotManagement.scala:356)
  at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:480)
  at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:468)
  at org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:74)
  at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:349)
  at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:343)
  at org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:74)
  at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshotAtInitInternal$1(SnapshotManagement.scala:304)
  at scala.Option.map(Option.scala:230)
  at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotAtInitInternal(SnapshotManagement.scala:301)
  at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotAtInitInternal$(SnapshotManagement.scala:298)
  at org.apache.spark.sql.delta.DeltaLog.createSnapshotAtInitInternal(DeltaLog.scala:74)
  at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:293)
  at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
  at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
  at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)
  at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:288)
  at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:287)
  at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)
  at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:57)
  at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)
  at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:790)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:785)
  at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
  at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
  at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:595)
  at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)
  at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
  at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
  at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:595)
  at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)
  at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)
  at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)
  at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:595)
  at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:784)
  at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:802)
  at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)
  at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
  at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
  at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
  at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
  at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:801)
  at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:811)
  at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:658)
  at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:85)
  at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:84)
  at org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:122)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:122)
  at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:103)
  at org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:178)
  at org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:230)
  at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
  at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
  at org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)
  at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:188)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)
  ... 47 elided
{code}
The issue does not occur with the following configurations:
 # _Spark 3.3.3, hadoop-aws:3.3.2, delta-core_2.12:1.2.1_
 # _Spark 3.4.1, hadoop-aws:3.3.4, delta-core_2.12:2.4.0_","Spark version: 3.5.0

Deployment mode: spark-shell

OS: Ubuntu (Docker image)

Java/JVM version: OpenJDK 11

Packages: hadoop-aws:3.3.4, delta-core_2.12:2.4.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Oct 19 10:04:17 UTC 2023,,,,,,,,,,"0|z1kjig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/23 10:04;sdaberdaku;Hello [~tanawatpan], you need to use the latest delta-spark version 3.0.0 which came out just yesterday. It now supports delta with Spark 3.5.0.
https://github.com/delta-io/delta/releases/tag/v3.0.0;;;",,,,,,,,,,,,,,
Make StatusTrackerSuite less fragile,SPARK-45283,13551693,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,xiongbo,xiongbo,xiongbo,22/Sep/23 17:16,18/Oct/23 05:22,30/Oct/23 17:26,04/Oct/23 00:02,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,4.0.0,,,,Spark Core,Tests,,,,0,pull-request-available,,,,"It's discovered from [Github Actions|https://github.com/xiongbo-sjtu/spark/actions/runs/6270601155/job/17028788767] that StatusTrackerSuite can run into random failures, as shown by the following stack trace (highlighted in red).  The proposed fix is to update the unit test to remove the nondeterministic behavior.
{quote}[info] StatusTrackerSuite:
[info] - basic status API usage (99 milliseconds)
[info] - getJobIdsForGroup() (56 milliseconds)
[info] - getJobIdsForGroup() with takeAsync() (48 milliseconds)
[info] - getJobIdsForGroup() with takeAsync() across multiple partitions (58 milliseconds)
[info] - getJobIdsForTag() *** FAILED *** (10 seconds, 77 milliseconds)
{color:#ff0000}[info] The code passed to eventually never returned normally. Attempted 651 times over 10.005059944000001 seconds. Last failure message: Set(3, 2, 1) was not equal to Set(1, 2). (StatusTrackerSuite.scala:148){color}
[info] org.scalatest.exceptions.TestFailedDueToTimeoutException:
[info] at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)
[info] at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)
[info] at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)
[info] at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)
[info] at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)
[info] at org.apache.spark.StatusTrackerSuite.$anonfun$new$21(StatusTrackerSuite.scala:148)
[info] at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
[info] at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
[info] at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
[info] at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
[info] at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
[info] at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
[info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info] at org.scalatest.Transformer.apply(Transformer.scala:22)
[info] at org.scalatest.Transformer.apply(Transformer.scala:20)
[info] at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info] at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info] at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info] at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
[info] at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info] at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info] at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info] at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info] at scala.collection.immutable.List.foreach(List.scala:333)
[info] at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info] at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info] at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info] at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info] at org.scalatest.Suite.run(Suite.scala:1114)
[info] at org.scalatest.Suite.run$(Suite.scala:1096)
[info] at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info] at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info] at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
[info] at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info] at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info] at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info] at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
[info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info] at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)
[info] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info] at java.lang.Thread.run(Thread.java:750)
[info] Cause: org.scalatest.exceptions.TestFailedException: Set(3, 2, 1) was not equal to Set(1, 2)
[info] at org.scalatest.matchers.MatchersHelper$.indicateFailure(MatchersHelper.scala:397)
[info] at org.scalatest.matchers.should.Matchers$ShouldMethodHelperClass.shouldMatcher(Matchers.scala:7299)
[info] at org.scalatest.matchers.should.Matchers$AnyShouldWrapper.should(Matchers.scala:7347)
[info] at org.apache.spark.StatusTrackerSuite.$anonfun$new$27(StatusTrackerSuite.scala:152)
[info] at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)
[info] at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)
[info] at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)
[info] at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)
[info] at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)
[info] at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)
[info] at org.apache.spark.StatusTrackerSuite.$anonfun$new$21(StatusTrackerSuite.scala:148)
[info] at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
[info] at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
[info] at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
[info] at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
[info] at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
[info] at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
[info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info] at org.scalatest.Transformer.apply(Transformer.scala:22)
[info] at org.scalatest.Transformer.apply(Transformer.scala:20)
[info] at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info] at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info] at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info] at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
[info] at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info] at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info] at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info] at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info] at scala.collection.immutable.List.foreach(List.scala:333)
[info] at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info] at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info] at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info] at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info] at org.scalatest.Suite.run(Suite.scala:1114)
[info] at org.scalatest.Suite.run$(Suite.scala:1096)
[info] at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info] at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info] at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
[info] at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info] at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info] at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info] at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
[info] at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info] at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info] at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)
[info] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info] at java.lang.Thread.run(Thread.java:750)
{quote}",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,,,,,Wed Oct 04 00:02:01 UTC 2023,,,,,,,,,,"0|z1kj7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/23 00:02;mridulm80;Issue resolved by pull request 43194
[https://github.com/apache/spark/pull/43194];;;",,,,,,,,,,,,,,
Join loses records for cached datasets,SPARK-45282,13551692,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,koert,koert,22/Sep/23 17:13,28/Sep/23 04:07,30/Oct/23 17:26,,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,correctness,CorrectnessBug,,,"we observed this issue on spark 3.4.1 but it is also present on 3.5.0. it is not present on spark 3.3.1.

it only shows up in distributed environment. i cannot replicate in unit test. however i did get it to show up on hadoop cluster, kubernetes, and on databricks 13.3

the issue is that records are dropped when two cached dataframes are joined. it seems in spark 3.4.1 in queryplan some Exchanges are dropped as an optimization while in spark 3.3.1 these Exhanges are still present. it seems to be an issue with AQE with canChangeCachedPlanOutputPartitioning=true.

to reproduce on distributed cluster these settings needed are:
{code:java}
spark.sql.adaptive.advisoryPartitionSizeInBytes 33554432
spark.sql.adaptive.coalescePartitions.parallelismFirst false
spark.sql.adaptive.enabled true
spark.sql.optimizer.canChangeCachedPlanOutputPartitioning true {code}
code using scala to reproduce is:
{code:java}
import java.util.UUID
import org.apache.spark.sql.functions.col

import spark.implicits._

val data = (1 to 1000000).toDS().map(i => UUID.randomUUID().toString).persist()

val left = data.map(k => (k, 1))
val right = data.map(k => (k, k)) // if i change this to k => (k, 1) it works!
println(""number of left "" + left.count())
println(""number of right "" + right.count())
println(""number of (left join right) "" +
  left.toDF(""key"", ""value1"").join(right.toDF(""key"", ""value2""), ""key"").count()
)

val left1 = left
  .toDF(""key"", ""value1"")
  .repartition(col(""key"")) // comment out this line to make it work
  .persist()
println(""number of left1 "" + left1.count())

val right1 = right
  .toDF(""key"", ""value2"")
  .repartition(col(""key"")) // comment out this line to make it work
  .persist()
println(""number of right1 "" + right1.count())

println(""number of (left1 join right1) "" +  left1.join(right1, ""key"").count()) // this gives incorrect result{code}
this produces the following output:
{code:java}
number of left 1000000
number of right 1000000
number of (left join right) 1000000
number of left1 1000000
number of right1 1000000
number of (left1 join right1) 859531 {code}
note that the last number (the incorrect one) actually varies depending on settings and cluster size etc.

 ",spark 3.4.1 on apache hadoop 3.3.6 or kubernetes 1.26 or databricks 13.3,,,,,,,,,,,,,,,,,,,SPARK-41048,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 28 04:05:51 UTC 2023,,,,,,,,,,"0|z1kj74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/23 18:40;koert;after reverting SPARK-41048 the issue went away.;;;","24/Sep/23 14:46;yumwang;cc [~ulysses] [~cloud_fan];;;","27/Sep/23 01:36;ulysses;I can not re-produce this issue in master branch (4.0.0), [~koert] have you tried master branch ?;;;","28/Sep/23 04:05;koert;yes i can reproduce it.

master branch on commit:
{code:java}
commit 7e8aafd2c0f1f6fcd03a69afe2b85fd3fda95d20 (HEAD -> master, upstream/master)
Author: lanmengran1 <lanmengran1@jd.com>
Date:   Tue Sep 26 21:01:02 2023 -0500    [SPARK-45334][SQL] Remove misleading comment in parquetSchemaConverter {code}
i build spark for k8s using:
{code:java}
$ dev/make-distribution.sh --name kubernetes --tgz -Pkubernetes -Phadoop-cloud {code}
created docker container using Dockerfile provided in resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile

launch pod and shell inside:
{code:java}
185@proxy:~/work-dir$ export SPARK_LOCAL_HOSTNAME=$(hostname -i
185@proxy:~/work-dir$ export SPARK_PUBLIC_DNS=$(hostname -i)                                                                                              185@proxy:~/work-dir$ /opt/spark/bin/spark-shell --master k8s://https://kubernetes.default:443 --deploy-mode client --num-executors 4 --executor-memory 2G --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kubernetes.namespace=default --conf spark.sql.adaptive.coalescePartitions.parallelismFirst=false --conf spark.sql.adaptive.enabled=true --conf spark.sql.adaptive.advisoryPartitionSizeInBytes=33554432 --conf spark.sql.optimizer.canChangeCachedPlanOutputPartitioning=true --conf spark.kubernetes.container.image=111111111111.dkr.ecr.us-east-1.amazonaws.com/spark:4.0.0-SNAPSHOT
23/09/28 03:44:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 4.0.0-SNAPSHOT
      /_/
         
Using Scala version 2.13.11 (OpenJDK 64-Bit Server VM, Java 21)
Type in expressions to have them evaluated.
Type :help for more information.
Spark context Web UI available at http://10.177.71.94:4040
Spark context available as 'sc' (master = k8s://https://kubernetes.default:443, app id = spark-5ab0957571944828866a2f23068ff180).
Spark session available as 'spark'.scala> :paste
// Entering paste mode (ctrl-D to finish)import java.util.UUID
import org.apache.spark.sql.functions.col
import spark.implicits._

val data = (1 to 1000000).toDS().map(i => UUID.randomUUID().toString).persist()
val left = data.map(k => (k, 1))
val right = data.map(k => (k, k)) // if i change this to k => (k, 1) it works!
println(""number of left "" + left.count())
println(""number of right "" + right.count())
println(""number of (left join right) "" +
  left.toDF(""key"", ""vertex"").join(right.toDF(""key"", ""state""), ""key"").count()
)

val left1 = left
  .toDF(""key"", ""vertex"")
  .repartition(col(""key"")) // comment out this line to make it work
  .persist()
println(""number of left1 "" + left1.count())
val right1 = right
  .toDF(""key"", ""state"")
  .repartition(col(""key"")) // comment out this line to make it work
  .persist()
println(""number of right1 "" + right1.count())
println(""number of (left1 join right1) "" +  left1.join(right1, ""key"").count()) // this gives incorrect result
// Exiting paste mode, now interpreting.
23/09/28 03:45:30 WARN TaskSetManager: Stage 0 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
23/09/28 03:45:34 WARN TaskSetManager: Stage 1 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
number of left 1000000                                                          
23/09/28 03:45:36 WARN TaskSetManager: Stage 4 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
number of right 1000000
23/09/28 03:45:39 WARN TaskSetManager: Stage 7 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
23/09/28 03:45:40 WARN TaskSetManager: Stage 8 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
number of (left join right) 1000000                                             
23/09/28 03:45:45 WARN TaskSetManager: Stage 16 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
number of left1 1000000                                                         
23/09/28 03:45:48 WARN TaskSetManager: Stage 24 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
number of right1 1000000                                                        
number of (left1 join right1) 850735                                            
import java.util.UUID
import org.apache.spark.sql.functions.col
import spark.implicits._
val data: org.apache.spark.sql.Dataset[String] = [value: string]
val left: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]
val right: org.apache.spark.sql.Dataset[(String, String)] = [_1: string, _2: string]
val left1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [key: string, vertex: int]
val right1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [key: string, state: string]

scala>   {code}
 ;;;",,,,,,,,,,,
Change Maven daily test use Java 17 for testing.,SPARK-45280,13551669,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,22/Sep/23 13:47,22/Sep/23 16:03,30/Oct/23 17:26,22/Sep/23 16:03,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Project Infra,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 22 16:03:12 UTC 2023,,,,,,,,,,"0|z1kj20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/23 16:03;dongjoon;Issue resolved by pull request 43057
[https://github.com/apache/spark/pull/43057];;;",,,,,,,,,,,,,,
replace function fails to handle null replace param,SPARK-45275,13551617,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,deolid,deolid,22/Sep/23 07:19,22/Sep/23 07:23,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"[replace |https://spark.apache.org/docs/latest/api/sql/#replace]function fails to handle null replace param, example below:

 
df.withColumn('test',F.expr('replace(col1, ""nUll"", 1)')).show()
|| ||col1||col2||test||
||0|person1|0.0|person1|
||1|person1|2.0|person1|
||2|person1|3.0|person1|
||3|person2|1.0|person2|
||4|None|2.0|None|
||5|nUll|None|1|

 
df.withColumn('test',F.expr('replace(col1, ""nUll"", null)')).show()
|| ||col1||col2||test||
||0|person1|0.0|None|
||1|person1|2.0|None|
||2|person1|3.0|None|
||3|person2|1.0|None|
||4|None|2.0|None|
||5|nUll|None|None|

 

 

This function has been ported over to 3.5.0 but I've not been able to test it on that yet",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/23 07:23;deolid;replace_bug.png;https://issues.apache.org/jira/secure/attachment/13063107/replace_bug.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-22 07:19:16.0,,,,,,,,,,"0|z1kiqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Http header Attack【HttpSecurityFilter】,SPARK-45273,13551615,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,chenyu-opensource,chenyu-opensource,22/Sep/23 06:26,16/Oct/23 14:53,30/Oct/23 17:26,16/Oct/23 14:53,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,pull-request-available,,,,There is an HTTP host header attack vulnerability in the target URL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 22 19:37:13 UTC 2023,,,,,,,,,,"0|z1kiq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/23 19:34;bjornjorgensen;Hi, [~chenyu-opensource] can you take this on mail to security@spark.apache.org 
CC [~srowen];;;","22/Sep/23 19:37;srowen;Yep we typically evaluate security reports on private@spark.apache.org first, not here;;;",,,,,,,,,,,,,
Arrow DurationWriter fails when vector is at capacity,SPARK-45256,13551547,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sgoos-db,sgoos-db,sgoos-db,21/Sep/23 15:04,22/Sep/23 16:17,30/Oct/23 17:26,22/Sep/23 16:17,3.4.0,3.4.1,3.4.2,3.5.0,3.5.1,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,The DurationWriter fails if more values are written than the initial capacity of the DurationVector (4032). Fix by using `setSafe` instead of `set` method. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 22 16:17:36 UTC 2023,,,,,,,,,,"0|z1kiaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/23 16:17;dongjoon;Issue resolved by pull request 43035
[https://github.com/apache/spark/pull/43035];;;",,,,,,,,,,,,,,
Spark connect client failing with java.lang.NoClassDefFoundError,SPARK-45255,13551543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,hvanhovell,haldefaiz,haldefaiz,21/Sep/23 14:29,02/Oct/23 17:07,30/Oct/23 17:26,02/Oct/23 17:06,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Connect,,,,,1,,,,,"java 1.8, sbt 1.9, scala 2.12

 

I have a very simple repo with the following dependency in `build.sbt`

```

{{libraryDependencies ++= Seq(""org.apache.spark"" %% ""spark-connect-client-jvm"" % ""3.5.0"")}}

```

A simple application

```

{{object Main extends App {}}
{{   val s = SparkSession.builder().remote(""sc://localhost"").getOrCreate()}}
{{   s.read.json(""/tmp/input.json"").repartition(10).show(false)}}
{{}}}

```

But when I run it, I get the following error

 

```

{{Exception in thread ""main"" java.lang.NoClassDefFoundError: org/sparkproject/connect/client/com/google/common/cache/CacheLoader}}
{{    at Main$.delayedEndpoint$Main$1(Main.scala:4)}}
{{    at Main$delayedInit$body.apply(Main.scala:3)}}
{{    at scala.Function0.apply$mcV$sp(Function0.scala:39)}}
{{    at scala.Function0.apply$mcV$sp$(Function0.scala:39)}}
{{    at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)}}
{{    at scala.App.$anonfun$main$1$adapted(App.scala:80)}}
{{    at scala.collection.immutable.List.foreach(List.scala:431)}}
{{    at scala.App.main(App.scala:80)}}
{{    at scala.App.main$(App.scala:78)}}
{{    at Main$.main(Main.scala:3)}}
{{    at Main.main(Main.scala)}}
{{Caused by: java.lang.ClassNotFoundException: org.sparkproject.connect.client.com.google.common.cache.CacheLoader}}
{{    at java.net.URLClassLoader.findClass(URLClassLoader.java:387)}}
{{    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)}}
{{    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)}}
{{    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)}}
{{    ... 11 more}}

```

I know the connect does a bunch of shading during assembly so it could be related to that. This application is not started via spark-submit or anything. It's not run neither under a `SPARK_HOME` ( I guess that's the whole point of connect client )

 

EDIT

Not sure if it's the right mitigation but explicitly adding guava worked but now I am in the 2nd territory of error

{{Sep 21, 2023 8:21:59 PM org.sparkproject.connect.client.io.grpc.NameResolverRegistry getDefaultRegistry}}
{{WARNING: No NameResolverProviders found via ServiceLoader, including for DNS. This is probably due to a broken build. If using ProGuard, check your configuration}}
{{Exception in thread ""main"" org.sparkproject.connect.client.com.google.common.util.concurrent.UncheckedExecutionException: org.sparkproject.connect.client.io.grpc.ManagedChannelRegistry$ProviderNotFoundException: No functional channel service provider found. Try adding a dependency on the grpc-okhttp, grpc-netty, or grpc-netty-shaded artifact}}
{{    at org.sparkproject.connect.client.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2085)}}
{{    at org.sparkproject.connect.client.com.google.common.cache.LocalCache.get(LocalCache.java:4011)}}
{{    at org.sparkproject.connect.client.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4034)}}
{{    at org.sparkproject.connect.client.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:5010)}}
{{    at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$1(SparkSession.scala:945)}}
{{    at scala.Option.getOrElse(Option.scala:189)}}
{{    at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:945)}}
{{    at Main$.delayedEndpoint$Main$1(Main.scala:4)}}
{{    at Main$delayedInit$body.apply(Main.scala:3)}}
{{    at scala.Function0.apply$mcV$sp(Function0.scala:39)}}
{{    at scala.Function0.apply$mcV$sp$(Function0.scala:39)}}
{{    at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)}}
{{    at scala.App.$anonfun$main$1$adapted(App.scala:80)}}
{{    at scala.collection.immutable.List.foreach(List.scala:431)}}
{{    at scala.App.main(App.scala:80)}}
{{    at scala.App.main$(App.scala:78)}}
{{    at Main$.main(Main.scala:3)}}
{{    at Main.main(Main.scala)}}
{{Caused by: org.sparkproject.connect.client.io.grpc.ManagedChannelRegistry$ProviderNotFoundException: No functional channel service provider found. Try adding a dependency on the grpc-okhttp, grpc-netty, or grpc-netty-shaded artifact}}
{{    at org.sparkproject.connect.client.io.grpc.ManagedChannelRegistry.newChannelBuilder(ManagedChannelRegistry.java:179)}}
{{    at org.sparkproject.connect.client.io.grpc.ManagedChannelRegistry.newChannelBuilder(ManagedChannelRegistry.java:155)}}
{{    at org.sparkproject.connect.client.io.grpc.Grpc.newChannelBuilder(Grpc.java:101)}}
{{    at org.sparkproject.connect.client.io.grpc.Grpc.newChannelBuilderForAddress(Grpc.java:111)}}
{{    at org.apache.spark.sql.connect.client.SparkConnectClient$Configuration.createChannel(SparkConnectClient.scala:633)}}
{{    at org.apache.spark.sql.connect.client.SparkConnectClient$Configuration.toSparkConnectClient(SparkConnectClient.scala:645)}}
{{    at org.apache.spark.sql.SparkSession$.create(SparkSession.scala:760)}}
{{    at org.apache.spark.sql.SparkSession$$anon$1.load(SparkSession.scala:736)}}
{{    at org.apache.spark.sql.SparkSession$$anon$1.load(SparkSession.scala:735)}}
{{    at org.sparkproject.connect.client.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3570)}}
{{    at org.sparkproject.connect.client.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2312)}}
{{    at org.sparkproject.connect.client.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2189)}}
{{    at org.sparkproject.connect.client.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2079)}}
{{    ... 17 more}}

 

The fix as per the error suggestion isn't working :/ 

I followed the doc exactly as described. Can somebody help",,,,,,,,,,,,,,,,,,,SPARK-45371,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Oct 02 17:04:50 UTC 2023,,,,,,,,,,"0|z1kia0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/23 13:25;boltonidze;I have the same issue. But adding guava dependency didn't help me


 
{code:java}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/sparkproject/connect/client/com/google/common/cache/CacheLoader 
at ... 
Caused by: java.lang.ClassNotFoundException: org.sparkproject.connect.client.com.google.common.cache.CacheLoader 
at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581) 
at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178) 
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522) 
... 2 more{code}
 ;;;","22/Sep/23 14:29;haldefaiz;For now, I unblocked myself by manually building spark connect

{{build/mvn -Pconnect -DskipTests clean package}}

{{and then running}}

{{mkdir connect-jars}}

{{./bin/spark-connect-scala-client-classpath | tr ':' '\n' | xargs -I{} cp {} connect-jars}}

 

{{Then, when starting your client application, have the connect-jars directory in your classpath. Not sure if this is the right way though}};;;","22/Sep/23 14:31;haldefaiz;to get past the error
`org/sparkproject/connect/client/com/google/common/cache/CacheLoader`
even after adding guava library, you need to copy their shading rules

```

    (assembly / assemblyShadeRules) := Seq(
      ShadeRule.rename(""io.grpc.**"" -> ""org.sparkproject.connect.client.io.grpc.@1"").inAll,
      ShadeRule.rename(""com.google.**"" -> ""org.sparkproject.connect.client.com.google.@1"").inAll,
      ShadeRule.rename(""io.netty.**"" -> ""org.sparkproject.connect.client.io.netty.@1"").inAll,
      ShadeRule.rename(""org.checkerframework.**"" -> ""org.sparkproject.connect.client.org.checkerframework.@1"").inAll,
      ShadeRule.rename(""javax.annotation.**"" -> ""org.sparkproject.connect.client.javax.annotation.@1"").inAll,
      ShadeRule.rename(""io.perfmark.**"" -> ""org.sparkproject.connect.client.io.perfmark.@1"").inAll,
      ShadeRule.rename(""org.codehaus.**"" -> ""org.sparkproject.connect.client.org.codehaus.@1"").inAll,
      ShadeRule.rename(""android.annotation.**"" -> ""org.sparkproject.connect.client.android.annotation.@1"").inAll
    ),

```;;;","25/Sep/23 09:56;boltonidze;Yes, thanks. After adding shading rules the error is gone. But anyway it's incorrect approach that I should add something additional libraries with shading rules for using the spark connect...;;;","02/Oct/23 17:04;hvanhovell;This has been fixed in SPARK-45371.

See [https://github.com/apache/spark/commit/e53abbbceaa2c41babaa23fe4c2f282f559b4c03];;;",,,,,,,,,,
Non-nullable schema is not effective in DF from JSON,SPARK-45254,13551536,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,michele_rastelli,michele_rastelli,21/Sep/23 13:41,21/Sep/23 13:41,30/Oct/23 17:26,,3.3.3,3.4.1,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"In Spark 3.3 and 3.4, when creating a DF with schema with non-nullable fields, the created DF ends up having schema with nullable fields.

 

 
{code:java}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{BooleanType, StructField, StructType}
object Foo extends App {
  val spark: SparkSession = SparkSession.builder()
    .appName(""foo"")
    .master(""local[*]"")
    .config(""spark.driver.host"", ""127.0.0.1"")
    .getOrCreate()
  val schema = StructType(Array(StructField(""a"", BooleanType, nullable = false)))
  import spark.implicits._
  val df = spark.read.schema(schema).json(Seq(
    """"""{""a"":null}"""""",
    """"""{""a"":true}"""""",
    """"""{""a"":false}"""""",
  ).toDS)
  df.collect()
    .map(_.toString())
    .foreach(println(_))
  schema.printTreeString()
  df.schema.printTreeString()
}
 
{code}
 

 

Produces:

 
{code:java}
[null]
[true]
[false]
root
 |-- a: boolean (nullable = false)
root
 |-- a: boolean (nullable = true)
{code}
 

 

 

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-21 13:41:58.0,,,,,,,,,,"0|z1ki8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the group of `ShiftLeft` and `ArraySize`,SPARK-45253,13551534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,podongfeng,podongfeng,podongfeng,21/Sep/23 13:11,22/Sep/23 03:56,30/Oct/23 17:26,22/Sep/23 03:51,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Documentation,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 22 03:51:48 UTC 2023,,,,,,,,,,"0|z1ki80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/23 03:51;gurwls223;Issue resolved by pull request 43033
[https://github.com/apache/spark/pull/43033];;;",,,,,,,,,,,,,,
`sbt doc` execution failed.,SPARK-45252,13551530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,21/Sep/23 12:52,22/Sep/23 13:39,30/Oct/23 17:26,21/Sep/23 23:06,3.3.4,3.4.2,3.5.1,4.0.0,,,,,,,,,,,,,,,,4.0.0,,,,Spark Core,,,,,0,pull-request-available,,,,"run 

 
{code:java}
build/sbt clean doc -Phadoop-3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pvolcano
{code}
 
{code:java}
[info] Main Scala API documentation successful.
[error] sbt.inc.Doc$JavadocGenerationFailed
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$cachedJavadoc$1(Doc.scala:51)
[error]         at sbt.inc.Doc$$anonfun$cachedJavadoc$2.run(Doc.scala:41)
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$prepare$1(Doc.scala:62)
[error]         at sbt.inc.Doc$$anonfun$prepare$5.run(Doc.scala:57)
[error]         at sbt.inc.Doc$.go$1(Doc.scala:73)
[error]         at sbt.inc.Doc$.$anonfun$cached$5(Doc.scala:82)
[error]         at sbt.inc.Doc$.$anonfun$cached$5$adapted(Doc.scala:81)
[error]         at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:220)
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$cached$1(Doc.scala:85)
[error]         at sbt.inc.Doc$$anonfun$cached$7.run(Doc.scala:68)
[error]         at sbt.Defaults$.$anonfun$docTaskSettings$4(Defaults.scala:2178)
[error]         at scala.Function1.$anonfun$compose$1(Function1.scala:49)
[error]         at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:63)
[error]         at sbt.std.Transform$$anon$4.work(Transform.scala:69)
[error]         at sbt.Execute.$anonfun$submit$2(Execute.scala:283)
[error]         at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:24)
[error]         at sbt.Execute.work(Execute.scala:292)
[error]         at sbt.Execute.$anonfun$submit$1(Execute.scala:283)
[error]         at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)
[error]         at sbt.CompletionService$$anon$2.call(CompletionService.scala:65)
[error]         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error]         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[error]         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[error]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[error]         at java.lang.Thread.run(Thread.java:750)
[error] sbt.inc.Doc$JavadocGenerationFailed
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$cachedJavadoc$1(Doc.scala:51)
[error]         at sbt.inc.Doc$$anonfun$cachedJavadoc$2.run(Doc.scala:41)
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$prepare$1(Doc.scala:62)
[error]         at sbt.inc.Doc$$anonfun$prepare$5.run(Doc.scala:57)
[error]         at sbt.inc.Doc$.go$1(Doc.scala:73)
[error]         at sbt.inc.Doc$.$anonfun$cached$5(Doc.scala:82)
[error]         at sbt.inc.Doc$.$anonfun$cached$5$adapted(Doc.scala:81)
[error]         at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:220)
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$cached$1(Doc.scala:85)
[error]         at sbt.inc.Doc$$anonfun$cached$7.run(Doc.scala:68)
[error]         at sbt.Defaults$.$anonfun$docTaskSettings$4(Defaults.scala:2178)
[error]         at scala.Function1.$anonfun$compose$1(Function1.scala:49)
[error]         at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:63)
[error]         at sbt.std.Transform$$anon$4.work(Transform.scala:69)
[error]         at sbt.Execute.$anonfun$submit$2(Execute.scala:283)
[error]         at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:24)
[error]         at sbt.Execute.work(Execute.scala:292)
[error]         at sbt.Execute.$anonfun$submit$1(Execute.scala:283)
[error]         at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)
[error]         at sbt.CompletionService$$anon$2.call(CompletionService.scala:65)
[error]         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error]         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[error]         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[error]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[error]         at java.lang.Thread.run(Thread.java:750)
[error] (network-yarn / Compile / doc) sbt.inc.Doc$JavadocGenerationFailed
[error] (network-shuffle / Compile / doc) sbt.inc.Doc$JavadocGenerationFailed
[error] Total time: 126 s (02:06), completed 2023-9-21 20:51:43
{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 21 23:06:44 UTC 2023,,,,,,,,,,"0|z1ki74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/23 23:06;dongjoon;Issue resolved by pull request 43032
[https://github.com/apache/spark/pull/43032];;;",,,,,,,,,,,,,,
Why isCheckpointed returns false while as checkpointing is eager by default?,SPARK-45249,13551508,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kondziolka9ld,kondziolka9ld,21/Sep/23 09:28,21/Sep/23 09:33,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Hi!

I consider why `isCheckpointed` method return `false` - could someone explain it? I would expect `true`.
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/
         
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.13)
Type in expressions to have them evaluated.
Type :help for more information.
scala> sc.setCheckpointDir(""file:///tmp/"")
scala> val df1 = Seq(1,2,3,4).toDF
df1: org.apache.spark.sql.DataFrame = [value: int]
scala> val df2 = df1.checkpoint()
df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: int]
scala> df2.rdd.isCheckpointed
res2: Boolean = false  // why false?{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-21 09:28:29.0,,,,,,,,,,"0|z1ki28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encourage using latest jinja2 other than documentation build,SPARK-45246,13551484,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,itholic,itholic,21/Sep/23 05:01,22/Sep/23 03:39,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Build,Pandas API on Spark,,,,0,pull-request-available,,,,"Pandas 2.0.0 requires latest jinja2, so we need latest jinja2 for Pandas API on Spark",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-21 05:01:35.0,,,,,,,,,,"0|z1khww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonWorkerFactory create gets stuck in case of worker failures,SPARK-45245,13551480,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,rangadi,rangadi,21/Sep/23 04:00,30/Oct/23 14:04,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,pull-request-available,,,,"`createSimpleWorker()` method in `PythonWorkerFactory` does not return if the worker fails to connect back to the server. 



This is because it calls accept() without a timeout. If the worker does not connect back, accept() waits forever. 

See PR for the fix. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-21 04:00:30.0,,,,,,,,,,"0|z1khw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RADIX sort is not stable and can produce different results for first/collect_list aggs,SPARK-45243,13551449,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,revans2,revans2,20/Sep/23 17:53,20/Sep/23 17:53,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"I just set the version as 3.3.0. I think it is all versions, but I have not tested it to be sure.

I simple query like
{code:java}
spark.read.parquet(""/tmp/TEST"").groupBy(""a"").agg(first(col(""b""))).show()
{code}
can produce different results depending on if RADIX sort is enabled or not. In this case ""a"" is a LONG and ""b"" is a STRING. The STRING forces the aggregation to be a sort based aggregation and b being a long causes the sort to default to a RADIX sort.

In practice first, last, and collect_list are not deterministic because the order that a shuffle arrives to a task is a race so the order that the rows arrive after the second stage of the aggregation is totally up in the air. first and last might just be called pick_one. But in this case the data is small enough that there is a single partition so it should be deterministic. But it is not.
{code:java}
scala> spark.read.parquet(""/tmp/TEST"").show(100, false)
+--------------------+----+
|a                   |b   |
+--------------------+----+
|4080731634774120135 |HH  |
|-7996385019137306797|AA  |
|4891386765580059730 |BI  |
|-2578026341565473682|DE  |
|-7264635988756013877|CH  |
|5656737394922367923 |AG  |
|-6183011807271780569|BG  |
|109827782918242415  |CD  |
|-4058328039203991995|FA  |
|null                |FG  |
|4080731634774120135 |ID  |
|-7996385019137306797|GG  |
|4891386765580059730 |AC  |
|-2578026341565473682|null|
|-7264635988756013877|HF  |
|5656737394922367923 |II  |
|-6183011807271780569|FC  |
|109827782918242415  |DI  |
|-4058328039203991995|IH  |
|null                |FE  |
|4080731634774120135 |HA  |
|-7996385019137306797|ID  |
|4891386765580059730 |GI  |
|-2578026341565473682|GB  |
|-7264635988756013877|EC  |
|5656737394922367923 |DA  |
|-6183011807271780569|BB  |
|109827782918242415  |AE  |
|-4058328039203991995|FE  |
|null                |AE  |
|4080731634774120135 |BC  |
|-7996385019137306797|HF  |
+--------------------+----+

scala> spark.read.parquet(""/tmp/TEST"").groupBy(""a"").agg(first(col(""b""))).show()
+--------------------+--------+
|                   a|first(b)|
+--------------------+--------+
|                null|      FG|
|-7996385019137306797|      GG|
|-7264635988756013877|      CH|
|-6183011807271780569|      BG|
|-4058328039203991995|      FA|
|-2578026341565473682|      DE|
|  109827782918242415|      CD|
| 4080731634774120135|      HH|
| 4891386765580059730|      AC|
| 5656737394922367923|      AG|
+--------------------+--------+

scala> spark.conf.set(""spark.sql.sort.enableRadixSort"", false)

scala> spark.read.parquet(""/tmp/TEST"").groupBy(""a"").agg(first(col(""b""))).show()
+--------------------+--------+
|                   a|first(b)|
+--------------------+--------+
|                null|      FG|
|-7996385019137306797|      AA|
|-7264635988756013877|      CH|
|-6183011807271780569|      BG|
|-4058328039203991995|      FA|
|-2578026341565473682|      DE|
|  109827782918242415|      CD|
| 4080731634774120135|      HH|
| 4891386765580059730|      BI|
| 5656737394922367923|      AG|
+--------------------+--------+
{code}
Here the values for -7996385019137306797 changed from GG with radix sort on to AA with it off.  AA is technially correct, because it appears on line 2 of the input where as GG shows up on line 12. 

 

I honestly don't know if Spark expects the sort to be stable or not. Looking at the code SortExec and UnsafeExternalSorter do not make any claims about being stable and https://issues.apache.org/jira/browse/SPARK-23973 indicates that sort is not stable, so this might just works as designed.  I just find it odd that in most cases it is stable, so I guess that was just by accident.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-20 17:53:20.0,,,,,,,,,,"0|z1khp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the default value of `spark.history.store.hybridStore.diskBackend` in `monitoring.md`,SPARK-45237,13551399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,20/Sep/23 12:36,20/Sep/23 15:47,30/Oct/23 17:26,20/Sep/23 15:47,3.4.2,3.5.1,4.0.0,,,,,,,,,,,,,,,,,3.4.2,3.5.1,4.0.0,,Documentation,,,,,0,pull-request-available,,,,"SPARK-42277 change to use RocksDB for spark.history.store.hybridStore.diskBackend by default, but in `monitoring.md`, the default value is still set as LEVELDB.
 
 
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 20 15:47:24 UTC 2023,,,,,,,,,,"0|z1khe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/23 15:47;dongjoon;Issue resolved by pull request 43015
[https://github.com/apache/spark/pull/43015];;;",,,,,,,,,,,,,,
The `sql()` method doesn't support map and array parameters in Python connect client,SPARK-45235,13551383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,20/Sep/23 11:18,21/Sep/23 06:05,30/Oct/23 17:26,21/Sep/23 06:05,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,"At the moment, the Python connect client fails when we pass a map or an array as a parameter to the `sql()` method. This ticket aims to fix the behaviour to reach feature parity with the regular PySpark.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 21 06:05:42 UTC 2023,,,,,,,,,,"0|z1khag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/23 06:05;maxgekk;Issue resolved by pull request 43014
[https://github.com/apache/spark/pull/43014];;;",,,,,,,,,,,,,,
Update `test_axis_on_dataframe` when Pandas regression is fixed,SPARK-45228,13551313,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,itholic,itholic,20/Sep/23 03:39,20/Sep/23 03:40,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Pandas API on Spark,Tests,,,,0,,,,,"We manually cast the datatype when testing `test_axis_on_dataframe` from [https://github.com/apache/spark/pull/43002,|https://github.com/apache/spark/pull/43002.] but it's not a normal way to test properly.

After the regression of Pandas is resolved, we should return the test back to normal way.

See Pandas regression: https://github.com/pandas-dev/pandas/issues/55194",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-20 03:39:21.0,,,,,,,,,,"0|z1kguw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix non-deterministic seeded Dataset APIs,SPARK-45216,13551225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,19/Sep/23 12:00,21/Sep/23 07:32,30/Oct/23 17:26,21/Sep/23 00:36,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Connect,SQL,,,,0,pull-request-available,,,,"If we run the following example the result is the expected equal 2 columns:

{noformat}
val c = rand()
df.select(c, c)

+--------------------------+--------------------------+
|rand(-4522010140232537566)|rand(-4522010140232537566)|
+--------------------------+--------------------------+
|        0.4520819282997137|        0.4520819282997137|
+--------------------------+--------------------------+
{noformat}

 
But if we run use other similar APIs their result is incorrect:

{noformat}
val r1 = random()
val r2 = uuid()
val r3 = shuffle(col(""x""))
val x = df.select(r1, r1, r2, r2, r3, r3)

+------------------+------------------+--------------------+--------------------+----------+----------+
|            rand()|            rand()|              uuid()|              uuid()|shuffle(x)|shuffle(x)|
+------------------+------------------+--------------------+--------------------+----------+----------+
|0.7407604956381952|0.7957319451135009|e55bc4b0-74e6-4b0...|a587163b-d06b-4bb...| [1, 2, 3]| [2, 1, 3]|
+------------------+------------------+--------------------+--------------------+----------+----------+
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 21 00:36:38 UTC 2023,,,,,,,,,,"0|z1kgbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/23 00:36;gurwls223;Issue resolved by pull request 42997
[https://github.com/apache/spark/pull/42997];;;",,,,,,,,,,,,,,
Columns should not be visible for filter after projection,SPARK-45214,13551202,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jwozniak,jwozniak,19/Sep/23 08:48,19/Sep/23 08:48,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Columns are visible for filtering but not for select after projection. Moreover the behaviour is different when after a union (in this case columns are not visible for filtering anymore).
{code:java}
from pyspark.sql import SparkSession
from pyspark.sql.types import *

data1 = []
data2 = []

for i in range(2): 
    data1.append( (1,i) )
    data2.append( (2,i+10))



schema1 = StructType([
        StructField('f1', IntegerType(), True),
         StructField('f2', IntegerType(), True)
])



df1 = spark.createDataFrame(data1, schema1)
df2 = spark.createDataFrame(data2, schema1)


df1.show()
df2.show()


#works, f1 is available for filter (though it should not be)
df1.select('f2').where('f1=1').show()

#error, f1 is not available
df1.select('f2').union(df2.select('f2')).where('f1=1').show()

#this is semantically not symmetric -> incorrect. 

{code}

This is similar to this one: https://issues.apache.org/jira/browse/SPARK-30421
Perhaps it gives a bit more argumentation why this should be fixed as it is logically not correct. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-19 08:48:15.0,,,,,,,,,,"0|z1kg68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Install independent Python linter dependencies for branch-3.5,SPARK-45212,13551176,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,19/Sep/23 04:12,19/Sep/23 07:30,30/Oct/23 17:26,19/Sep/23 07:30,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Project Infra,,,,,0,,,,,"Python linter failed in branch -3.5 daily test:
 * [https://github.com/apache/spark/actions/runs/6221638911/job/16884068430]

{code:java}
Run PYTHON_EXECUTABLE=python3.9 ./dev/lint-python
12starting python compilation test...
13python compilation succeeded.
14
15starting black test...
16black checks failed:
17Oh no! 💥 💔 💥 The required version `22.6.0` does not match the running version `23.9.1`!
18Please run 'dev/reformat-python' script.
191
20Error: Process completed with exit code 1. {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 19 07:30:35 UTC 2023,,,,,,,,,,"0|z1kg0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/23 07:30;dongjoon;Issue resolved by pull request 42990
[https://github.com/apache/spark/pull/42990];;;",,,,,,,,,,,,,,
Scala 2.13 daily  test failed,SPARK-45211,13551172,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,19/Sep/23 03:16,19/Sep/23 08:57,30/Oct/23 17:26,19/Sep/23 08:57,3.5.1,4.0.0,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Connect,,,,,0,,,,,"* [https://github.com/apache/spark/actions/runs/6215331575/job/16868131377]

{code:java}
[info] - abandoned query gets INVALID_HANDLE.OPERATION_ABANDONED error *** FAILED *** (157 milliseconds)
19991[info]   Expected exception org.apache.spark.SparkException to be thrown, but java.lang.StackOverflowError was thrown (ReattachableExecuteSuite.scala:172)
19992[info]   org.scalatest.exceptions.TestFailedException:
19993[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
19994[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
19995[info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1564)
19996[info]   at org.scalatest.Assertions.intercept(Assertions.scala:756)
19997[info]   at org.scalatest.Assertions.intercept$(Assertions.scala:746)
19998[info]   at org.scalatest.funsuite.AnyFunSuite.intercept(AnyFunSuite.scala:1564)
19999[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$18(ReattachableExecuteSuite.scala:172)
20000[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$18$adapted(ReattachableExecuteSuite.scala:168)
20001[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withCustomBlockingStub(SparkConnectServerTest.scala:222)
20002[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withCustomBlockingStub$(SparkConnectServerTest.scala:216)
20003[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.withCustomBlockingStub(ReattachableExecuteSuite.scala:30)
20004[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$16(ReattachableExecuteSuite.scala:168)
20005[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$16$adapted(ReattachableExecuteSuite.scala:151)
20006[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withClient(SparkConnectServerTest.scala:199)
20007[info]   at org.apache.spark.sql.connect.SparkConnectServerTest.withClient$(SparkConnectServerTest.scala:191)
20008[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.withClient(ReattachableExecuteSuite.scala:30)
20009[info]   at org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.$anonfun$new$15(ReattachableExecuteSuite.scala:151)
20010[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
20011[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
20012[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
20013[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
20014[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
20015[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
20016[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
20017[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
20018[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
20019[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
20020[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
20021[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
20022[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
20023[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
20024[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
20025[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
20026[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
20027[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
20028[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
20029[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
20030[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
20031[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
20032[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
20033[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
20034[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
20035[info]   at scala.collection.immutable.List.foreach(List.scala:333)
20036[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
20037[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
20038[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
20039[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
20040[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
20041[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
20042[info]   at org.scalatest.Suite.run(Suite.scala:1114)
20043[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
20044[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
20045[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
20046[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
20047[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
20048[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
20049[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
20050[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
20051[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
20052[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
20053[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
20054[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
20055[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
20056[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)
20057[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
20058[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
20059[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
20060[info]   at java.lang.Thread.run(Thread.java:750)
20061[info]   Cause: java.lang.StackOverflowError:
20062[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20063[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20064[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20065[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20066[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20067[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20068[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20069[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20070[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20071[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20072[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20073[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20074[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20075[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20076[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20077[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20078[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20079[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20080[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20081[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20082[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20083[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20084[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20085[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36)
20086[info]   at org.apache.spark.sql.connect.client.WrappedCloseableIterator.hasNext(CloseableIterator.scala:36) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 19 08:57:12 UTC 2023,,,,,,,,,,"0|z1kfzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/23 08:57;LuciferYang;Issue resolved by pull request 42981
[https://github.com/apache/spark/pull/42981];;;",,,,,,,,,,,,,,
Switch languages consistently across docs for all code snippets (Spark 3.4 and below),SPARK-45210,13551170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,19/Sep/23 02:56,19/Sep/23 05:52,30/Oct/23 17:26,19/Sep/23 05:52,3.1.3,3.2.4,3.3.2,3.4.1,,,,,,,,,,,,,,,,3.3.4,3.4.2,,,Documentation,,,,,0,,,,,Similar with SPARK-44820 but needs different change as they were refactored at https://github.com/apache/spark/commit/12b9b771c7ad75cb90c0a51cd2d0581dd3c719e2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 19 05:52:49 UTC 2023,,,,,,,,,,"0|z1kfz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/23 05:52;gurwls223;Issue resolved by pull request 42989
[https://github.com/apache/spark/pull/42989];;;",,,,,,,,,,,,,,
Website doesn't have horizontal scrollbar,SPARK-45208,13551159,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dcoliversun,dcoliversun,19/Sep/23 01:15,20/Sep/23 00:12,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,,,,,"I find a recent issue with the official Spark documentation on the website. Specifically, the Kubernetes configuration lists on the right-hand side are not visible and doc doesn't have a horizontal scrollbar.

 
- [https://spark.apache.org/docs/3.5.0/running-on-kubernetes.html#configuration]
- [https://spark.apache.org/docs/3.4.1/running-on-kubernetes.html#configuration]

Wide tables are broken in the same way.

- https://spark.apache.org/docs/latest/spark-standalone.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-19 01:15:29.0,,,,,,,,,,"0|z1kfwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Since version 3.2.0, Spark SQL has taken longer to execute ""show paritions"",probably because of changes introduced by  SPARK-35278",SPARK-45205,13551114,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yorksity,yorksity,yorksity,18/Sep/23 16:58,10/Oct/23 06:38,30/Oct/23 17:26,10/Oct/23 06:37,3.2.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,SQL,,,,,0,pull-request-available,,,,"After SPARK-35378 was changed, I noticed that the execution of statements such as ‘show parititions test' became slower.

The change point is that the execution process changes from ExecutedCommandEnec to CommandResultExec, but ExecutedCommandExec originally implemented the following method

override def executeToIterator(): Iterator[InternalRow] = sideEffectResult.iterator

CommandResultExec is not rewritten, so when the hasNext method is executed, a job process is created, resulting in increased time-consuming",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 10 06:37:22 UTC 2023,,,,,,,,,,"0|z1kfmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/23 06:37;cloud_fan;Issue resolved by pull request 43270
[https://github.com/apache/spark/pull/43270];;;",,,,,,,,,,,,,,
lint-js only covers core module,SPARK-45202,13551096,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,18/Sep/23 14:44,19/Sep/23 03:06,30/Oct/23 17:26,19/Sep/23 03:06,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 19 03:06:02 UTC 2023,,,,,,,,,,"0|z1kfio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/23 03:06;yao;Issue resolved by pull request 42982
[https://github.com/apache/spark/pull/42982];;;",,,,,,,,,,,,,,
NoClassDefFoundError: InternalFutureFailureAccess when compiling Spark 3.5.0,SPARK-45201,13551072,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,sdaberdaku,sdaberdaku,18/Sep/23 12:32,11/Oct/23 09:58,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Connect,,,,,0,,,,,"I am trying to compile Spark 3.5.0 and make a distribution that supports Spark Connect and Kubernetes. The compilation seems to complete correctly, but when I try to run the Spark Connect server on kubernetes I get a ""NoClassDefFoundError"" as follows:
{code:java}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/sparkproject/guava/util/concurrent/internal/InternalFutureFailureAccess
    at java.base/java.lang.ClassLoader.defineClass1(Native Method)
    at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)
    at java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)
    at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:862)
    at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:760)
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:681)
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:639)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
    at java.base/java.lang.ClassLoader.defineClass1(Native Method)
    at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)
    at java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)
    at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:862)
    at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:760)
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:681)
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:639)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
    at java.base/java.lang.ClassLoader.defineClass1(Native Method)
    at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)
    at java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)
    at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:862)
    at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:760)
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:681)
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:639)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
    at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.<init>(LocalCache.java:3511)
    at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.<init>(LocalCache.java:3515)
    at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2168)
    at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2079)
    at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4011)
    at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4034)
    at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:5010)
    at org.apache.spark.storage.BlockManagerId$.getCachedBlockManagerId(BlockManagerId.scala:146)
    at org.apache.spark.storage.BlockManagerId$.apply(BlockManagerId.scala:127)
    at org.apache.spark.storage.BlockManager.initialize(BlockManager.scala:536)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:625)
    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
    at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
    at scala.Option.getOrElse(Option.scala:189)
    at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
    at org.apache.spark.sql.connect.service.SparkConnectServer$.main(SparkConnectServer.scala:34)
    at org.apache.spark.sql.connect.service.SparkConnectServer.main(SparkConnectServer.scala)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.sparkproject.guava.util.concurrent.internal.InternalFutureFailureAccess
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
    ... 56 more{code}
My build command is as follows:
{code:java}
export MAVEN_OPTS=""-Xss256m -Xmx8g -XX:ReservedCodeCacheSize=2g""
./dev/make-distribution.sh --name spark --pip -Pscala-2.12 -Pconnect -Pkubernetes -Phive -Phive-thriftserver -Phadoop-3 -Dhadoop.version=""3.3.4"" -Dhive.version=""2.3.9"" -Dhive23.version=""2.3.9"" -Dhive.version.short=""2.3""{code}
I am building Spark using Debian Bookworm, with Java 8 8u382 and Maven 3.8.8.
I get the same error even when I omitt the -Pconnect profile from Maven, and simply add the ""org.apache.spark:spark-connect_2.12:3.5.0"" jar with the appropriate spark config. On the other hand, if I download the pre-built spark package, and add the spark-connect jar, this error does not appear. 
What could I be possibly missing in my build environment? I have omitted the  yarn, mesos and sparkr profiles (which are used in the distributed built) on purpose, but I do not see how these affect spark connect.

Any help will be appreciated!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/23 16:27;sdaberdaku;Dockerfile;https://issues.apache.org/jira/secure/attachment/13063000/Dockerfile","21/Sep/23 16:57;sdaberdaku;spark-3.5.0.patch;https://issues.apache.org/jira/secure/attachment/13063101/spark-3.5.0.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 11 09:43:30 UTC 2023,,,,,,,,,,"0|z1kfdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/23 12:44;sdaberdaku;I also tried building and then running Spark with Java17 instead of Java8 with no changes.;;;","18/Sep/23 16:35;sdaberdaku;I also added a Dockerfile which builds Spark 3.5.0 using the eclipse-temurin:11-jdk-focal image (Ubuntu based), I still get the same error. 

My container command is as follows:
{code:java}
command:
  - /bin/bash
  - '-c'      
args:
  - >-
    /opt/entrypoint.sh
    /opt/spark/sbin/start-connect-server.sh
    --properties-file /opt/spark/spark-properties.conf{code}
This whole configuration works without issues with Spark 3.4.1. 

 ;;;","18/Sep/23 16:46;sdaberdaku;Out of completeness, this is what my spark-properties.conf looks like (i've censored some info):
{code:java}
spark.connect.grpc.binding.port 15002
spark.driver.host 10.0.0.246
spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 0
spark.dynamicAllocation.maxExecutors 5
spark.dynamicAllocation.executorAllocationRatio 0.25
spark.dynamicAllocation.schedulerBacklogTimeout 1s
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 120s
spark.dynamicAllocation.executorIdleTimeout     300s
spark.dynamicAllocation.cachedExecutorIdleTimeout 1800s
spark.jars.ivy /tmp/.ivy
spark.kubernetes.allocation.driver.readinessTimeout 60s
spark.kubernetes.executor.container.image 0123456789.dkr.ecr.some-aws-region.amazonaws.com/spark-connect-3.5.0:v1.0.18
spark.kubernetes.container.image.pullPolicy Always
spark.kubernetes.driver.pod.name spark-connect-0
spark.kubernetes.executor.podTemplateFile /opt/spark/executor-pod-template.yaml
spark.kubernetes.executor.request.cores 12000m
spark.driver.cores 1
spark.executor.cores 64
spark.kubernetes.namespace spark-connect
spark.master k8s://https://kubernetes.default.svc.cluster.local:443
spark.ui.port 4040
spark.executor.memory 40000m
spark.executor.memoryOverhead 8000m
spark.driver.memory 10240m
spark.driver.memoryOverhead 2048m
spark.executor.extraJavaOptions -XX:+ExitOnOutOfMemoryError -XX:+UseCompressedOops -XX:+UseG1GC
spark.driver.extraJavaOptions -XX:+ExitOnOutOfMemoryError -XX:+UseCompressedOops -XX:+UseG1GC
spark.sql.parquet.datetimeRebaseModeInWrite CORRECTED
spark.driver.extraClassPath /opt/spark/jars/*
spark.driver.extraLibraryPath /opt/hadoop/lib/native
spark.executor.extraClassPath /opt/spark/jars/*
spark.executor.extraLibraryPath /opt/hadoop/lib/native
spark.sql.hive.metastore.jars builtin
spark.hadoop.aws.region some-aws-region
spark.sql.catalogImplementation hive
spark.sql.execution.arrow.pyspark.enabled true
spark.sql.execution.arrow.pyspark.fallback.enabled true
spark.eventLog.enabled false
spark.sql.extensions io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.delta.logStore.s3a.impl io.delta.storage.S3DynamoDBLogStore
spark.io.delta.storage.S3DynamoDBLogStore.ddb.tableName dynamodb-table-delta-table-lock
spark.io.delta.storage.S3DynamoDBLogStore.ddb.region some-aws-region
spark.databricks.delta.replaceWhere.constraintCheck.enabled false
spark.databricks.delta.replaceWhere.dataColumns.enabled true
spark.databricks.delta.schema.autoMerge.enabled false
spark.databricks.delta.merge.repartitionBeforeWrite.enabled true
spark.databricks.delta.optimize.repartition.enabled true
spark.databricks.delta.checkpoint.partSize 10
spark.databricks.delta.properties.defaults.dataSkippingNumIndexedCols -1
spark.kubernetes.authenticate.driver.serviceAccountName spark-connect
spark.kubernetes.authenticate.executor.serviceAccountName spark-connect
spark.kubernetes.executor.annotation.eks.amazonaws.com/role-arn arn:aws:iam::0123456789:role/spark-connect-irsa
spark.kubernetes.authenticate.submission.caCertFile /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
spark.kubernetes.authenticate.submission.oauthTokenFile /var/run/secrets/kubernetes.io/serviceaccount/token
spark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.WebIdentityTokenCredentialsProvider
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3.impl org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.fast.upload true
spark.hadoop.fs.s3a.experimental.input.fadvise random
spark.hive.imetastoreclient.factory.class com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory
spark.aws.glue.cache.table.enable true
spark.aws.glue.cache.table.size 1000
spark.aws.glue.cache.table.ttl-mins 30
spark.aws.glue.cache.db.enable true
spark.aws.glue.cache.db.size 1000
spark.aws.glue.cache.db.ttl-mins 30
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.hadoop.mapreduce.input.fileinputformat.list-status.num-threads 64
spark.sql.sources.parallelPartitionDiscovery.threshold 256
spark.connect.grpc.maxInboundMessageSize 1073741824
spark.connect.grpc.arrow.maxBatchSize 64m
spark.sql.catalog.spark_catalog.defaultDatabase experimental {code};;;","21/Sep/23 15:55;sdaberdaku;After spending hours analyzing the project pom files, I discovered two things.

First, the shade plugin is relocating the guava/failureaccess package twice in the connect jars (once by the module shade plugin, once by the base project plugin). I created a simple patch to prevent the relocation of failureacces by the base plugin. I am adding the patch file [^spark-3.5.0.patch] to this Jira issue, I do not have time to create a pull request, you can apply the patch by navigating inside the source folder and running:
{code:java}
patch -p1 <spark-3.5.0.patch {code}
Second, the spark-connect-common jar produced by make-distribution is redundant and was the cause of the class loading issues. Removing it resolved all these issues I had. ;;;","09/Oct/23 15:03;nsuke;We've experienced the same issue and resolved it in the same way (by removing the connect common JAR and applying the patch).

For some reason the issue did not always reproduce. Using the same container image on a same Kubernetes cluster, it seems that it only happens on certain nodes. I suspect that it is because of the use of wildcard in Spark classpath that JVM probably resolves to the actual filepaths using system call without any guaranteed ordering in the result (just a guess from the behavior).;;;","09/Oct/23 16:11;sdaberdaku;Hello [~nsuke], 
I am happy my patch was useful to you!
The JVM class loader inconsistency seems to be a very plausible cause, I experienced something similar with my docker image working locally (with docker) but not on my EKS cluster (to be clear, same docker image).;;;","11/Oct/23 09:43;xieshuaihu;[~sdaberdaku] I alse have the same issue. I solved it by putting spark-connect.jar in spark-submit --jars, instead of SPARK_HOME/jars;;;",,,,,,,,
Spark 3.4.0 always using default log4j profile,SPARK-45200,13551047,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jd-gen,jd-gen,18/Sep/23 10:13,10/Oct/23 07:56,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,pull-request-available,,,,"I've been using Spark core 3.2.2 and was upgrading to 3.4.0

On execution of my Java code with the 3.4.0,  it generates some extra set of logs but don't face this issue with 3.2.2.

 

I noticed that logs says _Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties._

 

Is this a bug or do we have a  a configuration to disable the using of default log4j profile?

I didn't see anything in the documentation

 

+*Output:*+
{code:java}
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
23/09/18 20:05:08 INFO SparkContext: Running Spark version 3.4.0
23/09/18 20:05:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/09/18 20:05:08 INFO ResourceUtils: ==============================================================
23/09/18 20:05:08 INFO ResourceUtils: No custom resources configured for spark.driver.
23/09/18 20:05:08 INFO ResourceUtils: ==============================================================
23/09/18 20:05:08 INFO SparkContext: Submitted application: XYZ
23/09/18 20:05:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/09/18 20:05:08 INFO ResourceProfile: Limiting resource is cpu
23/09/18 20:05:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/09/18 20:05:08 INFO SecurityManager: Changing view acls to: jd
23/09/18 20:05:08 INFO SecurityManager: Changing modify acls to: jd
23/09/18 20:05:08 INFO SecurityManager: Changing view acls groups to: 
23/09/18 20:05:08 INFO SecurityManager: Changing modify acls groups to: 
23/09/18 20:05:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: jd; groups with view permissions: EMPTY; users with modify permissions: jd; groups with modify permissions: EMPTY
23/09/18 20:05:08 INFO Utils: Successfully started service 'sparkDriver' on port 39155.
23/09/18 20:05:08 INFO SparkEnv: Registering MapOutputTracker
23/09/18 20:05:08 INFO SparkEnv: Registering BlockManagerMaster
23/09/18 20:05:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/09/18 20:05:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/09/18 20:05:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/09/18 20:05:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-226d599c-1511-4fae-b0e7-aae81b684012
23/09/18 20:05:08 INFO MemoryStore: MemoryStore started with capacity 2004.6 MiB
23/09/18 20:05:08 INFO SparkEnv: Registering OutputCommitCoordinator
23/09/18 20:05:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/09/18 20:05:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/09/18 20:05:09 INFO Executor: Starting executor ID driver on host jd
23/09/18 20:05:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/09/18 20:05:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32819.
23/09/18 20:05:09 INFO NettyBlockTransferService: Server created on jd:32819
23/09/18 20:05:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/09/18 20:05:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, jd, 32819, None)
23/09/18 20:05:09 INFO BlockManagerMasterEndpoint: Registering block manager jd:32819 with 2004.6 MiB RAM, BlockManagerId(driver, jd, 32819, None)
23/09/18 20:05:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, jd, 32819, None)
23/09/18 20:05:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, jd, 32819, None)
{code}
 

 

 

I'm using Spark core dependency in one of my Jars, the jar code contains the following:

 

*+Code:+*
{code:java}
SparkSession
          .builder()
          .appName(""XYZ"")
          .config(""spark.master"", ""local"")
          .config(""spark.io.compression.codec"", ""snappy"")
          .config(""spark.io.compression.snappy.blockSize"", blockSize)
          .getOrCreate(); {code}
 

When it calls *getOrCreate()* method, I get the above stated extra logs.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,,,2023-09-18 10:13:33.0,,,,,,,,,,"0|z1kf7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
problem using broadcast join with parquet/iceberg tables,SPARK-45198,13551034,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,,,tonyuwarov,tonyuwarov,18/Sep/23 08:38,18/Sep/23 08:45,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,Build,,,,,0,,,,,"We have 2 Parquet tables: load_test_full_warehouse.gen_document_type and load_test_full_warehouse.generation_document_part.
Trying to make a left join of these two tables onto each other gives a strange result. In the case where on the left side of the join we use a large table load_test_full_warehouse.generation_document_part, the optimizer uses a broadcast join.
However, in the case when on the left in the join we use a small reference table, the optimizer chooses to execute the query using the merge sort. Although it would seem that the small table on the left in a left join should initiate a broadcast join.
  An attempt to use hints and collect statistics did not yield results. The following queries were used:

spark.sql(f""""""create table iceberg_warehouse.t1 using iceberg 
              as SELECT /*+ BROADCAST(doc_tp) */
                doc.DOCUMENT_DATE
                , doc_tp.NAME as DOCUMENT_TYPE
                , COUNT(*) as DOC_QTY
              FROM load_test_full_warehouse.generation_document_part doc
              LEFT JOIN load_test_full_warehouse.gen_document_type doc_tp ON doc.DOCUMENT_TYPE_ID_INT = doc_tp.DOCUMENT_TYPE_ID_INT
              GROUP BY doc.DOCUMENT_DATE, doc_tp.NAME"""""")

== Physical Plan ==
AtomicCreateTableAsSelect (25)
+- AdaptiveSparkPlan (24)
   +- == Final Plan ==
      * HashAggregate (15)
      +- AQEShuffleRead (14)
         +- ShuffleQueryStage (13), Statistics(sizeInBytes=16.7 MiB, rowCount=3.12E+5)
            +- Exchange (12)
               +- * HashAggregate (11)
                  +- * Project (10)
                     +- * BroadcastHashJoin LeftOuter BuildRight (9)
                        :- * Project (3)
                        :  +- * ColumnarToRow (2)
                        :     +- Scan parquet spark_catalog.load_test_full_warehouse.generation_document_part (1)
                        +- BroadcastQueryStage (8), Statistics(sizeInBytes=1031.8 KiB, rowCount=1.00E+3)
                           +- BroadcastExchange (7)
                              +- * Filter (6)
                                 +- * ColumnarToRow (5)
                                    +- Scan parquet spark_catalog.load_test_full_warehouse.gen_document_type (4)
   +- == Initial Plan ==
      HashAggregate (23)
      +- Exchange (22)
         +- HashAggregate (21)
            +- Project (20)
               +- BroadcastHashJoin LeftOuter BuildRight (19)
                  :- Project (16)
                  :  +- Scan parquet spark_catalog.load_test_full_warehouse.generation_document_part (1)
                  +- BroadcastExchange (18)
                     +- Filter (17)
                        +- Scan parquet spark_catalog.load_test_full_warehouse.gen_document_type (4)

 

spark.sql(f""""""create table iceberg_warehouse.t2 using iceberg 
              as SELECT /*+ BROADCAST(doc_tp) */
                doc.DOCUMENT_DATE
                , doc_tp.NAME as DOCUMENT_TYPE
                , COUNT(*) as DOC_QTY
              FROM load_test_full_warehouse.gen_document_type doc_tp
              LEFT JOIN load_test_full_warehouse.generation_document_part doc ON doc.DOCUMENT_TYPE_ID_INT = doc_tp.DOCUMENT_TYPE_ID_INT
              GROUP BY doc.DOCUMENT_DATE, doc_tp.NAME"""""")

== Physical Plan ==
AtomicCreateTableAsSelect (34)
+- AdaptiveSparkPlan (33)
   +- == Final Plan ==
      * HashAggregate (21)
      +- AQEShuffleRead (20)
         +- ShuffleQueryStage (19), Statistics(sizeInBytes=1695.3 KiB, rowCount=3.10E+4)
            +- Exchange (18)
               +- * HashAggregate (17)
                  +- * Project (16)
                     +- * SortMergeJoin LeftOuter (15)
                        :- * Sort (6)
                        :  +- AQEShuffleRead (5)
                        :     +- ShuffleQueryStage (4), Statistics(sizeInBytes=46.9 KiB, rowCount=1.00E+3)
                        :        +- Exchange (3)
                        :           +- * ColumnarToRow (2)
                        :              +- Scan parquet spark_catalog.load_test_full_warehouse.gen_document_type (1)
                        +- * Sort (14)
                           +- AQEShuffleRead (13)
                              +- ShuffleQueryStage (12), Statistics(sizeInBytes=234.7 GiB, rowCount=1.05E+10)
                                 +- Exchange (11)
                                    +- * Project (10)
                                       +- * Filter (9)
                                          +- * ColumnarToRow (8)
                                             +- Scan parquet spark_catalog.load_test_full_warehouse.generation_document_part (7)
   +- == Initial Plan ==
      HashAggregate (32)
      +- Exchange (31)
         +- HashAggregate (30)
            +- Project (29)
               +- SortMergeJoin LeftOuter (28)
                  :- Sort (23)
                  :  +- Exchange (22)
                  :     +- Scan parquet spark_catalog.load_test_full_warehouse.gen_document_type (1)
                  +- Sort (27)
                     +- Exchange (26)
                        +- Project (25)
                           +- Filter (24)
                              +- Scan parquet spark_catalog.load_test_full_warehouse.generation_document_part (7)",,2203200,2203200,,0%,2203200,2203200,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/23 08:45;tonyuwarov;T1-Details-for-Query.png;https://issues.apache.org/jira/secure/attachment/13062984/T1-Details-for-Query.png","18/Sep/23 08:45;tonyuwarov;T2-Details-for-Query.png;https://issues.apache.org/jira/secure/attachment/13062983/T2-Details-for-Query.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-18 08:38:06.0,,,,,,,,,,"0|z1kf4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Parquet reads fail with ""RuntimeException: Unable to create Parquet converter for data type ""timestamp_ntz"" due to incorrect schema inference",SPARK-45194,13550996,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ivan.sadikov,ivan.sadikov,18/Sep/23 04:59,18/Sep/23 05:03,30/Oct/23 17:26,,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"I found that Parquet reads could fail due to incorrect schema inference with two conflicting types exist in files. This is caused by the fact that schema inference only considers one file by default which could contain different types than what in other file.

We have {{spark.sql.parquet.mergeSchema}} is set to `false` by default. This causes schema inference to pick a file (depending on the order the file system returns files) and infer schema based on that file. However, if you have conflicting types or a smaller/narrower type is selected, instead of failing during schema inference, an exception is thrown during the subsequent read.

In this case, we infer schema based on the file with TIMESTAMP_NTZ and fail to read the file that contains TIMESTAMP_LTZ:
{code:java}
[info]   Cause: java.lang.RuntimeException: Unable to create Parquet converter for data type ""timestamp_ntz"" whose Parquet type is int64 time(TIMESTAMP(MILLIS,true))
[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.convertErrorForTimestampNTZ(ParquetVectorUpdaterFactory.java:209)
[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.validateTimestampType(ParquetVectorUpdaterFactory.java:203)
[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:121)
[info]   at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175){code}
Note that if the file with TIMESTAMP_LTZ is selected, the read succeeds.

 

Here is the repro as a unit test that you can run in Spark master. Just add the test to ParquetIOSuite or some other test suite.
{code:java}
import org.apache.hadoop.conf._
import org.apache.hadoop.fs._
import org.apache.parquet.example.data.simple._
import org.apache.parquet.hadoop.example._
import org.apache.parquet.schema._

// Creates a Parquet file with two simple columns: integer and timestamp.
// Depending on isUTC flag, the timestamp is either NTZ or LTZ.
private def createParquetFile(path: String, isUTC: Boolean): Unit = {
  val schema = MessageTypeParser.parseMessageType(
    s""""""
    message schema {
      optional int32 a;
      optional int64 ts (TIMESTAMP(MILLIS, $isUTC));
    }
    """"""
  )
  val conf = new Configuration(false)
  conf.set(""parquet.example.schema"", schema.toString)
  val writer = ExampleParquetWriter.builder(new Path(path)).withConf(conf).build()
  for (i <- 0 until 2) {
    val group = new SimpleGroup(schema)
    group.add(""a"", 1)
    group.add(""ts"", System.currentTimeMillis)
    writer.write(group)
  }
  writer.close()
}

test(""repro"") {
  withTempPath { dir =>
    createParquetFile(dir + ""/file-1.parquet"", false) // NTZ
    createParquetFile(dir + ""/file-2.parquet"", true) // LTZ    

    val df = spark.read.parquet(dir.getAbsolutePath)
    df.show() // fails
  }
} {code}
If you run the repro as is, you will get: 
{code:java}
[info]   Cause: java.lang.RuntimeException: Unable to create Parquet converter for data type ""timestamp_ntz"" whose Parquet type is int64 time(TIMESTAMP(MILLIS,true)) {code}
If you swap the files (file names), the read succeeds.
{code:java}
+---+--------------------+
|  a|                  ts|
+---+--------------------+
|  1|2023-09-17 21:59:...|
|  1|2023-09-17 21:59:...|
|  1|2023-09-17 21:59:...|
|  1|2023-09-17 21:59:...|
+---+--------------------+ {code}
If you set spark.sql.parquet.mergeSchema to true, the schema inference fails with
{code:java}
[info]   org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:
[info] Initial schema:
[info] ""STRUCT<a: INT, ts: TIMESTAMP_NTZ>""
[info] Schema that cannot be merged with the initial schema:
[info] ""STRUCT<a: INT, ts: TIMESTAMP>"". {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 18 05:00:22 UTC 2023,,,,,,,,,,"0|z1kewg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/23 05:00;ivan.sadikov;cc [~gengliang] [~cloud_fan];;;",,,,,,,,,,,,,,
lineInterpolate for graphviz edge is overdue,SPARK-45192,13550989,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,yao,yao,yao,18/Sep/23 02:58,20/Sep/23 02:02,30/Oct/23 17:26,20/Sep/23 02:02,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Web UI,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 20 02:02:07 UTC 2023,,,,,,,,,,"0|z1keuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/23 02:02;yao;Issue resolved by pull request 42969
[https://github.com/apache/spark/pull/42969];;;",,,,,,,,,,,,,,
Fix WorkerPage to use the same pattern for `logPage` urls,SPARK-45187,13550960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,17/Sep/23 08:45,17/Sep/23 17:36,30/Oct/23 17:26,17/Sep/23 17:34,3.2.4,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.1,4.0.0,Spark Core,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Sep 17 17:34:42 UTC 2023,,,,,,,,,,"0|z1keog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/23 17:34;dongjoon;Issue resolved by pull request 42959
[https://github.com/apache/spark/pull/42959];;;",,,,,,,,,,,,,,
Ignore type check for preventing unexpected linter failure,SPARK-45185,13550910,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,itholic,itholic,16/Sep/23 05:54,16/Sep/23 21:01,30/Oct/23 17:26,16/Sep/23 21:01,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Build,PySpark,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-16 05:54:08.0,,,,,,,,,,"0|z1kedc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorPodsLifecycleManager delete a pod multi times.,SPARK-45183,13550903,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,hgs19921112,hgs19921112,16/Sep/23 01:20,19/Sep/23 13:57,30/Oct/23 17:26,,3.2.0,,,,,,,,,,,,,,,,,,,,,,,Kubernetes,,,,,0,,,,," Because `ExecutorPodsLifecycleManager`.`removedExecutorsCache` is not thread safe, will cause a pod deleted many times when  `ExecutorPodsLifecycleManager`.`onNewSnapshots` called by multi threads.",Spark 3.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 19 13:57:05 UTC 2023,,,,,,,,,,"0|z1kebs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/23 01:22;hgs19921112;[~dongjoon] , will you have a look at this.;;;","18/Sep/23 20:55;dongjoon;Thank you for creating a JIRA, but Apache Spark 3.2.0 is EOL already. 
To make it sure that this is a valid issue still, could you try Apache Spark 3.5.0 and update `Affects Version` field, [~hgs19921112]?;;;","19/Sep/23 13:57;hgs19921112;I have compared  Spark 3.2.0 with Spark 3.5.0. The deletion of pod is nothing different in `ExecutorPodsLifecycleManager`.So I suspect the version 3.5.0 may have the same issure.[~dongjoon] ;;;",,,,,,,,,,,,
Ignore task completion from old stage after retrying indeterminate stages,SPARK-45182,13550884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mayurb31,mayurb31,mayurb31,15/Sep/23 18:39,26/Sep/23 03:06,30/Oct/23 17:26,26/Sep/23 03:06,3.3.2,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Spark Core,,,,,0,pull-request-available,,,,"SPARK-25342 Added a support for rolling back shuffle map stage so that all tasks of the stage can be retried when the stage output is indeterminate. This is done by clearing all map outputs at the time of stage submission. This approach workouts well except for this case:

Assume both Shuffle 1 and 2 are indeterminate

ShuffleMapStage1 –{-}–{-}> Shuffle 1 ---–> ShuffleMapStage2 ----> Shuffle 2 ----> ResultStage
 * ShuffleMapStage1 is complete
 * A task from ShuffleMapStage2 fails with FetchFailed. Other tasks are still running
 * Both ShuffleMapStage1 and ShuffleMapStage2 are retried
 * ShuffleMapStage1 is retried and completes
 * ShuffleMapStage2 reattempt is scheduled for execution
 * Before all tasks of ShuffleMapStage2 reattempt could finish, one/more laggard tasks from the original attempt of ShuffleMapStage2 finish and ShuffleMapStage2 also gets marked as complete
 * Result Stage gets scheduled and finishes

Internally within Uber, we have been using the stage rollback functionality even for deterministic stages from Spark 2.4.3 to add fault tolerance from server going down in [remote shuffle service |https://github.com/uber/RemoteShuffleService]and have faced this scenario quite often

Ideally, such laggard tasks should not be considered towards the partition completion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 26 03:06:10 UTC 2023,,,,,,,,,,"0|z1ke7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/23 03:06;cloud_fan;Issue resolved by pull request 42950
[https://github.com/apache/spark/pull/42950];;;",,,,,,,,,,,,,,
Fallback to use single batch executor for Trigger.AvailableNow with unsupported sources rather than using wrapper,SPARK-45178,13550783,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,15/Sep/23 04:52,21/Sep/23 01:58,30/Oct/23 17:26,20/Sep/23 02:05,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Structured Streaming,,,,,0,pull-request-available,,,,"We have observed the case where wrapper implementation of Trigger.AvailableNow (
AvailableNowDataStreamWrapper and subclasses) is not fully compatible with 3rd party data source and brought up correctness issue.
 
While we could persuade 3rd party data source to support Trigger.AvailableNow, pursuing all 3rd parties to do this is too aggressive and challenging goal we never be able to make. Also, it may not be also possible to come up with the wrapper implementation which would have zero issue with any arbitrary source.
 
As a mitigation, we want to make a slight behavioral change for such case, falling back to single batch execution (a.k.a. Trigger.Once) rather than using wrapper implementation. The exact behavior between Trigger.AvailableNow and Trigger.Once are different so it's technically behavioral change, but it's probably lot less surprised than failing the query.
 
For extreme case where users are confident that there will be no issue at all on using wrapper, we will come up with a flag to provide the previous behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 20 02:05:23 UTC 2023,,,,,,,,,,"0|z1kdl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/23 04:53;kabhwan;PR will be available sooner.;;;","20/Sep/23 02:05;kabhwan;Issue resolved by pull request 42940
[https://github.com/apache/spark/pull/42940];;;",,,,,,,,,,,,,
AggregatingAccumulator with TypedImperativeAggregate throwing ClassCastException,SPARK-45176,13550779,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,hcampbell,hcampbell,15/Sep/23 03:48,15/Sep/23 03:50,30/Oct/23 17:26,,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Probably related to SPARK-39044. But potentially also this comment in Executor.scala.
{quote}// TODO: do not serialize value twice
val directResult = new DirectTaskResult(valueByteBuffer, accumUpdates, metricPeaks)
{quote}
The class cast exception I'm seeing is
{quote}
java.lang.ClassCastException: class [B cannot be cast to class org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir
{quote}
But I've seen it with other aggregation buffers like QuantileSummaries as well.

It's my belief that withBufferSerialized() for the Aggregating Accumulator is being called twice, leading to on serializeAggregateBuffernPlace(buffer)
also being called twice for the an Imperative aggregate, the second time round, the buffer is already a byte array and the asInstanceOf[T] in getBufferObject is throwing.

This doesn't appear to happen on all runs, and it might be its only occurring when there's a transitive exception. I have a further suspicion that the cause might originate with
{quote}
SerializationDebugger.improveException
{quote}
which is traversing the task and forcing writeExternal, to be called.

Setting
|spark.serializer.extraDebugInfo|false|

Seems to make things a bit more reliable (I haven't seen the error while this setting is on), and points strongly in that direction.

Stack trace:
{quote}
Job aborted due to stage failure: Authorized committer (attemptNumber=0, stage=15, partition=10) failed; but task commit success, data duplication may happen. reason=ExceptionFailure(java.io.IOException,java.lang.ClassCastException: class [B cannot be cast to class org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir ([B is in module java.base of loader 'bootstrap'; org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir is in unnamed module of loader 'app'),[Ljava.lang.StackTraceElement;@7fe2f462,java.io.IOException: java.lang.ClassCastException: class [B cannot be cast to class org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir ([B is in module java.base of loader 'bootstrap'; org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir is in unnamed module of loader 'app')
at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1502)
at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)
at java.base/java.io.ObjectOutputStream.writeExternalData(Unknown Source)
at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(Unknown Source)
at java.base/java.io.ObjectOutputStream.writeObject0(Unknown Source)
at java.base/java.io.ObjectOutputStream.writeObject(Unknown Source)
at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
at org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:643)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassCastException: class [B cannot be cast to class org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir ([B is in module java.base of loader 'bootstrap'; org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir is in unnamed module of loader 'app')
at org.apache.spark.sql.catalyst.expressions.aggregate.ReservoirSample.serialize(ReservoirSample.scala:33)
at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.serializeAggregateBufferInPlace(interfaces.scala:624)
at org.apache.spark.sql.execution.AggregatingAccumulator.withBufferSerialized(AggregatingAccumulator.scala:206)
at org.apache.spark.sql.execution.AggregatingAccumulator.withBufferSerialized(AggregatingAccumulator.scala:33)
at org.apache.spark.util.AccumulatorV2.writeReplace(AccumulatorV2.scala:186)
at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.base/java.lang.reflect.Method.invoke(Unknown Source)
at java.base/java.io.ObjectStreamClass.invokeWriteReplace(Unknown Source)
at java.base/java.io.ObjectOutputStream.writeObject0(Unknown Source)
at java.base/java.io.ObjectOutputStream.writeObject(Unknown Source)
at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$2(TaskResult.scala:62)
at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$2$adapted(TaskResult.scala:62)
at scala.collection.immutable.Vector.foreach(Vector.scala:1856)
at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:62)
at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1495)
... 11 more
 
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-15 03:48:59.0,,,,,,,,,,"0|z1kdk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove some unnecessary sourceMapping files in UI,SPARK-45173,13550774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,15/Sep/23 02:27,16/Sep/23 01:52,30/Oct/23 17:26,16/Sep/23 01:52,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Web UI,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Sep 16 01:52:12 UTC 2023,,,,,,,,,,"0|z1kdj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/23 01:52;dongjoon;Issue resolved by pull request 42935
[https://github.com/apache/spark/pull/42935];;;",,,,,,,,,,,,,,
GenerateExec fails to initialize non-deterministic expressions before use,SPARK-45171,13550722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,14/Sep/23 16:37,15/Sep/23 04:23,30/Oct/23 17:26,15/Sep/23 04:23,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,SQL,,,,,0,pull-request-available,,,,"The following query fails:
{noformat}
select *
from explode(
  transform(sequence(0, cast(rand()*1000 as int) + 1), x -> x * 22)
);
{noformat}
The error is:
{noformat}
23/09/14 09:27:25 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.IllegalArgumentException: requirement failed: Nondeterministic expression org.apache.spark.sql.catalyst.expressions.Rand should be initialized before eval.
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.sql.catalyst.expressions.Nondeterministic.eval(Expression.scala:497)
	at org.apache.spark.sql.catalyst.expressions.Nondeterministic.eval$(Expression.scala:495)
	at org.apache.spark.sql.catalyst.expressions.RDG.eval(randomExpressions.scala:35)
	at org.apache.spark.sql.catalyst.expressions.BinaryArithmetic.eval(arithmetic.scala:384)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:543)
	at org.apache.spark.sql.catalyst.expressions.BinaryArithmetic.eval(arithmetic.scala:384)
	at org.apache.spark.sql.catalyst.expressions.Sequence.eval(collectionOperations.scala:3062)
	at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval(higherOrderFunctions.scala:275)
	at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval$(higherOrderFunctions.scala:274)
	at org.apache.spark.sql.catalyst.expressions.ArrayTransform.eval(higherOrderFunctions.scala:308)
	at org.apache.spark.sql.catalyst.expressions.ExplodeBase.eval(generators.scala:375)
	at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$8(GenerateExec.scala:108)
...        
{noformat}
However, this query succeeds:
{noformat}
select *
from explode(
  sequence(0, cast(rand()*1000 as int) + 1)
);
{noformat}
The difference is that {{transform}} turns off whole-stage codegen, which exposes a bug in {{GenerateExec}} where the non-deterministic expression passed to the generator function is not initialized before being used.

An even simpler reprod case is:
{noformat}
set spark.sql.codegen.wholeStage=false;

select explode(array(rand()));
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 15 04:23:03 UTC 2023,,,,,,,,,,"0|z1kd7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/23 04:23;gurwls223;Issue resolved by pull request 42933
[https://github.com/apache/spark/pull/42933];;;",,,,,,,,,,,,,,
Python Spark Connect client does not call `releaseAll`,SPARK-45167,13550691,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,grundprinzip-db,grundprinzip-db,grundprinzip-db,14/Sep/23 12:49,05/Oct/23 04:02,30/Oct/23 17:26,17/Sep/23 09:02,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Connect,,,,,0,pull-request-available,,,,The Python client does not call release all previous responses on the server and thus does not properly close the queries.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-43754,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Sep 17 09:02:14 UTC 2023,,,,,,,,,,"0|z1kd0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/23 09:02;dongjoon;Issue resolved by pull request 42929
[https://github.com/apache/spark/pull/42929];;;",,,,,,,,,,,,,,
Unsupported map/array constructors via `call_function` in parameterized `sql()`,SPARK-45162,13550622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,14/Sep/23 06:04,14/Sep/23 08:32,30/Oct/23 17:26,14/Sep/23 08:32,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,"The example below demonstrates the issue:

{code:scala}
scala> sql(""SELECT element_at(:mapParam, 'a')"", Map(""mapParam"" -> call_function(""map"", lit(""a""), lit(1))))
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNBOUND_SQL_PARAMETER] Found the unbound parameter: mapParam. Please, fix `args` and provide a mapping of the parameter to a SQL literal.; line 1 pos 18;
'NameParameterizedQuery [mapParam='map(a, 1)]
+- 'Project [unresolvedalias('element_at(namedparameter(mapParam), a), None)]
   +- OneRowRelation
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 14 08:32:06 UTC 2023,,,,,,,,,,"0|z1kclc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/23 08:32;maxgekk;Issue resolved by pull request 42894
[https://github.com/apache/spark/pull/42894];;;",,,,,,,,,,,,,,
Pyspark DecisionTreeClassifier: results and tree structure in spark3 very different from that of the spark2 version on the same data and with the same hyperparameters.,SPARK-45154,13550538,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,oumarnour,oumarnour,13/Sep/23 13:39,25/Sep/23 16:11,30/Oct/23 17:26,,3.0.0,3.2.4,3.3.1,3.3.2,3.3.3,3.4.0,3.4.1,,,,,,,,,,,,,,,,,ML,MLlib,PySpark,Spark Core,,0,decisiontree,pyspark3,spark2,spark3,"Hello,
I have an engine running on spark2 using a DecisionTreeClassifier model using the CrossValidator. 

 
{code:java}
dt  = DecisionTreeClassifier(maxBins=10000, seed=0)   
cv_dt_evaluator = BinaryClassificationEvaluator(
            metricName="""", 
            rawPredictionCol=""probability"")

# Create param grid and cross validator for model selection
dt_grid = ParamGridBuilder()\
            .addGrid(
                dt.minInstancesPerNode, [100]
        )\
            .addGrid(
                dt.maxDepth, [10]
        )\
            .build()
cv = CrossValidator(
            estimator=dt, estimatorParamMaps=dt_grid, evaluator=cv_dt_evaluator,
            parallelism=4
            numFolds=4
        ){code}
 

I want to {*}migrate from spark2  to spark3{*}. I've run *DecisionTreeClassifier* on the same data with the same parameter values. But unfortunately my results are {*}completely different, especially in terms of tree structure{*}. I have trees with less depth and fewer splits on spark3. I've tried to read the documentation but I haven't found an answer to my question.

 

Can you help me find a solution to this problem?

Thanks in advance for your help 

        

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,pyspark,Python3,Spark2,spark3,2023-09-13 13:39:00.0,,,,,,,,,,"0|z1kc2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rebalance testing time for `pyspark-pandas-connect-part1`,SPARK-45153,13550536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,13/Sep/23 13:32,18/Sep/23 19:02,30/Oct/23 17:26,18/Sep/23 19:02,3.4.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Build,PySpark,,,,0,pull-request-available,,,,pyspark-pandas-connect-part1 takes much more time than other splited tests. We should rebalance the test.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 18 19:02:35 UTC 2023,,,,,,,,,,"0|z1kc28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/23 19:02;dongjoon;Issue resolved by pull request 42909
[https://github.com/apache/spark/pull/42909];;;",,,,,,,,,,,,,,
ChromeUISeleniumSuite test failed,SPARK-45150,13550473,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,LuciferYang,LuciferYang,13/Sep/23 08:57,14/Sep/23 06:51,30/Oct/23 17:26,14/Sep/23 06:51,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Spark Core,Tests,,,,0,pull-request-available,,,,"build/sbt -Dguava.version=31.1-jre -Dspark.test.webdriver.chrome.driver=/path/to/chromedriver -Dtest.default.exclude.tags="""" -Phive -Phive-thriftserver ""core/testOnly org.apache.spark.ui.ChromeUISeleniumSuite""

 
{code:java}
[info] ChromeUISeleniumSuite:
Starting ChromeDriver 117.0.5938.62 (25a7172909a4cba7355365cf424d7d7eb35231f4-refs/branch-heads/5938@{#1146}) on port 25714
Only local connections are allowed.
Please see https://chromedriver.chromium.org/security-considerations for suggestions on keeping ChromeDriver safe.
ChromeDriver was started successfully.
[info] - SPARK-31534: text for tooltip should be escaped (1 second, 279 milliseconds)
[info] - SPARK-31882: Link URL for Stage DAGs should not depend on paged table. *** FAILED *** (10 seconds, 476 milliseconds)
[info]   The code passed to eventually never returned normally. Attempted 123 times over 10.143342708 seconds. Last failure message: 2 was not equal to 3. (RealBrowserUISeleniumSuite.scala:86)
[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:
[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)
[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)
[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)
[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)
[info]   at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$5(RealBrowserUISeleniumSuite.scala:86)
[info]   at org.apache.spark.LocalSparkContext$.withSpark(LocalSparkContext.scala:65)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$4(RealBrowserUISeleniumSuite.scala:83)
[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)
[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
[info]   at java.base/java.lang.Thread.run(Thread.java:1583)
[info]   Cause: org.scalatest.exceptions.TestFailedException: 2 was not equal to 3
[info]   at org.scalatest.matchers.MatchersHelper$.indicateFailure(MatchersHelper.scala:397)
[info]   at org.scalatest.matchers.should.Matchers$ShouldMethodHelperClass.shouldMatcher(Matchers.scala:7299)
[info]   at org.scalatest.matchers.should.Matchers$AnyShouldWrapper.should(Matchers.scala:7347)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$8(RealBrowserUISeleniumSuite.scala:95)
[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)
[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)
[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)
[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)
[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)
[info]   at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$5(RealBrowserUISeleniumSuite.scala:86)
[info]   at org.apache.spark.LocalSparkContext$.withSpark(LocalSparkContext.scala:65)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$4(RealBrowserUISeleniumSuite.scala:83)
[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)
[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
[info]   at java.base/java.lang.Thread.run(Thread.java:1583)
[info] - SPARK-31886: Color barrier execution mode RDD correctly *** FAILED *** (10 seconds, 109 milliseconds)
[info]   The code passed to eventually never returned normally. Attempted 69 times over 10.037747416999999 seconds. Last failure message: 0 did not equal 1. (RealBrowserUISeleniumSuite.scala:108)
[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:
[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)
[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)
[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)
[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)
[info]   at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$10(RealBrowserUISeleniumSuite.scala:108)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$10$adapted(RealBrowserUISeleniumSuite.scala:105)
[info]   at org.apache.spark.LocalSparkContext$.withSpark(LocalSparkContext.scala:65)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$9(RealBrowserUISeleniumSuite.scala:105)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)
[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
[info]   at java.base/java.lang.Thread.run(Thread.java:1583)
[info]   Cause: org.scalatest.exceptions.TestFailedException: 0 did not equal 1
[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$12(RealBrowserUISeleniumSuite.scala:121)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)
[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)
[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)
[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)
[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)
[info]   at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$10(RealBrowserUISeleniumSuite.scala:108)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$10$adapted(RealBrowserUISeleniumSuite.scala:105)
[info]   at org.apache.spark.LocalSparkContext$.withSpark(LocalSparkContext.scala:65)
[info]   at org.apache.spark.ui.RealBrowserUISeleniumSuite.$anonfun$new$9(RealBrowserUISeleniumSuite.scala:105)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)
[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
[info]   at java.base/java.lang.Thread.run(Thread.java:1583)
[info] - Search text for paged tables should not be saved (986 milliseconds)
[info] Run completed in 24 seconds, 603 milliseconds.
[info] Total number of tests run: 4
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 2, failed 2, canceled 0, ignored 0, pending 0
[info] *** 2 TESTS FAILED *** {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 14 06:51:45 UTC 2023,,,,,,,,,,"0|z1kbo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/23 06:51;yao;Issue resolved by pull request 42907
[https://github.com/apache/spark/pull/42907];;;",,,,,,,,,,,,,,
flaky test: RocksDBBackendChromeUIHistoryServerSuite,SPARK-45149,13550471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,LuciferYang,LuciferYang,13/Sep/23 08:52,13/Sep/23 11:05,30/Oct/23 17:26,13/Sep/23 11:05,3.5.1,4.0.0,,,,,,,,,,,,,,,,,,,,,,Spark Core,Tests,,,,0,,,,,"build/sbt -Dguava.version=31.1-jre -Dspark.test.webdriver.chrome.driver=/path/to/chromedriver -Dtest.default.exclude.tags="""" -Phive -Phive-thriftserver ""core/testOnly org.apache.spark.deploy.history.RocksDBBackendChromeUIHistoryServerSuite""

 
{code:java}
info] RocksDBBackendChromeUIHistoryServerSuite:
Starting ChromeDriver 117.0.5938.62 (25a7172909a4cba7355365cf424d7d7eb35231f4-refs/branch-heads/5938@{#1146}) on port 17382
Only local connections are allowed.
Please see https://chromedriver.chromium.org/security-considerations for suggestions on keeping ChromeDriver safe.
ChromeDriver was started successfully.
[info] - ajax rendered relative links are prefixed with uiRoot (spark.ui.proxyBase) *** FAILED *** (5 seconds, 553 milliseconds)
[info]   2 was not greater than 4 (RealBrowserUIHistoryServerSuite.scala:150)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.matchers.MatchersHelper$.indicateFailure(MatchersHelper.scala:392)
[info]   at org.scalatest.matchers.should.Matchers$ShouldMethodHelperClass.shouldMatcher(Matchers.scala:7304)
[info]   at org.scalatest.matchers.should.Matchers$AnyShouldWrapper.should(Matchers.scala:7347)
[info]   at org.apache.spark.deploy.history.RealBrowserUIHistoryServerSuite.$anonfun$new$1(RealBrowserUIHistoryServerSuite.scala:150)
[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)
[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
[info]   at java.base/java.lang.Thread.run(Thread.java:1583)
[info] Run completed in 8 seconds, 61 milliseconds.
[info] Total number of tests run: 1
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 0, failed 1, canceled 0, ignored 0, pending 0
[info] *** 1 TEST FAILED *** {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 13 11:05:16 UTC 2023,,,,,,,,,,"0|z1kbns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/23 11:05;LuciferYang;It may be a problem with my local disk, close this one;;;",,,,,,,,,,,,,,
Specify the range for Spark Connect dependencies in pyspark base image,SPARK-45142,13550445,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,13/Sep/23 05:10,13/Sep/23 12:22,30/Oct/23 17:26,13/Sep/23 12:20,4.0.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Project Infra,,,,,0,pull-request-available,,,,The build fails after the image was regenerated. The cause seems to be changed dependencies. We should specify the range in Spark connect for pyspark.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 13 12:20:36 UTC 2023,,,,,,,,,,"0|z1kbi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/23 12:20;gurwls223;Issue resolved by pull request 42898
[https://github.com/apache/spark/pull/42898];;;",,,,,,,,,,,,,,
Unsupported map and array constructors by `sql()` in connect clients,SPARK-45137,13550393,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,12/Sep/23 16:25,18/Sep/23 17:14,30/Oct/23 17:26,18/Sep/23 17:14,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Connect,,,,,0,pull-request-available,,,,"The code below demonstrates the issue:

{code:scala}
spark.sql(""select element_at(?, 1)"", Array(array(lit(1)))).collect()
{code}

It fails with the error:

{code:java}
[info]   java.lang.UnsupportedOperationException: literal unresolved_function {
[info]   function_name: ""array""
[info]   arguments {
[info]     literal {
[info]       integer: 1
[info]     }
[info]   }
[info] }
[info]  not supported (yet).
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 18 17:14:29 UTC 2023,,,,,,,,,,"0|z1kb6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/23 17:14;maxgekk;Issue resolved by pull request 42931
[https://github.com/apache/spark/pull/42931];;;",,,,,,,,,,,,,,
Data duplication may occur when fallback to origin shuffle block,SPARK-45134,13550347,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,gaoyajun02,gaoyajun02,12/Sep/23 10:19,01/Oct/23 01:05,30/Oct/23 17:26,,3.2.0,3.3.0,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,,,Shuffle,,,,,0,pull-request-available,,,,"One possible situation that has been found is that, during the process of requesting mergedBlockMeta, when the channel is closed, it may trigger two callback callbacks and result in duplicate data for the original shuffle blocks.
 # The first time is when the channel is inactivated, the responseHandler will execute the callback for all outstandingRpcs.
 # The second time is when the listener corresponding to shuffleClient.writeAndFlush executes the callback after the channel is closed.

Some Error Logs:
{code:java}
23/09/08 09:22:21 ERROR shuffle-client-7-1 TransportResponseHandler: Still have 1 requests outstanding when connection from host/ip:prot is closed
23/09/08 09:22:21 ERROR shuffle-client-7-1 PushBasedFetchHelper: Failed to get the meta of push-merged block for (3, 54) from host:port
java.io.IOException: Connection from host:port closed
        at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147)
        at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
        at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
        at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
        at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
        at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818)
        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:745)
 
23/09/08 09:22:21 ERROR shuffle-client-7-1 PushBasedFetchHelper: Failed to get the meta of push-merged block for (3, 54) from host:port
java.io.IOException: Failed to send RPC RPC 8079698359363123411 to host/ip:port: java.nio.channels.ClosedChannelException
        at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:433)
        at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:409)
        at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
        at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)
        at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
        at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
        at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)
        at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
        at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:790)
        at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:767)
        at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:790)
        at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:767)
        at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)
        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
        at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
        ... 18 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 20 03:29:41 UTC 2023,,,,,,,,,,"0|z1kaw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/23 10:43;gaoyajun02;Hi, [~csingh] [~vsowrirajan] [~mshen] , Can you take a look? hope some suggestions.;;;","20/Sep/23 03:29;snoot;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/43004;;;",,,,,,,,,,,,,
Fix IDENTIFIER clause for functions,SPARK-45132,13550339,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srielau,srielau,srielau,12/Sep/23 09:24,12/Oct/23 13:35,30/Oct/23 17:26,12/Oct/23 13:35,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Spark Core,,,,,0,pull-request-available,,,,"Due to a quirk in the grammar IDENTIFIER('foo')(<somearg>) does not resolve depending on <somearg>.

Example:
SELECT IDENTIFIER('abs')(-1) works, but
SELECT IDENTIFIER('abs')(c1) FROM VALUES(-1) AS T(c1) does not.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Oct 12 13:35:36 UTC 2023,,,,,,,,,,"0|z1kaug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/23 13:35;cloud_fan;Issue resolved by pull request 42888
[https://github.com/apache/spark/pull/42888];;;",,,,,,,,,,,,,,
Do not use local user ID for Local Relations,SPARK-45124,13550303,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,12/Sep/23 03:08,12/Sep/23 06:00,30/Oct/23 17:26,12/Sep/23 06:00,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Connect,,,,,0,pull-request-available,,,,Allowing a fetch of a local relation using user-provided information is a potential security risk since this allows users to fetch arbitrary local relations.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 12 06:00:07 UTC 2023,,,,,,,,,,"0|z1kamg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/23 06:00;gurwls223;Issue resolved by pull request 42880
[https://github.com/apache/spark/pull/42880];;;",,,,,,,,,,,,,,
Raise TypeError for DataFrame.interpolate when all columns are object-dtype.,SPARK-45123,13550301,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,itholic,itholic,12/Sep/23 02:55,13/Sep/23 00:25,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Pandas API on Spark,,,,,0,pull-request-available,,,,To match the pandas behavior.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-12 02:55:12.0,,,,,,,,,,"0|z1kam0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade d3 from v3 to v7(v7.8.5) and apply api changes in UI,SPARK-45120,13550296,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,12/Sep/23 02:28,14/Sep/23 06:51,30/Oct/23 17:26,14/Sep/23 06:51,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Web UI,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 14 06:51:27 UTC 2023,,,,,,,,,,"0|z1kakw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/23 01:51;yao;Issue resolved by pull request 42879
[https://github.com/apache/spark/pull/42879];;;","13/Sep/23 13:49;hudson;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42907;;;","14/Sep/23 00:15;gurwls223;Reverted in https://github.com/apache/spark/commit/d26e77aabd2901066419353707c2411ddde4a07f;;;","14/Sep/23 06:51;yao;Issue resolved by pull request 42907
[https://github.com/apache/spark/pull/42907];;;",,,,,,,,,,,
Implement missing otherCopyArgs for the MultiCommutativeOp expression,SPARK-45117,13550258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scnakandala,scnakandala,scnakandala,11/Sep/23 17:28,12/Sep/23 15:53,30/Oct/23 17:26,12/Sep/23 15:53,3.4.1,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,SQL,,,,,0,pull-request-available,,,,Calling toJSON on a `MultiCommutativeOp` throws an assertion error as it does not implement the `otherCopyArgs` method.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 12 15:53:05 UTC 2023,,,,,,,,,,"0|z1kacg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/23 15:53;cloud_fan;Issue resolved by pull request 42873
[https://github.com/apache/spark/pull/42873];;;",,,,,,,,,,,,,,
Upgrade rocksdbjni to 8.5.3,SPARK-45110,13550128,Bug,Reopened,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,panbingkun,panbingkun,10/Sep/23 08:31,13/Sep/23 07:33,30/Oct/23 17:26,,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,,,,,Build,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 12 15:50:54 UTC 2023,,,,,,,,,,"0|z1k9js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/23 07:29;dongjoon;Issue resolved by pull request 42862
[https://github.com/apache/spark/pull/42862];;;","12/Sep/23 15:50;dongjoon;This is reverted due to the following.

https://github.com/apache/spark/actions/runs/6156730315/job/16706042686

{code}
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fc89e1ef7e7, pid=83569, tid=0x00007fc87fffd640
#
# JRE version: OpenJDK Runtime Environment (8.0_382-b05) (build 1.8.0_382-b05)
# Java VM: OpenJDK 64-Bit Server VM (25.382-b05 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [librocksdbjni8661574108371776748.so+0x3ef7e7]  rocksdb::DBImpl::GetProperty(rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&, std::string*)+0x57
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /home/runner/work/spark/spark/sql/core/hs_err_pid83569.log
Compiled method (nm)  246314 30882     n 0       org.rocksdb.RocksDB::getProperty (native)
 total in heap  [0x00007fc8fc7af7d0,0x00007fc8fc7afb60] = 912
 relocation     [0x00007fc8fc7af8f8,0x00007fc8fc7af940] = 72
 main code      [0x00007fc8fc7af940,0x00007fc8fc7afb58] = 536
 oops           [0x00007fc8fc7afb58,0x00007fc8fc7afb60] = 8
#
# If you would like to submit a bug report, please visit:
#   https://github.com/adoptium/adoptium-support/issues
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
{code};;;",,,,,,,,,,,,,
Fix eas_decrypt and ln in connect,SPARK-45109,13550127,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,10/Sep/23 08:25,12/Sep/23 07:24,30/Oct/23 17:26,11/Sep/23 00:07,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,4.0.0,,Connect,,,,,0,pull-request-available,,,,"The current {{eas_descrypt}} reference to {{aes_encrypt}} is clearly a bug. The {{ln}} reference to {{log}} is more like a cosmetic issue, but because {{ln}} and {{log}} function implementations are different in Spark SQL we should use the same implementation in Spark Connect too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 12 07:24:29 UTC 2023,,,,,,,,,,"0|z1k9jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/23 00:07;podongfeng;Issue resolved by pull request 42863
[https://github.com/apache/spark/pull/42863];;;","12/Sep/23 07:24;dongjoon;This landed at branch-3.4 via https://github.com/apache/spark/pull/42872;;;",,,,,,,,,,,,,
 percentile_cont gets internal error when user input fails runtime replacement's input type check,SPARK-45106,13550053,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,08/Sep/23 14:19,08/Sep/23 19:40,30/Oct/23 17:26,08/Sep/23 19:40,3.3.2,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,3.5.1,,,,SQL,,,,,0,pull-request-available,,,,"This query throws an internal error rather than producing a useful error message:
{noformat}
select percentile_cont(b) WITHIN GROUP (ORDER BY a DESC) as x 
from (values (12, 0.25), (13, 0.25), (22, 0.25)) as (a, b);

[INTERNAL_ERROR] Cannot resolve the runtime replaceable expression ""percentile_cont(a, b)"". The replacement is unresolved: ""percentile(a, b, 1)"".
org.apache.spark.SparkException: [INTERNAL_ERROR] Cannot resolve the runtime replaceable expression ""percentile_cont(a, b)"". The replacement is unresolved: ""percentile(a, b, 1)"".
	at org.apache.spark.SparkException$.internalError(SparkException.scala:92)
	at org.apache.spark.SparkException$.internalError(SparkException.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:313)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:277)
...
{noformat}
It should instead inform the user that the input expression must be foldable.

{{PercentileCont}} does not check the user's input. If the runtime replacement (an instance of {{Percentile}}) rejects the user's input, the runtime replacement ends up unresolved.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 08 19:40:13 UTC 2023,,,,,,,,,,"0|z1k934:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/23 19:40;dongjoon;This is resolved via https://github.com/apache/spark/pull/42857;;;",,,,,,,,,,,,,,
Upgrade graphlib-dot.min.js to 1.0.2,SPARK-45104,13549982,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,08/Sep/23 03:29,09/Sep/23 05:08,30/Oct/23 17:26,08/Sep/23 20:32,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Web UI,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 08 20:32:45 UTC 2023,,,,,,,,,,"0|z1k8nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/23 03:45;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42853;;;","08/Sep/23 20:32;dongjoon;Issue resolved by pull request 42853
[https://github.com/apache/spark/pull/42853];;;",,,,,,,,,,,,,
Update ORC to 1.8.5,SPARK-45103,13549959,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,07/Sep/23 19:34,07/Sep/23 22:37,30/Oct/23 17:26,07/Sep/23 22:37,3.4.2,,,,,,,,,,,,,,,,,,,3.4.2,,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 07 22:37:29 UTC 2023,,,,,,,,,,"0|z1k8i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/23 22:37;dongjoon;Issue resolved by pull request 42851
[https://github.com/apache/spark/pull/42851];;;",,,,,,,,,,,,,,
Spark UI: A stage is still active even when all of it's tasks are succeeded,SPARK-45101,13549936,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,RickyMa,RickyMa,07/Sep/23 14:00,19/Sep/23 08:14,30/Oct/23 17:26,,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"In the stage UI, we can see all the tasks' statuses are SUCCESS.

But the stage is still marked as active.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/23 14:03;RickyMa;1.png;https://issues.apache.org/jira/secure/attachment/13062748/1.png","07/Sep/23 14:03;RickyMa;2.png;https://issues.apache.org/jira/secure/attachment/13062747/2.png","07/Sep/23 14:03;RickyMa;3.png;https://issues.apache.org/jira/secure/attachment/13062749/3.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-09-07 14:00:47.0,,,,,,,,,,"0|z1k8d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
reflect() fails with an internal error on NULL class and method,SPARK-45100,13549931,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,07/Sep/23 13:22,08/Sep/23 16:00,30/Oct/23 17:26,08/Sep/23 08:13,3.3.2,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.0,4.0.0,SQL,,,,,0,pull-request-available,,,,"The example below demonstrates the issue:

{code:sql}
spark-sql (default)> select reflect('java.util.UUID', CAST(NULL AS STRING));
[INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace.
{code}
",,,,,,,,,,,,,,,SPARK-45079,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 08 15:09:18 UTC 2023,,,,,,,,,,"0|z1k8c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/23 08:13;maxgekk;Issue resolved by pull request 42849
[https://github.com/apache/spark/pull/42849];;;","08/Sep/23 15:09;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/42855;;;",,,,,,,,,,,,,
Custom jekyll-rediect-from redirect.html template,SPARK-45098,13549906,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,07/Sep/23 10:10,09/Sep/23 05:11,30/Oct/23 17:26,08/Sep/23 17:24,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Documentation,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 08 17:24:37 UTC 2023,,,,,,,,,,"0|z1k86g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/23 17:24;dongjoon;Issue resolved by pull request 42848
[https://github.com/apache/spark/pull/42848];;;",,,,,,,,,,,,,,
isEmpty on union of RDDs sharing the same trait crash when the first RDD is empty,SPARK-45094,13549829,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,jeanfrancisroy,jeanfrancisroy,06/Sep/23 18:58,06/Sep/23 19:00,30/Oct/23 17:26,,3.2.1,3.4.0,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Given two RDDs of different types, but sharing a common trait, one can obtain a union of the two RDDs by doing the following:
{code:java}
> import org.apache.spark.rdd.RDD

> trait Foo { val a: Int }
> case class Bar(a: Int) extends Foo
> case class Baz(a: Int, b: Int) extends Foo

> val bars = spark.sparkContext.parallelize(List(Bar(1), Bar(2))).asInstanceOf[RDD[Foo]]
> val bazs = spark.sparkContext.parallelize(List(Baz(1, 42), Baz(2, 42))).asInstanceOf[RDD[Foo]]

> val union = bars.union(bazs){code}
 

When doing so, `count()` and `isEmpty()` are behaving as expected:
{code:java}
> union.count()
4

> union.isEmpty()
false{code}
However, if the first RDD is empty, `count()` will behave as expected, but `isEmpty()` will throw a `java.lang.ArrayStoreException`:
{code:java}
> val bars = spark.sparkContext.parallelize(List.empty[Bar]).asInstanceOf[RDD[Foo]]
> val union = bars.union(bazs)

> union.count()
2

> union.isEmpty()
BOOM{code}
Full stack trace:
{code:java}
ERROR Executor: Exception in task 4.0 in stage 8.0 (TID 134)
java.lang.ArrayStoreException: $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Baz
    at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:74)
    at scala.Array$.slowcopy(Array.scala:157)
    at scala.Array$.copy(Array.scala:183)
    at scala.collection.mutable.ResizableArray.copyToArray(ResizableArray.scala:80)
    at scala.collection.mutable.ResizableArray.copyToArray$(ResizableArray.scala:78)
    at scala.collection.mutable.ArrayBuffer.copyToArray(ArrayBuffer.scala:49)
    at scala.collection.TraversableOnce.copyToArray(TraversableOnce.scala:334)
    at scala.collection.TraversableOnce.copyToArray$(TraversableOnce.scala:333)
    at scala.collection.AbstractTraversable.copyToArray(Traversable.scala:108)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:342)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractTraversable.toArray(Traversable.scala:108)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
    at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1462)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
    at org.apache.spark.scheduler.Task.run(Task.scala:139)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
23/09/06 14:55:18 ERROR Executor: Exception in task 14.0 in stage 8.0 (TID 144)
java.lang.ArrayStoreException: $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Baz
    at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:74)
    at scala.Array$.slowcopy(Array.scala:157)
    at scala.Array$.copy(Array.scala:183)
    at scala.collection.mutable.ResizableArray.copyToArray(ResizableArray.scala:80)
    at scala.collection.mutable.ResizableArray.copyToArray$(ResizableArray.scala:78)
    at scala.collection.mutable.ArrayBuffer.copyToArray(ArrayBuffer.scala:49)
    at scala.collection.TraversableOnce.copyToArray(TraversableOnce.scala:334)
    at scala.collection.TraversableOnce.copyToArray$(TraversableOnce.scala:333)
    at scala.collection.AbstractTraversable.copyToArray(Traversable.scala:108)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:342)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractTraversable.toArray(Traversable.scala:108)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
    at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1462)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
    at org.apache.spark.scheduler.Task.run(Task.scala:139)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
23/09/06 14:55:18 WARN TaskSetManager: Lost task 4.0 in stage 8.0 (TID 134) (192.168.86.57 executor driver): java.lang.ArrayStoreException: $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Baz
    at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:74)
    at scala.Array$.slowcopy(Array.scala:157)
    at scala.Array$.copy(Array.scala:183)
    at scala.collection.mutable.ResizableArray.copyToArray(ResizableArray.scala:80)
    at scala.collection.mutable.ResizableArray.copyToArray$(ResizableArray.scala:78)
    at scala.collection.mutable.ArrayBuffer.copyToArray(ArrayBuffer.scala:49)
    at scala.collection.TraversableOnce.copyToArray(TraversableOnce.scala:334)
    at scala.collection.TraversableOnce.copyToArray$(TraversableOnce.scala:333)
    at scala.collection.AbstractTraversable.copyToArray(Traversable.scala:108)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:342)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractTraversable.toArray(Traversable.scala:108)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
    at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1462)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
    at org.apache.spark.scheduler.Task.run(Task.scala:139)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)23/09/06 14:55:18 ERROR TaskSetManager: Task 4 in stage 8.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 8.0 failed 1 times, most recent failure: Lost task 4.0 in stage 8.0 (TID 134) (192.168.86.57 executor driver): java.lang.ArrayStoreException: Baz
    at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:74)
    at scala.Array$.slowcopy(Array.scala:157)
    at scala.Array$.copy(Array.scala:183)
    at scala.collection.mutable.ResizableArray.copyToArray(ResizableArray.scala:80)
    at scala.collection.mutable.ResizableArray.copyToArray$(ResizableArray.scala:78)
    at scala.collection.mutable.ArrayBuffer.copyToArray(ArrayBuffer.scala:49)
    at scala.collection.TraversableOnce.copyToArray(TraversableOnce.scala:334)
    at scala.collection.TraversableOnce.copyToArray$(TraversableOnce.scala:333)
    at scala.collection.AbstractTraversable.copyToArray(Traversable.scala:108)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:342)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractTraversable.toArray(Traversable.scala:108)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
    at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1462)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
    at org.apache.spark.scheduler.Task.run(Task.scala:139)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
  at scala.Option.foreach(Option.scala:407)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
  at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1462)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
  at org.apache.spark.rdd.RDD.take(RDD.scala:1435)
  at org.apache.spark.rdd.RDD.$anonfun$isEmpty$1(RDD.scala:1572)
  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
  at org.apache.spark.rdd.RDD.isEmpty(RDD.scala:1572)
  ... 47 elided
Caused by: java.lang.ArrayStoreException: Baz
  at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:74)
  at scala.Array$.slowcopy(Array.scala:157)
  at scala.Array$.copy(Array.scala:183)
  at scala.collection.mutable.ResizableArray.copyToArray(ResizableArray.scala:80)
  at scala.collection.mutable.ResizableArray.copyToArray$(ResizableArray.scala:78)
  at scala.collection.mutable.ArrayBuffer.copyToArray(ArrayBuffer.scala:49)
  at scala.collection.TraversableOnce.copyToArray(TraversableOnce.scala:334)
  at scala.collection.TraversableOnce.copyToArray$(TraversableOnce.scala:333)
  at scala.collection.AbstractTraversable.copyToArray(Traversable.scala:108)
  at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:342)
  at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
  at scala.collection.AbstractTraversable.toArray(Traversable.scala:108)
  at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
  at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
  at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1462)
  at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
  at org.apache.spark.scheduler.Task.run(Task.scala:139)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
  at java.base/java.lang.Thread.run(Thread.java:829) {code}","Tested on Spark 3.2.1 and Spark 3.4.0, using Scala 2.12.17 and OpenJDK 11.0.20.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,,,,2023-09-06 18:58:20.0,,,,,,,,,,"0|z1k7pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encoders.bean does no longer work with read-only properties,SPARK-45081,13549685,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,gbloisi-openaire,gbloisi-openaire,05/Sep/23 14:42,18/Sep/23 20:40,30/Oct/23 17:26,12/Sep/23 14:17,3.4.1,,,,,,,,,,,,,,,,,,,3.4.2,3.5.1,4.0.0,,SQL,,,,,0,pull-request-available,,,,"Since Spark 3.4.x an exception is thrown when Encoders.bean is called providing a bean having read-only properties, such as:

 
{code:java}
public static class ReadOnlyPropertyBean implements Serializable {
    public boolean isEmpty() {
      return true;
    }
} {code}
 

 
Encoders.bean(ReadOnlyPropertyBean.class) will throw:
{code:java}
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:529)
        at scala.None$.get(Option.scala:527)
        at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$deserializerFor$8(ScalaReflection.scala:359)
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
        at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
        at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
        at scala.collection.TraversableLike.map(TraversableLike.scala:286)
        at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
        at scala.collection.AbstractTraversable.map(Traversable.scala:108)
        at org.apache.spark.sql.catalyst.ScalaReflection$.deserializerFor(ScalaReflection.scala:348)
        at org.apache.spark.sql.catalyst.ScalaReflection$.deserializerFor(ScalaReflection.scala:183)
        at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:56)
        at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.javaBean(ExpressionEncoder.scala:62)
        at org.apache.spark.sql.Encoders$.bean(Encoders.scala:179)
        at org.apache.spark.sql.Encoders.bean(Encoders.scala) {code}
This problem is described also in [link Encoders.bean doesn't work anymore on a Java POJO, with Spark 3.4.0|https://stackoverflow.com/questions/76036349/encoders-bean-doesnt-work-anymore-on-a-java-pojo-with-spark-3-4-0]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 18 20:40:37 UTC 2023,,,,,,,,,,"0|z1k6tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/23 20:40;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/42913;;;",,,,,,,,,,,,,,
Kafka DSv2 streaming source implementation calls planInputPartitions 4 times per microbatch,SPARK-45080,13549669,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,05/Sep/23 13:31,07/Sep/23 10:43,30/Oct/23 17:26,07/Sep/23 10:43,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Structured Streaming,,,,,0,,,,,"I was tracking through method calls for DSv2 streaming source, and figured out planInputPartitions is called 4 times per microbatch.

It turned out that multiple calls of planInputPartitions is due to `DataSourceV2ScanExecBase.supportsColumnar`, though it is called through `MicroBatchScanExec.inputPartitions` which is defined as lazy, hence shouldn't happen.

The behavior seems to be coupled with catalyst and very hard to figure out why, but with SPARK-44505, we can at least fix this per each data source.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 07 10:43:14 UTC 2023,,,,,,,,,,"0|z1k6ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/23 13:31;kabhwan;Working on this. Will submit a PR sooner.;;;","06/Sep/23 03:41;snoot;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/42823;;;","07/Sep/23 10:43;kabhwan;Issue resolved by pull request 42823
[https://github.com/apache/spark/pull/42823];;;",,,,,,,,,,,,
percentile_approx() fails with an internal error on NULL accuracy,SPARK-45079,13549658,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,05/Sep/23 11:46,07/Sep/23 13:22,30/Oct/23 17:26,06/Sep/23 07:33,3.3.2,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.0,4.0.0,SQL,,,,,0,,,,,"The example below demonstrates the issue:

{code:sql}
spark-sql (default)> SELECT percentile_approx(col, array(0.5, 0.4, 0.1), NULL) FROM VALUES (0), (1), (2), (10) AS tab(col);
[INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace.
{code}
",,,,,,,,,,,,,,SPARK-45100,SPARK-45060,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 06 10:33:07 UTC 2023,,,,,,,,,,"0|z1k6nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/23 11:59;aparna.garg;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/42817;;;","06/Sep/23 07:33;maxgekk;Issue resolved by pull request 42817
[https://github.com/apache/spark/pull/42817];;;","06/Sep/23 10:33;aparna.garg;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/42835;;;",,,,,,,,,,,,
The ArrayInsert function should make explicit casting when element type not equals derived component type,SPARK-45078,13549654,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,taoran,taoran,05/Sep/23 11:24,18/Sep/23 18:56,30/Oct/23 17:26,17/Sep/23 08:17,3.4.1,,,,,,,,,,,,,,,,,,,3.4.2,3.5.1,4.0.0,,SQL,,,,,0,pull-request-available,,,,"Generally speaking, array_insert has same insert semantic with  array_prepend/array_append. however, if we run sql use element cast like below, array_prepend/array_append can get right result. but array_insert failed.
{code:java}
spark-sql (default)> select array_prepend(array(1), cast(2 as tinyint));
[2,1]
Time taken: 0.123 seconds, Fetched 1 row(s) {code}
{code:java}
spark-sql (default)> select array_append(array(1), cast(2 as tinyint)); 
[1,2] 
Time taken: 0.206 seconds, Fetched 1 row(s)
{code}
{code:java}
spark-sql (default)> select array_insert(array(1), 2, cast(2 as tinyint));
[DATATYPE_MISMATCH.ARRAY_FUNCTION_DIFF_TYPES] Cannot resolve ""array_insert(array(1), 2, CAST(2 AS TINYINT))"" due to data type mismatch: Input to `array_insert` should have been ""ARRAY"" followed by a value with same element type, but it's [""ARRAY<INT>"", ""TINYINT""].; line 1 pos 7;
'Project [unresolvedalias(array_insert(array(1), 2, cast(2 as tinyint)), None)]
+- OneRowRelation {code}
The reported error is clear, however, we may should do explicit casting here. because multiset type such as array or map allow the operands of same type family  to coexist.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 18 18:56:22 UTC 2023,,,,,,,,,,"0|z1k6mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/23 08:17;maxgekk;Issue resolved by pull request 42951
[https://github.com/apache/spark/pull/42951];;;","18/Sep/23 18:56;dongjoon;This is resolved via https://github.com/apache/spark/pull/42960;;;",,,,,,,,,,,,,
Alter table with invalid default value will not report error,SPARK-45075,13549612,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,05/Sep/23 03:49,12/Sep/23 06:33,30/Oct/23 17:26,08/Sep/23 20:18,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,3.4.2,3.5.1,4.0.0,,SQL,,,,,0,pull-request-available,,,,"create table t(i boolean, s bigint);
alter table t alter column s set default badvalue;
 
The code wouldn't report error on DataSource V2, not align with V1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 12 06:29:08 UTC 2023,,,,,,,,,,"0|z1k6d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/23 14:18;ignitetcbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42810;;;","08/Sep/23 20:18;dongjoon;Issue resolved by pull request 42810
[https://github.com/apache/spark/pull/42810];;;","12/Sep/23 06:29;dongjoon;This landed at branch-3.4 via https://github.com/apache/spark/pull/42876;;;",,,,,,,,,,,,
Fix Outerscopes for same cell evaluation,SPARK-45072,13549588,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,04/Sep/23 16:43,05/Sep/23 15:36,30/Oct/23 17:26,05/Sep/23 15:36,3.5.0,,,,,,,,,,,,,,,,,,,3.5.1,,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 05 15:36:16 UTC 2023,,,,,,,,,,"0|z1k67s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/23 15:36;dongjoon;This is resolved via https://github.com/apache/spark/pull/42807;;;",,,,,,,,,,,,,,
SQL variable should always be resolved after outer reference,SPARK-45069,13549561,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,04/Sep/23 12:32,11/Sep/23 14:58,30/Oct/23 17:26,11/Sep/23 14:58,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 11 14:58:04 UTC 2023,,,,,,,,,,"0|z1k61s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/23 09:08;githubbot;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/42803;;;","11/Sep/23 14:58;cloud_fan;Issue resolved by pull request 42803
[https://github.com/apache/spark/pull/42803];;;",,,,,,,,,,,,,
to_char() fails with an internal error on NULL format,SPARK-45060,13549469,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,02/Sep/23 14:44,05/Sep/23 11:46,30/Oct/23 17:26,04/Sep/23 07:36,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,,,,,"The example below demonstrates the issue:

{code:sql}
spark-sql (default)> SELECT to_char(x'537061726b2053514c', CAST(NULL AS STRING));
[INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace.
{code}
",,,,,,,,,,,,,,SPARK-45079,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 04 07:36:14 UTC 2023,,,,,,,,,,"0|z1k5hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/23 07:36;maxgekk;Issue resolved by pull request 42781
[https://github.com/apache/spark/pull/42781];;;",,,,,,,,,,,,,,
Deadlock caused by rdd replication level of 2,SPARK-45057,13549452,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,01/Sep/23 23:32,28/Sep/23 23:53,30/Oct/23 17:26,28/Sep/23 23:52,3.4.1,,,,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.1,4.0.0,Spark Core,,,,,0,pull-request-available,,,," 
When 2 tasks try to compute same rdd with replication level of 2 and running on only 2 executors. Deadlock will happen.

Task only release lock after writing into local machine and replicate to remote executor.

 
||Time||Exe 1 (Task Thread T1)||Exe 1 (Shuffle Server Thread T2)||Exe 2 (Task Thread T3)||Exe 2 (Shuffle Server Thread T4)||
|T0|write lock of rdd| | | |
|T1| | |write lock of rdd| |
|T2|replicate -> UploadBlockSync (blocked by T4)| | | |
|T3| | | |Received UploadBlock request from T1 (blocked by T4)|
|T4| | |replicate -> UploadBlockSync (blocked by T2)| |
|T5| |Received UploadBlock request from T3 (blocked by T1)| | |
|T6|Deadlock|Deadlock|Deadlock|Deadlock|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 28 23:52:57 UTC 2023,,,,,,,,,,"0|z1k5dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/23 06:37;Ngone51;In the case of ""Received UploadBlock request from T1 (blocked by T4)"", shouldn't it be blocked by T3?;;;","28/Sep/23 23:52;mridulm80;Issue resolved by pull request 43067
[https://github.com/apache/spark/pull/43067];;;",,,,,,,,,,,,,
Do not transpose windows if they conflict on ORDER BY / PROJECT clauses,SPARK-45055,13549439,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,gubichev,gubichev,01/Sep/23 18:59,12/Sep/23 13:09,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,pull-request-available,,,,"TransposeWindows rule reorders parent and child window functions. Currently it incorrectly reorders the window functions in cases where the top window function orders by on the result of the bottom window function, e.g. {{sum1}} in the following example:

{{SELECT ROW_NUMBER() OVER (PARTITION BY C ORDER BY sum1)  FROM (SELECT ROW_NUMBER() OVER (PARTITION BY C) as sum1 FROM T) }}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 06 09:39:50 UTC 2023,,,,,,,,,,"0|z1k5ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/23 09:39;aparna.garg;User 'agubichev' has created a pull request for this issue:
https://github.com/apache/spark/pull/42778;;;",,,,,,,,,,,,,,
HiveExternalCatalog.listPartitions should restore Spark SQL stats,SPARK-45054,13549437,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,01/Sep/23 18:14,02/Sep/23 03:25,30/Oct/23 17:26,02/Sep/23 03:22,3.2.4,3.3.2,3.4.1,,,,,,,,,,,,,,,,,3.4.2,3.5.0,4.0.0,,SQL,,,,,0,,,,,"If partitions are stored in HMS with Spark populated stats such as {{spark.sql.statistics.totalSize}}, currently {{HiveExternalCatalog.listPartitions}} doesn't call {{restorePartitionMetadata}} to restore those stats.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Sep 02 03:22:50 UTC 2023,,,,,,,,,,"0|z1k5a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/23 03:22;csun;Issue resolved by pull request 42777
[https://github.com/apache/spark/pull/42777];;;",,,,,,,,,,,,,,
SPARK-43183 broke various tests in 3rd party streaming data sources,SPARK-45045,13549358,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kabhwan,kabhwan,kabhwan,01/Sep/23 06:40,04/Sep/23 02:42,30/Oct/23 17:26,04/Sep/23 02:42,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Structured Streaming,,,,,0,,,,,"SPARK-43183 made a change regarding behavior to the StreamingQueryListener as well as StreamingQuery API, while the intention was more about introducing the change in the former one.

I just got some reports from 3rd party data sources that they encountered innocent test failures during upgrade to 3.5.0 (RC), and it looks like they are mostly concerned about the behavioral change for StreamingQuery API. (It's lot less convenient for developers to rely on streaming query listener for testing.)

That said, to avoid 3rd party ecosystem be bugging with new behavior, we should probably revert the behavior for StreamingQuery API at least.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 04 02:42:31 UTC 2023,,,,,,,,,,"0|z1k4so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/23 02:42;kabhwan;Issue resolved by pull request 42773
[https://github.com/apache/spark/pull/42773];;;",,,,,,,,,,,,,,
spark using --proxy-user GSS init failed when `hive.metastore.token.signature` not empty,SPARK-45041,13549345,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,Zing,Zing,01/Sep/23 00:12,06/Sep/23 13:28,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"In spark, we can using --proxy-user to proxy the other user in kerberos env. But we will make  GSS init failed exception when connect to hive metastore and `hive.metastore.token.signature` not empty.
{code:java}
```
spark-sql  --conf spark.driver.extraClassPath=/home/hive/conf   --proxy-user test_user 
```{code}

if we set conf in `hive-site.xml`
{code:java}
```
    <property>
       <name>hive.metastore.token.signature</name>
       <value>spark_delegation_token</value>
    </property>
```{code}
we will get 

 
{code:java}
```
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
    at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)
    at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:95)
    at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271)
    at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:38)
    at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)
    at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)
    at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:478)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)
```{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 06 13:28:44 UTC 2023,,,,,,,,,,"0|z1k4ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/23 13:28;Zing;https://github.com/apache/spark/pull/42760;;;",,,,,,,,,,,,,,
Caught Hive MetaException attempting to get partition metadata by filter from Hive,SPARK-45040,13549339,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,potatoly,potatoly,31/Aug/23 23:01,31/Aug/23 23:02,30/Oct/23 17:26,,2.4.0,,,,,,,,,,,,,,,,,,,,,,,Java API,,,,,0,,,,,"We are integrating Spark 2.4 with our AWS Glue ETL job.
And we recently realized that a lot of our jobs are failed because of the below error:


{{Exception in User Class: java.lang.RuntimeException : Caught Hive MetaException attempting to get partition metadata by filter from Hive. You can set the Spark configuration setting spark.sql.hive.manageFilesourcePartitions to false to work around this problem, however this will result in degraded performance. Please report a bug: }}{{https://issues.apache.org/jira/browse/SPARK
}}

This error first happens from Aug 30th, and it occurs from time to time. It is gone for several hours and occurs again. During the time that the error occurs, most of the jobs fail, only few succeed. 

And we tried to set {{spark.sql.hive.manageFilesourcePartitions}} to false but it did not work out. Some other issue is coming out.

Can you look into the error and let me know if there is any work around to mitigate the issue? 

Let me know if you need anything from my end.",AWS Glue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/23 23:02;potatoly;Screenshot Capture - 2023-08-31 - 19-02-31.png;https://issues.apache.org/jira/secure/attachment/13062643/Screenshot+Capture+-+2023-08-31+-+19-02-31.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-31 23:01:34.0,,,,,,,,,,"0|z1k4og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'default' not found (state=08S01,code=0)",SPARK-45020,13549144,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,warriersruthi,warriersruthi,30/Aug/23 11:16,13/Sep/23 07:19,30/Oct/23 17:26,,3.1.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,pull-request-available,,,,"There is an alert that fires up when a Spark 3.1 cluster is created using shared metastore with Spark 2.4. The alert says DefaultDatabase does not exist. This is misleading and thus we need to suppress this alert. 
In the class SessionCatalog.scala, the method requireDbExists() is not handling the case when the db = defaultDB. This needs to be added to suppress this misleading alert. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-30 11:16:04.0,,,,,,,,,,"0|z1k3h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up fileserver when cleaning up files, jars and archives in SparkContext",SPARK-45014,13549096,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,30/Aug/23 05:43,31/Aug/23 06:21,30/Oct/23 17:26,31/Aug/23 06:21,4.0.0,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Connect,,,,,0,,,,,"In SPARK-44348, we clean up Spark Context's added files but we don't clean up the ones in fileserver.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 31 06:21:36 UTC 2023,,,,,,,,,,"0|z1k36g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/23 00:24;ignitetcbot;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/42731;;;","31/Aug/23 00:25;ggintegration;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/42731;;;","31/Aug/23 06:21;gurwls223;Issue resolved by pull request 42731
[https://github.com/apache/spark/pull/42731];;;",,,,,,,,,,,,
fix merged pull requests resolution,SPARK-45007,13548963,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,29/Aug/23 10:36,29/Aug/23 15:22,30/Oct/23 17:26,29/Aug/23 15:22,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Project Infra,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 29 15:22:01 UTC 2023,,,,,,,,,,"0|z1k2cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/23 15:22;dongjoon;Issue resolved by pull request 42722
[https://github.com/apache/spark/pull/42722];;;",,,,,,,,,,,,,,
Add support for rack information from an environment variable,SPARK-44992,13548880,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,holden,holden,28/Aug/23 20:04,28/Aug/23 20:04,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,This would allow us to use things like EC2_AVAILABILITY_ZONE for locality for Kube (or other clusters) which span multiple AZs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-28 20:04:59.0,,,,,,,,,,"0|z1k1ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark json schema inference and fromJson api having inconsistent behavior,SPARK-44991,13548876,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tenstriker,tenstriker,28/Aug/23 19:12,30/Aug/23 19:22,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Spark json reader can infer datatype of a fields. I am ingesting millions of datapoints and  generating a `DataFrameA`. what i notice that Schema inference mark datatype of a field with tons of Integers and Empty Strings as a Long. That is an okay behavior as I don't set `primitivesAsString` as I do want  primitive type inference. I store `DataFrameA` into `TableA` 

Now, this inference behavior is not respected by `fromJson` of `from_json` api when I am trying to write new data on `TableA`. Means, if I read a chunk of new input data into using `spark.read.schema(fromJson(getStruct(TableA)).json('/path/to/more/data')` reader complains that EmptyString cannot be cast to Long .

ps - `getStruct(TableA)` is psuedo method that returns `struct` of TableA schema somehow. and `/path/to/more/data` is new dataset which has some records with value for this fields as an empty string.

 

I think if reader doesn't complain about Empty string during schema inference it shouldn't complain either on reading without inference. May be treat Empty as Null just like during schema inference. Empty string is a legal value for String type field but not Number types fields so I don't see any reason not to treat it as a Null. Another option is to give additional reader option - treatEmptyAsNull so it's more explicit? 

ps - I marked it as bug but could be more suited as improvements.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-28 19:12:24.0,,,,,,,,,,"0|z1k1tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV conversion performance severely degraded for null fields,SPARK-44990,13548875,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,atulpayapilly_amazon,atulpayapilly_amazon,28/Aug/23 18:40,01/Sep/23 16:39,30/Oct/23 17:26,30/Aug/23 17:55,3.3.0,3.3.1,3.3.2,3.3.3,3.3.4,3.4.0,3.4.1,3.5.0,3.5.1,4.0.0,,,,,,,,,,3.3.4,3.4.2,3.5.0,,SQL,,,,,0,,,,," 
[https://github.com/apache/spark/pull/36110/files]
 introduced a SQLConf access in a critical section for every field processed in a record that is null.

This causes severe degradation of performance causing one workload that was completing in a couple of seconds to now take around 8 minutes.

This conf needs to be moved out of the critical path, there's no need for it to be in this location.

The version of Spark prior to this commit didn't exhibit the slowdown. I also generated a patch on an affected version with the suspected line removed and the problem went away.","Ran on Spark 3.3.1/EMR 6.10.0 with driver r5.xlarge and 4 x r5.16xlarge core nodes. The workload was:

spark.read.parquet(""<redacted HDFS location>"").repartition(100).write.format(""com.databricks.spark.csv"").option(""compression"",""gzip"").option(""header"", ""true"").option(""encoding"",""utf-8"").option(""charset"",""utf-8"").option(""escape"", """").option(""quote"", """").option(""quote"", ""\u0000"").option(""emptyValue"", """").option(""delimiter"", ""\t"").mode(""overwrite"").save(""<redacted HDFS location>"")

Input data contained 5 parquet data files 41MB each.

Most of the fields were null values.

Schema was very wide (1099 columns).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 01 16:39:22 UTC 2023,,,,,,,,,,"0|z1k1tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/23 17:55;dongjoon;Issue resolved by pull request 42738
[https://github.com/apache/spark/pull/42738];;;","31/Aug/23 04:28;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42744;;;","31/Aug/23 04:28;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42744;;;","01/Sep/23 16:39;atulpayapilly_amazon;Thanks for fixing this. I see that a null test was added and removed. While I agree that an all null test is not very meaningful a mostly null test is still valid and would be good to avoid this regression again.;;;",,,,,,,,,,,
"Parquet INT64 (TIMESTAMP(NANOS,false)) throwing Illegal Parquet type",SPARK-44988,13548859,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,flavioodas,flavioodas,28/Aug/23 15:47,09/Oct/23 09:56,30/Oct/23 17:26,,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,1,,,,,"This bug seems similar to https://issues.apache.org/jira/browse/SPARK-40819, except that it's a problem with INT64 (TIMESTAMP(NANOS,false)), instead of INT64 (TIMESTAMP(NANOS,true)).

The error happens whenever I'm trying to read:
{code:java}
org.apache.spark.sql.AnalysisException: Illegal Parquet type: INT64 (TIMESTAMP(NANOS,false)).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.illegalParquetTypeError(QueryCompilationErrors.scala:1762)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.illegalType$1(ParquetSchemaConverter.scala:206)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertPrimitiveField$2(ParquetSchemaConverter.scala:283)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertPrimitiveField(ParquetSchemaConverter.scala:224)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertField(ParquetSchemaConverter.scala:187)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3(ParquetSchemaConverter.scala:147)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3$adapted(ParquetSchemaConverter.scala:117)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.immutable.Range.foreach(Range.scala:158)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertInternal(ParquetSchemaConverter.scala:117)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convert(ParquetSchemaConverter.scala:87)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readSchemaFromFooter$2(ParquetFileFormat.scala:493)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readSchemaFromFooter(ParquetFileFormat.scala:493)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$2(ParquetFileFormat.scala:473)
	at scala.collection.immutable.Stream.map(Stream.scala:418)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:473)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:464)
	at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Oct 09 09:56:14 UTC 2023,,,,,,,,,,"0|z1k1ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/23 13:27;fanjia;Have you tried setting spark.sql.legacy.parquet.nanosAsLong to true？;;;","09/Oct/23 09:56;milesgranger;[~fanjia]that ""worked"" for me, but then of course need to cast the resulting bigint to a timestamp, which I feel is error prone. Would be nice if spark supported timestamp[ns] though.;;;",,,,,,,,,,,,,
Fix inherited namedtuples to work in createDataFrame,SPARK-44980,13548757,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,28/Aug/23 02:06,28/Aug/23 06:47,30/Oct/23 17:26,28/Aug/23 06:47,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Connect,PySpark,,,,0,,,,,"{code}
from collections import namedtuple
MyTuple = namedtuple(""MyTuple"", [""zz"", ""b"", ""a""])

class MyInheritedTuple(MyTuple):
    pass

df = spark.createDataFrame([MyInheritedTuple(1, 2, 3), MyInheritedTuple(11, 22, 33)])
df.collect()
{code}

{code}
[Row(zz=None, b=None, a=None), Row(zz=None, b=None, a=None)]
{code}

should be

{code}
[Row(zz=1, b=2, a=3), Row(zz=11, b=22, a=33)]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 28 06:47:42 UTC 2023,,,,,,,,,,"0|z1k134:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/23 06:47;gurwls223;Issue resolved by pull request 42693
[https://github.com/apache/spark/pull/42693];;;",,,,,,,,,,,,,,
Preserve full principal user name on executor side,SPARK-44976,13548705,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,eub,eub,26/Aug/23 02:55,28/Aug/23 12:46,30/Oct/23 17:26,,3.2.3,3.3.3,3.4.1,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"SPARK-6558 changes the behavior of {{Utils.getCurrentUserName()}} to use shortname instead of full principal name.
Due to this, it doesn't respect {{hadoop.security.auth_to_local}} rule on the side of non-kerberized hdfs namenode.
For example, I use 2 hdfs cluster. One is kerberized, the other one is not kerberized.
I make a rule to add some prefix to username on the non-kerberized cluster if some one access it from the kerberized cluster.


{code}
  <property>
    <name>hadoop.security.auth_to_local</name>
    <value xml:space=""preserve"">
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/(.+)@.*/_ex_$1/
RULE:[2:$1@$0](.*@EXAMPLE.COM)s/(.+)@.*/_ex_$1/
DEFAULT</value>
  </property>
{code}

However, if I submit spark job with keytab & principal option, hdfs directory and files ownership is not coherent.

(I change some words for privacy.)

{code}
$ hdfs dfs -ls hdfs:///user/eub/some/path/20230510/23
Found 52 items
-rw-rw-rw-   3 _ex_eub hdfs          0 2023-05-11 00:16 hdfs:///user/eub/some/path/20230510/23/_SUCCESS
-rw-r--r--   3 eub      hdfs  134418857 2023-05-11 00:15 hdfs:///user/eub/some/path/20230510/23/part-00000-b781be38-9dbc-41da-8d0e-597a7f343649-c000.txt.gz
-rw-r--r--   3 eub      hdfs  153410049 2023-05-11 00:16 hdfs:///user/eub/some/path/20230510/23/part-00001-b781be38-9dbc-41da-8d0e-597a7f343649-c000.txt.gz
-rw-r--r--   3 eub      hdfs  157260989 2023-05-11 00:16 hdfs:///user/eub/some/path/20230510/23/part-00002-b781be38-9dbc-41da-8d0e-597a7f343649-c000.txt.gz
-rw-r--r--   3 eub      hdfs  156222760 2023-05-11 00:16 hdfs:///user/eub/some/path/20230510/23/part-00003-b781be38-9dbc-41da-8d0e-597a7f343649-c000.txt.gz
{code}

Another interesting point is that if I submit spark job without keytab and principal option but with kerberos authentication with {{kinit}}, it will not follow {{hadoop.security.auth_to_local}} rule completely.

{code}
$ hdfs dfs -ls  hdfs:///user/eub/output/
Found 3 items
-rw-rw-r--+  3 eub hdfs          0 2023-08-25 12:31 hdfs:///user/eub/output/_SUCCESS
-rw-rw-r--+  3 eub hdfs        512 2023-08-25 12:31 hdfs:///user/eub/output/part-00000.gz
-rw-rw-r--+  3 eub hdfs        574 2023-08-25 12:31 hdfs:///user/eub/output/part-00001.gz
{code}


I finally found that if I submit spark job with {{--principal}} and {{--keytab}} option, ugi will be different.
(refer to https://github.com/apache/spark/blob/2583bd2c16a335747895c0843f438d0966f47ecd/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L905).

Only file ({{_SUCCESS}}) and output directory created by driver (application master side) will respect {{hadoop.security.auth_to_local}} on the non-kerberized namenode only if {{--principal}} and {{--keytab}] options are provided.

No matter how hdfs files or directory are created by executor or driver, those should respect {{hadoop.security.auth_to_local}} rule and should be the same.


Workaround is to pass additional argument to change {{SPARK_USER}} on the executor side.
e.g. {{--conf spark.executorEnv.SPARK_USER=_ex_eub}}

{{--conf spark.yarn.appMasterEnv.SPARK_USER=_ex_eub}} will make an error. There are some logics to append environment value with {{:}} (colon) as a separator.

- https://github.com/apache/spark/blob/4748d858b4478ea7503b792050d4735eae83b3cd/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L893
- https://github.com/apache/spark/blob/4748d858b4478ea7503b792050d4735eae83b3cd/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnSparkHadoopUtil.scala#L52
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Aug 26 14:25:40 UTC 2023,,,,,,,,,,"0|z1k0rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/23 04:54;eub;https://github.com/apache/spark/pull/42690;;;","26/Aug/23 14:25;eub;I think it is also related to https://issues.apache.org/jira/browse/SPARK-31551.;;;",,,,,,,,,,,,,
"CONV('-9223372036854775808', 10, -2) throws ArrayIndexOutOfBoundsException",SPARK-44973,13548693,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jira.shegalov,jira.shegalov,25/Aug/23 21:05,25/Aug/23 21:05,30/Oct/23 17:26,,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"
{code:scala}
scala> sql(s""SELECT CONV('${Long.MinValue}', 10, -2)"").show(false)
java.lang.ArrayIndexOutOfBoundsException: -1
  at org.apache.spark.sql.catalyst.util.NumberConverter$.convert(NumberConverter.scala:183)
  at org.apache.spark.sql.catalyst.expressions.Conv.nullSafeEval(mathExpressions.scala:463)
  at org.apache.spark.sql.catalyst.expressions.TernaryExpression.eval(Expression.scala:821)
  at org.apache.spark.sql.catalyst.expressions.ToPrettyString.eval(ToPrettyString.scala:57)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:81)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:91)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-25 21:05:37.0,,,,,,,,,,"0|z1k0ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[BUG Fix] PySpark StreamingQuerProgress fromJson ,SPARK-44971,13548686,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WweiL,WweiL,WweiL,25/Aug/23 18:35,31/Aug/23 00:30,30/Oct/23 17:26,31/Aug/23 00:30,3.5.0,3.5.1,,,,,,,,,,,,,,,,,,3.5.1,,,,Structured Streaming,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 31 00:30:55 UTC 2023,,,,,,,,,,"0|z1k0nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/23 00:30;gurwls223;Fixed in https://github.com/apache/spark/pull/42686;;;",,,,,,,,,,,,,,
Spark History File Uploads Can Fail on S3,SPARK-44970,13548679,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,holden,holden,holden,25/Aug/23 16:59,25/Aug/23 16:59,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,Sometimes if the driver OOMs the history log will not upload finish.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-25 16:59:31.0,,,,,,,,,,"0|z1k0ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Taking sum of two columns behaves differently from sum aggregation function,SPARK-44947,13548526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Invalid,,mroels,mroels,24/Aug/23 14:22,25/Aug/23 03:31,30/Oct/23 17:26,25/Aug/23 03:31,3.4.1,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"Taking the sum of two columns behaves differently when there are NULL values than taking the SUM of a column. This is odd and confusing for users

Reproducible example: 
{code:java}
$ from pyspark.sql import SparkSession
$ import pyspark.sql.functions as f
$ spark = SparkSession.builder.getOrCreate()

$ df = spark.createDataFrame([(1, 2), (2, None)], [""foo"", ""bar""])
$ df.show()
> 
+---+----+
|foo| bar|
+---+----+
|  1|   2|
|  2|null|
+---+----+

$ df.select(f.sum(""foo""), f.sum(""bar"")).show()
>
+--------+--------+
|sum(foo)|sum(bar)|
+--------+--------+
|       3|       2|
+--------+--------+

$ df.select((f.col(""foo"") + f.col(""bar"")).alias(""sum(foobar)"")).show()
> 
+-----------+
|sum(foobar)|
+-----------+
|          3|
|       null|
+-----------+

// I expected to get, but I was surprised to see the result above
+-----------+
|sum(foobar)|
+-----------+
|          3|
|          2|
+-----------+
{code}
 ","* Docker container: python:3.10-slim-bullseye
 * Java: openjdk-17-jre-headless
 * Spark 3.4.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 25 03:31:27 UTC 2023,,,,,,,,,,"0|z1jzns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/23 03:31;gurwls223;Simply `sum` is null tolerant but the arithmetic operators aren't. I believe all DBMSes work like that? ;;;",,,,,,,,,,,,,,
toPandas() gives FutureWarning when containing columns of datatype timestamp,SPARK-44946,13548525,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Cannot Reproduce,,mroels,mroels,24/Aug/23 13:57,25/Aug/23 05:00,30/Oct/23 17:26,25/Aug/23 03:33,3.4.1,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"When converting a Spark DataFrame into a pandas DataFrame, we get a FutureWarning when the DataFrame contains columns of type {{timestamp. }}

Reproducible example (that you can run locally):
{code:java}
from datetime import datetime

from pyspark.sql import SparkSession
import pandas as pd

spark = SparkSession.builder.getOrCreate()

df = pd.DataFrame({""foo"": [datetime(2023, 1, 1), datetime(2023, 1, 1)]})

df_sp = spark.createDataFrame(df)

test = df_sp.toPandas()

// warning logs: 
/usr/local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead

{code}
Note that if we enable arrow (by setting {{{}config(""spark.sql.execution.arrow.pyspark.enabled"", ""true""){}}}), this warning is gone! Although I admit that I have seen it popping up once with arrow enabled too, but I could not create a reproducible example out of that. 

 

Note that this basically means that I cannot use Spark with pandas 2.0 without Arrow enabled... ","For my test, I ran it in a docker container: 
 * Python version: python 3.10 (base image python:3.10-slim-bullseye)
 * Java: openjdk-17-jre-headless
 * Spark: 3.4.1
 * pandas: 1.5.3
 * pyarrow: 11.0.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 25 05:00:45 UTC 2023,,,,,,,,,,"0|z1jznk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/23 03:33;gurwls223;This will be addressed together at https://issues.apache.org/jira/browse/SPARK-44101;;;","25/Aug/23 03:33;gurwls223;and actually this is fixed in the master branch.;;;","25/Aug/23 04:53;mroels;Nice! So pandas 2.0 support will only be available in Spark 4.0 then (if I understand the ticket you linked correctly)?  ;;;","25/Aug/23 05:00;gurwls223;Yup, correct. The change is a bit invasive so won't be fixed in 3.5.;;;",,,,,,,,,,,
Automate PySpark error class documentation,SPARK-44945,13548519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,24/Aug/23 13:16,28/Aug/23 04:14,30/Oct/23 17:26,28/Aug/23 04:14,3.5.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Documentation,PySpark,,,,0,,,,,We need to automate the process for PySpark error class documentation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 28 04:14:19 UTC 2023,,,,,,,,,,"0|z1jzm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/23 04:14;gurwls223;Issue resolved by pull request 42658
[https://github.com/apache/spark/pull/42658];;;",,,,,,,,,,,,,,
"CONV produces incorrect result near Long.MIN_VALUE, fails to detect overflow",SPARK-44943,13548480,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jira.shegalov,jira.shegalov,24/Aug/23 07:49,24/Aug/23 09:20,30/Oct/23 17:26,,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Signed conversion does not detect overflow 
{code:java}
>>> spark.conf.set('spark.sql.ansi.enabled', True)
>>> sql(""SELECT conv('-9223372036854775809', 10, -10)"").show(truncate=False)
+-----------------------------------+
|conv(-9223372036854775809, 10, -10)|
+-----------------------------------+
|-9223372036854775807               |
+-----------------------------------+
{code}

Unsigned conversion produces -1 but does not throw in the ANSI mode
{code}
>>> sql(""SELECT conv('-9223372036854775809', 10, 10)"").show(truncate=False)
+----------------------------------+
|conv(-9223372036854775809, 10, 10)|
+----------------------------------+
|18446744073709551615              |
+----------------------------------+
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 24 09:20:15 UTC 2023,,,,,,,,,,"0|z1jzdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/23 09:19;githubbot;User 'gerashegalov' has created a pull request for this issue:
https://github.com/apache/spark/pull/42652;;;","24/Aug/23 09:20;githubbot;User 'gerashegalov' has created a pull request for this issue:
https://github.com/apache/spark/pull/42652;;;",,,,,,,,,,,,,
"Improve performance of JSON parsing when ""spark.sql.json.enablePartialResults"" is enabled",SPARK-44940,13548471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,24/Aug/23 06:38,26/Sep/23 20:20,30/Oct/23 17:26,04/Sep/23 21:37,3.4.0,3.5.0,4.0.0,,,,,,,,,,,,,,,,,3.4.2,3.5.0,,,SQL,,,,,0,correctness,pull-request-available,,,"Follow-up on https://issues.apache.org/jira/browse/SPARK-40646.

I found that JSON parsing is significantly slower due to exception creation in control flow. Also, some fields are not parsed correctly and the exception is thrown in certain cases: 
{code:java}
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.util.GenericArrayData cannot be cast to org.apache.spark.sql.catalyst.InternalRow
	at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getStruct(rows.scala:51)
	at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getStruct$(rows.scala:51)
	at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getStruct(rows.scala:195)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:590)
	... 39 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,SPARK-40646,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 26 20:19:49 UTC 2023,,,,,,,,,,"0|z1jzbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/23 06:51;ivan.sadikov;I have prototyped the fix and will open a PR shortly.;;;","25/Aug/23 02:29;ivan.sadikov;Opened https://github.com/apache/spark/pull/42667.;;;","31/Aug/23 04:12;snoot;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/42667;;;","04/Sep/23 04:05;snoot;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/42790;;;","04/Sep/23 04:06;snoot;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/42790;;;","04/Sep/23 04:21;snoot;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/42792;;;","04/Sep/23 21:37;dongjoon;Issue resolved by pull request 42790
[https://github.com/apache/spark/pull/42790];;;","04/Sep/23 21:39;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/42792;;;","26/Sep/23 20:19;tgraves; I noticed this went into 3.5.0  ([https://github.com/apache/spark/commits/v3.5.0)] so updating the fixed versions.;;;",,,,,,
Fix `RELEASE` file to have the correct information in Docker images,SPARK-44935,13548438,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,23/Aug/23 21:23,23/Aug/23 23:01,30/Oct/23 17:26,23/Aug/23 23:01,2.4.8,3.0.3,3.1.3,3.2.4,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,3.3.4,3.4.2,3.5.0,4.0.0,Kubernetes,,,,,0,,,,,"{code}
$ docker run -it --rm apache/spark:latest ls -al /opt/spark/RELEASE
-rw-r--r-- 1 spark spark 0 Jun 25 03:13 /opt/spark/RELEASE

$ docker run -it --rm apache/spark:v3.1.3 ls -al /opt/spark/RELEASE | tail -n1
-rw-r--r-- 1 root root 0 Feb 21  2022 /opt/spark/RELEASE
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 23 23:01:39 UTC 2023,,,,,,,,,,"0|z1jz48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 23:01;dongjoon;Issue resolved by pull request 42636
[https://github.com/apache/spark/pull/42636];;;",,,,,,,,,,,,,,
PushdownPredicatesAndPruneColumnsForCTEDef creates invalid plan when called over CTE with duplicate attributes,SPARK-44934,13548434,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,wenyuen-db,wenyuen-db,wenyuen-db,23/Aug/23 20:47,24/Aug/23 17:57,30/Oct/23 17:26,24/Aug/23 15:06,3.3.3,3.4.1,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Optimizer,,,,,0,,,,,"When running the query
{code:java}
with cte as (
 select c1, c1, c2, c3 from t where random() > 0
)
select cte.c1, cte2.c1, cte.c2, cte2.c3 from
 (select c1, c2 from cte) cte
 inner join
 (select c1, c3 from cte) cte2
 on cte.c1 = cte2.c1 {code}
 
The query fails with the error
{code:java}
org.apache.spark.scheduler.DAGScheduler: Failed to update accumulator 9523 (Unknown class) for task 1
org.apache.spark.SparkException: attempted to access non-existent accumulator 9523{code}
Further investigation shows that the rule PushdownPredicatesAndPruneColumnsForCTEDef creates an invalid plan when the output of a CTE contains duplicate expression IDs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 24 15:06:56 UTC 2023,,,,,,,,,,"0|z1jz3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/23 15:06;ptoth;Issue resolved by pull request 42635
[https://github.com/apache/spark/pull/42635];;;",,,,,,,,,,,,,,
Spark structured streaming performance regression in latency times reading/writing to kafka since 3.0.2,SPARK-44933,13548431,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ebaggott,ebaggott,23/Aug/23 19:49,23/Aug/23 20:33,30/Oct/23 17:26,,2.4.8,3.0.2,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,,,,,,,,Structured Streaming,,,,,0,,,,," During a migration from spark 2.4.4 to spark 3.4.0 I have noticed slower latency times in spark structured streaming when reading and writing to kafka. I have tested using both CONTINUOUS and MICROBATCH.

In simple read and write to kafka using CONTINUOUS mode in spark 2.4.4 I usually see latency times of ~5ms in our appllication. When moving to spark 3.4.0 this increased to ~15ms.

I stripped it back to a very simple test where I send 2 data fields in csv format to a kafka topic using a simple producer. Then I have a simple consumer which reads from the input topic and writes to an output topic. The 2 fields are an ID and an amount value. I read from both topics and retrieve the kafka timestamp value for all rows. I then subtract the input timestamp from the output timestamp to get the latency. To keep things as simple as possible I am using 1 kafka partition and I am using local[1] as the spark master.

Version    latency (ms)    Trigger
2.4.4    3.25    CONTINUOUS
3.4.0    7.23    CONTINUOUS
2.4.4    640    MICROBATCH
3.4.0    693    MICROBATCH
I have tried all versions of spark 3.x and I believe this issue was introduced in 3.0.2. I also tried different versions of spark 2.4.x and I see the same behaviour when going from 2.4.7 to 2.4.8.

In the simple test I only use a few jars. One of these is spark-sql-kafka-0-10_2.12 When running on spark 3.0.2 using the 3.0.2 version of this jar I see the slower times. When I run again on spark 3.0.2 and use the 3.0.1 version of this jar I see the faster times.

The same thing happens between 2.4.7 version and the 2.4.8 version. The 2.4.8 version has the slower times.

Has anyone else observed a slow down in latency in structured streaming when reading from kafka ?

Are there any settings I need to change when moving to these versions ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-23 19:49:51.0,,,,,,,,,,"0|z1jz2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Continuous Structured Streaming not reporting streaming metrics,SPARK-44932,13548418,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,bryanqiang,bryanqiang,23/Aug/23 16:22,23/Aug/23 16:25,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,Structured Streaming,,,,,0,,,,,"Hello, we've been running spark continuous structured streaming on standalone cluster and happy with the performance however we noticed streaming metrics like input rate and process rate are not updated by the `ProgressReporter` in `ContinuousExecution` because the [`finishTrigger`|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala#L293] function is never invoked in `ContinuousExecution`. I'm wondering why and how may get metrics like in micro-batch structured streaming.

 

!https://preview.redd.it/mzh4oc0cbojb1.png?width=1901&format=png&auto=webp&s=8d649ae515e6adb7d6ce853802e0a4134c9fa277!!https://preview.redd.it/8ou3uyuofojb1.png?width=1523&format=png&auto=webp&s=ac6bf7fa05cb90b09cb10b9ac815f64b5e97175e!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-23 16:22:52.0,,,,,,,,,,"0|z1jyzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix JSON Serailization for Spark Connect Event Listener,SPARK-44931,13548366,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,grundprinzip-db,grundprinzip-db,23/Aug/23 10:29,25/Aug/23 03:34,30/Oct/23 17:26,25/Aug/23 03:34,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,SPARK-44861,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 23 15:03:12 UTC 2023,,,,,,,,,,"0|z1jyo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 15:03;ggintegration;User 'grundprinzip' has created a pull request for this issue:
https://github.com/apache/spark/pull/42630;;;",,,,,,,,,,,,,,
K8s default service token file should not be materialized into token,SPARK-44925,13548314,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,23/Aug/23 03:08,23/Aug/23 05:52,30/Oct/23 17:26,23/Aug/23 05:52,2.4.8,3.0.3,3.1.3,3.2.4,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,3.3.4,3.4.2,3.5.0,4.0.0,Kubernetes,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 23 05:52:24 UTC 2023,,,,,,,,,,"0|z1jyco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 03:20;snoot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/42624;;;","23/Aug/23 03:20;snoot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/42624;;;","23/Aug/23 05:52;dongjoon;Issue resolved by pull request 42624
[https://github.com/apache/spark/pull/42624];;;",,,,,,,,,,,,
Disable o.a.p.h.InternalParquetRecordWriter logs for tests to reduce the log volume,SPARK-44922,13548309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,23/Aug/23 01:51,23/Aug/23 05:51,30/Oct/23 17:26,23/Aug/23 05:51,4.0.0,,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,,,Tests,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 23 05:51:07 UTC 2023,,,,,,,,,,"0|z1jybk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 05:51;yao;Issue resolved by https://github.com/apache/spark/pull/42614;;;",,,,,,,,,,,,,,
Spark 3.4 multi-column sum slows with many columns,SPARK-44912,13548238,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,brbickel,brbickel,22/Aug/23 14:16,11/Sep/23 19:43,30/Oct/23 17:26,11/Sep/23 19:43,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"The code below is a minimal reproducible example of an issue I discovered with Pyspark 3.4.x. I want to sum the values of multiple columns and put the sum of those columns (per row) into a new column. This code works and returns in a reasonable amount of time in Pyspark 3.3.x, but is extremely slow in Pyspark 3.4.x when the number of columns grows. See below for execution timing summary as N varies.
{code:java}
import pyspark.sql.functions as F
import random
import string
from functools import reduce
from operator import add
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# generate a dataframe N columns by M rows with random 8 digit column 
# names and random integers in [-5,10]
N = 30
M = 100
columns = [''.join(random.choices(string.ascii_uppercase +
                                  string.digits, k=8))
           for _ in range(N)]
data = [tuple([random.randint(-5,10) for _ in range(N)])
        for _ in range(M)]

df = spark.sparkContext.parallelize(data).toDF(columns)
# 3 ways to add a sum column, all of them slow for high N in spark 3.4
df = df.withColumn(""col_sum1"", sum(df[col] for col in columns))
df = df.withColumn(""col_sum2"", reduce(add, [F.col(col) for col in columns]))
df = df.withColumn(""col_sum3"", F.expr(""+"".join(columns))) {code}
Timing results for Spark 3.3:
||N||Exe Time (s)||
|5|0.514|
|10|0.248|
|15|0.327|
|20|0.403|
|25|0.279|
|30|0.322|
|50|0.430|

Timing results for Spark 3.4:
||N||Exe Time (s)||
|5|0.379|
|10|0.318|
|15|0.405|
|20|1.32|
|25|28.8|
|30|448|
|50|>10000 (did not finish)|",,,,,,,,,,,,,,,,SPARK-45071,,,SPARK-45071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 11 19:43:46 UTC 2023,,,,,,,,,,"0|z1jxvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/23 17:15;bersprockets;It looks like this was fixed with SPARK-45071. Your issue was reported earlier, but missed somehow.;;;","11/Sep/23 19:43;brbickel;Verified build containing linked issue fix solved the problem.;;;",,,,,,,,,,,,,
Encoders.bean does not support superclasses with generic type arguments,SPARK-44910,13548200,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gbloisi-openaire,gbloisi-openaire,gbloisi-openaire,22/Aug/23 11:54,19/Sep/23 04:39,30/Oct/23 17:26,19/Sep/23 04:37,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,,3.4.2,3.5.1,4.0.0,,SQL,,,,,0,pull-request-available,,,,"As per SPARK-44634 another unsupported feature of bean encoder is when the superclass of the bean has generic type arguments. For example:
{code:java}
class JavaBeanWithGenericsA<T> {
    public T getPropertyA() {
        return null;
    }

    public void setPropertyA(T a) {

    }
}

class JavaBeanWithGenericBase extends JavaBeanWithGenericsA<String> {
}

Encoders.bean(JavaBeanWithGenericBase.class); // Exception

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 19 04:39:19 UTC 2023,,,,,,,,,,"0|z1jxnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/23 16:52;ignitetcbot;User 'gbloisi-openaire' has created a pull request for this issue:
https://github.com/apache/spark/pull/42634;;;","19/Sep/23 04:37;dongjoon;Issue resolved by pull request 42634
[https://github.com/apache/spark/pull/42634];;;","19/Sep/23 04:39;dongjoon;This landed at branch-3.4 via https://github.com/apache/spark/pull/42914;;;",,,,,,,,,,,,
"Fix spark connect ML crossvalidator ""foldCol"" param",SPARK-44908,13548167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,weichenxu123,weichenxu123,weichenxu123,22/Aug/23 08:45,23/Aug/23 10:19,30/Oct/23 17:26,23/Aug/23 10:19,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Connect,ML,,,,0,,,,,"Fix spark connect ML crossvalidator ""foldCol"" param.

 

Currently it calls `df.rdd` APIs but it is not supported in spark connect",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 23 10:19:50 UTC 2023,,,,,,,,,,"0|z1jxg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/23 09:03;githubbot;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/42605;;;","23/Aug/23 10:19;weichenxu123;Issue resolved by pull request 42605
[https://github.com/apache/spark/pull/42605];;;",,,,,,,,,,,,,
`DataFrame.join` should throw IllegalArgumentException for invalid join types,SPARK-44907,13548155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,podongfeng,podongfeng,podongfeng,22/Aug/23 07:20,23/Aug/23 00:01,30/Oct/23 17:26,23/Aug/23 00:01,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Connect,PySpark,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 23 00:01:45 UTC 2023,,,,,,,,,,"0|z1jxdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 00:01;podongfeng;Issue resolved by pull request 42603
[https://github.com/apache/spark/pull/42603];;;",,,,,,,,,,,,,,
NullPointerException on stateful expression evaluation,SPARK-44905,13548142,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,22/Aug/23 05:46,23/Aug/23 05:49,30/Oct/23 17:26,23/Aug/23 05:49,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 23 05:49:16 UTC 2023,,,,,,,,,,"0|z1jxao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/23 09:11;githubbot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42601;;;","23/Aug/23 05:49;yao;Issue resolved by https://github.com/apache/spark/pull/42601;;;",,,,,,,,,,,,,
The precision of LongDecimal is inconsistent with Hive.,SPARK-44902,13548125,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,wforget,wforget,22/Aug/23 02:08,29/Aug/23 07:26,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"The precision of LongDecimal in Hive is 19 but it is 20 in Spark. This leads to type conversion errors in some cases.

 

Relevant code:

[https://github.com/apache/spark/blob/4646991abd7f4a47a1b8712e2017a2fae98f7c5a/sql/api/src/main/scala/org/apache/spark/sql/types/DecimalType.scala#L129|https://github.com/apache/spark/blob/4646991abd7f4a47a1b8712e2017a2fae98f7c5a/sql/api/src/main/scala/org/apache/spark/sql/types/DecimalType.scala#L129C51-L129C51]

[https://github.com/apache/hive/blob/3d3acc7a19399d749a39818573a76a0dbbaf2598/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/HiveDecimalUtils.java#L76]

 

Reproduce:

create table and view in hive:
{code:java}
create table t (value bigint);
create view v as select value * 0.1 from t; {code}
read in spark:
{code:java}
select * from v; {code}
error occurred:
{code:java}
org.apache.spark.sql.AnalysisException: [CANNOT_UP_CAST_DATATYPE] Cannot up cast `(value * 0.1)` from ""DECIMAL(22,1)"" to ""DECIMAL(21,1)"".The type path of the target object is:
You can either add an explicit cast to the input data or choose a higher precision type of the field in the target object	at org.apache.spark.sql.errors.QueryCompilationErrors$.upCastFailureError(QueryCompilationErrors.scala:285)	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveUpCast$$fail(Analyzer.scala:3627)	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$57$$anonfun$applyOrElse$235.applyOrElse(Analyzer.scala:3658)	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$57$$anonfun$applyOrElse$235.applyOrElse(Analyzer.scala:3635) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 29 07:26:22 UTC 2023,,,,,,,,,,"0|z1jx6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/23 07:26;wforget;I tried changing it to 19 but some test cases failed.

https://github.com/wForget/spark/actions/runs/5934710599;;;",,,,,,,,,,,,,,
Cached DataFrame keeps growing,SPARK-44900,13548114,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,,,varun2807,varun2807,21/Aug/23 23:26,31/Aug/23 03:20,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,1,,,,,"Scenario :

We have a kafka streaming application where the data lookups are happening by joining  another DF which is cached, and the caching strategy is MEMORY_AND_DISK.

However the size of the cached DataFrame keeps on growing for every micro batch the streaming application process and that's being visible under storage tab.

A similar stack overflow thread was already raised.

https://stackoverflow.com/questions/55601779/spark-dataframe-cache-keeps-growing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 31 03:20:53 UTC 2023,,,,,,,,,,"0|z1jx4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 17:14;varun2807;[~yao] , is there a way I could prioritize this issue as it's causing us production impact ?;;;","24/Aug/23 02:34;yao;How about releasing the cached rdds if you never touch it again;;;","24/Aug/23 02:39;varun2807;[~yao]  Thanks for the comment. However we can't release as cached RDD's are being used in every micro batch that's why could not unpersist.;;;","24/Aug/23 06:01;yao;Please note that there is a storage limit in place. Adding items without removing any may lead to cache evictions and recomputations. In such cases, caching may not be as effective as direct computation, as extra write paths are introduced. Probably, you should optimize your program.

 

 ;;;","24/Aug/23 17:45;varun2807;We do not keep on adding , they are existing (RDD ID's also wont change) cached RDD's which keeps on growing, tested with spark3 as well, seeing same behavior and also changed the persisting strategy to just MEMORY instead of MEMORY_AND_DISK, but no luck.

 ;;;","25/Aug/23 07:54;yao;Set spark.cleaner.periodicGC.interval to a smaller value(3min) might help;;;","25/Aug/23 18:37;yaud;[~yao] we tried spark.cleaner.periodicGC.interval=1min but it didn't help. 

Here are my observations:
 * this happens even in a very simple scenario - see the example to reproduce below
 * it happens after join
 * uncontrollable growth of disk usage starts only when any portion of RDD got spilled to disk
 * if cached RDD remains 100% in memory this issue doesn't happen
 * when an executor dies then ""Size on Disk"" on Storage tab gets reduced by the amount of storage blocks held by that dead executor (makes sense)

It looks like some storage blocks (shuffle blocks?) are being tracked under that cached RDD and never (or at least not in a reasonable time) released until the executor dies.

Our worry is whether it is just disk size usage tracking bug or those blocks are actually kept on the disk because our production job disk usage (per Spark UI) grew by 6TB in a span of 10 hours.

Here's the code to reproduce:
{code:java}
val conf = new SparkConf().set(""spark.master"", ""yarn"")
val spark = SparkSession.builder().config(conf).getOrCreate()

import spark.implicits._

val sc = spark.sparkContext
val ssc = new StreamingContext(sc, Seconds(10))
// create a pseudo stream
val rddQueue = new mutable.Queue[RDD[Long]]()
val stream = ssc.queueStream(rddQueue, oneAtATime = true)
// create a simple lookup table
val lookup = sc.range(start = 0, end = 50000000, numSlices = 10)
    .toDF(""id"")
    .withColumn(""value"", md5(rand().cast(StringType)))
    .cache()
// for every micro-batch perform value lookup via join
stream.foreachRDD { rdd =>
  val df = rdd.toDF(""id"")
  df.join(lookup, Seq(""id""), ""leftouter"").count()
}
// run the streaming
ssc.start()
for (_ <- 1 to 1000000) {
  rddQueue.synchronized {
    val firstId = Random.nextInt(50000000)
    rddQueue += sc.range(start = firstId, end = firstId + 10000, numSlices = 4)
  }
  Thread.sleep(10)
}
ssc.stop() {code}
Submit parameters (selected to create storage memory deficit and cause cache to be spilled):
{code:java}
--executor-cores 2 --num-executors 5 --executor-memory 1250m --driver-memory 1g \
--conf spark.dynamicAllocation.enabled=false --conf spark.sql.shuffle.partitions=10 {code}
When executed, disk usage of that cached lookup DF grows really fast.

Same thing happens in Spark 2.4 and 3.3;;;","28/Aug/23 19:20;varun2807;[~yao] hope you got a chance to look into what [~yaud] mentioned.;;;","29/Aug/23 04:57;yxzhang;Hi [~varun2807]  [~yaud]  did you check the actual cached file size on disk, on the yarn node manager local filesystem?  Is it really ever growing?;;;","29/Aug/23 05:09;yaud;[~yxzhang] looks like it is just disk usage tracking issue as disk space is not used as much.

However it affects effectiveness of cached data since Spark spills it to disk as it believes it doesn't fit memory anymore so eventually it becomes 100% stored on disk.;;;","31/Aug/23 03:20;varun2807;[~yxzhang] / [~yao] any update for us ?;;;",,,,
Local Property Propagation to Subquery Broadcast Exec,SPARK-44897,13548099,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mikechen,mikechen,mikechen,21/Aug/23 16:49,28/Aug/23 02:57,30/Oct/23 17:26,28/Aug/23 02:57,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"https://issues.apache.org/jira/browse/SPARK-32748 was opened and then I believe mistakenly reverted to address this issue. The claim was local properties propagation in SubqueryBroadcastExec to the dynamic pruning thread is not necessary because they will be propagated by broadcast threads anyways. However, in a scenario where the dynamic pruning thread is first to initialize the broadcast relation future, the local properties will not be propagated correctly. This is because the local properties being propagated to the broadcast threads would already be incorrect.
I do not have a good way of reproducing this consistently because generally the SubqueryBroadcastExec is not the first to initialize the broadcast relation future, but by adding a Thread.sleep(1) into the doPrepare method of SubqueryBroadcastExec, the following test always fails.
{code:java}
withSQLConf(StaticSQLConf.SUBQUERY_BROADCAST_MAX_THREAD_THRESHOLD.key -> ""1"") {
  withTable(""a"", ""b"") {
    val confKey = ""spark.sql.y""
    val confValue1 = UUID.randomUUID().toString()
    val confValue2 = UUID.randomUUID().toString()
    Seq((confValue1, ""1"")).toDF(""key"", ""value"")
      .write
      .format(""parquet"")
      .partitionBy(""key"")
      .mode(""overwrite"")
      .saveAsTable(""a"")
    val df1 = spark.table(""a"")

    def generateBroadcastDataFrame(confKey: String, confValue: String): Dataset[String] = {
      val df = spark.range(1).mapPartitions { _ =>
        Iterator(TaskContext.get.getLocalProperty(confKey))
      }.filter($""value"".contains(confValue)).as(""c"")
      df.hint(""broadcast"")
    }

    // set local property and assert
    val df2 = generateBroadcastDataFrame(confKey, confValue1)
    spark.sparkContext.setLocalProperty(confKey, confValue1)
    val checkDF = df1.join(df2).where($""a.key"" === $""c.value"").select($""a.key"", $""c.value"")
    val checks = checkDF.collect()
    assert(checks.forall(_.toSeq == Seq(confValue1, confValue1)))

    // change local property and re-assert
    Seq((confValue2, ""1"")).toDF(""key"", ""value"")
      .write
      .format(""parquet"")
      .partitionBy(""key"")
      .mode(""overwrite"")
      .saveAsTable(""b"")
    val df3 = spark.table(""b"")
    val df4 = generateBroadcastDataFrame(confKey, confValue2)
    spark.sparkContext.setLocalProperty(confKey, confValue2)
    val checks2DF = df3.join(df4).where($""b.key"" === $""c.value"").select($""b.key"", $""c.value"")
    val checks2 = checks2DF.collect()
    assert(checks2.forall(_.toSeq == Seq(confValue2, confValue2)))
    assert(checks2.nonEmpty)
  }
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 28 02:57:40 UTC 2023,,,,,,,,,,"0|z1jx14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/23 02:57;cloud_fan;Issue resolved by pull request 42587
[https://github.com/apache/spark/pull/42587];;;",,,,,,,,,,,,,,
Need to update the golden files of SQLQueryTestSuite for Java 21.,SPARK-44888,13548016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,21/Aug/23 04:44,21/Aug/23 10:02,30/Oct/23 17:26,21/Aug/23 10:02,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,Tests,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 21 10:02:34 UTC 2023,,,,,,,,,,"0|z1jwio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/23 10:02;LuciferYang;Issue resolved by pull request 42580
[https://github.com/apache/spark/pull/42580];;;",,,,,,,,,,,,,,
NullPointerException is thrown when column with ROWID type contains NULL values,SPARK-44885,13547998,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tnieradzik,tnieradzik,tnieradzik,20/Aug/23 19:47,22/Aug/23 10:49,30/Oct/23 17:26,22/Aug/23 10:48,3.4.1,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Spark Core,SQL,,,,0,,,,,"A row ID may be NULL in an Oracle table. When this is the case, the following exception is thrown:

{{[info] Cause: java.lang.NullPointerException:}}
{{{}[info] at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$12(JdbcUtils.scala:452){}}}{{{}[info] at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$12$adapted(JdbcUtils.scala:451){}}}
{{{}[info] at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:361){}}}{{{}[info] at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:343){}}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Scala,,,,Tue Aug 22 10:48:30 UTC 2023,,,,,,,,,,"0|z1jweo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/23 19:54;tnieradzik;PR submitted: https://github.com/apache/spark/pull/42576;;;","22/Aug/23 10:48;gurwls223;Issue resolved by pull request 42576
[https://github.com/apache/spark/pull/42576];;;",,,,,,,,,,,,,
Spark doesn't create SUCCESS file in Spark 3.3.0+ when partitionOverwriteMode is dynamic,SPARK-44884,13547988,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dipayandev,dipayandev,20/Aug/23 12:34,31/Aug/23 14:27,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"The issue is not happening in Spark 2.x (I am using 2.4.0), but only in 3.3.0 (tested with 3.4.1 as well)

Code to reproduce the issue

 
{code:java}
scala> spark.conf.set(""spark.sql.sources.partitionOverwriteMode"", ""dynamic"") 
scala> val DF = Seq((""test1"", 123)).toDF(""name"", ""num"")
scala> DF.write.option(""path"", ""gs://test_bucket/table"").mode(""overwrite"").partitionBy(""num"").format(""orc"").saveAsTable(""test_schema.test_tb1"") {code}
 

The above code succeeds and creates external Hive table, but {*}there is no SUCCESS file generated{*}.

Adding the content of the bucket after table creation

!image-2023-08-25-13-01-42-137.png|width=500,height=130!

 The same code when running with spark 2.4.0 (with or without external path), generates the SUCCESS file.
{code:java}
scala> DF.write.mode(SaveMode.Overwrite).partitionBy(""num"").format(""orc"").saveAsTable(""test_schema.test_tb1""){code}
!image-2023-08-20-18-46-53-342.png|width=465,height=166!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/23 13:16;dipayandev;image-2023-08-20-18-46-53-342.png;https://issues.apache.org/jira/secure/attachment/13062293/image-2023-08-20-18-46-53-342.png","25/Aug/23 07:31;dipayandev;image-2023-08-25-13-01-42-137.png;https://issues.apache.org/jira/secure/attachment/13062437/image-2023-08-25-13-01-42-137.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 31 14:27:51 UTC 2023,,,,,,,,,,"0|z1jwcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/23 17:38;stevel@apache.org;this is created in the committer; for hadoop-mapreduce ones  ""mapreduce.fileoutputcommitter.marksuccessfuljobs""; is the flag to enable this; if it is not being created then it'll be down to how saveAsTable commits work;;;","21/Aug/23 18:10;dipayandev;[~stevel@apache.org] , have set that also, but still no _SUCCESS file when we pass an external path. I am not using any custom committer. Its the default Hadoop-mapreduce one. Can you please point me to the code? 
{code:java}
spark.conf.set(""spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs"", true) {code};;;","21/Aug/23 18:13;dipayandev;There is no reason to disable this feature in Spark 3.3.0. There can be lots of downstream applications that are dependent on the _SUCCESS file and this feature change wasn't mention anywhere in the release. Any workaround for this? [~stevel@apache.org] ;;;","22/Aug/23 16:00;stevel@apache.org;[~dipayandev] i don't think think anyone has disabled the option; doesn't surface in my test setup (manifest and s3a committers).

Afraid you are going to have to debug it yourself, as it is your env which has the problem.

does everything work if you use .saveAs() rather than .saveAsTable()?;;;","22/Aug/23 16:09;dipayandev;[~stevel@apache.org] , I am running on DataProc but I am able to replicate the same from my local machine as well.

_SUCCESS file is created
 * Spark 2.x (2.4.0 I am using) with .saveAsTable() with or without external path.

_SUCCESS file is not created
 * Spark 3.3.0 with .saveAsTable() with or without external path.

As mentioned, I have set the following config, but no help.
spark.conf.set(""spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs"", true) 
Are you able to replicate the issue with the snippet I have shared or the _SUCCESS file is generating at your end when an external path is passed?;;;","23/Aug/23 17:23;stevel@apache.org;i'm not trying to replicate it; i have too many other things to do. in open source, sadly, everyone gets to fend for themselves, and i'm not actually a spark developer. i'd suggest looking at what changed in .saveAsTable to see what possible changes may be to blame...;;;","25/Aug/23 08:24;dipayandev;I am able to narrow down the problem. This issue is happening when we're setting  *_partitionOverwriteMode to dynamic._  __* This config is required to do incremental updates.
{code:java}
spark.conf.set(""spark.sql.sources.partitionOverwriteMode"", ""dynamic"").{code}
When I am setting it to 'static' , __SUCCESS_ file is generated.;;;","25/Aug/23 14:11;stevel@apache.org;so using insert overwrite. yes, what happens there is that the entire job is written to a temporary subdir, and on job complete the partition directories are updated, but not any files on the root path created by job committer itself;;;","25/Aug/23 14:19;dipayandev;Right, the behaviour is same in Spark 2 and 3. However, in Spark 2.x after renaming the temporary subdir, it writes the _SUCCESS file on the root path but NOT in Spark 3.x when that param is passed. 

I see this part of the code ([Hadoop Committer|https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java#L433[]]) is not changed in the latest hadoop-mapreduce, but somewhere probably _partitionOverwriteMode_ {color:#172b4d}option is broken when passed from latest Spark Dataframewriter. {color}

In Spark 2.x, the _SUCCESS file gets updated everytime you do insert overwrite. 

 ;;;","31/Aug/23 14:27;medb;Is this a duplicate of the https://issues.apache.org/jira/browse/SPARK-35279?;;;",,,,,
Spark insertInto with location GCS bucket root causes NPE,SPARK-44883,13547984,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,dipayandev,dipayandev,20/Aug/23 11:00,21/Aug/23 11:04,30/Oct/23 17:26,21/Aug/23 11:04,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"In our Organisation, we are using GCS bucket root location to point to our Hive table. Dataproc's latest 2.1 uses *Spark* *3.3.0* and this needs to be fixed.

Spark Scala code to reproduce this issue
{noformat}
val DF = Seq((""test1"", 123)).toDF(""name"", ""num"")
DF.write.option(""path"", ""gs://test_dd123/"").mode(SaveMode.Overwrite).partitionBy(""num"").format(""orc"").saveAsTable(""schema_name.table_name"")


val DF1 = Seq((""test2"", 125)).toDF(""name"", ""num"")
DF.write.mode(SaveMode.Overwrite).format(""orc"").insertInto(""schema_name.table_name"")


java.lang.NullPointerException
  at org.apache.hadoop.fs.Path.<init>(Path.java:141)
  at org.apache.hadoop.fs.Path.<init>(Path.java:120)
  at org.apache.hadoop.fs.Path.suffix(Path.java:441)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.$anonfun$getCustomPartitionLocations$1(InsertIntoHadoopFsRelationCommand.scala:254) {noformat}
Looks like the issue is coming from Hadoop Path. 
{noformat}
scala> import org.apache.hadoop.fs.Path
import org.apache.hadoop.fs.Path
scala> val path: Path = new Path(""gs://test_dd123/"")
path: org.apache.hadoop.fs.Path = gs://test_dd123/

scala> path.suffix(""/num=123"")
java.lang.NullPointerException
  at org.apache.hadoop.fs.Path.<init>(Path.java:150)
  at org.apache.hadoop.fs.Path.<init>(Path.java:129)
  at org.apache.hadoop.fs.Path.suffix(Path.java:450){noformat}
Path.suffix throughs NPE when writing into GS buckets root. 

 ",,,,,,,,,,,,,,,,,,,HADOOP-18856,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-20 11:00:54.0,,,,,,,,,,"0|z1jwbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor stucked on retrying to fetch shuffle data when `java.lang.OutOfMemoryError. unable to create native thread` exception occurred.,SPARK-44881,13547969,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,hgs19921112,hgs19921112,20/Aug/23 02:25,20/Aug/23 05:24,30/Oct/23 17:26,,3.2.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Aug 20 05:24:40 UTC 2023,,,,,,,,,,"0|z1jw88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/23 05:24;ignitetcbot;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/42572;;;",,,,,,,,,,,,,,
Remove unnecessary curly braces at the end of the thread locks info,SPARK-44880,13547952,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,19/Aug/23 16:52,20/Aug/23 01:41,30/Oct/23 17:26,20/Aug/23 01:41,3.3.2,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Web UI,,,,,0,,,,,Remove unnecessary curly braces at the end of the thread locks info,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Aug 20 01:41:08 UTC 2023,,,,,,,,,,"0|z1jw4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/23 01:41;yumwang;Issue resolved by pull request 42571
[https://github.com/apache/spark/pull/42571];;;",,,,,,,,,,,,,,
Enable and fix test_parity_arrow_python_udf,SPARK-44876,13547895,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ueshin,ueshin,ueshin,18/Aug/23 17:55,21/Aug/23 00:20,30/Oct/23 17:26,21/Aug/23 00:20,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,PySpark,,,,,0,,,,,"{{pyspark.sql.tests.connect.test_parity_arrow_python_udf}} is not listed in {{dev/sparktestsupport/modules.py}}, and it fails when running manually.

{code}
======================================================================
ERROR [0.072s]: test_register (pyspark.sql.tests.connect.test_parity_arrow_python_udf.ArrowPythonUDFParityTests)
----------------------------------------------------------------------
Traceback (most recent call last):
...
pyspark.errors.exceptions.base.PySparkRuntimeError: [SCHEMA_MISMATCH_FOR_PANDAS_UDF] Result vector from pandas_udf was not the required length: expected 1, got 38.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 21 00:20:37 UTC 2023,,,,,,,,,,"0|z1jvrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/23 00:20;gurwls223;Issue resolved by pull request 42568
[https://github.com/apache/spark/pull/42568];;;",,,,,,,,,,,,,,
Alter nested view fails because of HMS client,SPARK-44873,13547889,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kyle.rong,kyle.rong,kyle.rong,18/Aug/23 16:30,19/Aug/23 01:34,30/Oct/23 17:26,18/Aug/23 18:07,3.1.0,3.1.1,3.1.2,3.1.3,3.2.0,3.2.1,3.2.2,3.2.3,3.2.4,3.3.0,3.3.1,3.3.2,3.3.3,3.3.4,3.4.0,3.4.1,3.4.2,3.5.0,3.5.1,3.5.0,4.0.0,,,SQL,,,,,0,,,,,"Currently, 
{code:java}
CREATE OR REPLACE VIEW t AS SELECT "" +
        ""struct(id AS `$col2`, struct(id AS `$col`) AS s1) AS s2 FROM RANGE(5)
ALTER VIEW t SET TBLPROPERTIES ('x' = 'y'){code}
would fail when calling HMS's updateTable, because HMS does not support nested struct in view. We can fix this by passing HMS an empty schema, since we store the actual view schema in the table's properties already. This fix is similar to https://github.com/apache/spark/pull/37364",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Aug 19 01:34:09 UTC 2023,,,,,,,,,,"0|z1jvqg:",9223372036854775807,,,,,Gengliang.Wang,,,,,,,,,,,,,,,,,,"18/Aug/23 18:07;Gengliang.Wang;Issue resolved by pull request 42532
[https://github.com/apache/spark/pull/42532];;;","19/Aug/23 01:34;paulk;User 'kylerong-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/42566;;;",,,,,,,,,,,,,
Fix PERCENTILE_DISC behaviour,SPARK-44871,13547866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,petertoth,petertoth,petertoth,18/Aug/23 12:54,23/Aug/23 08:02,30/Oct/23 17:26,22/Aug/23 16:27,3.3.0,3.3.1,3.3.2,3.3.3,3.4.0,3.4.1,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.0,4.0.0,SQL,,,,,0,,,,,"Currently {{percentile_disc()}} returns incorrect results in some cases:

E.g.:
{code:java}
SELECT
  percentile_disc(0.0) WITHIN GROUP (ORDER BY a) as p0,
  percentile_disc(0.1) WITHIN GROUP (ORDER BY a) as p1,
  percentile_disc(0.2) WITHIN GROUP (ORDER BY a) as p2,
  percentile_disc(0.3) WITHIN GROUP (ORDER BY a) as p3,
  percentile_disc(0.4) WITHIN GROUP (ORDER BY a) as p4,
  percentile_disc(0.5) WITHIN GROUP (ORDER BY a) as p5,
  percentile_disc(0.6) WITHIN GROUP (ORDER BY a) as p6,
  percentile_disc(0.7) WITHIN GROUP (ORDER BY a) as p7,
  percentile_disc(0.8) WITHIN GROUP (ORDER BY a) as p8,
  percentile_disc(0.9) WITHIN GROUP (ORDER BY a) as p9,
  percentile_disc(1.0) WITHIN GROUP (ORDER BY a) as p10
FROM VALUES (0), (1), (2), (3), (4) AS v(a)
{code}
returns:
{code:java}
+---+---+---+---+---+---+---+---+---+---+---+
| p0| p1| p2| p3| p4| p5| p6| p7| p8| p9|p10|
+---+---+---+---+---+---+---+---+---+---+---+
|0.0|0.0|0.0|1.0|1.0|2.0|2.0|2.0|3.0|3.0|4.0|
+---+---+---+---+---+---+---+---+---+---+---+
{code}
but it should return:
{noformat}
+---+---+---+---+---+---+---+---+---+---+---+
| p0| p1| p2| p3| p4| p5| p6| p7| p8| p9|p10|
+---+---+---+---+---+---+---+---+---+---+---+
|0.0|0.0|0.0|1.0|1.0|2.0|2.0|3.0|3.0|4.0|4.0|
+---+---+---+---+---+---+---+---+---+---+---+
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 22 16:27:35 UTC 2023,,,,,,,,,,"0|z1jvlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/23 17:44;tgraves;Can you add a description to this please;;;","18/Aug/23 18:37;petertoth;[~tgraves], sure, I've just updated it.

It looks like my PR didn't get linked here automatically, so here it is: https://github.com/apache/spark/pull/42559;;;","22/Aug/23 16:27;maxgekk;Issue resolved by pull request 42610
[https://github.com/apache/spark/pull/42610];;;",,,,,,,,,,,,
Spark wrongly map the BOOLEAN Type to BIT(1) in Snowflake,SPARK-44866,13547843,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hayssam,hayssam,hayssam,18/Aug/23 09:20,08/Sep/23 22:15,30/Oct/23 17:26,08/Sep/23 22:14,3.4.1,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,"In Snowflake the Boolean type is represented by the Boolean data type ([https://docs.snowflake.com/en/sql-reference/data-types-logical]), but Spark rely on the default JdbcDialect to generate the mapping which maps _Boolean_ to _BIT(1)_

This should be probably handled by a dialect specific to Snowflake.",,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 08 22:14:37 UTC 2023,,,,,,,,,,"0|z1jvg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/23 13:26;hayssam;Created PR [https://github.com/apache/spark/pull/42545];;;","08/Sep/23 22:14;dongjoon;Issue resolved by pull request 42545
[https://github.com/apache/spark/pull/42545];;;",,,,,,,,,,,,,
[CONNECT] jsonignore SparkListenerConnectOperationStarted.planRequest,SPARK-44861,13547799,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jgauthier,jgauthier,jgauthier,17/Aug/23 23:54,25/Aug/23 03:34,30/Oct/23 17:26,23/Aug/23 16:41,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"SparkListenerConnectOperationStarted was added as part of SPARK-43923.

SparkListenerConnectOperationStarted.planRequest cannot be serialized & deserialized from json as it has recursive objects.

Add @JsonIgnoreProperties(\{ ""planRequest"" }) to avoid failures",,,,,,,,,,,,,,,,,,SPARK-44931,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-17 23:54:28.0,,,,,,,,,,"0|z1jv6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix getBaseURI error in Spark Worker LogPage UI buttons,SPARK-44857,13547788,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,17/Aug/23 21:42,22/Aug/23 22:11,30/Oct/23 17:26,18/Aug/23 01:07,3.2.0,3.2.4,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.0,4.0.0,Spark Core,Web UI,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-35822,,,,,,,,,,"17/Aug/23 21:42;dongjoon;Screenshot 2023-08-17 at 2.38.45 PM.png;https://issues.apache.org/jira/secure/attachment/13062258/Screenshot+2023-08-17+at+2.38.45+PM.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 22 22:11:04 UTC 2023,,,,,,,,,,"0|z1jv40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/23 01:07;dongjoon;Issue resolved by pull request 42546
[https://github.com/apache/spark/pull/42546];;;","22/Aug/23 22:11;dongjoon;Thank you for fixing the `Fix Version`, [~yumwang].;;;",,,,,,,,,,,,,
Python timedelta to DayTimeIntervalType edge cases bug,SPARK-44854,13547756,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,hdaly0,hdaly0,17/Aug/23 16:57,22/Aug/23 02:56,30/Oct/23 17:26,22/Aug/23 02:56,3.4.0,,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,4.0.0,,PySpark,,,,,0,pull-request-available,,,,"h1. Python Timedelta to PySpark DayTimeIntervalType bug

There is a bug that exists which means certain Python datetime.timedelta objects get converted to a PySpark DayTimeIntervalType column with a different value to that which is stored in the Python timedelta.

A simple illustrative example can be produced with the below code:

 
{code:java}
from datetime import timedelta
from pyspark.sql.types import DayTimeIntervalType, StructField, StructType

spark = ...spark session setup here...

td = timedelta(days=4498031, seconds=16054, microseconds=999981)
df = spark.createDataFrame([(td,)], StructType([StructField(name=""timedelta_col"", dataType=DayTimeIntervalType(), nullable=False)]))
df.show(truncate=False)

> +------------------------------------------------+
> |timedelta_col                                   | 
> +------------------------------------------------+ 
> |INTERVAL '4498031 04:27:35.999981' DAY TO SECOND| 
> +------------------------------------------------+

print(str(td))

>  '4498031 days, 4:27:34.999981' {code}
In the above example, look at the seconds. The original python timedelta object has 34 seconds, the pyspark DayTimeIntervalType column has 35 seconds.
h1. Fix

This issue arises because the current conversion from python timedelta uses the timedelta function `.total_seconds()` to get the number of seconds, and then adds the microsecond component back in afterwards. Unfortunately the `.total_seconds()` function with some timedeltas (ones with microsecond entries close to 1_000_000 I believe) ends up rounding *up* to the nearest second (probably due to floating point precision), with the microseconds then added on top of that. The effect is that 1 second gets added incorrectly.

The issue can be fixed by doing the processing in a slightly different way. Instead of doing:

 
{code:java}
(math.floor(dt.total_seconds()) * 1000000) + dt.microseconds{code}
 

Instead we construct the timedelta from its components:

 
{code:java}
(((dt.days * 86400) + dt.seconds) * 1_000_000) + dt.microseconds {code}
 
h1. Tests

An illustrative edge case example for timedeltas is the above (which can also be written as `datetime.timedelta(microseconds=388629894454999981)`)

 

A related edge case which is already handled but not tested exists for the situation where there are positive and negative components to the created timedelta object. An entry for this edge case is also included as it is related.
h1. PR

Link to the PR addressing this issue: https://github.com/apache/spark/pull/42541
h1. Keywords to help people searching for this issue:

datetime.timedelta

timedelta

pyspark.sql.types.DayTimeIntervalType

DayTimeIntervalType

 ",,10800,10800,,0%,10800,10800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 22 02:56:40 UTC 2023,,,,,,,,,,"0|z1juww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/23 02:56;gurwls223;Issue resolved by pull request 42541
[https://github.com/apache/spark/pull/42541];;;",,,,,,,,,,,,,,
Need exclude `junit-jupiter-api` from `curator-test`,SPARK-44852,13547746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,17/Aug/23 15:38,18/Aug/23 16:05,30/Oct/23 17:26,18/Aug/23 16:05,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Spark Core,Tests,,,,0,,,,,"SPARK-44792 Upgrade curator to 5.2.0,  `curator-test` depends on `junit-jupiter-api`, but Apache Spark currently does not support testing with JUnit5, so it will report the following error when executing `

build/mvn clean install -pl core -am -Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest`.

 
{code:java}
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[ERROR] TestEngine with ID 'junit-vintage' failed to discover tests
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Spark Project Parent POM 4.0.0-SNAPSHOT:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [  4.631 s]
[INFO] Spark Project Tags ................................. SUCCESS [  9.044 s]
[INFO] Spark Project Local DB ............................. SUCCESS [ 12.686 s]
[INFO] Spark Project Common Utils ......................... SUCCESS [ 12.216 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 54.368 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 14.355 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [ 12.321 s]
[INFO] Spark Project Launcher ............................. SUCCESS [ 10.019 s]
[INFO] Spark Project Core ................................. FAILURE [01:06 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  03:16 min
[INFO] Finished at: 2023-08-17T23:30:36+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.1.2:test (default-test) on project spark-core_2.12: 
[ERROR] 
[ERROR] Please refer to /Users/yangjie01/SourceCode/git/spark-mine-12/core/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] There was an error in the forked process
[ERROR] TestEngine with ID 'junit-vintage' failed to discover tests
[ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: There was an error in the forked process
[ERROR] TestEngine with ID 'junit-vintage' failed to discover tests
[ERROR]     at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:628)
[ERROR]     at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:285)
[ERROR]     at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:250)
[ERROR]     at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1203)
[ERROR]     at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1055)
[ERROR]     at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:871)
[ERROR]     at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
[ERROR]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2(MojoExecutor.java:370)
[ERROR]     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute(MojoExecutor.java:351)
[ERROR]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:215)
[ERROR]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:171)
[ERROR]     at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:163)
[ERROR]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
[ERROR]     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
[ERROR]     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
[ERROR]     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
[ERROR]     at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:299)
[ERROR]     at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
[ERROR]     at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
[ERROR]     at org.apache.maven.cli.MavenCli.execute(MavenCli.java:963)
[ERROR]     at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:296)
[ERROR]     at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
[ERROR]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[ERROR]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[ERROR]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[ERROR]     at java.lang.reflect.Method.invoke(Method.java:498)
[ERROR]     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:282)
[ERROR]     at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:225)
[ERROR]     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:406)
[ERROR]     at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:347)
[ERROR] 
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :spark-core_2.12
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 18 16:05:14 UTC 2023,,,,,,,,,,"0|z1juuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/23 16:05;dongjoon;Issue resolved by pull request 42539
[https://github.com/apache/spark/pull/42539];;;",,,,,,,,,,,,,,
MLlib GBTClassifier has wrong impurity method 'variance' instead of 'gini' or 'entropy'. ,SPARK-44848,13547715,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,lisi,lisi,17/Aug/23 12:04,03/Oct/23 11:40,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,1,,,,,"Impurity method 'variance' should only be used for regressors, *not* classifiers. For classifiers gini and entropy should be available as it is already the case for the RandomForestClassifier [https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.html] .

Because of this bug 'minInfoGain' hyperparameter cannot be tuned to combat overfitting. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Oct 03 11:40:48 UTC 2023,,,,,,,,,,"0|z1juns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/23 11:40;oumarnour;Hello,

I have the same issue. I want to know if that issue is solved ?

Thanks;;;",,,,,,,,,,,,,,
Couldn't submit Spark application to Kubenetes in versions v1.27.3,SPARK-44847,13547707,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,siddaraju_gc,siddaraju_gc,17/Aug/23 11:03,17/Aug/23 11:03,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Kubernetes,Spark Submit,,,,0,,,,,"Spark-submit ( cluster mode on Kubernetes ) results error *io.fabric8.kubernetes.client.KubernetesClientException* on my 3 nodes k8s cluster.

Steps followed:
 * using IBM cloud, created 3 Instances
 * 1st Instance act as master node and another two acts as worker nodes

 
{noformat}
root@vsi-spark-master:/opt# kubectl get nodes
NAME                 STATUS   ROLES                  AGE   VERSION
vsi-spark-master     Ready    control-plane,master   2d    v1.27.3+k3s1
vsi-spark-worker-1   Ready    <none>                 47h   v1.27.3+k3s1
vsi-spark-worker-2   Ready    <none>                 47h   v1.27.3+k3s1{noformat}
 * Copy spark-3.4.1-bin-hadoop3.tgz in to /opt/spark folder 
 * Ran spark by using below command

 
{noformat}
root@vsi-spark-master:/opt# /opt/spark/bin/spark-submit --master k8s://http://<master_node_IP>:6443 --conf spark.kubernetes.authenticate.submission.oauthToken=$TOKEN --deploy-mode cluster --name spark-pi --class org.apache.spark.examples.SparkPi --conf spark.executor.instances=5 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  --conf spark.kubernetes.container.image=sushmakorati/testrepo:pyrandomGB local:///opt/spark/examples/jars/spark-examples_2.12-3.4.1.jar{noformat}
 * And getting below error message.

{noformat}
3/07/27 12:56:26 WARN Utils: Kubernetes master URL uses HTTP instead of HTTPS.
23/07/27 12:56:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/07/27 12:56:26 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
23/07/27 12:56:26 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
23/07/27 12:56:27 ERROR Client: Please check ""kubectl auth can-i create pod"" first. It should be yes.
Exception in thread ""main"" io.fabric8.kubernetes.client.KubernetesClientException: An error has occurred.
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:129)
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:122)
    at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:44)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:1113)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:93)
    at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:153)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5(KubernetesClientApplication.scala:250)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5$adapted(KubernetesClientApplication.scala:244)
    at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2786)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:244)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:216)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1020)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: Connection reset
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:535)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:558)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleCreate(OperationSupport.java:349)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:711)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:93)
    at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)
    ... 15 more
Caused by: java.net.SocketException: Connection reset
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
    at okio.Okio$2.read(Okio.java:140)
    at okio.AsyncTimeout$2.read(AsyncTimeout.java:237)
    at okio.RealBufferedSource.read(RealBufferedSource.java:47)
    at okhttp3.internal.http1.Http1Codec$AbstractSource.read(Http1Codec.java:363)
    at okhttp3.internal.http1.Http1Codec$UnknownLengthSource.read(Http1Codec.java:507)
    at okio.RealBufferedSource.exhausted(RealBufferedSource.java:57)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientImpl$OkHttpAsyncBody.doConsume(OkHttpClientImpl.java:127)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
23/07/27 12:56:27 INFO ShutdownHookManager: Shutdown hook called
23/07/27 12:56:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-70ee50ef-d9e9-4220-91f4-15a282031095{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-17 11:03:12.0,,,,,,,,,,"0|z1jum0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushFoldableIntoBranches in complex grouping expressions may cause bindReference error,SPARK-44846,13547697,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhuml,zhuml,zhuml,17/Aug/23 09:05,04/Sep/23 12:48,30/Oct/23 17:26,04/Sep/23 12:48,3.4.1,,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,4.0.0,,SQL,,,,,0,,,,,"SQL:
{code:java}
select c*2 as d from
(select if(b > 1, 1, b) as c from
(select if(a < 0, 0 ,a) as b from t group by b) t1
group by c) t2 {code}
ERROR:
{code:java}
Couldn't find _groupingexpression#15 in [if ((_groupingexpression#15 > 1)) 1 else _groupingexpression#15#16]
java.lang.IllegalStateException: Couldn't find _groupingexpression#15 in [if ((_groupingexpression#15 > 1)) 1 else _groupingexpression#15#16]
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1241)
    at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1240)
    at org.apache.spark.sql.catalyst.expressions.BinaryExpression.mapChildren(Expression.scala:653)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.TernaryLike.mapChildren(TreeNode.scala:1272)
    at org.apache.spark.sql.catalyst.trees.TernaryLike.mapChildren$(TreeNode.scala:1271)
    at org.apache.spark.sql.catalyst.expressions.If.mapChildren(conditionalExpressions.scala:41)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)
    at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)
    at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:533)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:405)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)
    at scala.collection.immutable.List.map(List.scala:293)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)
    at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:360)
    at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:538)
    at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.doProduce(AggregateCodegenSupport.scala:69)
    at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.doProduce$(AggregateCodegenSupport.scala:65)
    at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:49)
    at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
    at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:49)
    at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:660)
    at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:723)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
    at org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:93)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$doExecute$1(AdaptiveSparkPlanExec.scala:386)
    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
    at org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3857)
    at org.apache.spark.sql.Dataset.rdd(Dataset.scala:3855)
    at org.apache.spark.sql.QueryTest$.$anonfun$getErrorMessageInCheckAnswer$1(QueryTest.scala:266)
    at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:209)
    at org.apache.spark.sql.QueryTest$.getErrorMessageInCheckAnswer(QueryTest.scala:266)
    at org.apache.spark.sql.QueryTest$.checkAnswer(QueryTest.scala:243)
    at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:151)
    at org.apache.spark.sql.DataFrameSuite.$anonfun$new$737(DataFrameSuite.scala:3676)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:95)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withTempView(SQLTestUtils.scala:276)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withTempView$(SQLTestUtils.scala:274)
    at org.apache.spark.sql.DataFrameSuite.withTempView(DataFrameSuite.scala:60)
    at org.apache.spark.sql.DataFrameSuite.$anonfun$new$736(DataFrameSuite.scala:3667)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
    at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
    at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
    at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
    at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
    at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
    at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
    at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
    at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
    at org.scalatest.Transformer.apply(Transformer.scala:22)
    at org.scalatest.Transformer.apply(Transformer.scala:20)
    at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
    at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
    at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
    at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
    at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
    at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
    at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
    at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
    at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
    at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
    at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
    at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
    at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
    at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
    at org.scalatest.Suite.run(Suite.scala:1114)
    at org.scalatest.Suite.run$(Suite.scala:1096)
    at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
    at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
    at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
    at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
    at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
    at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
    at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
    at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
    at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
    at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
    at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:47)
    at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1321)
    at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1315)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1315)
    at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:992)
    at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:970)
    at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1481)
    at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:970)
    at org.scalatest.tools.Runner$.run(Runner.scala:798)
    at org.scalatest.tools.Runner.run(Runner.scala)
    at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2or3(ScalaTestRunner.java:38)
    at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:25)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 04 12:48:17 UTC 2023,,,,,,,,,,"0|z1jujs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 09:17;githubbot;User 'zml1206' has created a pull request for this issue:
https://github.com/apache/spark/pull/42531;;;","31/Aug/23 16:52;ignitetcbot;User 'zml1206' has created a pull request for this issue:
https://github.com/apache/spark/pull/42633;;;","04/Sep/23 12:48;yumwang;Issue resolved by pull request 42633
[https://github.com/apache/spark/pull/42633];;;",,,,,,,,,,,,
spark job copies jars repeatedly if fs.defaultFS and application jar are same url,SPARK-44845,13547678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zheju_he,zheju_he,zheju_he,17/Aug/23 07:22,07/Sep/23 04:16,30/Oct/23 17:26,07/Sep/23 04:16,3.4.1,,,,,,,,,,,,,,,,,,,4.0.0,,,,YARN,,,,,0,,,,,"In the org.apache.spark.deploy.yarn.Client#compareUri method, hdfs://hadoop81:8020 and hdfs://192.168.0.81:8020 are regarded as different file systems (hadoop81 corresponds to 192.168.0.81). The specific reason is that in the last pr, different URIs of user information are also regarded as different file systems. Uri.getauthority is used to determine the user information, but authority contains the host so the URI above must be different from authority. To determine whether the user authentication information is different, you only need to determine URI.getUserInfo.

 

the last pr and issue link:
https://issues.apache.org/jira/browse/SPARK-22587

https://github.com/apache/spark/pull/19885",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Sep 07 04:16:30 UTC 2023,,,,,,,,,,"0|z1jufk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/23 07:34;zheju_he;my pr https://github.com/apache/spark/pull/42529;;;","21/Aug/23 03:56;snoot;User 'zekai-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/42529;;;","21/Aug/23 03:57;snoot;User 'zekai-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/42529;;;","07/Sep/23 04:16;mridulm80;Issue resolved by pull request 42529
[https://github.com/apache/spark/pull/42529];;;",,,,,,,,,,,
Exclude `python/build/*` for local `lint-python` testing,SPARK-44844,13547666,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,itholic,itholic,17/Aug/23 06:02,21/Aug/23 03:56,30/Oct/23 17:26,21/Aug/23 03:56,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Tests,,,,,0,,,,,"In local testing, when executing `dev/lint-python` it displays bunch of errors since dummy files from `python/build/*`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 21 03:56:21 UTC 2023,,,,,,,,,,"0|z1jucw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/23 03:56;itholic;It's not a general issue.;;;",,,,,,,,,,,,,,
flaky test: RocksDBStateStoreStreamingAggregationSuite,SPARK-44843,13547663,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,LuciferYang,LuciferYang,17/Aug/23 05:57,17/Aug/23 05:57,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,SQL,Tests,,,,0,,,,,"I've seen this more than once, let's record it for now.

[https://github.com/apache/spark/actions/runs/5875252243/job/15931264374]
{code:java}
2023-08-16T06:49:14.0550627Z [0m[[0m[0minfo[0m] [0m[0m[31m- SPARK-35896: metrics in StateOperatorProgress are output correctly (RocksDBStateStore) *** FAILED *** (1 minute, 1 second)[0m[0m
2023-08-16T06:49:14.0560354Z [0m[[0m[0minfo[0m] [0m[0m[31m  Timed out waiting for stream: The code passed to failAfter did not complete within 60 seconds.[0m[0m
2023-08-16T06:49:14.0568703Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.lang.Thread.getStackTrace(Thread.java:1564)[0m[0m
2023-08-16T06:49:14.0578526Z [0m[[0m[0minfo[0m] [0m[0m[31m  	org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:277)[0m[0m
2023-08-16T06:49:14.0600495Z [0m[[0m[0minfo[0m] [0m[0m[31m  	org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)[0m[0m
2023-08-16T06:49:14.0609443Z [0m[[0m[0minfo[0m] [0m[0m[31m  	org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)[0m[0m
2023-08-16T06:49:14.0630028Z [0m[[0m[0minfo[0m] [0m[0m[31m  	org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)[0m[0m
2023-08-16T06:49:14.0638142Z [0m[[0m[0minfo[0m] [0m[0m[31m  	org.apache.spark.sql.streaming.StreamTest.$anonfun$testStream$7(StreamTest.scala:481)[0m[0m
2023-08-16T06:49:14.0704798Z [0m[[0m[0minfo[0m] [0m[0m[31m  	org.apache.spark.sql.streaming.StreamTest.$anonfun$testStream$7$adapted(StreamTest.scala:480)[0m[0m
2023-08-16T06:49:14.0732716Z [0m[[0m[0minfo[0m] [0m[0m[31m  	scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)[0m[0m
2023-08-16T06:49:14.0743783Z [0m[[0m[0minfo[0m] [0m[0m[31m  	scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)[0m[0m
2023-08-16T06:49:14.0753421Z [0m[[0m[0minfo[0m] [0m[0m[31m  	scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)[0m[0m
2023-08-16T06:49:14.0765553Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.0773522Z [0m[[0m[0minfo[0m] [0m[0m[31m  	Caused by: 	null[0m[0m
2023-08-16T06:49:14.0787123Z [0m[[0m[0minfo[0m] [0m[0m[31m  	java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)[0m[0m
2023-08-16T06:49:14.0796604Z [0m[[0m[0minfo[0m] [0m[0m[31m  		java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2173)[0m[0m
2023-08-16T06:49:14.0808419Z [0m[[0m[0minfo[0m] [0m[0m[31m  		org.apache.spark.sql.execution.streaming.StreamExecution.awaitOffset(StreamExecution.scala:481)[0m[0m
2023-08-16T06:49:14.0817018Z [0m[[0m[0minfo[0m] [0m[0m[31m  		org.apache.spark.sql.streaming.StreamTest.$anonfun$testStream$8(StreamTest.scala:482)[0m[0m
2023-08-16T06:49:14.0824218Z [0m[[0m[0minfo[0m] [0m[0m[31m  		scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m[0m
2023-08-16T06:49:14.0831608Z [0m[[0m[0minfo[0m] [0m[0m[31m  		org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)[0m[0m
2023-08-16T06:49:14.0838059Z [0m[[0m[0minfo[0m] [0m[0m[31m  		org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)[0m[0m
2023-08-16T06:49:14.0847335Z [0m[[0m[0minfo[0m] [0m[0m[31m  		org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)[0m[0m
2023-08-16T06:49:14.0854180Z [0m[[0m[0minfo[0m] [0m[0m[31m  		org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)[0m[0m
2023-08-16T06:49:14.0861298Z [0m[[0m[0minfo[0m] [0m[0m[31m  		org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)[0m[0m
2023-08-16T06:49:14.0866845Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.0872599Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.0880688Z [0m[[0m[0minfo[0m] [0m[0m[31m  == Progress ==[0m[0m
2023-08-16T06:49:14.0887109Z [0m[[0m[0minfo[0m] [0m[0m[31m     StartStream(ProcessingTimeTrigger(0),org.apache.spark.util.SystemClock@432e877b,Map(spark.sql.shuffle.partitions -> 3),null)[0m[0m
2023-08-16T06:49:14.0901797Z [0m[[0m[0minfo[0m] [0m[0m[31m     AddData to MemoryStream[value#19338]: 3,2,1,3[0m[0m
2023-08-16T06:49:14.0913348Z [0m[[0m[0minfo[0m] [0m[0m[31m  => CheckLastBatch: [3,2],[2,1],[1,1][0m[0m
2023-08-16T06:49:14.0919235Z [0m[[0m[0minfo[0m] [0m[0m[31m     AssertOnQuery(<condition>, Check total state rows = List(3), updated state rows = List(3), rows dropped by watermark = List(0), removed state rows = Some(List(0)))[0m[0m
2023-08-16T06:49:14.0925028Z [0m[[0m[0minfo[0m] [0m[0m[31m     AddData to MemoryStream[value#19338]: 1,4[0m[0m
2023-08-16T06:49:14.0931608Z [0m[[0m[0minfo[0m] [0m[0m[31m     CheckLastBatch: [3,2],[2,1],[1,2],[4,1][0m[0m
2023-08-16T06:49:14.0938968Z [0m[[0m[0minfo[0m] [0m[0m[31m     AssertOnQuery(<condition>, Check operator progress metrics: operatorName = stateStoreSave, numShufflePartitions = 3, numStateStoreInstances = 3)[0m[0m
2023-08-16T06:49:14.0944383Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.0985018Z [0m[[0m[0minfo[0m] [0m[0m[31m  == Stream ==[0m[0m
2023-08-16T06:49:14.1010235Z [0m[[0m[0minfo[0m] [0m[0m[31m  Output Mode: Complete[0m[0m
2023-08-16T06:49:14.1015512Z [0m[[0m[0minfo[0m] [0m[0m[31m  Stream state: {}[0m[0m
2023-08-16T06:49:14.1020511Z [0m[[0m[0minfo[0m] [0m[0m[31m  Thread state: alive[0m[0m
2023-08-16T06:49:14.1026531Z [0m[[0m[0minfo[0m] [0m[0m[31m  Thread stack trace: sun.misc.Unsafe.park(Native Method)[0m[0m
2023-08-16T06:49:14.1032459Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)[0m[0m
2023-08-16T06:49:14.1038406Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)[0m[0m
2023-08-16T06:49:14.1044652Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)[0m[0m
2023-08-16T06:49:14.1052001Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)[0m[0m
2023-08-16T06:49:14.1058244Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)[0m[0m
2023-08-16T06:49:14.1063878Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)[0m[0m
2023-08-16T06:49:14.1069740Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)[0m[0m
2023-08-16T06:49:14.1081581Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)[0m[0m
2023-08-16T06:49:14.1089123Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:974)[0m[0m
2023-08-16T06:49:14.1096254Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.SparkContext.runJob(SparkContext.scala:2405)[0m[0m
2023-08-16T06:49:14.1104050Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)[0m[0m
2023-08-16T06:49:14.1112551Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)[0m[0m
2023-08-16T06:49:14.1120764Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:307)[0m[0m
2023-08-16T06:49:14.1141786Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:318)[0m[0m
2023-08-16T06:49:14.1149758Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)[0m[0m
2023-08-16T06:49:14.1159611Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)[0m[0m
2023-08-16T06:49:14.1166283Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)[0m[0m
2023-08-16T06:49:14.1172538Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)[0m[0m
2023-08-16T06:49:14.1208809Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3585)[0m[0m
2023-08-16T06:49:14.1215346Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.Dataset$$Lambda$2415/1714895648.apply(Unknown Source)[0m[0m
2023-08-16T06:49:14.1228588Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)[0m[0m
2023-08-16T06:49:14.1234684Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.Dataset$$Lambda$2442/14653745.apply(Unknown Source)[0m[0m
2023-08-16T06:49:14.1246038Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)[0m[0m
2023-08-16T06:49:14.1246768Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)[0m[0m
2023-08-16T06:49:14.1273061Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.Dataset$$Lambda$2416/619810288.apply(Unknown Source)[0m[0m
2023-08-16T06:49:14.1273847Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)[0m[0m
2023-08-16T06:49:14.1274620Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$$$Lambda$2366/419259595.apply(Unknown Source)[0m[0m
2023-08-16T06:49:14.1291096Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:209)[0m[0m
2023-08-16T06:49:14.1291944Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:115)[0m[0m
2023-08-16T06:49:14.1292713Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$$$Lambda$2353/1369125389.apply(Unknown Source)[0m[0m
2023-08-16T06:49:14.1293407Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)[0m[0m
2023-08-16T06:49:14.1294142Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)[0m[0m
2023-08-16T06:49:14.1305624Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)[0m[0m
2023-08-16T06:49:14.1306571Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.Dataset.collect(Dataset.scala:3585)[0m[0m
2023-08-16T06:49:14.1307380Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)[0m[0m
2023-08-16T06:49:14.1308412Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution$$Lambda$2352/335540119.apply(Unknown Source)[0m[0m
2023-08-16T06:49:14.1318048Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)[0m[0m
2023-08-16T06:49:14.1324073Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$$$Lambda$2366/419259595.apply(Unknown Source)[0m[0m
2023-08-16T06:49:14.1326362Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:209)[0m[0m
2023-08-16T06:49:14.1330068Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:115)[0m[0m
2023-08-16T06:49:14.1330922Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$$$Lambda$2353/1369125389.apply(Unknown Source)[0m[0m
2023-08-16T06:49:14.1418063Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)[0m[0m
2023-08-16T06:49:14.1418867Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)[0m[0m
2023-08-16T06:49:14.1419759Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)[0m[0m
2023-08-16T06:49:14.1420680Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution$$Lambda$2351/1752953868.apply(Unknown Source)[0m[0m
2023-08-16T06:49:14.1421623Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:405)[0m[0m
2023-08-16T06:49:14.1422639Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:403)[0m[0m
2023-08-16T06:49:14.1423746Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)[0m[0m
2023-08-16T06:49:14.1424717Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)[0m[0m
2023-08-16T06:49:14.1425711Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)[0m[0m
2023-08-16T06:49:14.1426644Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution$$Lambda$2053/1800928896.apply$mcV$sp(Unknown Source)[0m[0m
2023-08-16T06:49:14.1427389Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m[0m
2023-08-16T06:49:14.1428211Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:405)[0m[0m
2023-08-16T06:49:14.1678185Z 06:49:14.167 WARN org.apache.spark.sql.execution.streaming.ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
2023-08-16T06:49:14.1749754Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:403)[0m[0m
2023-08-16T06:49:14.1760827Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)[0m[0m
2023-08-16T06:49:14.1767884Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)[0m[0m
2023-08-16T06:49:14.1772443Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution$$Lambda$2050/1379075459.apply$mcZ$sp(Unknown Source)[0m[0m
2023-08-16T06:49:14.1775920Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)[0m[0m
2023-08-16T06:49:14.1779547Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)[0m[0m
2023-08-16T06:49:14.1780654Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)[0m[0m
2023-08-16T06:49:14.1781762Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$2042/403495914.apply$mcV$sp(Unknown Source)[0m[0m
2023-08-16T06:49:14.1787505Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m[0m
2023-08-16T06:49:14.1789663Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)[0m[0m
2023-08-16T06:49:14.1824717Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)[0m[0m
2023-08-16T06:49:14.1825945Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)[0m[0m
2023-08-16T06:49:14.1826944Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$2038/346934547.apply$mcV$sp(Unknown Source)[0m[0m
2023-08-16T06:49:14.1827779Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m[0m
2023-08-16T06:49:14.1828564Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)[0m[0m
2023-08-16T06:49:14.1829439Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)[0m[0m
2023-08-16T06:49:14.1830060Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.1830461Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.1830895Z [0m[[0m[0minfo[0m] [0m[0m[31m  == Sink ==[0m[0m
2023-08-16T06:49:14.1831298Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.1831680Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.1832072Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.1832499Z [0m[[0m[0minfo[0m] [0m[0m[31m  == Plan ==[0m[0m
2023-08-16T06:49:14.1833000Z [0m[[0m[0minfo[0m] [0m[0m[31m  == Parsed Logical Plan ==[0m[0m
2023-08-16T06:49:14.1833702Z [0m[[0m[0minfo[0m] [0m[0m[31m  WriteToMicroBatchDataSource MemorySink, 0214edd5-ddb7-4c00-befd-a225a61a800e, Complete, 0[0m[0m
2023-08-16T06:49:14.1835181Z [0m[[0m[0minfo[0m] [0m[0m[31m  +- Aggregate [value#19338], [value#19338, count(1) AS count(1)#19343L][0m[0m
2023-08-16T06:49:14.1836058Z [0m[[0m[0minfo[0m] [0m[0m[31m     +- StreamingDataSourceV2Relation [value#19338], org.apache.spark.sql.execution.streaming.MemoryStreamScanBuilder@65f877f4, MemoryStream[value#19338], -1, 0[0m[0m
2023-08-16T06:49:14.1836684Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.1837076Z [0m[[0m[0minfo[0m] [0m[0m[31m  == Analyzed Logical Plan ==[0m[0m
2023-08-16T06:49:14.1837697Z [0m[[0m[0minfo[0m] [0m[0m[31m  WriteToMicroBatchDataSource MemorySink, 0214edd5-ddb7-4c00-befd-a225a61a800e, Complete, 0[0m[0m
2023-08-16T06:49:14.1838328Z [0m[[0m[0minfo[0m] [0m[0m[31m  +- Aggregate [value#19338], [value#19338, count(1) AS count(1)#19343L][0m[0m
2023-08-16T06:49:14.1839185Z [0m[[0m[0minfo[0m] [0m[0m[31m     +- StreamingDataSourceV2Relation [value#19338], org.apache.spark.sql.execution.streaming.MemoryStreamScanBuilder@65f877f4, MemoryStream[value#19338], -1, 0[0m[0m
2023-08-16T06:49:14.1839983Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.1840380Z [0m[[0m[0minfo[0m] [0m[0m[31m  == Optimized Logical Plan ==[0m[0m
2023-08-16T06:49:14.1841274Z [0m[[0m[0minfo[0m] [0m[0m[31m  WriteToDataSourceV2 MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@26d87efa][0m[0m
2023-08-16T06:49:14.1842056Z [0m[[0m[0minfo[0m] [0m[0m[31m  +- Aggregate [value#19338], [value#19338, count(1) AS count(1)#19343L][0m[0m
2023-08-16T06:49:14.1842987Z [0m[[0m[0minfo[0m] [0m[0m[31m     +- StreamingDataSourceV2Relation [value#19338], org.apache.spark.sql.execution.streaming.MemoryStreamScanBuilder@65f877f4, MemoryStream[value#19338], -1, 0[0m[0m
2023-08-16T06:49:14.1843620Z [0m[[0m[0minfo[0m] [0m[0m  [0m
2023-08-16T06:49:14.1843994Z [0m[[0m[0minfo[0m] [0m[0m[31m  == Physical Plan ==[0m[0m
2023-08-16T06:49:14.1845106Z [0m[[0m[0minfo[0m] [0m[0m[31m  WriteToDataSourceV2 MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@26d87efa], org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$2290/663905427@7505e54c[0m[0m
2023-08-16T06:49:14.1846414Z [0m[[0m[0minfo[0m] [0m[0m[31m  +- *(4) HashAggregate(keys=[value#19338], functions=[count(1)], output=[value#19338, count(1)#19343L])[0m[0m
2023-08-16T06:49:14.1847513Z [0m[[0m[0minfo[0m] [0m[0m[31m     +- StateStoreSave [value#19338], state info [ checkpoint = file:/home/runner/work/spark/spark/target/tmp/streaming.metadata-9b48d443-954d-4338-85cf-fdf0d8ccb029/state, runId = 61c94e63-d753-41bf-b5b3-cdbe2d844d26, opId = 0, ver = 0, numPartitions = 3], Complete, 0, 0, 2[0m[0m
2023-08-16T06:49:14.1848443Z [0m[[0m[0minfo[0m] [0m[0m[31m        +- *(3) HashAggregate(keys=[value#19338], functions=[merge_count(1)], output=[value#19338, count#19372L])[0m[0m
2023-08-16T06:49:14.1849537Z [0m[[0m[0minfo[0m] [0m[0m[31m           +- StateStoreRestore [value#19338], state info [ checkpoint = file:/home/runner/work/spark/spark/target/tmp/streaming.metadata-9b48d443-954d-4338-85cf-fdf0d8ccb029/state, runId = 61c94e63-d753-41bf-b5b3-cdbe2d844d26, opId = 0, ver = 0, numPartitions = 3], 2[0m[0m
2023-08-16T06:49:14.1850484Z [0m[[0m[0minfo[0m] [0m[0m[31m              +- *(2) HashAggregate(keys=[value#19338], functions=[merge_count(1)], output=[value#19338, count#19372L])[0m[0m
2023-08-16T06:49:14.1851143Z [0m[[0m[0minfo[0m] [0m[0m[31m                 +- Exchange hashpartitioning(value#19338, 3), ENSURE_REQUIREMENTS, [plan_id=89400][0m[0m
2023-08-16T06:49:14.1851838Z [0m[[0m[0minfo[0m] [0m[0m[31m                    +- *(1) HashAggregate(keys=[value#19338], functions=[partial_count(1)], output=[value#19338, count#19372L])[0m[0m
2023-08-16T06:49:14.1852387Z [0m[[0m[0minfo[0m] [0m[0m[31m                       +- *(1) Project [value#19338][0m[0m
2023-08-16T06:49:14.1852988Z [0m[[0m[0minfo[0m] [0m[0m[31m                          +- MicroBatchScan[value#19338] MemoryStreamDataSource (StreamTest.scala:462)[0m[0m
2023-08-16T06:49:14.1853598Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.scalatest.exceptions.TestFailedException:[0m[0m
2023-08-16T06:49:14.1854372Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)[0m[0m
2023-08-16T06:49:14.1912224Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)[0m[0m
2023-08-16T06:49:14.1913222Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1564)[0m[0m
2023-08-16T06:49:14.1914065Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Assertions.fail(Assertions.scala:933)[0m[0m
2023-08-16T06:49:14.1914653Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Assertions.fail$(Assertions.scala:929)[0m[0m
2023-08-16T06:49:14.1915455Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuite.fail(AnyFunSuite.scala:1564)[0m[0m
2023-08-16T06:49:14.1916150Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.streaming.StreamTest.failTest$1(StreamTest.scala:462)[0m[0m
2023-08-16T06:49:14.1917119Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.streaming.StreamTest.liftedTree1$1(StreamTest.scala:800)[0m[0m
2023-08-16T06:49:14.1917876Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.streaming.StreamTest.testStream(StreamTest.scala:776)[0m[0m
2023-08-16T06:49:14.1918617Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.streaming.StreamTest.testStream$(StreamTest.scala:342)[0m[0m
2023-08-16T06:49:14.1919669Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.streaming.StreamingAggregationSuite.testStream(StreamingAggregationSuite.scala:54)[0m[0m
2023-08-16T06:49:14.1920735Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.streaming.StreamingAggregationSuite.$anonfun$new$86(StreamingAggregationSuite.scala:849)[0m[0m
2023-08-16T06:49:14.1921511Z [0m[[0m[0minfo[0m] [0m[0m[31m  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m[0m
2023-08-16T06:49:14.1922369Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.streaming.RocksDBStateStoreTest.$anonfun$test$2(RocksDBStateStoreTest.scala:39)[0m[0m
2023-08-16T06:49:14.1923184Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54)[0m[0m
2023-08-16T06:49:14.1923962Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38)[0m[0m
2023-08-16T06:49:14.1924964Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.streaming.StreamingAggregationSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(StreamingAggregationSuite.scala:54)[0m[0m
2023-08-16T06:49:14.1925885Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:247)[0m[0m
2023-08-16T06:49:14.1926655Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:245)[0m[0m
2023-08-16T06:49:14.1927567Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.streaming.StreamingAggregationSuite.withSQLConf(StreamingAggregationSuite.scala:54)[0m[0m
2023-08-16T06:49:14.1928495Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.streaming.RocksDBStateStoreTest.$anonfun$test$1(RocksDBStateStoreTest.scala:39)[0m[0m
2023-08-16T06:49:14.1929226Z [0m[[0m[0minfo[0m] [0m[0m[31m  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m[0m
2023-08-16T06:49:14.1929847Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)[0m[0m
2023-08-16T06:49:14.1930491Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)[0m[0m
2023-08-16T06:49:14.1931279Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)[0m[0m
2023-08-16T06:49:14.1931975Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)[0m[0m
2023-08-16T06:49:14.1932654Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)[0m[0m
2023-08-16T06:49:14.1933321Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)[0m[0m
2023-08-16T06:49:14.1934040Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)[0m[0m
2023-08-16T06:49:14.1934683Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)[0m[0m
2023-08-16T06:49:14.1935267Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m[0m
2023-08-16T06:49:14.1935918Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m[0m
2023-08-16T06:49:14.1936614Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m[0m
2023-08-16T06:49:14.1937347Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)[0m[0m
2023-08-16T06:49:14.1938043Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)[0m[0m
2023-08-16T06:49:14.1938800Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)[0m[0m
2023-08-16T06:49:14.1939622Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)[0m[0m
2023-08-16T06:49:14.1940287Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m[0m
2023-08-16T06:49:14.1940965Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)[0m[0m
2023-08-16T06:49:14.1941680Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)[0m[0m
2023-08-16T06:49:14.1942528Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)[0m[0m
2023-08-16T06:49:14.1943259Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)[0m[0m
2023-08-16T06:49:14.1989522Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)[0m[0m
2023-08-16T06:49:14.1995333Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)[0m[0m
2023-08-16T06:49:14.1996628Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)[0m[0m
2023-08-16T06:49:14.1997346Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)[0m[0m
2023-08-16T06:49:14.1998091Z [0m[[0m[0minfo[0m] [0m[0m[31m  at scala.collection.immutable.List.foreach(List.scala:431)[0m[0m
2023-08-16T06:49:14.1998995Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m[0m
2023-08-16T06:49:14.1999670Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)[0m[0m
2023-08-16T06:49:14.2000302Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)[0m[0m
2023-08-16T06:49:14.2000993Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)[0m[0m
2023-08-16T06:49:14.2004880Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)[0m[0m
2023-08-16T06:49:14.2005664Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)[0m[0m
2023-08-16T06:49:14.2006251Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Suite.run(Suite.scala:1114)[0m[0m
2023-08-16T06:49:14.2007449Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Suite.run$(Suite.scala:1096)[0m[0m
2023-08-16T06:49:14.2008151Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)[0m[0m
2023-08-16T06:49:14.2009643Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)[0m[0m
2023-08-16T06:49:14.2010290Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.runImpl(Engine.scala:535)[0m[0m
2023-08-16T06:49:14.2011063Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)[0m[0m
2023-08-16T06:49:14.2013323Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)[0m[0m
2023-08-16T06:49:14.2014289Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)[0m[0m
2023-08-16T06:49:14.2017752Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)[0m[0m
2023-08-16T06:49:14.2018563Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)[0m[0m
2023-08-16T06:49:14.2019222Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)[0m[0m
2023-08-16T06:49:14.2020623Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)[0m[0m
2023-08-16T06:49:14.2021440Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)[0m[0m
2023-08-16T06:49:14.2022124Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)[0m[0m
2023-08-16T06:49:14.2029114Z [0m[[0m[0minfo[0m] [0m[0m[31m  at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)[0m[0m
2023-08-16T06:49:14.2029922Z [0m[[0m[0minfo[0m] [0m[0m[31m  at java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m[0m
2023-08-16T06:49:14.2031313Z [0m[[0m[0minfo[0m] [0m[0m[31m  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m[0m
2023-08-16T06:49:14.2032083Z [0m[[0m[0minfo[0m] [0m[0m[31m  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m[0m
2023-08-16T06:49:14.2036045Z [0m[[0m[0minfo[0m] [0m[0m[31m  at java.lang.Thread.run(Thread.java:750)[0m[0m
2023-08-16T06:50:14.2328885Z 06:50:14.232 WARN org.apache.spark.sql.execution.streaming.state.RocksDB StateStoreId(opId=0,partId=0,name=default): Error closing RocksDB
2023-08-16T06:50:14.2340748Z org.apache.spark.SparkException: [CANNOT_LOAD_STATE_STORE.UNRELEASED_THREAD_ERROR] An error occurred during loading state. StateStoreId(opId=0,partId=0,name=default): RocksDB instance could not be acquired by [ThreadId: Some(2023)] as it was not released by [ThreadId: Some(2020), task: partition 0.0 in stage 541.0, TID 1401] after 120010 ms.
2023-08-16T06:50:14.2342031Z Thread holding the lock has trace: org.apache.spark.sql.execution.streaming.state.StateStore$.getStateStoreProvider(StateStore.scala:555)
2023-08-16T06:50:14.2342817Z org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:544)
2023-08-16T06:50:14.2343724Z org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:126)
2023-08-16T06:50:14.2344430Z org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
2023-08-16T06:50:14.2344964Z org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
2023-08-16T06:50:14.2345513Z org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
2023-08-16T06:50:14.2346107Z org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
2023-08-16T06:50:14.2346618Z org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
2023-08-16T06:50:14.2347140Z org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2023-08-16T06:50:14.2347736Z org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
2023-08-16T06:50:14.2348282Z org.apache.spark.scheduler.Task.run(Task.scala:141)
2023-08-16T06:50:14.2348805Z org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2023-08-16T06:50:14.2349373Z org.apache.spark.executor.Executor$TaskRunner$$Lambda$2925/1900945640.apply(Unknown Source)
2023-08-16T06:50:14.2350017Z org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2023-08-16T06:50:14.2350714Z org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2023-08-16T06:50:14.2351555Z org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:95)
2023-08-16T06:50:14.2352082Z org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2023-08-16T06:50:14.2352688Z java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2023-08-16T06:50:14.2353420Z java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2023-08-16T06:50:14.2353927Z java.lang.Thread.run(Thread.java:750)
2023-08-16T06:50:14.2354527Z 	at org.apache.spark.sql.errors.QueryExecutionErrors$.unreleasedThreadError(QueryExecutionErrors.scala:2637)
2023-08-16T06:50:14.2355260Z 	at org.apache.spark.sql.execution.streaming.state.RocksDB.acquire(RocksDB.scala:562)
2023-08-16T06:50:14.2356039Z 	at org.apache.spark.sql.execution.streaming.state.RocksDB.close(RocksDB.scala:467)
2023-08-16T06:50:14.2356887Z 	at org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider.close(RocksDBStateStoreProvider.scala:222)
2023-08-16T06:50:14.2357743Z 	at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$unload$1(StateStore.scala:606)
2023-08-16T06:50:14.2358482Z 	at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$unload$1$adapted(StateStore.scala:606)
2023-08-16T06:50:14.2359162Z 	at scala.Option.foreach(Option.scala:407)
2023-08-16T06:50:14.2359724Z 	at org.apache.spark.sql.execution.streaming.state.StateStore$.unload(StateStore.scala:606)
2023-08-16T06:50:14.2360502Z 	at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$4(StateStore.scala:663)
2023-08-16T06:50:14.2361318Z 	at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$4$adapted(StateStore.scala:663)
2023-08-16T06:50:14.2362645Z 	at scala.collection.mutable.HashMap$$anon$1.$anonfun$foreach$2(HashMap.scala:153)
2023-08-16T06:50:14.2363154Z 	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
2023-08-16T06:50:14.2363683Z 	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
2023-08-16T06:50:14.2364196Z 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
2023-08-16T06:50:14.2364675Z 	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:153)
2023-08-16T06:50:14.2365279Z 	at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$2(StateStore.scala:663)
2023-08-16T06:50:14.2365989Z 	at org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask$$anon$1.run(StateStore.scala:471)
2023-08-16T06:50:14.2366577Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2023-08-16T06:50:14.2367068Z 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
2023-08-16T06:50:14.2367706Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
2023-08-16T06:50:14.2368448Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
2023-08-16T06:50:14.2369108Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2023-08-16T06:50:14.2369683Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2023-08-16T06:50:14.2370116Z 	at java.lang.Thread.run(Thread.java:750)
2023-08-16T06:50:14.2554839Z 06:50:14.253 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 541.0 (TID 1403) (localhost executor driver): TaskKilled (Stage cancelled: Job 274 cancelled part of cancelled job group 61c94e63-d753-41bf-b5b3-cdbe2d844d26)
2023-08-16T06:50:14.2727333Z 06:50:14.268 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 541.0 (TID 1401) (localhost executor driver): TaskKilled (Stage cancelled: Job 274 cancelled part of cancelled job group 61c94e63-d753-41bf-b5b3-cdbe2d844d26)
2023-08-16T06:50:14.2979761Z 06:50:14.294 ERROR org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@77a1d641] is aborting.
2023-08-16T06:50:14.2981538Z 06:50:14.294 ERROR org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@77a1d641] aborted. {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-17 05:57:56.0,,,,,,,,,,"0|z1juc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
array_insert() give wrong results for ngative index,SPARK-44840,13547640,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,srielau,srielau,17/Aug/23 00:14,24/Aug/23 16:57,30/Oct/23 17:26,22/Aug/23 18:04,3.4.0,,,,,,,,,,,,,,,,,,,3.4.2,3.5.1,4.0.0,,Spark Core,,,,,0,,,,,"Unlike in Snowflake we decided that array_inert() is 1 based.
This means 1 is the first element in an array and -1 is the last. 
This matches the behavior of functions such as substr() and element_at().

 
{code:java}
> SELECT array_insert(array('a', 'b', 'c'), 1, 'z');
[""z"",""a"",""b"",""c""]
> SELECT array_insert(array('a', 'b', 'c'), 0, 'z');
Error
> SELECT array_insert(array('a', 'b', 'c'), -1, 'z');
[""a"",""b"",""c"",""z""]
> SELECT array_insert(array('a', 'b', 'c'), 5, 'z');
[""a"",""b"",""c"",NULL,""z""]
> SELECT array_insert(array('a', 'b', 'c'), -5, 'z');
[""z"",NULL,""a"",""b"",""c""]
> SELECT array_insert(array('a', 'b', 'c'), 2, cast(NULL AS STRING));
[""a"",NULL,""b"",""c""]
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 22 18:04:44 UTC 2023,,,,,,,,,,"0|z1ju74:",9223372036854775807,,,,,maxgekk,,,,,,,,,,,,,,,,,,"20/Aug/23 14:46;srowen;What is the desired behavior - is Snowflake's behavior 'standard'?;;;","20/Aug/23 19:40;srielau;[~srowen] There is no standard as such.
However, there are multiple reasons not to be compatible with Snowflake:
1. Precedence: SUBSTR('Hello', 1, 1) => 'H', SUBSTR('Hello', -1, 1) => 'o' (not 'l').
2. array access has been a mixed bag for us (some 0, some 1-based), but we have tried to move towards 1-based as well. e.g., element_at() is 1-based, and we use -1 (!) to get the last element.

3. Snowflake had no choice but to use -1 for the second last element because 1 is their second element. Because they are 0-based they are unable to use array_insert() to append an element (short of passing the (length - 1) as parameter. So the proposal is  objectively more powerful.;;;","21/Aug/23 09:46;aparna.garg;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/42564;;;","22/Aug/23 18:04;maxgekk;Issue resolved by pull request 42564
[https://github.com/apache/spark/pull/42564];;;",,,,,,,,,,,
Expose uploadAllArtifactClasses in ArtifactManager to `sql` package,SPARK-44829,13547541,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,vicennial,vicennial,16/Aug/23 11:39,16/Aug/23 19:06,30/Oct/23 17:26,16/Aug/23 19:06,3.5.1,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"Currently, the [uploadAllClassFilesArtifacts|https://github.com/apache/spark/blob/master/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/ArtifactManager.scala#L144-L146] method is private[client] but this limits the ability of non-client features to use UDFs (which require the class files). Currently, this is not an issue because classfiles are uploaded in all analyze/execute operations. Any new code paths would suffer from CNFE if they are not able to call this method. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-16 11:39:34.0,,,,,,,,,,"0|z1jtl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade ORC to 1.9.1,SPARK-44828,13547522,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,16/Aug/23 09:21,16/Aug/23 14:55,30/Oct/23 17:26,16/Aug/23 14:55,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 16 14:55:06 UTC 2023,,,,,,,,,,"0|z1jtgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/23 09:23;githubbot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/42511;;;","16/Aug/23 14:55;yao;Issue resolved by pull request 42511
[https://github.com/apache/spark/pull/42511];;;",,,,,,,,,,,,,
Fix test_assert_approx_equal_decimaltype_custom_rtol_pass when SPARK_ANSI_SQL_MODE=true,SPARK-44827,13547517,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,16/Aug/23 09:01,25/Aug/23 00:09,30/Oct/23 17:26,25/Aug/23 00:09,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,PySpark,Tests,,,,0,,,,,"https://github.com/apache/spark/actions/runs/5873530086/job/15926967006

!image-2023-08-16-17-01-43-819.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/23 09:01;panbingkun;image-2023-08-16-17-01-43-819.png;https://issues.apache.org/jira/secure/attachment/13062218/image-2023-08-16-17-01-43-819.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 25 00:09:37 UTC 2023,,,,,,,,,,"0|z1jtfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/23 00:09;gurwls223;Issue resolved by pull request 42513
[https://github.com/apache/spark/pull/42513];;;",,,,,,,,,,,,,,
Resolve testing timeout issue from Spark Connect,SPARK-44826,13547504,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,itholic,itholic,16/Aug/23 06:43,22/Oct/23 00:56,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Connect,Pandas API on Spark,Tests,,,0,,,,,DiffFramesParitySetItemSeriesTests.test_series_iloc_setitem is failing on Spark Connect due to unexpected timeout issue: https://github.com/itholic/spark/actions/runs/5850534247/job/15860127608,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Oct 22 00:56:36 UTC 2023,,,,,,,,,,"0|z1jtcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/23 00:56;geoff_langenderfer;this passes with python 3.11. Maybe it's an issue with 3.9?

{code:bash}
✦ ~/work/spark master* ⇣
myenv ❯ python/run-tests --testnames pyspark.pandas.tests.connect.diff_frames_ops.test_parity_setitem_series
Running PySpark tests. Output is in /home/g/work/spark/python/unit-tests.log
Will test against the following Python executables: ['/home/g/work/spark/python/myenv/bin/python3', 'python']
Will test the following Python tests: ['pyspark.pandas.tests.connect.diff_frames_ops.test_parity_setitem_series']
/home/g/work/spark/python/myenv/bin/python3 python_implementation is CPython
/home/g/work/spark/python/myenv/bin/python3 version is: Python 3.11.5
python python_implementation is CPython
python version is: Python 3.11.5
Starting test(/home/g/work/spark/python/myenv/bin/python3): pyspark.pandas.tests.connect.diff_frames_ops.test_parity_setitem_series (temp output: /home/g/work/spark/python/target/921e1ed4-2292-4d47-afee-c637bf47b24b/home_g_work_spark_python_myenv_bin_python3__pyspark.pandas.tests.connect.diff_frames_ops.test_parity_setitem_series__rvygfv5b.log)
Starting test(python): pyspark.pandas.tests.connect.diff_frames_ops.test_parity_setitem_series (temp output: /home/g/work/spark/python/target/3e101881-644e-4ff9-8cec-5418f69795ed/python__pyspark.pandas.tests.connect.diff_frames_ops.test_parity_setitem_series__75h3h5fc.log)
Finished test(/home/g/work/spark/python/myenv/bin/python3): pyspark.pandas.tests.connect.diff_frames_ops.test_parity_setitem_series (0s)
Finished test(python): pyspark.pandas.tests.connect.diff_frames_ops.test_parity_setitem_series (0s)
Tests passed in 0 seconds
{code};;;",,,,,,,,,,,,,,
The JIRA Python misses our assignee when it searches user again,SPARK-44813,13547346,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,15/Aug/23 08:14,21/Aug/23 07:54,30/Oct/23 17:26,18/Aug/23 18:04,4.0.0,,,,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.0,4.0.0,Project Infra,,,,,0,,,,,"{code:java}
>>> assignee = asf_jira.user(""yao"")
>>> ""SPARK-44801""'SPARK-44801'

>>> asf_jira.assign_issue(issue.key, assignee.name)

response text = {""errorMessages"":[],""errors"":{""assignee"":""User 'airhot' cannot be assigned issues.""}} {code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 18 18:04:55 UTC 2023,,,,,,,,,,"0|z1jsds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/23 15:04;srowen;Issue resolved by pull request 42496
[https://github.com/apache/spark/pull/42496];;;","18/Aug/23 18:04;yao;Issue resolved by pull request 42496
[https://github.com/apache/spark/pull/42496];;;",,,,,,,,,,,,,
Data lost after union using spark.sql.parquet.enableNestedColumnVectorizedReader=true,SPARK-44805,13547273,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,jwozniak,jwozniak,14/Aug/23 14:46,13/Sep/23 15:32,30/Oct/23 17:26,08/Sep/23 20:11,3.3.1,3.4.1,,,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.1,4.0.0,Spark Core,,,,,0,correctness,pull-request-available,,,"When union-ing two DataFrames read from parquet containing nested structures (2 fields of array types where one is double and second is integer) data from the second field seems to be lost (zeros are set instead). 

This seems to be the case only if nested vectorised reader is used (spark.sql.parquet.enableNestedColumnVectorizedReader=true). 

The following Python code reproduces the problem: 
{code:java}
from pyspark.sql import SparkSession
from pyspark.sql.types import *

# PREPARING DATA
data1 = []
data2 = []

for i in range(2): 
    data1.append( (([1,2,3],[1,1,2]),i))
    data2.append( (([1.0,2.0,3.0],[1,1]),i+10))

schema1 = StructType([
        StructField('value', StructType([
             StructField('f1', ArrayType(IntegerType()), True),
             StructField('f2', ArrayType(IntegerType()), True)             
             ])),
         StructField('id', IntegerType(), True)
])

schema2 = StructType([
        StructField('value', StructType([
             StructField('f1', ArrayType(DoubleType()), True),
             StructField('f2', ArrayType(IntegerType()), True)             
             ])),
         StructField('id', IntegerType(), True)
])

spark = SparkSession.builder.getOrCreate()
data_dir = ""/user/<user>/""

df1 = spark.createDataFrame(data1, schema1)
df1.write.mode('overwrite').parquet(data_dir + ""data1"") 
df2 = spark.createDataFrame(data2, schema2)
df2.write.mode('overwrite').parquet(data_dir + ""data2"") 


# READING DATA
parquet1 = spark.read.parquet(data_dir + ""data1"")
parquet2 = spark.read.parquet(data_dir + ""data2"")


# UNION
out = parquet1.union(parquet2)


parquet1.select(""value.f2"").distinct().show()
out.select(""value.f2"").distinct().show()
print(parquet1.collect())
print(out.collect()) {code}
Output: 
{code:java}
+---------+
|       f2|
+---------+
|[1, 1, 2]|
+---------+

+---------+
|       f2|
+---------+
|[0, 0, 0]|
|   [1, 1]|
+---------+


[
Row(value=Row(f1=[1, 2, 3], f2=[1, 1, 2]), id=0), 
Row(value=Row(f1=[1, 2, 3], f2=[1, 1, 2]), id=1)
]

[
Row(value=Row(f1=[1.0, 2.0, 3.0], f2=[0, 0, 0]), id=0), 
Row(value=Row(f1=[1.0, 2.0, 3.0], f2=[0, 0, 0]), id=1), 
Row(value=Row(f1=[1.0, 2.0, 3.0], f2=[1, 1]), id=10), 
Row(value=Row(f1=[1.0, 2.0, 3.0], f2=[1, 1]), id=11)
] {code}
Please notice that values for the field f2 are lost after the union is done. This only happens when this data is read from parquet files. 

Could you please look into this? 

Best regards,

Jakub","pySpark, linux, hadoop, parquet. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 08 20:11:31 UTC 2023,,,,,,,,,,"0|z1jrxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/23 00:14;bersprockets;[~sunchao] 

It seems to be some weird interaction between Parquet nested vectorization and the {{Cast}} expression:
{noformat}
drop table if exists t1;

create table t1 using parquet as
select * from values
(named_struct('f1', array(1, 2, 3), 'f2', array(1, 1, 2)))
as (value);

select value from t1;
{""f1"":[1,2,3],""f2"":[1,1,2]}         <== this is expected
Time taken: 0.126 seconds, Fetched 1 row(s)

select cast(value as struct<f1:array<double>,f2:array<int>>) AS value from t1;
{""f1"":[1.0,2.0,3.0],""f2"":[0,0,0]}   <== this is not expected
Time taken: 0.102 seconds, Fetched 1 row(s)

set spark.sql.parquet.enableNestedColumnVectorizedReader=false;

select cast(value as struct<f1:array<double>,f2:array<int>>) AS value from t1;
{""f1"":[1.0,2.0,3.0],""f2"":[1,1,2]}   <== now has expected value
Time taken: 0.244 seconds, Fetched 1 row(s)
{noformat}
The union operation adds this {{Cast}} expression because {{value}} has different datatypes between your two dataframes.;;;","28/Aug/23 15:21;jwozniak;Hello,

Is it possible to know any ETA on this one?

Is this something that could potentially be fixed in the next version of Spark or rather not? 

Thanks,

Jakub;;;","05/Sep/23 23:50;bersprockets;I looked at this yesterday and I think I have a handle on what's going on. I will make a PR in the coming days.;;;","06/Sep/23 07:30;jwozniak;Would be great, thanks! ;;;","07/Sep/23 15:47;bersprockets;PR here: https://github.com/apache/spark/pull/42850;;;","08/Sep/23 03:23;snoot;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/42850;;;","08/Sep/23 20:11;dongjoon;Issue resolved by pull request 42850
[https://github.com/apache/spark/pull/42850];;;",,,,,,,,
SQL Page does not capture failed queries in analyzer ,SPARK-44801,13547185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,14/Aug/23 06:16,06/Sep/23 04:15,30/Oct/23 17:26,24/Aug/23 15:32,3.2.4,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,,,,4.0.0,,,,SQL,Web UI,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 06 04:15:51 UTC 2023,,,,,,,,,,"0|z1jre0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/23 03:13;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42481;;;","24/Aug/23 15:32;Qin Yao;Issue resolved by pull request 42481
[https://github.com/apache/spark/pull/42481];;;","06/Sep/23 04:15;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42825;;;",,,,,,,,,,,,
Fix Scala 2.13 mima check after SPARK-44705 merged,SPARK-44798,13547165,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,14/Aug/23 03:59,15/Aug/23 03:46,30/Oct/23 17:26,15/Aug/23 03:45,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Build,,,,,0,,,,,"{code:java}
[error] spark-core: Failed binary compatibility check against org.apache.spark:spark-core_2.13:3.4.0! Found 1 potential problems (filtered 4019)
8107[error]  * method this(org.apache.spark.api.python.BasePythonRunner,java.io.DataInputStream,org.apache.spark.api.python.BasePythonRunner#WriterThread,Long,org.apache.spark.SparkEnv,java.net.Socket,scala.Option,java.util.concurrent.atomic.AtomicBoolean,org.apache.spark.TaskContext)Unit in class org.apache.spark.api.python.BasePythonRunner#ReaderIterator's type is different in current version, where it is (org.apache.spark.api.python.BasePythonRunner,java.io.DataInputStream,org.apache.spark.api.python.BasePythonRunner#Writer,Long,org.apache.spark.SparkEnv,org.apache.spark.api.python.PythonWorker,scala.Option,java.util.concurrent.atomic.AtomicBoolean,org.apache.spark.TaskContext)Unit instead of (org.apache.spark.api.python.BasePythonRunner,java.io.DataInputStream,org.apache.spark.api.python.BasePythonRunner#WriterThread,Long,org.apache.spark.SparkEnv,java.net.Socket,scala.Option,java.util.concurrent.atomic.AtomicBoolean,org.apache.spark.TaskContext)Unit
8108[error]    filter with: ProblemFilters.exclude[IncompatibleMethTypeProblem](""org.apache.spark.api.python.BasePythonRunner#ReaderIterator.this"")
8109[error] java.lang.RuntimeException: Failed binary compatibility check against org.apache.spark:spark-core_2.13:3.4.0! Found 1 potential problems (filtered 4019)
8110[error] 	at scala.sys.package$.error(package.scala:30)
8111[error] 	at com.typesafe.tools.mima.plugin.SbtMima$.reportModuleErrors(SbtMima.scala:89)
8112[error] 	at com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$2(MimaPlugin.scala:36)
8113[error] 	at com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$2$adapted(MimaPlugin.scala:26)
8114[error] 	at scala.collection.Iterator.foreach(Iterator.scala:943)
8115[error] 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
8116[error] 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
8117[error] 	at com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$1(MimaPlugin.scala:26)
8118[error] 	at com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$1$adapted(MimaPlugin.scala:25)
8119[error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49)
8120[error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:63)
8121[error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:69)
8122[error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:283)
8123[error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:24)
8124[error] 	at sbt.Execute.work(Execute.scala:292)
8125[error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:283)
8126[error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)
8127[error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:65)
8128[error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
8129[error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
8130[error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
8131[error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
8132[error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
8133[error] 	at java.lang.Thread.run(Thread.java:750)
8134[error] (core / mimaReportBinaryIssues) Failed binary compatibility check against org.apache.spark:spark-core_2.13:3.4.0! Found 1 potential problems (filtered 4019)
8135[error] Total time: 162 s (02:42), completed Aug 13, 2023 7:28:17 PM
8136
8137Error:  running /home/runner/work/spark/spark/dev/mima -Phadoop-3 -Pscala-2.13 -Pconnect -Pmesos -Pkubernetes -Phadoop-cloud -Pspark-ganglia-lgpl -Pkinesis-asl -Phive -Pdocker-integration-tests -Pyarn -Pvolcano -Phive-thriftserver ; received return code 1
8138Error: Process completed with exit code 17. {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 15 03:46:40 UTC 2023,,,,,,,,,,"0|z1jr9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/23 03:45;LuciferYang;Issue resolved by pull request 42479
[https://github.com/apache/spark/pull/42479];;;","15/Aug/23 03:46;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/42479;;;",,,,,,,,,,,,,
Many PySpark-related test cases failed in Java 17 daily tests,SPARK-44797,13547164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,LuciferYang,LuciferYang,14/Aug/23 03:56,15/Aug/23 04:16,30/Oct/23 17:26,15/Aug/23 04:16,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Connect,SQL,Tests,,,0,,,,,"The failures started two days ago.
 * [https://github.com/apache/spark/actions/runs/5837423492]
 * [https://github.com/apache/spark/actions/runs/5843658110]
 * [https://github.com/apache/spark/actions/runs/5849761680]

{code:java}
======================================================================
2381FAIL [0.008s]: test_udtf_with_analyze_taking_wrong_number_of_arguments (pyspark.sql.tests.connect.test_parity_udtf.UDTFParityTests)
2382----------------------------------------------------------------------
2383pyspark.errors.exceptions.connect.AnalysisException: [TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON] Failed to analyze the Python user defined table function: Traceback (most recent call last):
2384  File ""/__w/spark/spark/python/lib/pyspark.zip/pyspark/sql/worker/analyze_udtf.py"", line 112, in main
2385    result = handler.analyze(*args)  # type: ignore[attr-defined]
2386TypeError: analyze() takes 0 positional arguments but 1 was given
2387
2388
2389During handling of the above exception, another exception occurred:
2390
2391Traceback (most recent call last):
2392  File ""/__w/spark/spark/python/pyspark/sql/tests/test_udtf.py"", line 1541, in test_udtf_with_analyze_taking_wrong_number_of_arguments
2393    func(lit(1)).collect()
2394AssertionError: ""analyze\(\) missing 1 required positional argument: 'b'"" does not match ""[TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON] Failed to analyze the Python user defined table function: Traceback (most recent call last):
2395  File ""/__w/spark/spark/python/lib/pyspark.zip/pyspark/sql/worker/analyze_udtf.py"", line 112, in main
2396    result = handler.analyze(*args)  # type: ignore[attr-defined]
2397TypeError: analyze() takes 0 positional arguments but 1 was given
2398""
2399
2400----------------------------------------------------------------------
2401Ran 174 tests in 84.628s
2402
2403FAILED (failures=21, errors=7, skipped=6)
2404 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 14 03:57:54 UTC 2023,,,,,,,,,,"0|z1jr9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/23 03:57;LuciferYang;cc [~ruifengz] [~gurwls223] ;;;",,,,,,,,,,,,,,
WholeStageCodeGen pipelineTime metric incorrect result.,SPARK-44793,13547061,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,abmodi,abmodi,13/Aug/23 05:27,13/Aug/23 05:27,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,PipelineTime metric of wholestagecodegen is added multiple times if hasNext is called multiple times after there are no rows left.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-13 05:27:02.0,,,,,,,,,,"0|z1jqzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make ArrowDeserializer work with REPL generated classes,SPARK-44791,13547057,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,hvanhovell,hvanhovell,hvanhovell,13/Aug/23 01:22,14/Aug/23 09:15,30/Oct/23 17:26,14/Aug/23 00:39,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 14 09:15:44 UTC 2023,,,,,,,,,,"0|z1jqyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/23 09:15;githubbot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/42485;;;","14/Aug/23 09:15;githubbot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/42485;;;",,,,,,,,,,,,,
Failure in testing `SparkSessionE2ESuite` using Maven,SPARK-44784,13547036,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,hvanhovell,LuciferYang,LuciferYang,12/Aug/23 13:29,27/Aug/23 03:33,30/Oct/23 17:26,27/Aug/23 03:29,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Connect,Tests,,,,0,,,,,"[https://github.com/apache/spark/actions/runs/5832898984/job/15819181762]

 

The following failures exist in the daily Maven tests, we should fix them before Apache Spark 3.5.0 release:
{code:java}
SparkSessionE2ESuite:
4638- interrupt all - background queries, foreground interrupt *** FAILED ***
4639  The code passed to eventually never returned normally. Attempted 30 times over 20.092924822 seconds. Last failure message: Some(""unexpected failure in q1: org.apache.spark.SparkException: org/apache/spark/sql/connect/client/SparkResult"") was not empty Error not empty: Some(unexpected failure in q1: org.apache.spark.SparkException: org/apache/spark/sql/connect/client/SparkResult). (SparkSessionE2ESuite.scala:71)
4640- interrupt all - foreground queries, background interrupt *** FAILED ***
4641  ""org/apache/spark/sql/connect/client/SparkResult"" did not contain ""OPERATION_CANCELED"" Unexpected exception: org.apache.spark.SparkException: org/apache/spark/sql/connect/client/SparkResult (SparkSessionE2ESuite.scala:105)
4642- interrupt tag *** FAILED ***
4643  The code passed to eventually never returned normally. Attempted 30 times over 20.069445587 seconds. Last failure message: ListBuffer() had length 0 instead of expected length 2 Interrupted operations: ListBuffer().. (SparkSessionE2ESuite.scala:199)
4644- interrupt operation *** FAILED ***
4645  org.apache.spark.SparkException: org/apache/spark/sql/connect/client/SparkResult
4646  at org.apache.spark.sql.connect.client.GrpcExceptionConverter$.toThrowable(GrpcExceptionConverter.scala:89)
4647  at org.apache.spark.sql.connect.client.GrpcExceptionConverter$.convert(GrpcExceptionConverter.scala:38)
4648  at org.apache.spark.sql.connect.client.GrpcExceptionConverter$$anon$1.hasNext(GrpcExceptionConverter.scala:46)
4649  at org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:83)
4650  at org.apache.spark.sql.connect.client.SparkResult.operationId(SparkResult.scala:174)
4651  at org.apache.spark.sql.SparkSessionE2ESuite.$anonfun$new$31(SparkSessionE2ESuite.scala:243)
4652  at org.apache.spark.sql.connect.client.util.RemoteSparkSession.$anonfun$test$1(RemoteSparkSession.scala:243)
4653  at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
4654  at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
4655  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
4656  ... {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Aug 27 03:33:05 UTC 2023,,,,,,,,,,"0|z1jqu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/23 10:08;juliuszsompolski;This is a classloader issue with the serialization of an UDF pulling classes from the client's class context that it doesn't necessarily need, and then resulting in ClassNotFoundException on the server.
org.apache.spark.SparkException: org/apache/spark/sql/connect/client/SparkResult
It's not specifically related to the tests in that suite, but can happen anyplace with UDFs.;;;","16/Aug/23 23:02;hvanhovell;Hmmm, this was supposed to be fixed by stubbing. The underlying issue is that for lambda deserialization we need to be able to load its capturing class. When that class contains any reference to a class we don't have on the classpath (and one we definitely don't want), it will fail with a CNFE.

I will do some digging.;;;","17/Aug/23 06:01;LuciferYang;Because in https://github.com/apache/spark/pull/42523, SparkResult will be moved to the Connect Common module, it will become a class shared by both client and server. Therefore, if https://github.com/apache/spark/pull/42523 is merged first, the existing test cases will not be able to reproduce this issue.;;;","25/Aug/23 04:59;gurwls223;[~LuciferYang] BTW, is this a real blocker? or test-only issue?;;;","25/Aug/23 05:13;LuciferYang;-Just a test-only issue,  not a real blocker, but we should at least make the maven test pass in 3.5 release, even if it means just removing these two cases.-

 

Sorry, I misunderstood the Jira ticket. I'm more inclined to think this is a testing issue now. What do you think? [~hvanhovell] ;;;","27/Aug/23 03:29;LuciferYang;Issue resolved by pull request 42591
[https://github.com/apache/spark/pull/42591];;;","27/Aug/23 03:33;snoot;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/42591;;;",,,,,,,,
SaveMode.ErrorIfExists does not work with kafka-sql,SPARK-44774,13546956,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dolfinus,dolfinus,11/Aug/23 09:46,11/Aug/23 09:48,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"I' trying to write batch dataframe to Kafka topic with {{mode=""error""}}, but when topic exists it does not raise exception. Instead it appends data to a topic.

Steps to reproduce:

1. Start Kafka:

docker-compose.yml
{code:yaml}
version: '3.9'

services:
  zookeeper:
    image: bitnami/zookeeper:3.8
    environment:
      ALLOW_ANONYMOUS_LOGIN: 'yes'

  kafka:
    image: bitnami/kafka:latest
    restart: unless-stopped
    ports:
    - 9093:9093
    environment:
      ALLOW_PLAINTEXT_LISTENER: 'yes'
      KAFKA_ENABLE_KRAFT: 'no'
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: INTERNAL_PLAINTEXT_ANONYMOUS
      KAFKA_CFG_LISTENERS: INTERNAL_PLAINTEXT_ANONYMOUS://:9092,EXTERNAL_PLAINTEXT_ANONYMOUS://:9093
      KAFKA_CFG_ADVERTISED_LISTENERS: INTERNAL_PLAINTEXT_ANONYMOUS://kafka:9092,EXTERNAL_PLAINTEXT_ANONYMOUS://localhost:9093
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL_PLAINTEXT_ANONYMOUS:PLAINTEXT,EXTERNAL_PLAINTEXT_ANONYMOUS:PLAINTEXT
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: 'true'
    depends_on:
    - zookeeper
{code}

{code:bash}
docker-compose up -d
{code}

2. Start Spark session:

{code:bash}
pip install pyspark[sql]==3.4.1
{code}


{code:python}
from pyspark.sql import SparkSession

spark = SparkSession.builder.config(""spark.jars.packages"", ""org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1"").getOrCreate()
{code}

3. Create DataFrame and write it to Kafka. First write using {{mode=""append""}} to create topic, then with {{mode=""error""}} to raise because topic already exist:
{code}
df = spark.createDataFrame([{""value"": ""string""}])
df.write.format(""kafka"").option(""kafka.bootstrap.servers"", ""localhost:9093"").option(""topic"", ""new_topic"").mode(""append"").save()

# no exception is raised
df.write.format(""kafka"").option(""kafka.bootstrap.servers"", ""localhost:9093"").option(""topic"", ""new_topic"").mode(""error"").save()
{code}

4. Check topic content - 2 rows are added to topic instead of one:
{code:python}
spark.read.format(""kafka"").option(""kafka.bootstrap.servers"", ""localhost:9093"").option(""subscribe"", ""new_topic"").load().show(10, False)
{code}
{code}
+----+-------------------+---------+---------+------+-----------------------+-------------+
|key |value              |topic    |partition|offset|timestamp              |timestampType|
+----+-------------------+---------+---------+------+-----------------------+-------------+
|null|[73 74 72 69 6E 67]|new_topic|0        |0     |2023-08-11 09:39:35.813|0            |
|null|[73 74 72 69 6E 67]|new_topic|0        |1     |2023-08-11 09:39:36.122|0            |
+----+-------------------+---------+---------+------+-----------------------+-------------+
{code}

It looks like mode is checked by KafkaSourceProvider, but is not used at all:
https://github.com/apache/spark/blob/v3.4.1/connector/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceProvider.scala#L172-L178

So data is always appended to topic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-11 09:46:54.0,,,,,,,,,,"0|z1jqc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading blocks from remote executors  causes timeout issue,SPARK-44772,13546909,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,,,nebiaydin,nebiaydin,11/Aug/23 03:54,14/Aug/23 05:47,30/Oct/23 17:26,,3.1.2,,,,,,,,,,,,,,,,,,,,,,,EC2,PySpark,Shuffle,Spark Core,,0,,,,,"I'm using EMR 6.5 with Spark 3.1.2

I'm shuffling 1.5 TiB of data with 3000 executors with 4 cores 23 gig memory for executors

Also speculative mode is on.
{code:java}
// df.repartition(6000) {code}
I see lots of failures with 
{code:java}
2023-08-11 01:01:09,846 ERROR org.apache.spark.network.server.ChunkFetchRequestHandler (shuffle-server-4-95): Error sending result ChunkFetchSuccess[streamChunkId=StreamChunkId[streamId=779084003612,chunkIndex=323],buffer=FileSegmentManagedBuffer[file=/mnt3/yarn/usercache/zeppelin/appcache/application_1691438567823_0012/blockmgr-0d82ca05-9429-4ff2-9f61-e779e8e60648/07/shuffle_5_114492_0.data,offset=1836997,length=618]] to /172.31.20.110:36654; closing connection
java.nio.channels.ClosedChannelException
	at org.sparkproject.io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
	at org.sparkproject.io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
	at org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:709)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:792)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:702)
	at org.sparkproject.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:110)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:709)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:792)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:702)
	at org.sparkproject.io.netty.handler.timeout.IdleStateHandler.write(IdleStateHandler.java:302)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:790)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:808)
	at org.sparkproject.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1025)
	at org.sparkproject.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:294)
	at org.apache.spark.network.server.ChunkFetchRequestHandler.respond(ChunkFetchRequestHandler.java:142)
	at org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:116)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
	at org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:470)
	at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:148)
	at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:111)
	at org.sparkproject.io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:362)
	at org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel.doWriteInternal(AbstractNioByteChannel.java:235)
	at org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel.doWrite0(AbstractNioByteChannel.java:209)
	at org.sparkproject.io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:400)
	at org.sparkproject.io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:930)
	at org.sparkproject.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:354)
	at org.sparkproject.io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:897)
	at org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1372)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:750)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:742)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:728)
	at org.sparkproject.io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:127)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:750)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:765)
	... 39 more {code}
 

I tried to set this for kernel

```

sudo ethtool -K eth0 tso off
sudo ethtool -K eth0 sg off

```

Didn't work. I guess external shuffle service is not able to send to data to other executors due to some reason.

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-11 03:54:09.0,,,,,,,,,,"0|z1jq1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve WSCG handling of row buffer by accounting for executor memory  .  Exploding nested arrays can easily lead to out of memory errors. ,SPARK-44768,13546887,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tafranky@gmail.com,tafranky@gmail.com,10/Aug/23 22:36,11/Aug/23 03:36,30/Oct/23 17:26,,3.3.2,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,,,,Optimizer,,,,,0,,,,,"The code sample below is to showcase the wholestagecodegen generated when exploding nested arrays.  The data sample in the dataframe is quite small so it will not trigger the Out of Memory error . 

However if the array is larger and the row size is large , you will definitely end up with an OOM error .  

 

consider a scenario where you flatten  a nested array 

// e.g you can use the following steps to create the dataframe 

//create a partClass case class
case class partClass (PARTNAME: String , PartNumber: String , PartPrice : Double )

//create a nested array array class
case  class array_array_class (
 col_int: Int,
 arr_arr_string : Seq[Seq[String]],
 arr_arr_bigint : Seq[Seq[Long]],
 col_string     : String,
 parts_s        : Seq[Seq[partClass]]
 
)

//create a dataframe
var df_array_array = sc.parallelize(
 Seq(
 (1,Seq(Seq(""a"",""b"" ,""c"" ,""d"") ,Seq(""aa"",""bb"" ,""cc"",""dd"")) , Seq(Seq(1000,20000), Seq(30000,-10000)),""ItemPart1"",
  Seq(Seq(partClass(""PNAME1"",""P1"",20.75),partClass(""PNAME1_1"",""P1_1"",30.75)))
 ) ,
 
 (2,Seq(Seq(""ab"",""bc"" ,""cd"" ,""de"") ,Seq(""aab"",""bbc"" ,""ccd"",""dde""),Seq(""aaaaaabbbbb"")) , Seq(Seq(-1000,-20000,-1,-2), Seq(0,30000,-10000)),""ItemPart2"",
  Seq(Seq(partClass(""PNAME2"",""P2"",170.75),partClass(""PNAME2_1"",""P2_1"",33.75),partClass(""PNAME2_2"",""P2_2"",73.75)))
 )
  
 )

).toDF(""c1"" ,""c2"" ,""c3"" ,""c4"" ,""c5"")

//explode a nested array 

var  result   =  df_array_array.select( col(""c1""), explode(col(""c2""))).select('c1 , explode('col))

result.explain

 

The physical for this operator is seen below.

-------------------------------------
Physical plan 

== Physical Plan ==
*(1) Generate explode(col#27), [c1#17|#17], false, [col#30|#30]
+- *(1) Filter ((size(col#27, true) > 0) AND isnotnull(col#27))
   +- *(1) Generate explode(c2#18), [c1#17|#17], false, [col#27|#27]
      +- *(1) Project [_1#6 AS c1#17, _2#7 AS c2#18|#6 AS c1#17, _2#7 AS c2#18]
         +- *(1) Filter ((size(_2#7, true) > 0) AND isnotnull(_2#7))
            +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple5, true]))._1 AS _1#6, mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -2), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -2), StringType, ObjectType(class java.lang.String)), true, false, true), validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), ArrayType(StringType,true), ObjectType(interface scala.collection.Seq)), None), knownnotnull(assertnotnull(input[0, scala.Tuple5, true]))._2, None) AS _2#7, mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -3), mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -4), assertnotnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -4), IntegerType, IntegerType)), validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -3), ArrayType(IntegerType,false), ObjectType(interface scala.collection.Seq)), None), knownnotnull(assertnotnull(input[0, scala.Tuple5, true]))._3, None) AS _3#8, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, scala.Tuple5, true]))._4, true, false, true) AS _4#9, mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -5), mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -6), if (isnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -6), StructField(PARTNAME,StringType,true), StructField(PartNumber,StringType,true), StructField(PartPrice,DoubleType,false), ObjectType(class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$partClass)))) null else named_struct(PARTNAME, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -6), StructField(PARTNAME,StringType,true), StructField(PartNumber,StringType,true), StructField(PartPrice,DoubleType,false), ObjectType(class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$partClass))).PARTNAME, true, false, true), PartNumber, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -6), StructField(PARTNAME,StringType,true), StructField(PartNumber,StringType,true), StructField(PartPrice,DoubleType,false), ObjectType(class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$partClass))).PartNumber, true, false, true), PartPrice, knownnotnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -6), StructField(PARTNAME,StringType,true), StructField(PartNumber,StringType,true), StructField(PartPrice,DoubleType,false), ObjectType(class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$partClass))).PartPrice), validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -5), ArrayType(StructType(StructField(PARTNAME,StringType,true),StructField(PartNumber,StringType,true),StructField(PartPrice,DoubleType,false)),true), ObjectType(interface scala.collection.Seq)), None), knownnotnull(assertnotnull(input[0, scala.Tuple5, true]))._5, None) AS _5#10]
               +- Scan[obj#5|#5]

 

 

Because the explode function can create multiple rows from a single row  , we should account for the memory available when adding rows to the buffer .  

 

This is even more important when we are exploding nested arrays . ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/23 03:32;tafranky@gmail.com;image-2023-08-10-20-32-55-684.png;https://issues.apache.org/jira/secure/attachment/13062044/image-2023-08-10-20-32-55-684.png","11/Aug/23 00:21;tafranky@gmail.com;spark-jira_wscg_code.txt;https://issues.apache.org/jira/secure/attachment/13062043/spark-jira_wscg_code.txt",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 11 03:33:02 UTC 2023,,,,,,,,,,"0|z1jpww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/23 03:33;tafranky@gmail.com;!image-2023-08-10-20-32-55-684.png!;;;",,,,,,,,,,,,,,
Fix a bug of promoting string as double in binary arithmetic with interval  ,SPARK-44763,13546858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,10/Aug/23 18:03,11/Aug/23 02:20,30/Oct/23 17:26,11/Aug/23 02:20,4.0.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"The following query works on branch-3.5 or below, but fails on the latest master:

```
select concat(DATE'2020-12-31', ' ', date_format('09:03:08', 'HH:mm:ss')) + (INTERVAL '03' HOUR)
```
 
The direct reason is now we mark `cast(date as string)` as resolved during type coercion after changes [https://github.com/apache/spark/pull/42089.] As a result, there are two transforms from CombinedTypeCoercionRule
```
Rule ConcatCoercion Transformed concat(2020-12-31,  , date_format(cast(09:03:08 as timestamp), HH:mm:ss, Some(America/Los_Angeles))) to concat(cast(2020-12-31 as string),  , date_format(cast(09:03:08 as timestamp), HH:mm:ss, Some(America/Los_Angeles)))

Rule PromoteStrings Transformed (concat(cast(2020-12-31 as string),  , date_format(cast(09:03:08 as timestamp), HH:mm:ss, Some(America/Los_Angeles))) + INTERVAL '03' HOUR) to (cast(concat(cast(2020-12-31 as string),  , date_format(cast(09:03:08 as timestamp), HH:mm:ss, Some(America/Los_Angeles))) as double) + INTERVAL '03' HOUR)
```  
The second transform doesn't happen in previous releases since cast(2020-12-31 as string)  used to be unresolved after the first transform.
 
The fix is simple, the analyzer should not promote string as double in binary arithmetic with ANSI interval. The changes in [https://github.com/apache/spark/pull/42089|https://github.com/apache/spark/pull/42089.] are valid and we should keep it.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 11 02:20:47 UTC 2023,,,,,,,,,,"0|z1jpqg:",9223372036854775807,,,,,,,,,,,,,4.0.0,,,,,,,,,,"11/Aug/23 02:20;Gengliang.Wang;Resolved in https://github.com/apache/spark/pull/42436;;;",,,,,,,,,,,,,,
Index Out Of Bound for JIRA resolution in merge_spark_pr,SPARK-44760,13546809,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,10/Aug/23 10:20,11/Aug/23 10:23,30/Oct/23 17:26,11/Aug/23 10:21,4.0.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Project Infra,,,,,0,,,,,I,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 11 10:21:42 UTC 2023,,,,,,,,,,"0|z1jpfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 15:08;dongjoon;Issue resolved by pull request 42429
[https://github.com/apache/spark/pull/42429];;;","11/Aug/23 02:15;gurwls223;Reverted at https://github.com/apache/spark/commit/3164ff51dd249670b8505a5ea8d361cf53c9db94;;;","11/Aug/23 10:21;yao;Issue resolved by pull request 42429
[https://github.com/apache/spark/pull/42429];;;",,,,,,,,,,,,
Do not combine multiple Generate operators in the same WholeStageCodeGen node because it can  easily cause OOM failures if arrays are relatively large,SPARK-44759,13546803,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tafranky@gmail.com,tafranky@gmail.com,10/Aug/23 09:23,13/Aug/23 06:03,30/Oct/23 17:26,,3.0.0,3.0.1,3.0.2,3.0.3,3.1.0,3.1.1,3.1.2,3.1.3,3.2.0,3.2.1,3.2.2,3.2.3,3.2.4,3.3.0,3.3.1,3.3.2,3.4.0,3.4.1,,,,,,Deploy,Optimizer,Spark Core,,,0,,,,,"This is an issue since the WSCG  implementation of the generate node. 

Because WSCG compute rows in batches , the combination of WSCG and the explode operation consume a lot of the dedicated executor memory. This is even more true when the WSCG node contains multiple explode nodes. This is the case when flattening a nested array.

The generate node used to flatten array generally  produces an amount of output rows that is significantly higher than the input rows.

the number of output rows generated is even drastically higher when flattening a nested array .

When we combine more that 1 generate node in the same WholeStageCodeGen  node, we run  a high risk of running out of memory for multiple reasons. 

1- As you can see from snapshots added in the comments ,  the rows created in the nested loop are saved in a writer buffer.  In this case because the rows were big , the job failed with an Out Of Memory Exception error .

2_ The generated WholeStageCodeGen result in a nested loop that for each row  , will explode the parent array and then explode the inner array.  The rows are accumulated in the writer buffer without accounting for the row size.

Please view the attached Spark Gui and Spark Dag 

In my case the wholestagecodegen includes 2 explode nodes. 

Because the array elements are large , we end up with an Out Of Memory error. 

 

I recommend that we do not merge  multiple explode nodes in the same whole stage code gen node . Doing so leads to potential memory issues.

In our case , the job execution failed with an  OOM error because the the WSCG executed  into a nested for loop . 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 16:27;tafranky@gmail.com;image-2023-08-10-09-27-24-124.png;https://issues.apache.org/jira/secure/attachment/13062036/image-2023-08-10-09-27-24-124.png","10/Aug/23 16:29;tafranky@gmail.com;image-2023-08-10-09-29-24-804.png;https://issues.apache.org/jira/secure/attachment/13062037/image-2023-08-10-09-29-24-804.png","10/Aug/23 16:32;tafranky@gmail.com;image-2023-08-10-09-32-46-163.png;https://issues.apache.org/jira/secure/attachment/13062038/image-2023-08-10-09-32-46-163.png","10/Aug/23 16:33;tafranky@gmail.com;image-2023-08-10-09-33-47-788.png;https://issues.apache.org/jira/secure/attachment/13062039/image-2023-08-10-09-33-47-788.png","10/Aug/23 09:25;tafranky@gmail.com;wholestagecodegen_wc1_debug_wholecodegen_passed;https://issues.apache.org/jira/secure/attachment/13062030/wholestagecodegen_wc1_debug_wholecodegen_passed",,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 10 16:34:45 UTC 2023,,,,,,,,,,"0|z1jpe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 16:27;tafranky@gmail.com;WSCG  generated code that calls  generate_doConsume_0

!image-2023-08-10-09-27-24-124.png!;;;","10/Aug/23 16:29;tafranky@gmail.com;WSCG  generated code for first Generate node 

!image-2023-08-10-09-29-24-804.png!;;;","10/Aug/23 16:33;tafranky@gmail.com;WSCG  generated code for second Generate node 

!image-2023-08-10-09-32-46-163.png!;;;","10/Aug/23 16:34;tafranky@gmail.com;Spark Dag for the use case . The failure is from the execution of WholeStageCodeGen(2)

!image-2023-08-10-09-33-47-788.png!;;;",,,,,,,,,,,
Vulnerabilities in Spark3.4,SPARK-44757,13546799,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,abalasub,abalasub,10/Aug/23 09:16,11/Oct/23 08:14,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"We are seeing below list of TPLS's with vulnerabilities bundled with Spark3.4 package with StackRox scan , is there any ETA on fixing them ? Kindly apprise us on the same .
h2. Vulnerabilities in Spark3.4
|*CVE*|*Description*|*Severity*|
|CVE-2018-21234|Jodd before 5.0.4 performs Deserialization of Untrusted JSON Data when setClassMetadataName is set.|CVSS Score:9.8Critical|
|CVE-2022-42004|In FasterXML jackson-databind before 2.13.4, resource exhaustion can occur because of a lack of a check in BeanDeserializer._deserializeFromArray to prevent use of deeply nested arrays. An application is vulnerable only with certain customized choices for deserialization.|CVSS Score 7.5Important|
| CVE-2022-42003|In FasterXML jackson-databind before 2.14.0-rc1, resource exhaustion can occur because of a lack of a check in primitive value deserializers to avoid deep wrapper array nesting, when the UNWRAP_SINGLE_VALUE_ARRAYS feature is enabled. Additional fix version in 2.13.4.1 and 2.12.17.1|CVSS Score 7.5Important|
|CVE-2022-40152|Those using Woodstox to parse XML data may be vulnerable to Denial of Service attacks (DOS) if DTD support is enabled. If the parser is running on user supplied input, an attacker may supply content that causes the parser to crash by stackoverflow. This effect may support a denial of service attack.|CVSS Score 7.5Important|
|CVE-2022-3171|A parsing issue with binary data in protobuf-java core and lite versions prior to 3.21.7, 3.20.3, 3.19.6 and 3.16.3 can lead to a denial of service attack. Inputs containing multiple instances of non-repeated embedded messages with repeated or unknown fields causes objects to be converted back-n-forth between mutable and immutable forms, resulting in potentially long garbage collection pauses. We recommend updating to the versions mentioned above.|CVSS Score 7.5Important|
|CVE-2021-34538|Apache Hive before 3.1.3 ""CREATE"" and ""DROP"" function operations does not check for necessary authorization of involved entities in the query. It was found that an unauthorized user can manipulate an existing UDF without having the privileges to do so. This allowed unauthorized or underprivileged users to drop and recreate UDFs pointing them to new jars that could be potentially malicious.|CVSS Score 7.5Important|
|CVE-2020-13949|In Apache Thrift 0.9.3 to 0.13.0, malicious RPC clients could send short messages which would result in a large memory allocation, potentially leading to denial of service.|CVSS Score 7.5Important|
|CVE-2018-10237|Unbounded memory allocation in Google Guava 11.0 through 24.x before 24.1.1 allows remote attackers to conduct denial of service attacks against servers that depend on this library and deserialize attacker-provided data, because the AtomicDoubleArray class (when serialized with Java serialization) and the CompoundOrdering class (when serialized with GWT serialization) perform eager allocation without appropriate checks on what a client has sent and whether the data size is reasonable.|CVSS 5.9Moderate|
|CVE-2021-22569|An issue in protobuf-java allowed the interleaving of com.google.protobuf.UnknownFieldSet fields in such a way that would be processed out of order. A small malicious payload can occupy the parser for several minutes by creating large numbers of short-lived objects that cause frequent, repeated pauses. We recommend upgrading libraries beyond the vulnerable versions.|CVSS 5.9Moderate|
|CVE-2020-8908|A temp directory creation vulnerability exists in all versions of Guava, allowing an attacker with access to the machine to potentially access data in a temporary directory created by the Guava API [com.google.common.io|https://urldefense.com/v3/__http:/com.google.common.io/__;!!KpaPruflFCEp!hUy3fNZoxf_mnbeTP7GUWkbaKtRLDswR2fRnQ9Gm_AoaeVUncE_plq53EqTWyd1ZfAI7tIFOgmmEBPoGRw$].Files.createTempDir(). By default, on unix-like systems, the created directory is world-readable (readable by an attacker with access to the system). The method in question has been marked @Deprecated in versions 30.0 and later and should not be used. For Android developers, we recommend choosing a temporary directory API provided by Android, such as context.getCacheDir(). For other Java developers, we recommend migrating to the Java 7 API java.nio.file.Files.createTempDirectory() which explicitly configures permissions of 700, or configuring the Java runtime's [java.io|https://urldefense.com/v3/__http:/java.io/__;!!KpaPruflFCEp!hUy3fNZoxf_mnbeTP7GUWkbaKtRLDswR2fRnQ9Gm_AoaeVUncE_plq53EqTWyd1ZfAI7tIFOgmmRx77EAw$].tmpdir system property to point to a location whose permissions are appropriately configured.|CVSS 3.3Low|

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 11 08:11:29 UTC 2023,,,,,,,,,,"0|z1jpdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/23 08:11;julienlau;Hi,

In addition to this I'd like to add the following CVE:
h1. CVE-2022-1471 (High) detected in snakeyaml-1.33.jar

SnakeYaml's Constructor() class does not restrict types which can be instantiated during deserialization. Deserializing yaml content provided by an attacker can lead to remote code execution. We recommend using SnakeYaml's SafeConsturctor when parsing untrusted content to restrict deserialization.

Publish Date: 2022-12-01

URL: [CVE-2022-1471|https://www.mend.io/vulnerability-database/CVE-2022-1471];;;",,,,,,,,,,,,,,
Executor hangs when RetryingBlockTransferor fails to initiate retry,SPARK-44756,13546779,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hdaikoku,hdaikoku,hdaikoku,10/Aug/23 07:36,26/Sep/23 16:39,30/Oct/23 17:26,26/Sep/23 16:08,3.3.1,,,,,,,,,,,,,,,,,,,4.0.0,,,,Shuffle,Spark Core,,,,0,pull-request-available,,,,"We have been observing this issue several times in our production where some executors are being stuck at BlockTransferService#fetchBlockSync().

After some investigation, the issue seems to be caused by an unhandled edge case in RetryingBlockTransferor.

1. Shuffle transfer fails for whatever reason
{noformat}
java.io.IOException: Cannot allocate memory
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at org.apache.spark.network.shuffle.SimpleDownloadFile$SimpleDownloadWritableChannel.write(SimpleDownloadFile.java:78)
	at org.apache.spark.network.shuffle.OneForOneBlockFetcher$DownloadCallback.onData(OneForOneBlockFetcher.java:340)
	at org.apache.spark.network.client.StreamInterceptor.handle(StreamInterceptor.java:79)
	at org.apache.spark.network.util.TransportFrameDecoder.feedInterceptor(TransportFrameDecoder.java:263)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:87)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
{noformat}
2. The above exception caught by [AbstractChannelHandlerContext#invokeChannelRead()|https://github.com/netty/netty/blob/netty-4.1.74.Final/transport/src/main/java/io/netty/channel/AbstractChannelHandlerContext.java#L381], and propagated to the exception handler

3. Exception reaches [RetryingBlockTransferor#initiateRetry()|https://github.com/apache/spark/blob/v3.3.1/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RetryingBlockTransferor.java#L178-L180], and it tries to initiate retry
{noformat}
23/08/09 16:58:37 shuffle-client-4-2 INFO RetryingBlockTransferor: Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms
{noformat}
4. Retry initiation fails (in our case, it fails to create a new thread)

5. Exception caught by [AbstractChannelHandlerContext#invokeExceptionCaught()|https://github.com/netty/netty/blob/netty-4.1.74.Final/transport/src/main/java/io/netty/channel/AbstractChannelHandlerContext.java#L305-L309], and not further processed
{noformat}
23/08/09 16:58:53 shuffle-client-4-2 DEBUG AbstractChannelHandlerContext: An exception java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:719)
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:957)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1378)
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112)
	at org.apache.spark.network.shuffle.RetryingBlockTransferor.initiateRetry(RetryingBlockTransferor.java:182)
	at org.apache.spark.network.shuffle.RetryingBlockTransferor.access$500(RetryingBlockTransferor.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.handleBlockTransferFailure(RetryingBlockTransferor.java:230)
	at org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.onBlockFetchFailure(RetryingBlockTransferor.java:260)
	at org.apache.spark.network.shuffle.OneForOneBlockFetcher.failRemainingBlocks(OneForOneBlockFetcher.java:318)
	at org.apache.spark.network.shuffle.OneForOneBlockFetcher.access$300(OneForOneBlockFetcher.java:55)
	at org.apache.spark.network.shuffle.OneForOneBlockFetcher$DownloadCallback.onFailure(OneForOneBlockFetcher.java:357)
	at org.apache.spark.network.client.StreamInterceptor.exceptionCaught(StreamInterceptor.java:56)
	at org.apache.spark.network.util.TransportFrameDecoder.exceptionCaught(TransportFrameDecoder.java:231)
	at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
{noformat}
6. After all, retry never happens and the executor thread ends up being stuck at [BlockTransferService#fetchBlockSync()|https://github.com/apache/spark/blob/v3.3.1/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala#L103], waiting for the transfer to complete/fail
{noformat}
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)
scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)
scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)
org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)
org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1154)
org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1098)
{noformat}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 26 16:08:03 UTC 2023,,,,,,,,,,"0|z1jp8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 12:13;hdaikoku;PR raised: https://github.com/apache/spark/pull/42426;;;","10/Aug/23 12:20;ggintegration;User 'hdaikoku' has created a pull request for this issue:
https://github.com/apache/spark/pull/42426;;;","26/Sep/23 16:08;mridulm80;Issue resolved by pull request 42426
[https://github.com/apache/spark/pull/42426];;;",,,,,,,,,,,,
Local tmp data is not cleared while using spark streaming consuming from kafka,SPARK-44755,13546776,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,xleesf,xleesf,10/Aug/23 06:56,10/Aug/23 06:56,30/Oct/23 17:26,,3.2.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"we are using spark 3.2 consuming data from kafka and then using `collectAsMap` to send to driver, we found the local temp file do not get cleared if the data consumed from kafka is larger than 200m(spark.network.maxRemoteBlockSizeFetchToMem)

!https://intranetproxy.alipay.com/skylark/lark/0/2023/png/320711/1691419276170-2dd0964f-4cf4-4b15-9fbe-9622116671da.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-10 06:56:38.0,,,,,,,,,,"0|z1jp88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Conflicting attribute during join two times the same table (AQE is disabled),SPARK-44739,13546646,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kondziolka9ld,kondziolka9ld,09/Aug/23 08:58,09/Aug/23 09:06,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"h2. Issue

I come across a something that seems to be bug in *pyspark* (when I disable adaptive queries). It is about joining two times the same dataframe (please look at reproduction steps below). 
----
h2. Reproduction steps
{code:java}
pyspark --conf spark.sql.adaptive.enabled=false
Python 3.8.10 (default, Nov 14 2022, 12:59:47) 
[GCC 9.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
23/08/09 10:18:54 WARN Utils: Your hostname, kondziolka-dd-laptop resolves to a loopback address: 127.0.1.1; using 192.168.0.18 instead (on interface wlp0s20f3)
23/08/09 10:18:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/08/09 10:18:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/Using Python version 3.8.10 (default, Nov 14 2022 12:59:47)
Spark context Web UI available at http://192.168.0.18:4040
Spark context available as 'sc' (master = local[*], app id = local-1691569137130).
SparkSession available as 'spark'.

>>> sc.setCheckpointDir(""file:///tmp"")
>>> df1=spark.createDataFrame([(1, 42)], [""id"", ""fval""])
>>> df2=spark.createDataFrame([(1, 0, ""jeden"")], [""id"", ""target"", ""aux""]) 
>>> df2.explain()
== Physical Plan ==
*(1) Scan ExistingRDD[id#4L,target#5L,aux#6]
>>> j1=df1.join(df2, [""id""]).select(""fval"", ""aux"").checkpoint()
>>> j1.explain()
== Physical Plan ==
*(1) Scan ExistingRDD[fval#1L,aux#6]
>>> # we see that both j1 and df2 refers to the same attribute aux#6
>>> # let's join df2 to j1. Both of them has aux column.
>>> j1.join(df2, ""aux"")                                                      
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/kondziolkadd/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py"", line 1539, in join
    jdf = self._jdf.join(other._jdf, on, how)
  File ""/home/kondziolkadd/.local/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py"", line 1321, in __call__
  File ""/home/kondziolkadd/.local/lib/python3.8/site-packages/pyspark/sql/utils.py"", line 196, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: 
Failure when resolving conflicting references in Join:
'Join Inner
:- LogicalRDD [fval#1L, aux#6], false
+- LogicalRDD [id#4L, target#5L, aux#6], false

Conflicting attributes: aux#6
;
'Join Inner
:- LogicalRDD [fval#1L, aux#6], false
+- LogicalRDD [id#4L, target#5L, aux#6], false
{code}
 
----
h2. Workaround

The workaround is about renaming columns twice times - I mean identity rename `X -> X' -> X`. It looks like it forces rewrite of metadata (change attribute id) and in this way it avoids conflict.
{code:java}
>>> sc.setCheckpointDir(""file:///tmp"")
>>> df1=spark.createDataFrame([(1, 42)], [""id"", ""fval""])
>>> df2=spark.createDataFrame([(1, 0, ""jeden"")], [""id"", ""target"", ""aux""])
>>> df2.explain()
== Physical Plan ==
*(1) Scan ExistingRDD[id#4L,target#5L,aux#6]
>>> j1=df1.join(df2, [""id""]).select(""fval"", ""aux"").withColumnRenamed(""aux"", ""_aux"").withColumnRenamed(""_aux"", ""aux"").checkpoint()
>>> j1.explain()                                                                
== Physical Plan ==
*(1) Scan ExistingRDD[fval#1L,aux#19]
>>> j1.join(df2, ""aux"")
>>>
{code}
----
h2. Others
 * Repartition before checkpoint is workaround as well (it does not change id of attribute)

{code:java}
>>> j1=df1.join(df2, [""id""]).select(""fval"", ""aux"").repartition(100).checkpoint() 
>>> j1.join(df2, ""aux"") {code}
 * Without `checkpoint` issue does not occur (although id is the same)

{code:java}
>>> j1=df1.join(df2, [""id""]).select(""fval"", ""aux"")
>>> j1.join(df2, ""aux"") {code}
 * Without disabling `AQE` it does not occur
 * I was not able to reproduce it on spark -  by saying that I mean that I reproduced it only in `pyspark`.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,pyspark,python,,,2023-08-09 08:58:28.0,,,,,,,,,,"0|z1jofc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Connect Reattach misses metadata propagation,SPARK-44738,13546636,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,grundprinzip-db,grundprinzip-db,grundprinzip-db,09/Aug/23 07:15,09/Aug/23 20:36,30/Oct/23 17:26,09/Aug/23 09:08,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Connect,,,,,0,,,,,"Currently, in the Spark Connect Reattach handler client metadata is not propgated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-43754,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 09 09:12:10 UTC 2023,,,,,,,,,,"0|z1jod4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/23 09:08;gurwls223;Issue resolved by pull request 42409
[https://github.com/apache/spark/pull/42409];;;","09/Aug/23 09:12;githubbot;User 'grundprinzip' has created a pull request for this issue:
https://github.com/apache/spark/pull/42409;;;",,,,,,,,,,,,,
Should not display json format errors on SQL page for non-SparkThrowables,SPARK-44737,13546625,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,09/Aug/23 04:04,11/Aug/23 07:03,30/Oct/23 17:26,11/Aug/23 07:03,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,SQL,Web UI,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 11 07:03:32 UTC 2023,,,,,,,,,,"0|z1joao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/23 04:19;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42407;;;","09/Aug/23 04:20;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42407;;;","11/Aug/23 07:03;yao;Issue resolved by pull request 42407
[https://github.com/apache/spark/pull/42407];;;",,,,,,,,,,,,
Spark Connect: Cleaner thread not stopped when SparkSession stops,SPARK-44730,13546602,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Not A Problem,,juliuszsompolski,juliuszsompolski,08/Aug/23 23:50,17/Aug/23 16:10,30/Oct/23 17:26,17/Aug/23 16:10,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,,,,,Connect,,,,,0,,,,,"Spark Connect scala client SparkSession has a cleaner, which starts a daemon thread to clean up Closeable objects after GC. This daemon thread is never stopped, and every SparkSession creates a new one.

Cleaner implements a stop() function, but no-one ever calls it. Possibly because even after SparkSession.stop(), the cleaner may still be needed when remaining references are GCed... For this reason it seems that the Cleaner should rather be a global singleton than within a session.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 17 16:09:52 UTC 2023,,,,,,,,,,"0|z1jo5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/23 16:09;juliuszsompolski;Silly me, Cleaner is already global and I looked wrong.;;;",,,,,,,,,,,,,,
INSET hash hset set to None when plan exported into JSON,SPARK-44724,13546575,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,minterlandi,minterlandi,08/Aug/23 18:45,08/Aug/23 18:45,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"I am exporting optimized plans using `_jdf.queryExecution().optimizedPlan().toJSON()`. I noticed that when the plan contains a `INSET` operator the `hset` attribute is None (instead of containing the set elements).

 

When printing directly `_jdf.queryExecution().optimizedPlan()` the `INSET` operator has all the elements so I guess that the problem is with the `toJSON` method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-08 18:45:16.0,,,,,,,,,,"0|z1jnzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
reattach.py: AttributeError: 'NoneType' object has no attribute 'message',SPARK-44722,13546551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,08/Aug/23 14:35,08/Aug/23 23:59,30/Oct/23 17:26,08/Aug/23 23:59,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-43754,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 08 23:59:18 UTC 2023,,,,,,,,,,"0|z1jnu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 23:59;gurwls223;Issue resolved by pull request 42397
[https://github.com/apache/spark/pull/42397];;;",,,,,,,,,,,,,,
NoClassDefFoundError when using Hive UDF,SPARK-44719,13546530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,08/Aug/23 12:18,15/Aug/23 03:43,30/Oct/23 17:26,12/Aug/23 04:47,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Build,SQL,,,,0,,,,,"How to reproduce:
{noformat}
spark-sql (default)> add jar /Users/yumwang/Downloads/HiveUDFs-1.0-SNAPSHOT.jar;
Time taken: 0.413 seconds
spark-sql (default)> CREATE TEMPORARY FUNCTION long_to_ip as 'net.petrabarus.hiveudfs.LongToIP';
Time taken: 0.038 seconds
spark-sql (default)> SELECT long_to_ip(2130706433L) FROM range(10);
23/08/08 20:17:58 ERROR SparkSQLDriver: Failed in [SELECT long_to_ip(2130706433L) FROM range(10)]
java.lang.NoClassDefFoundError: org/codehaus/jackson/map/type/TypeFactory
	at org.apache.hadoop.hive.ql.udf.UDFJson.<clinit>(UDFJson.java:64)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
...
{noformat}
",,,,,,,,,,,,,,,,,,,,SPARK-43225,,,,,,,,,,"08/Aug/23 12:18;yumwang;HiveUDFs-1.0-SNAPSHOT.jar;https://issues.apache.org/jira/secure/attachment/13061981/HiveUDFs-1.0-SNAPSHOT.jar",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Aug 12 04:47:41 UTC 2023,,,,,,,,,,"0|z1jnpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 12:25;yumwang;There are two ways to fix it:
1. Upgrade the built-in hive to 2.3.10 with the following patch.
2. Revert SPARK-43225.

https://github.com/apache/hive/pull/4562
https://github.com/apache/hive/pull/4563
https://github.com/apache/hive/pull/4564;;;","08/Aug/23 13:28;mauzhang;Is there a 2.3.10 release?;;;","08/Aug/23 16:07;dongjoon;No, there is no Apache Hive 2.3.10 release yet.

Given that there is no Apache Hive 2.3.10 yet, +1 for reverting SPARK-43225 (which is also created by [~yumwang]).;;;","11/Aug/23 03:39;snoot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/42447;;;","12/Aug/23 04:47;Qin Yao;Issue resolved by pull request 42446
[https://github.com/apache/spark/pull/42446];;;",,,,,,,,,,
"""pyspark.pandas.resample"" is incorrect when DST is overlapped and setting ""spark.sql.timestampType"" to TIMESTAMP_NTZ does not help",SPARK-44717,13546451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,attilapiros,attilapiros,08/Aug/23 04:01,09/Aug/23 02:04,30/Oct/23 17:26,09/Aug/23 02:04,3.4.0,3.4.1,4.0.0,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Pandas API on Spark,,,,,0,,,,,"Use one of the existing test:
- ""11H"" case of test_dataframe_resample (pyspark.pandas.tests.test_resample.ResampleTests) 
- ""1001H"" case of test_series_resample (pyspark.pandas.tests.test_resample.ResampleTests) 

After setting the TZ for example to New York (like by using the following python code in a ""setUpClass"":  
{noformat}
os.environ[""TZ""] = 'America/New_York'
{noformat})

You will get the error for the latter mentioned test:

{noformat}
======================================================================
FAIL [4.219s]: test_series_resample (pyspark.pandas.tests.test_resample.ResampleTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/__w/spark/spark/python/pyspark/pandas/tests/test_resample.py"", line 276, in test_series_resample
    self._test_resample(self.pdf3.A, self.psdf3.A, [""1001H""], ""right"", ""right"", ""sum"")
  File ""/__w/spark/spark/python/pyspark/pandas/tests/test_resample.py"", line 259, in _test_resample
    self.assert_eq(
  File ""/__w/spark/spark/python/pyspark/testing/pandasutils.py"", line 457, in assert_eq
    _assert_pandas_almost_equal(lobj, robj)
  File ""/__w/spark/spark/python/pyspark/testing/pandasutils.py"", line 228, in _assert_pandas_almost_equal
    raise PySparkAssertionError(
pyspark.errors.exceptions.base.PySparkAssertionError: [DIFFERENT_PANDAS_SERIES] Series are not almost equal:
Left:
Freq: 1001H
float64
Right:
float64
{noformat}

The problem is the in the pyspark resample there will be more resampled rows in the result. The DST change will cause those extra lines as the computed __tmp_resample_bin_col__ be something like:

{noformat}
| __index_level_0__  | __tmp_resample_bin_col__ | A
.....
|2011-03-08 00:00:00|2011-03-26 11:00:00     |0.3980551570183919  |
|2011-03-09 00:00:00|2011-03-26 11:00:00     |0.6511376673995046  |
|2011-03-10 00:00:00|2011-03-26 11:00:00     |0.6141085426890365  |
|2011-03-11 00:00:00|2011-03-26 11:00:00     |0.11557638066163867 |
|2011-03-12 00:00:00|2011-03-26 11:00:00     |0.4517788243490799  |
|2011-03-13 00:00:00|2011-03-26 11:00:00     |0.8637060550157284  |
|2011-03-14 00:00:00|2011-03-26 10:00:00     |0.8169499149450166  |
|2011-03-15 00:00:00|2011-03-26 10:00:00     |0.4585916249356583  |
|2011-03-16 00:00:00|2011-03-26 10:00:00     |0.8362472880832088  |
|2011-03-17 00:00:00|2011-03-26 10:00:00     |0.026716901748386812|
|2011-03-18 00:00:00|2011-03-26 10:00:00     |0.9086816462089563  |
{noformat}

You can see the extra lines around when the DST kicked in on 2011-03-13 in New York.

Even setting the conf ""spark.sql.timestampType"" to""TIMESTAMP_NTZ"" does not help.

You can see my tests here:
https://github.com/attilapiros/spark/pull/5

Pandas timestamps are TZ less:
`
{noformat}
import pandas as pd
a = pd.Timestamp(year=2011, month=3, day=13, hour=1)
b = pd.Timedelta(hours=1)

>> a 
Timestamp('2011-03-13 01:00:00')
>>> a+b
Timestamp('2011-03-13 02:00:00')
>>> a+b+b
Timestamp('2011-03-13 03:00:00')
{noformat}

But pyspark TimestampType uses TZ and DST:

{noformat}
>>> sql(""select  TIMESTAMP '2011-03-13 01:00:00'"").show()
+-------------------------------+
|TIMESTAMP '2011-03-13 01:00:00'|
+-------------------------------+
|            2011-03-13 01:00:00|
+-------------------------------+

>>> sql(""select  TIMESTAMP '2011-03-13 01:00:00' + make_interval(0,0,0,0,1,0,0)"").show()
+--------------------------------------------------------------------+
|TIMESTAMP '2011-03-13 01:00:00' + make_interval(0, 0, 0, 0, 1, 0, 0)|
+--------------------------------------------------------------------+
|                                                 2011-03-13 03:00:00|
+--------------------------------------------------------------------+
{noformat}

The current resample code uses the above interval based calculation.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 09 02:04:12 UTC 2023,,,,,,,,,,"0|z1jn7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 04:05;attilapiros;cc [~itholic] [~gurwls223]
;;;","08/Aug/23 04:14;gurwls223;cc [~podongfeng];;;","08/Aug/23 05:26;gurwls223;I think we should at least make this respects {{spark.sql.timestampType}} when it;s set to {{TIMESTAMP_NTZ}}. Took a quick look, and I think there's some problem in calculation logic in https://github.com/apache/spark/blob/master/python/pyspark/pandas/resample.py#L277-L310 especially date_trunc returns always {{TimestampType}}. We might need a dedicated internal expression cc [~podongfeng];;;","08/Aug/23 07:51;gurwls223;[~attilapiros] which time zone are you in? Would you mind trying this one below:

{code}
sql(""select  TIMESTAMP_NTZ '2011-03-13 01:00:00' + make_interval(0,0,0,0,1,0,0)"").show()
{code};;;","08/Aug/23 08:54;attilapiros;The TIMESTAMP_NTZ would work for sure.

Here is the test:

{noformat}
$ export TZ=""America/New_York""
$ ./bin/pyspark
....
>>> sql(""select  TIMESTAMP_NTZ '2011-03-13 01:00:00' + make_interval(0,0,0,0,1,0,0)"").show()

+------------------------------------------------------------------------+
|TIMESTAMP_NTZ '2011-03-13 01:00:00' + make_interval(0, 0, 0, 0, 1, 0, 0)|
+------------------------------------------------------------------------+
|                                                     2011-03-13 02:00:00|
+------------------------------------------------------------------------+

>>> sql(""select  TIMESTAMP '2011-03-13 01:00:00' + make_interval(0,0,0,0,1,0,0)"").show()
+--------------------------------------------------------------------+
|TIMESTAMP '2011-03-13 01:00:00' + make_interval(0, 0, 0, 0, 1, 0, 0)|
+--------------------------------------------------------------------+
|                                                 2011-03-13 03:00:00|
+--------------------------------------------------------------------+
{noformat}

;;;","08/Aug/23 11:17;gurwls223;Made a quick fix: https://github.com/apache/spark/pull/42392. I believe there are other corner cases like this a lot .. but the PR fixes this one alone for now.;;;","09/Aug/23 02:04;gurwls223;Issue resolved by pull request 42392
[https://github.com/apache/spark/pull/42392];;;",,,,,,,,
Fix flow control in ExecuteGrpcResponseSender,SPARK-44709,13546423,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,07/Aug/23 22:24,08/Aug/23 16:33,30/Oct/23 17:26,08/Aug/23 16:33,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-43754,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-07 22:24:23.0,,,,,,,,,,"0|z1jn1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProductEncoder not working as expected within User Defined Fuction (UDF),SPARK-44706,13546384,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jcblancomartinez,jcblancomartinez,07/Aug/23 15:39,07/Aug/23 16:11,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Hi,

When running the following code in Databricks' notebook:
{code:java}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}
  
  case class DataRow(field1: String)
  val sparkSession = SparkSession.builder.getOrCreate()
  import sparkSession.implicits._
  val udf1 = udf((x: String) => {
    val testData = Seq(DataRow(""test1""), DataRow(""test2"")).toDF(""test"") // This is failing at runtime
    3
  })
  val df1 = Seq(DataRow(""test1""), DataRow(""test2"")).toDF(""test"").withColumn(""udf"", udf1($""test"")) // This is working
  display(df1)
{code}
I get a ScalaReflectionException at runtime:
{code:java}
ScalaReflectionException: class $linedb3da7b2933d4b63a62b3d2a21c675f2141.$read in JavaMirror with com.databricks.backend.daemon.driver.DriverLocal$DriverLocalClassLoader@717115ad of type class com.databricks.backend.daemon.driver.DriverLocal$DriverLocalClassLoader with classpath [] and parent being com.databricks.backend.daemon.driver.ClassLoaders$ReplWrappingClassLoader@1670897 of type class com.databricks.backend.daemon.driver.ClassLoaders$ReplWrappingClassLoader with classpath [<unknown>] and parent being com.databricks.backend.daemon.driver.ClassLoaders$LibraryClassLoader@1a42da0a of type class com.databricks.backend.daemon.driver.ClassLoaders$LibraryClassLoader with classpath [file:/local_disk0/tmp/repl/spark-4071811259476162981-8e526ae9-25fb-4545-8d3f-963a8661cd2b/] and parent being sun.misc.Launcher$AppClassLoader@43ee72e6 of type class sun.misc.Launcher$AppClassLoader with classpath [...] not found. 
at scala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:141) 
at scala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:29) 
at $linedb3da7b2933d4b63a62b3d2a21c675f2196.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$c35395a9233a7197629c985e87dce75$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$typecreator6$1.apply(command-2013905963200886:11) 
at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe$lzycompute(TypeTags.scala:237) 
at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe(TypeTags.scala:237) 
at org.apache.spark.sql.catalyst.ScalaReflection$.encoderFor(ScalaReflection.scala:848) 
at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:55) 
at org.apache.spark.sql.Encoders$.product(Encoders.scala:312) 
at org.apache.spark.sql.LowPrioritySQLImplicits.newProductEncoder(SQLImplicits.scala:302) 
at org.apache.spark.sql.LowPrioritySQLImplicits.newProductEncoder$(SQLImplicits.scala:302) 
at org.apache.spark.sql.SQLImplicits.newProductEncoder(SQLImplicits.scala:34) 
at $linedb3da7b2933d4b63a62b3d2a21c675f2196.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$c35395a9233a7197629c985e87dce75$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.$anonfun$udf1$1(command-2013905963200886:11) {code}
Declaring the case class within the UDF:
{code:java}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}
 
  case class DataRow(field1: String)
  val sparkSession = SparkSession.builder.getOrCreate()
  import sparkSession.implicits._  
  val udf1 = udf((x: String) => {
    case class DataRow(field1: String)
    val testData = Seq(DataRow(""test1""), DataRow(""test2"")).toDF(""test"") // This is failing at runtime
    3
  })  
  val df1 = Seq(DataRow(""test1""), DataRow(""test2"")).toDF(""test"").withColumn(""udf"", udf1($""test"")) // This is working
  display(df1) {code}
I get a different error
{code:java}
error: value toDF is not a member of Seq[DataRow] val testData = Seq(DataRow(""test1""), DataRow(""test2"")).toDF(""test""){code}

Questions:
* Is this an expected behaviour or a bug?

Thanks.

Thanks.","DBR (Databricks) 13.2, Spark 3.4.0 and Scala 2.12.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Scala,,,,Mon Aug 07 16:11:55 UTC 2023,,,,,,,,,,"0|z1jmt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/23 16:11;jcblancomartinez;Using [Tuple1|https://www.scala-lang.org/api/2.13.4/scala/Tuple1.html] instead of DataRow serves as a workaround.;;;",,,,,,,,,,,,,,
Cannot derive ExpressionEncoder when using types tagged by a trait,SPARK-44702,13546321,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,ben_hemsi,ben_hemsi,07/Aug/23 09:18,07/Aug/23 09:26,30/Oct/23 17:26,,3.1.3,3.2.4,3.3.2,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,spark-sql,,,,"Steps to reproduce:
{code:java}
trait Tag
case class Foo(x: Int)
case class Bar(x: Foo with Tag)
ExpressionEncoder.apply[Bar](){code}
the ExpressionEncoder throws the following error (this was for 3.1.2):
{code:java}
scala.MatchError: Foo with Tag (of class scala.reflect.internal.Types$RefinedType0)
  at org.apache.spark.sql.catalyst.ScalaReflection.getConstructorParameters(ScalaReflection.scala:931)
  at org.apache.spark.sql.catalyst.ScalaReflection.getConstructorParameters$(ScalaReflection.scala:928)
  at org.apache.spark.sql.catalyst.ScalaReflection$.getConstructorParameters(ScalaReflection.scala:49) {code}
The bug is [on this line (on master)|https://github.com/apache/spark/blob/master/sql/api/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L461].
{code:java}
val TypeRef(_, _, actualTypeArgs) = dealiasedTpe {code}
which is an incomplete pattern match. The pattern match needs be extended to include matching on RefinedType as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 07 09:26:27 UTC 2023,,,,,,,,,,"0|z1jmf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/23 09:26;ben_hemsi;This is my first time working on spark. I'm happy to be assigned to the issue, but I don't know how to assign myself to the issue.

I've added the affects versions which I've reproduced the issue on, but I think the bug affects all version.;;;",,,,,,,,,,,,,,
Create table like other table should also copy table stats.,SPARK-44698,13546297,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,zhuqi,zhuqi,07/Aug/23 06:36,07/Aug/23 07:44,30/Oct/23 17:26,07/Aug/23 07:44,3.4.1,4.0.0,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"For example:
describe table extended tbl;

col0                    int
col1                    int
col2                    int
col3                    int

Detailed Table Information
Catalog                 spark_catalog
Database                default
Table                   tbl
Owner                   zhuqi
Created Time            Mon Aug 07 14:02:30 CST 2023
Last Access             UNKNOWN
Created By              Spark 4.0.0-SNAPSHOT
Type                    MANAGED
Provider                hive
Table Properties        [transient_lastDdlTime=1691388473]
Statistics              30 bytes
Location                [file:/Users/zhuqi/spark/spark/spark-warehouse/tbl|file:///Users/zhuqi/spark/spark/spark-warehouse/tbl]
Serde Library           org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat             org.apache.hadoop.mapred.TextInputFormat
OutputFormat            org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Storage Properties      [serialization.format=1]
Partition Provider      Catalog
Time taken: 0.032 seconds, Fetched 23 row(s)

create table tbl2 like tbl;
23/08/07 14:14:07 WARN HiveMetaStore: Location: [file:/Users/zhuqi/spark/spark/spark-warehouse/tbl2|file:///Users/zhuqi/spark/spark/spark-warehouse/tbl2] specified for non-external table:tbl2
Time taken: 0.098 seconds
spark-sql (default)> describe table extended tbl2;
col0                    int
col1                    int
col2                    int
col3                    int

Detailed Table Information
Catalog                 spark_catalog
Database                default
Table                   tbl2
Owner                   zhuqi
Created Time            Mon Aug 07 14:14:07 CST 2023
Last Access             UNKNOWN
Created By              Spark 4.0.0-SNAPSHOT
Type                    MANAGED
Provider                hive
Table Properties        [transient_lastDdlTime=1691388847]
Location                [file:/Users/zhuqi/spark/spark/spark-warehouse/tbl2|file:///Users/zhuqi/spark/spark/spark-warehouse/tbl2]
Serde Library           org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat             org.apache.hadoop.mapred.TextInputFormat
OutputFormat            org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Storage Properties      [serialization.format=1]
Partition Provider      Catalog
Time taken: 0.03 seconds, Fetched 22 row(s)

The table stats are missing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 07 07:44:26 UTC 2023,,,,,,,,,,"0|z1jm9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/23 07:44;zhuqi;Sorry i misunderstand, the create table like don't need to copy data actually!;;;",,,,,,,,,,,,,,
Improve error message for `DataFrame.toDF`.,SPARK-44695,13546287,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,07/Aug/23 03:30,09/Aug/23 01:08,30/Oct/23 17:26,09/Aug/23 01:08,4.0.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,PySpark,,,,,0,,,,,Improve error message,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 09 01:08:20 UTC 2023,,,,,,,,,,"0|z1jm7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/23 03:36;snoot;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/42369;;;","07/Aug/23 03:37;snoot;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/42369;;;","09/Aug/23 01:08;gurwls223;Issue resolved by pull request 42369
[https://github.com/apache/spark/pull/42369];;;",,,,,,,,,,,,
Logging level isn't passed to RocksDB state store provider correctly,SPARK-44683,13546200,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,siying,siying,siying,04/Aug/23 23:57,08/Aug/23 02:16,30/Oct/23 17:26,08/Aug/23 02:12,3.4.1,,,,,,,,,,,,,,,,,,,3.5.1,4.0.0,,,Structured Streaming,,,,,0,,,,,"We pass log4j's log level to RocksDB so that RocksDB debug log can go to log4j. However, we pass in log level after we create the logger. However, the way it is set isn't effective. This has two impacts: (1) setting DEBUG level don't make RocksDB generate DEBUG level logs; (2) setting WARN or ERROR level does prevent INFO level logging, but RocksDB still makes JNI calls to Scala, which is an unnecessary overhead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 08 02:12:49 UTC 2023,,,,,,,,,,"0|z1jlo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 02:12;kabhwan;Issue resolved by pull request 42354
[https://github.com/apache/spark/pull/42354];;;",,,,,,,,,,,,,,
parameter markers are not blocked from DEFAULT (and other places),SPARK-44680,13546172,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,srielau,srielau,04/Aug/23 16:33,08/Aug/23 08:26,30/Oct/23 17:26,08/Aug/23 08:26,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Spark Core,,,,,0,,,,,"scala> spark.sql(""CREATE TABLE t11(c1 int default :parm)"", args = Map(""parm"" -> 5)).show()

-> success

scala> spark.sql(""describe t11"");

[INVALID_DEFAULT_VALUE.UNRESOLVED_EXPRESSION] Failed to execute EXISTS_DEFAULT command because the destination table column `c1` has a DEFAULT value :parm, which fails to resolve as a valid expression.

This likely extends to other DDL-y places.
I can only find protection against placement in the body of a CREATE VIEW.

I see two ways out of this:
* Raise an error (as we do for CREATE VIEW v1(c1) AS SELECT ? )
 * Improve the way we persist queries/expressions to substitute the at-DDL-time bound parameter value (it' not a bug it's a feature....)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 08 08:26:59 UTC 2023,,,,,,,,,,"0|z1jli0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 08:26;maxgekk;Issue resolved by pull request 42365
[https://github.com/apache/spark/pull/42365];;;",,,,,,,,,,,,,,
java.lang.OutOfMemoryError: Requested array size exceeds VM limit,SPARK-44679,13546171,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,haitham,haitham,04/Aug/23 16:25,13/Oct/23 16:12,30/Oct/23 17:26,,3.2.1,,,,,,,,,,,,,,,,,,,,,,,EC2,PySpark,,,,0,,,,,"We get the following error from our Pyspark application in Production env:

_java.lang.OutOfMemoryError: Requested array size exceeds VM limit_

I simplified the code we used and shared it below so you can easily investigate the issue.

We use Pyspark to read 900 MB text file which has one record. We use foreach function to iterate over the Datafreme and apply some high order function. The error occurs once foreach action is triggered. I think the issue is related to the integer data type of the bytes array used to hold the serialized dataframe. Since the dataframe record was too big, it seems the serialized record exceeded the max integer value, hence the error occurred. 

Note that the same error happens when using foreachBatch function with writeStream. 

Our prod data has many records larger than 100 MB.  Appreciate your help to provide a fix or a solution to that issue.

 

*Find below the code snippet:*
from pyspark.sql import SparkSession,functions as f
 
def check_file_name(row):
    print(""check_file_name called"")
 
def main():
    spark=SparkSession.builder.enableHiveSupport().getOrCreate()
inputPath = ""s3://bucket-name/common/source/""
    inputDF = spark.read.text(inputPath, wholetext=True)
    inputDF = inputDF.select(f.date_format(f.current_timestamp(), 'yyyyMMddHH').astype('string').alias('insert_hr'),
                        f.col(""value"").alias(""raw_data""),
                        f.input_file_name().alias(""input_file_name""))
    inputDF.foreach(check_file_name)
 
if __name__ == ""__main__"":
    main()
*Find below spark-submit command used:*

spark-submit --master yarn --conf spark.serializer=org.apache.spark.serializer.KryoSerializer  --num-executors 15 --executor-cores 4 --executor-memory 20g --driver-memory 20g --name haitham_job --deploy-mode cluster big_file_process.py","We use Amazon EMR to run Pyspark jobs.
Amazon EMR version : emr-6.7.0
Installed applications : 
Tez 0.9.2, Spark 3.2.1, Hive 3.1.3, Sqoop 1.4.7, Hadoop 3.2.1, Zookeeper 3.5.7, HCatalog 3.1.3, Livy 0.7.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/23 16:33;haitham;code_sample.txt;https://issues.apache.org/jira/secure/attachment/13061931/code_sample.txt",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,,,Fri Oct 13 15:43:02 UTC 2023,,,,,,,,,,"0|z1jlhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/23 15:43;coder121;I think your problem is the same as SPARK-40622

We have seen the same error before. and the exception was thrown when the executor was serializing the result data.
{code:java}
2023-06-09T14:59:02,752 ERROR [Executor task launch worker for task 9.0 in stage 22.0 (TID 218)] executor.Executor : Exception in task 9.0 in stage 22.0 (TID 218)
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
	at java.util.Arrays.copyOf(Arrays.java:3236) ~[?:1.8.0_25]
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113) ~[?:1.8.0_25]
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) ~[?:1.8.0_25]
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140) ~[?:1.8.0_25]
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41) ~[spark-core]
	at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877) ~[?:1.8.0_25]
	at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786) ~[?:1.8.0_25]
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189) ~[?:1.8.0_25]
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) ~[?:1.8.0_25]
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) ~[?:1.8.0_25]
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) ~[?:1.8.0_25]
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) ~[?:1.8.0_25]
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) ~[?:1.8.0_25]
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44) ~[spark-core]
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101) ~[spark-core]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_25]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_25]
	at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_25] {code}
 

Our solution : 

The version we were using at the time was 3.2.x. Since we could not upgrade spark to 3.4 at that time, we decided to cherrypick the [commit|https://github.com/apache/spark/pull/38064/files#diff-d7a989c491f3cb77cca02c701496a9e2a3443f70af73b0d1ab0899239f3a789d] in this pr(SPARK-40622) to our own spark branch. Finally, we used the same scenario to test and found that the exception did not occur.;;;",,,,,,,,,,,,,,
Downgrade Hadoop to 3.3.4,SPARK-44678,13546169,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dongjoon,dongjoon,dongjoon,04/Aug/23 16:07,04/Aug/23 21:21,30/Oct/23 17:26,04/Aug/23 21:21,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Build,,,,,0,,,,,"There is a community report on S3A committer performance regression. Although it's one liner fix, there is no available Hadoop release with that fix at this time.

HADOOP-18757: Bump corePoolSize of HadoopThreadPoolExecutor in s3a committer",,,,,,,,,,,,,,,,,,,,,,,,,SPARK-44197,HADOOP-18757,SPARK-43448,SPARK-43880,SPARK-42913,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 04 21:21:02 UTC 2023,,,,,,,,,,"0|z1jlhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/23 21:21;dongjoon;Issue resolved by pull request 42345
[https://github.com/apache/spark/pull/42345];;;",,,,,,,,,,,,,,
Raise a ArrowIOError with empty messages,SPARK-44673,13546120,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,packyan,packyan,04/Aug/23 09:34,04/Aug/23 09:34,30/Oct/23 17:26,,2.4.4,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"We encountered an problem while using PySpark 2.4.4, where the Python Runner of a certain task threw a pyarrow.lib.ArrowIOError without any message, which is too confusing. And this task was scheduled multiple times, all of these task attempts failed for the same pyarrow.lib.ArrowIOError.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-04 09:34:08.0,,,,,,,,,,"0|z1jl6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the `test_to_excel` tests for python3.7,SPARK-44670,13546088,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,madhukar1118,madhukar1118,madhukar1118,04/Aug/23 04:12,06/Aug/23 01:24,30/Oct/23 17:26,06/Aug/23 01:24,3.4.1,,,,,,,,,,,,,,,,,,,3.4.2,,,,Pandas API on Spark,,,,,0,,,,,"With python3.7 and openpyxl installed got error:

======================================================================

ERROR: test_to_excel (pyspark.pandas.tests.test_dataframe_conversion.DataFrameConversionTest)

Traceback (most recent call last):

  File ""/workspace/apache-spark/python/pyspark/pandas/tests/test_dataframe_conversion.py"", line 102, in test_to_excel

    dataframes = self.get_excel_dfs(pandas_on_spark_location, pandas_location)

  File ""/workspace/apache-spark/python/pyspark/pandas/tests/test_dataframe_conversion.py"", line 89, in get_excel_dfs

    ""got"": pd.read_excel(pandas_on_spark_location, index_col=0),

  File ""/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py"", line 296, in wrapper

    return func(*args, **kwargs)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 304, in read_excel

    io = ExcelFile(io, engine=engine)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 867, in __init__

    self._reader = self._engines[engine](self._io)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py"", line 21, in __init__

    import_optional_dependency(""xlrd"", extra=err_msg)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/compat/_optional.py"", line 110, in import_optional_dependency

    raise ImportError(msg) from None

ImportError: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.

----------------------------------------------------------------------

 

 

 

But with xlrd 2.0.1 installed getting error

======================================================================

ERROR: test_to_excel (pyspark.pandas.tests.test_dataframe_conversion.DataFrameConversionTest)

----------------------------------------------------------------------

Traceback (most recent call last):

  File ""/workspace/apache-spark/python/pyspark/pandas/tests/test_dataframe_conversion.py"", line 102, in test_to_excel

    dataframes = self.get_excel_dfs(pandas_on_spark_location, pandas_location)

  File ""/workspace/apache-spark/python/pyspark/pandas/tests/test_dataframe_conversion.py"", line 89, in get_excel_dfs

    ""got"": pd.read_excel(pandas_on_spark_location, index_col=0),

  File ""/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py"", line 296, in wrapper

    return func(*args, **kwargs)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 304, in read_excel

    io = ExcelFile(io, engine=engine)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 867, in __init__

    self._reader = self._engines[engine](self._io)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py"", line 22, in __init__

    super().__init__(filepath_or_buffer)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 353, in __init__

    self.book = self.load_workbook(filepath_or_buffer)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py"", line 37, in load_workbook

    return open_workbook(filepath_or_buffer)

  File ""/opt/conda/lib/python3.7/site-packages/xlrd/__init__.py"", line 170, in open_workbook

    raise XLRDError(FILE_FORMAT_DESCRIPTIONS[file_format]+'; not supported')

xlrd.biffh.XLRDError: Excel xlsx file; not supported

----------------------------------------------------------------------

 ",,,,,,,,,,,,,,,SPARK-40353,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Aug 06 01:24:20 UTC 2023,,,,,,,,,,"0|z1jkzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/23 05:38;madhukar1118;Raised a PR for using openpyxl instead of xlrd - [https://github.com/apache/spark/pull/42339] ;;;","06/Aug/23 01:24;gurwls223;Issue resolved by pull request 42339
[https://github.com/apache/spark/pull/42339];;;",,,,,,,,,,,,,
ShuffleStatus.getMapStatus should return None instead of Some(null),SPARK-44658,13546040,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dongjoon,dongjoon,dongjoon,03/Aug/23 17:46,03/Aug/23 21:18,30/Oct/23 17:26,03/Aug/23 21:18,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-43043,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 03 21:18:43 UTC 2023,,,,,,,,,,"0|z1jkoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/23 21:18;dongjoon;Issue resolved by pull request 42323
[https://github.com/apache/spark/pull/42323];;;",,,,,,,,,,,,,,
Incorrect limit handling and config parsing in Arrow collect,SPARK-44657,13545991,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,03/Aug/23 12:46,08/Aug/23 08:32,30/Oct/23 17:26,08/Aug/23 08:32,3.4.0,3.4.1,3.4.2,3.5.0,,,,,,,,,,,,,,,,3.4.2,3.5.0,4.0.0,,Connect,,,,,0,,,,,"In the arrow writer [code|https://github.com/apache/spark/blob/6161bf44f40f8146ea4c115c788fd4eaeb128769/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala#L154-L163] , the conditions don’t seem to hold what the documentation says regd ""{_}maxBatchSize and maxRecordsPerBatch, respect whatever smaller""{_} since it seems to actually respect the conf which is ""larger"" (i.e less restrictive) due to _||_ operator.

 

Further, when the `{_}CONNECT_GRPC_ARROW_MAX_BATCH_SIZE{_}` conf is read, the value is not converted to bytes from Mib ([example|https://github.com/apache/spark/blob/3e5203c64c06cc8a8560dfa0fb6f52e74589b583/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/execution/SparkConnectPlanExecution.scala#L103]).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 08 08:32:04 UTC 2023,,,,,,,,,,"0|z1jkds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 08:32;gurwls223;Issue resolved by pull request 42321
[https://github.com/apache/spark/pull/42321];;;",,,,,,,,,,,,,,
non-trivial DataFrame unions should not break caching,SPARK-44653,13545948,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,03/Aug/23 06:08,14/Aug/23 14:21,30/Oct/23 17:26,04/Aug/23 03:33,3.3.0,,,,,,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 14 14:21:04 UTC 2023,,,,,,,,,,"0|z1jk48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/23 03:33;cloud_fan;Issue resolved by pull request 42315
[https://github.com/apache/spark/pull/42315];;;","14/Aug/23 14:21;ggintegration;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/42483;;;",,,,,,,,,,,,,
__repr__ broken for Row when the field is empty Row,SPARK-44643,13545910,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,02/Aug/23 20:05,03/Aug/23 03:05,30/Oct/23 17:26,03/Aug/23 03:05,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,PySpark,,,,,0,,,,,"PySpark {{Row}} raises and exception if the field is empty Row:

{code:python}
>>> repr(Row(Row()))
Traceback (most recent call last):
...
TypeError: not enough arguments for format string
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 03 03:05:19 UTC 2023,,,,,,,,,,"0|z1jjvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/23 03:05;gurwls223;Issue resolved by pull request 42303
[https://github.com/apache/spark/pull/42303];;;",,,,,,,,,,,,,,
Unable to read from JDBC data sources when using custom schema containing varchar,SPARK-44638,13545889,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,michaelsaid,michaelsaid,02/Aug/23 16:33,02/Aug/23 16:37,30/Oct/23 17:26,,3.1.0,3.2.4,3.3.2,3.4.1,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"When querying the data from JDBC databases with custom schema containing varchar I got this error :
{code:java}
[23/07/14 06:12:19 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) ( executor 1): java.sql.SQLException: Unsupported type varchar(100) at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedJdbcTypeError(QueryExecutionErrors.scala:818) 23/07/14 06:12:21 INFO TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2) on , executor 0: java.sql.SQLException (Unsupported type varchar(100)){code}
Code example: 
{code:java}
CUSTOM_SCHEMA=""ID Integer, NAME VARCHAR(100)""
df = spark.read.format(""jdbc"")
.option(""url"", ""jdbc:oracle:thin:@0.0.0.0:1521:db"")
.option(""driver"", ""oracle.jdbc.OracleDriver"")
.option(""dbtable"", ""table"")
.option(""customSchema"", CUSTOM_SCHEMA)
.option(""user"", ""user"")
.option(""password"", ""password"")
.load()
df.show(){code}
I tried to set {{spark.sql.legacy.charVarcharAsString = true}} to restore the behavior before Spark 3.1 but it doesn't help.
The issue occurs in version 3.1.0 and above. I believe that this issue is caused by https://issues.apache.org/jira/browse/SPARK-33480",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-02 16:33:22.0,,,,,,,,,,"0|z1jjr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecuteRelease needs to synchronize,SPARK-44637,13545878,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,02/Aug/23 14:54,02/Aug/23 19:08,30/Oct/23 17:26,02/Aug/23 19:08,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-43754,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-02 14:54:55.0,,,,,,,,,,"0|z1jjoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encoders.bean does no longer support nested beans with type arguments,SPARK-44634,13545823,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,gbloisi-openaire,gbloisi-openaire,02/Aug/23 09:13,07/Aug/23 14:14,30/Oct/23 17:26,07/Aug/23 14:14,3.4.1,3.5.0,4.0.0,,,,,,,,,,,,,,,,,3.4.2,3.5.0,4.0.0,,SQL,,,,,0,,,,,"Hi,

  while upgrading a project from spark 2.4.0 to 3.4.1 version, I have encountered the same problem described in [java - Encoders.bean attempts to check the validity of a return type considering its generic type and not its concrete class, with Spark 3.4.0 - Stack Overflow|https://stackoverflow.com/questions/76045255/encoders-bean-attempts-to-check-the-validity-of-a-return-type-considering-its-ge].

Put it short, starting from Spark 3.4.x Encoders.bean throws an exception when the passed class contains a field whose type is a nested bean with type arguments:

 
{code:java}
class A<T> {
   T value;
   // value getter and setter
}

class B {
   A<String> stringHolder;
   // stringHolder getter and setter
}

Encoders.bean(B.class); // throws ""SparkUnsupportedOperationException: [ENCODER_NOT_FOUND]...""{code}
 

 

It looks like this is a regression introduced with [SPARK-42093 SQL Move JavaTypeInference to AgnosticEncoders|https://github.com/apache/spark/commit/18672003513d5a4aa610b6b94dbbc15c33185d3#diff-1191737b908340a2f4c22b71b1c40ebaa0da9d8b40c958089c346a3bda26943b] while getting rid of TypeToken, that somehow managed that case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 07 14:14:33 UTC 2023,,,,,,,,,,"0|z1jjcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/23 14:14;gbloisi-openaire;PR has been merged;;;",,,,,,,,,,,,,,
pandas-on-Spark Dataframe.between_time fails when timestamp fields are present,SPARK-44633,13545812,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,teweiluo,teweiluo,02/Aug/23 07:46,02/Aug/23 07:49,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,Pandas API on Spark,,,,,0,,,,,"I tried to execute the between_time() method of a pandas-on-Spark dataframe that has a DatatimeIndex and contains fields of timestamp types, and hit the following error.

!image.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/23 07:48;teweiluo;image.png;https://issues.apache.org/jira/secure/attachment/13061850/image.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-08-02 07:46:33.0,,,,,,,,,,"0|z1jja0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert SPARK-43043 Improve the performance of MapOutputTracker.updateMapOutput,SPARK-44630,13545781,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,02/Aug/23 02:41,02/Aug/23 06:16,30/Oct/23 17:26,02/Aug/23 06:16,3.4.1,,,,,,,,,,,,,,,,,,,3.4.2,,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 02 06:16:03 UTC 2023,,,,,,,,,,"0|z1jj34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/23 06:16;gurwls223;Issue resolved by pull request 42285
[https://github.com/apache/spark/pull/42285];;;",,,,,,,,,,,,,,
org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils#resultSetToRows produces wrong data,SPARK-44627,13545772,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Not A Problem,,zhaomin,zhaomin,02/Aug/23 00:17,02/Aug/23 07:35,30/Oct/23 17:26,02/Aug/23 07:35,2.3.2,3.3.1,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"When the resultSet exists a timestmp column and it's value is null, but column define is not null. In the row it generates, this column will use the value of the same column in the previous row.  

 

In mysql, if a datetime column is defined, meanwhile it is not null. When a value is '0000-00-00 00:00:00', mysql provided a property of zeroDateTimeBehavior, it will return null.  

table define:
CREATE TABLE `test_timestamp` (
`id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键id',
`unbind_time` datetime NOT NULL DEFAULT '0000-00-00 00:00:00' ,
PRIMARY KEY (`id`)
)
example:

the value of resultSet

1, 2023-01-01 12:00:00

2, null

 

the value of row

1, 2023-01-01 12:00:00

2, 2023-01-01 12:00:00

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/23 06:01;zhaomin;image-2023-08-02-14-01-54-447.png;https://issues.apache.org/jira/secure/attachment/13061843/image-2023-08-02-14-01-54-447.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 02 07:35:40 UTC 2023,,,,,,,,,,"0|z1jj14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/23 06:02;zhaomin;!image-2023-08-02-14-01-54-447.png!

it only update isNull to true, but the value keep with last row.;;;","02/Aug/23 07:01;yao;Similar to SPARK-44280？;;;","02/Aug/23 07:35;yao;I have checked with the reporter offline. This issue only exists in the resultSetToRows, which is a dead function. And he considered it as a public API which turns out just an exposure of an internal method;;;",,,,,,,,,,,,
Hive Generic UDF support no longer supports short-circuiting of argument evaluation,SPARK-44616,13545591,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,andygrove,andygrove,31/Jul/23 22:06,01/Aug/23 16:45,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"PR [https://github.com/apache/spark/pull/39555] changed DeferredObject to no longer contain a function, and instead contains a value. This removes the deferred evaluation capability and means that HiveGenericUDF implementations can no longer short-circuit the evaluation of their arguments, which could be a performance issue for some users.

Here is a relevant javadoc comment from the Hive source for DeferredObject:

{code:java}
  /**
   * A Defered Object allows us to do lazy-evaluation and short-circuiting.
   * GenericUDF use DeferedObject to pass arguments.
   */
  public static interface DeferredObject {
{code}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-31 22:06:38.0,,,,,,,,,,"0|z1jhww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeduplicateRelations should retain Alias metadata when creating a new instance,SPARK-44610,13545555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,31/Jul/23 14:47,31/Jul/23 18:37,30/Oct/23 17:26,31/Jul/23 18:37,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jul 31 18:37:19 UTC 2023,,,,,,,,,,"0|z1jhow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/23 18:37;Gengliang.Wang;Issue resolved by pull request 42242
[https://github.com/apache/spark/pull/42242];;;",,,,,,,,,,,,,,
ExecutorPodsAllocator doesn't create new executors if no pod snapshot captured pod creation,SPARK-44609,13545545,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,alibiyeslambek,alibiyeslambek,31/Jul/23 13:01,02/Aug/23 14:25,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,Kubernetes,Scheduler,,,,0,,,,,"There’s a following race condition in ExecutorPodsAllocator when running a spark application with static allocation on kubernetes with numExecutors >= 1:
 * Driver requests an executor
 * exec-1 gets created and registers with driver
 * exec-1 is moved from {{newlyCreatedExecutors}} to {{schedulerKnownNewlyCreatedExecs}}
 * exec-1 got deleted very quickly (~1-30 sec) after registration
 * {{ExecutorPodsWatchSnapshotSource}} fails to catch the creation of the pod (e.g. websocket connection was reset, k8s-apiserver was down, etc.)
 * {{ExecutorPodsPollingSnapshotSource}} fails to catch the creation because it runs every 30 secs, but executor was removed much quicker after creation
 * exec-1 is never removed from {{schedulerKnownNewlyCreatedExecs}}
 * {{ExecutorPodsAllocator}} will never request new executor because it’s slot is occupied by exec-1, due to {{schedulerKnownNewlyCreatedExecs}} never being cleared.

 

Put up a fix here https://github.com/apache/spark/pull/42297",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-31 13:01:56.0,,,,,,,,,,"0|z1jhmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark 3.2+ can not read hive table with hbase serde when hbase StorefileSize  is 0,SPARK-44598,13545406,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,Zing,Zing,30/Jul/23 03:01,01/Aug/23 05:34,30/Oct/23 17:26,01/Aug/23 05:34,3.2.3,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"We using spark to read a hive table with hbase serde . We found that when the hbase table data is relatively small (hbase StorefileSize is 0), the data read by spark 3.2 or 3.5 is empty, and there is no error message.

But when using spark2.4 or hive to read, the data can be read normally. Other information shows that spark3.1 can also read data normally, can anyone provide some ideas?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 01 04:22:54 UTC 2023,,,,,,,,,,"0|z1jgrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/23 02:04;yumwang;How to reproduce this issue?;;;","01/Aug/23 02:32;ulysses;please try `--conf spark.hadoopRDD.ignoreEmptySplits=false`;;;","01/Aug/23 04:21;Zing;[~ulysses] yes , it fix my case , thanks~;;;","01/Aug/23 04:22;Zing;it seem is a hbase bug. and hbase fix this bug at hbase 2.5+

 

https://issues.apache.org/jira/browse/HBASE-26340;;;",,,,,,,,,,,
Add jobTags to SparkListenerSQLExecutionStart,SPARK-44591,13545357,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jgauthier,jgauthier,jgauthier,28/Jul/23 22:30,01/Aug/23 04:16,30/Oct/23 17:26,29/Jul/23 20:45,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"As part SPARK-43952, users can define job tags via SparkContext.addJobTag. These tags are then used to trigger cancelation via SparkContext.cancelJobsByTag. Furthermore, these tags can be used to logically group multiple jobs together.

Listener of job events can retrieve job tags via SparkListenerJobStart.props.getProperty(SparkContext.SPARK_JOB_TAGS)

Listener of SQL events can link SparkListenerJobStart & SparkListenerSQLExecutionStart via SparkListenerJobStart.props.getProperty(SQLExecution.EXECUTION_ID_KEY).

However, some SQL executions do not trigger jobs (i.e. commands). As such listeners of SQL executions cannot resolve job tags of all executions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 01 04:16:24 UTC 2023,,,,,,,,,,"0|z1jggw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/23 20:45;Gengliang.Wang;Issue resolved by pull request 42216
[https://github.com/apache/spark/pull/42216];;;","01/Aug/23 04:16;snoot;User 'jasonli-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/42244;;;",,,,,,,,,,,,,
Migrated shuffle blocks are encrypted multiple times when io.encryption is enabled ,SPARK-44588,13545325,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,henrymai,henrymai,henrymai,28/Jul/23 15:55,02/Aug/23 04:08,30/Oct/23 17:26,01/Aug/23 21:41,3.1.0,3.1.1,3.1.2,3.1.3,3.2.0,3.2.1,3.2.2,3.2.3,3.2.4,3.3.0,3.3.1,3.3.2,3.4.0,3.4.1,,,,,,3.3.3,3.4.2,3.5.0,,Spark Core,,,,,0,,,,,"Shuffle blocks upon migration are wrapped for encryption again when being written out to a file on the receiver side.

 

Pull request to fix this: https://github.com/apache/spark/pull/42214

 

Details:

Sender/Read side:

BlockManagerDecommissioner:run()
    blocks = bm.migratableResolver.getMigrationBlocks()
        *dataFile = IndexShuffleBlockResolver:getDataFile(...)*
       buffer = FileSegmentManagedBuffer(..., dataFile)
                       *^ This reads straight from disk without decryption*
    blocks.foreach((blockId, buffer) => bm.blockTransferService.uploadBlockSync(..., buffer, ...))
        -> uploadBlockSync() -> uploadBlock(..., buffer, ...)
            -> client.uploadStream(UploadBlockStream, buffer, ...)
 - Notice that there is no decryption here on the sender/read side.

Receiver/Write side:

NettyBlockRpcServer:receiveStream() <--- This is the UploadBlockStream handler
    putBlockDataAsStream()
        migratableResolver.putShuffleBlockAsStream()
            *-> file = IndexShuffleBlockResolver:getDataFile(...)*
            -> tmpFile = (file + .<uuid> extension)
            *-> Creates an encrypting writable channel to a tmpFile using serializerManager.wrapStream()*
            -> onData() writes the data into the channel
            -> onComplete() renames the tmpFile to the file
 - Notice:

 * Both getMigrationBlocks()[read] and putShuffleBlockAsStream()[write] target IndexShuffleBlockResolver:getDataFile()
 * The read path does not decrypt but the write path encrypts.
 * As a thought exercise: if this cycle happens more than once (where this receiver is now a sender) even if we assume that the shuffle blocks are initially unencrypted*, then bytes in the file will just have more and more layers of encryption applied to it each time it gets migrated.
 * *In practice, the shuffle blocks are encrypted on disk to begin with, this is just a thought exercise",,,,,,,,,,,,,,,,,,,,SPARK-20629,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-28 15:55:04.0,,,,,,,,,,"0|z1jg9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix warning condition in MLLib RankingMetrics ndcgAk,SPARK-44585,13545300,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gvuillier,gvuillier,gvuillier,28/Jul/23 13:23,28/Jul/23 22:30,30/Oct/23 17:26,28/Jul/23 22:30,3.4.1,,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,4.0.0,,MLlib,,,,,0,,,,,"The implementation of nDCG evaluation in MLLib with relevance score (added in 3.4.0, see https://issues.apache.org/jira/browse/SPARK-39446 and [pull request|https://github.com/apache/spark/pull/36843]) implements the following warning when the input data isn't correct: ""# of ground truth set and # of relevance value set should be equal, check input data""

 

The logic for raising warnings is faulty at the moment: it raises a warning when the following conditions are both true:
 # {{rel}} is empty
 # {{lab.size}} and {{rel.size}} are not equal.

 

With the current logic, RankingMetrics will:
 * raise incorrect warning when a user is using it in the ""binary"" mode (i.e. no relevance values in the input)
 * not raise warning (that could be necessary) when the user is using it in the ""non-binary"" model (i.e. with relevance values in the input)

 

The logic should be to raise a warning should be:
 # {{rel}} is *not empty*
 # {{lab.size}} and {{rel.size}} are not equal.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jul 28 22:30:38 UTC 2023,,,,,,,,,,"0|z1jg48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/23 22:30;srowen;Issue resolved by pull request 42207
[https://github.com/apache/spark/pull/42207];;;",,,,,,,,,,,,,,
AddArtifactsRequest and ArtifactStatusesRequest do not set client_type information,SPARK-44584,13545292,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,28/Jul/23 12:44,30/Jul/23 01:54,30/Oct/23 17:26,30/Jul/23 01:54,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jul 30 01:54:54 UTC 2023,,,,,,,,,,"0|z1jg2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/23 01:54;gurwls223;Issue resolved by pull request 42209
[https://github.com/apache/spark/pull/42209];;;",,,,,,,,,,,,,,
ShutdownHookManager get wrong hadoop user group information,SPARK-44581,13545256,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ylllllllll,ylllllllll,ylllllllll,28/Jul/23 09:18,09/Aug/23 05:58,30/Oct/23 17:26,09/Aug/23 05:58,3.2.1,3.3.2,3.4.1,,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.0,,Deploy,YARN,,,,1,,,,," I use spark 3.2.1 to run a job on yarn in cluster mode. 

when the job is finished, there is an exception that:
{code:java}
2023-07-28 10:57:16,324 ERROR yarn.ApplicationMaster: Failed to cleanup staging dir hdfs://dmp/user/ubd_dmp_test/.sparkStaging/application_1689318995305_0290 org.apache.hadoop.security.AccessControlException: Permission denied: user=yarn, access=WRITE, inode=""/user/ubd_dmp_test/.sparkStaging"":ubd_dmp_test:ubd_dmp_test:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:349) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943) at org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:105) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3266) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1128) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:725) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121) at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88) at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1656) at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:991) at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:988) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:998) at org.apache.spark.deploy.yarn.ApplicationMaster.cleanupStagingDir(ApplicationMaster.scala:686) at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$run$3(ApplicationMaster.scala:268) at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214) at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019) at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at scala.util.Try$.apply(Try.scala:213) at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188) at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=yarn, access=WRITE, inode=""/user/ubd_dmp_test/.sparkStaging"":ubd_dmp_test:ubd_dmp_test:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:349) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943) at org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:105) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3266) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1128) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:725) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1573) at org.apache.hadoop.ipc.Client.call(Client.java:1519) at org.apache.hadoop.ipc.Client.call(Client.java:1416) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129) at com.sun.proxy.$Proxy15.delete(Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:655) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) at com.sun.proxy.$Proxy16.delete(Unknown Source) at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1654) ... 20 more
 
  {code}
I used user ubd_dmp_test to run the job, but the program used user yarn to delete the staging file, this never happens before when I use spark2.4.

 

So I print some log about the current user when it tries to delete the staging file, turns out to be user yarn. Then I print the log about the current user when it execute the run method of ApplicationMaster object, turns out to be ubd_dmp_test.

 

I'm really confused about how this happened.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 09 05:58:39 UTC 2023,,,,,,,,,,"0|z1jfuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/23 10:22;ylllllllll;I found that the ShutDownHook Manager will start a new Thread when the JVM exists, so the UserGroupInformation will not be inherited from the SparkContext, then this hook will create a new ugi with user ""yarn"", which caused the exception.;;;","03/Aug/23 09:16;githubbot;User 'liangyu-1' has created a pull request for this issue:
https://github.com/apache/spark/pull/42295;;;","09/Aug/23 05:58;yao;Issue resolved by  [https://github.com/apache/spark/pull/42295];;;",,,,,,,,,,,,
RocksDB crashed when testing in GitHub Actions,SPARK-44580,13545220,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,LuciferYang,LuciferYang,28/Jul/23 05:30,13/Sep/23 02:21,30/Oct/23 17:26,,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,,,,,SQL,Tests,,,,0,,,,,"[https://github.com/LuciferYang/spark/actions/runs/5666554831/job/15395578871]

 
{code:java}
#
17177# A fatal error has been detected by the Java Runtime Environment:
17178#
17179#  SIGSEGV (0xb) at pc=0x00007f8a077d2743, pid=4403, tid=0x00007f89cadff640
17180#
17181# JRE version: OpenJDK Runtime Environment (8.0_372-b07) (build 1.8.0_372-b07)
17182# Java VM: OpenJDK 64-Bit Server VM (25.372-b07 mixed mode linux-amd64 compressed oops)
17183# Problematic frame:
17184# C  [librocksdbjni886380103972770161.so+0x3d2743]  rocksdb::DBImpl::FailIfCfHasTs(rocksdb::ColumnFamilyHandle const*) const+0x23
17185#
17186# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
17187#
17188# An error report file with more information is saved as:
17189# /home/runner/work/spark/spark/sql/core/hs_err_pid4403.log
17190#
17191# If you would like to submit a bug report, please visit:
17192#   https://github.com/adoptium/adoptium-support/issues
17193# The crash happened outside the Java Virtual Machine in native code.
17194# See problematic frame for where to report the bug.
17195# {code}
 

This is my first time encountering this problem, and I am  unsure of the root cause now

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/23 12:26;LuciferYang;image-2023-08-09-20-26-11-507.png;https://issues.apache.org/jira/secure/attachment/13062006/image-2023-08-09-20-26-11-507.png","10/Aug/23 01:44;panbingkun;image-2023-08-10-09-44-19-341.png;https://issues.apache.org/jira/secure/attachment/13062015/image-2023-08-10-09-44-19-341.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 13 02:06:34 UTC 2023,,,,,,,,,,"0|z1jfmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/23 04:11;LuciferYang;A new crash case

 

[https://github.com/apache/spark/actions/runs/5776211318/job/15655092778]

 
{code:java}
#
18752# A fatal error has been detected by the Java Runtime Environment:
18753#
18754#  SIGSEGV (0xb) at pc=0x00007f738a9d2743, pid=4842, tid=0x00007f739a9ff640
18755#
18756# JRE version: OpenJDK Runtime Environment (8.0_372-b07) (build 1.8.0_372-b07)
18757# Java VM: OpenJDK 64-Bit Server VM (25.372-b07 mixed mode linux-amd64 compressed oops)
18758# Problematic frame:
18759# C  [librocksdbjni7743674876323810807.so+0x3d2743]  rocksdb::DBImpl::FailIfCfHasTs(rocksdb::ColumnFamilyHandle const*) const+0x23
18760#
18761# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
18762#
18763# An error report file with more information is saved as:
18764# /home/runner/work/spark/spark/sql/core/hs_err_pid4842.log
18765#
18766# If you would like to submit a bug report, please visit:
18767#   https://github.com/adoptium/adoptium-support/issues
18768# The crash happened outside the Java Virtual Machine in native code.
18769# See problematic frame for where to report the bug.
18770#
18771Warning: Unable to read from client, please check on client for further details of the problem.
18772
18773Session terminated, killing shell... ...killed. {code};;;","09/Aug/23 12:26;LuciferYang;A new crash case

[https://github.com/yaooqinn/spark/actions/runs/5805477173/job/15736662791]

!image-2023-08-09-20-26-11-507.png!;;;","10/Aug/23 01:35;panbingkun;From this error, it seems that it is caused by the absence of `dfsRootDir`;;;","10/Aug/23 01:44;panbingkun;I have observed the logs of the above cases, and there are logs similar to  !image-2023-08-10-09-44-19-341.png! before each crash;;;","13/Sep/23 02:06;panbingkun;[https://github.com/apache/spark/pull/42862]

All errors occur in RocksDBStateStoreStreamingAggregationSuite;;;",,,,,,,,,,
INSERT BY NAME returns non-sensical error message,SPARK-44577,13545196,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,srielau,srielau,27/Jul/23 23:43,01/Sep/23 12:28,30/Oct/23 17:26,01/Sep/23 12:27,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,,,0,,,,,"CREATE TABLE bug(c1 INT);

INSERT INTO bug BY NAME SELECT 1 AS c2;

==> Multi-part identifier cannot be empty.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 01 12:27:58 UTC 2023,,,,,,,,,,"0|z1jfhc:",9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,,,"28/Jul/23 21:35;linhongliu-db;[~fanjia] could you make a followup PR to fix this?

The error is at: [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala#L243]

we should do something like:

```

val pathInfo = if (colPath.empty) {

  ""table""

} else {

  s""struct ${colPath.quoted}""

}

throw QueryCompilationErrors.incompatibleDataToTableExtraStructFieldsError(
     tableName,

    pathInfo,   // the changes

    extraCols
)

```;;;","28/Jul/23 21:35;linhongliu-db;cc [~cloud_fan] ;;;","29/Jul/23 01:01;fanjia;Let me fix this, thanks for report.;;;","29/Jul/23 01:51;fanjia;https://github.com/apache/spark/pull/42220;;;","31/Jul/23 09:03;githubbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42220;;;","01/Sep/23 12:27;cloud_fan;Issue resolved by pull request 42220
[https://github.com/apache/spark/pull/42220];;;",,,,,,,,,
Session Artifact update breaks XXWithState methods in KVGDS,SPARK-44576,13545192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,zhenli,zhenli,27/Jul/23 20:28,29/Aug/23 17:38,30/Oct/23 17:26,29/Aug/23 17:38,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"When changing the client test jar from system classloader to session classloader (https://github.com/apache/spark/compare/master...zhenlineo:spark:streaming-artifacts?expand=1), all XXWithState test suite failed with class loader errors: e.g.
```
23/07/25 16:13:14 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 16) (10.8.132.125 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 170 in stage 2.0 failed 1 times, most recent failure: Lost task 170.0 in stage 2.0 (TID 14) (10.8.132.125 executor driver): java.lang.ClassCastException: class org.apache.spark.sql.streaming.ClickState cannot be cast to class org.apache.spark.sql.streaming.ClickState (org.apache.spark.sql.streaming.ClickState is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @2c604965; org.apache.spark.sql.streaming.ClickState is in unnamed module of loader java.net.URLClassLoader @57751f4)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1514)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:592)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1480)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:595)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:)
23/07/25 16:13:14 ERROR Utils: Aborting task
java.lang.IllegalStateException: Error committing version 1 into HDFSStateStore[id=(op=0,part=5),dir=file:/private/var/folders/b0/f9jmmrrx5js7xsswxyf58nwr0000gp/T/temporary-02cca002-e189-4e32-afd8-964d6f8d5056/state/0/5]
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)
	at org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:183)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:611)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)
	at org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec.timeTakenMs(FlatMapGroupsWithStateExec.scala:374)
	at org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:183)
	at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1514)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:592)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1480)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:595)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
```

Fix the failure and change the client tests to install client jar on the session class loader.

This test only failed on maven builds as only maven builds needs the test jar to be installed separately. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 29 17:38:34 UTC 2023,,,,,,,,,,"0|z1jfgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/23 17:38;hvanhovell;This has been fixed by making the SBT build more hermetic (and fixing the class sync).;;;",,,,,,,,,,,,,,
Couldn't submit Spark application to Kubenetes in versions v1.27.3,SPARK-44573,13545137,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,,,siddaraju.g.c,siddaraju.g.c,27/Jul/23 13:02,10/Aug/23 07:16,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,Kubernetes,Spark Submit,,,,0,,,,,"Spark-submit ( cluster mode on Kubernetes ) results error *io.fabric8.kubernetes.client.KubernetesClientException* on my 3 nodes k8s cluster.

Steps followed:
 * using IBM cloud, created 3 Instances
 * 1st Instance act as master node and another two acts as worker nodes

 
{noformat}
root@vsi-spark-master:/opt# kubectl get nodes
NAME                 STATUS   ROLES                  AGE   VERSION
vsi-spark-master     Ready    control-plane,master   2d    v1.27.3+k3s1
vsi-spark-worker-1   Ready    <none>                 47h   v1.27.3+k3s1
vsi-spark-worker-2   Ready    <none>                 47h   v1.27.3+k3s1{noformat}
 * Copy spark-3.4.1-bin-hadoop3.tgz in to /opt/spark folder 
 * Ran spark by using below command

 
{noformat}
root@vsi-spark-master:/opt# /opt/spark/bin/spark-submit --master k8s://http://<master_node_IP>:6443 --conf spark.kubernetes.authenticate.submission.oauthToken=$TOKEN --deploy-mode cluster --name spark-pi --class org.apache.spark.examples.SparkPi --conf spark.executor.instances=5 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  --conf spark.kubernetes.container.image=sushmakorati/testrepo:pyrandomGB local:///opt/spark/examples/jars/spark-examples_2.12-3.4.1.jar{noformat}
 * And getting below error message.

{noformat}
3/07/27 12:56:26 WARN Utils: Kubernetes master URL uses HTTP instead of HTTPS.
23/07/27 12:56:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/07/27 12:56:26 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
23/07/27 12:56:26 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
23/07/27 12:56:27 ERROR Client: Please check ""kubectl auth can-i create pod"" first. It should be yes.
Exception in thread ""main"" io.fabric8.kubernetes.client.KubernetesClientException: An error has occurred.
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:129)
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:122)
    at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:44)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:1113)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:93)
    at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:153)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5(KubernetesClientApplication.scala:250)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5$adapted(KubernetesClientApplication.scala:244)
    at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2786)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:244)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:216)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1020)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: Connection reset
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:535)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:558)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleCreate(OperationSupport.java:349)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:711)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:93)
    at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)
    ... 15 more
Caused by: java.net.SocketException: Connection reset
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
    at okio.Okio$2.read(Okio.java:140)
    at okio.AsyncTimeout$2.read(AsyncTimeout.java:237)
    at okio.RealBufferedSource.read(RealBufferedSource.java:47)
    at okhttp3.internal.http1.Http1Codec$AbstractSource.read(Http1Codec.java:363)
    at okhttp3.internal.http1.Http1Codec$UnknownLengthSource.read(Http1Codec.java:507)
    at okio.RealBufferedSource.exhausted(RealBufferedSource.java:57)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientImpl$OkHttpAsyncBody.doConsume(OkHttpClientImpl.java:127)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
23/07/27 12:56:27 INFO ShutdownHookManager: Shutdown hook called
23/07/27 12:56:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-70ee50ef-d9e9-4220-91f4-15a282031095{noformat}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-27 13:02:50.0,,,,,,,,,,"0|z1jf48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong semantics for null IN (empty list),SPARK-44550,13544887,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jchen5,jchen5,jchen5,26/Jul/23 03:26,26/Sep/23 01:40,30/Oct/23 17:26,26/Sep/23 01:40,3.4.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,"{{null IN (empty list)}} incorrectly evaluates to null, when it should evaluate to false. (The reason it should be false is because a IN (b1, b2) is defined as a = b1 OR a = b2, and an empty IN list is treated as an empty OR which is false. This is specified by ANSI SQL.)

Many places in Spark execution (In, InSet, InSubquery) and optimization (OptimizeIn, NullPropagation) implemented this wrong behavior. Also note that the Spark behavior for the null IN (empty list) is inconsistent in some places - literal IN lists generally return null (incorrect), while IN/NOT IN subqueries mostly return false/true, respectively (correct) in this case.

This is a longstanding correctness issue which has existed since null support for IN expressions was first added to Spark.

Doc with more details: [https://docs.google.com/document/d/1k8AY8oyT-GI04SnP7eXttPDnDj-Ek-c3luF2zL6DPNU/edit] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 26 01:40:51 UTC 2023,,,,,,,,,,"0|z1jdl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/23 01:40;cloud_fan;Issue resolved by pull request 43068
[https://github.com/apache/spark/pull/43068];;;",,,,,,,,,,,,,,
Support correlated references under window functions,SPARK-44549,13544879,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gubichev,gubichev,gubichev,26/Jul/23 00:27,23/Aug/23 14:50,30/Oct/23 17:26,23/Aug/23 14:50,3.4.1,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,,,,,We should support subqueries with correlated references under a window function operator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 23 14:50:10 UTC 2023,,,,,,,,,,"0|z1jdjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 14:50;cloud_fan;Issue resolved by pull request 42383
[https://github.com/apache/spark/pull/42383];;;",,,,,,,,,,,,,,
BlockManagerDecommissioner throws exceptions when migrating RDD cached blocks to fallback storage,SPARK-44547,13544848,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ukby1234,ukby1234,ukby1234,25/Jul/23 18:53,25/Aug/23 05:20,30/Oct/23 17:26,25/Aug/23 05:20,3.4.1,,,,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.0,4.0.0,Spark Core,,,,,0,,,,,"Looks like the RDD cache doesn't support fallback storage and we should stop the migration if the only viable peer is the fallback storage. 

  [^spark-error.log] 23/07/25 05:12:58 WARN BlockManager: Failed to replicate rdd_18_25 to BlockManagerId(fallback, remote, 7337, None), failure #0
java.io.IOException: Failed to connect to remote:7337
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.netty.NettyBlockTransferService.uploadBlock(NettyBlockTransferService.scala:168)
	at org.apache.spark.network.BlockTransferService.uploadBlockSync(BlockTransferService.scala:121)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$replicate(BlockManager.scala:1784)
	at org.apache.spark.storage.BlockManager.$anonfun$replicateBlock$2(BlockManager.scala:1721)
	at org.apache.spark.storage.BlockManager.$anonfun$replicateBlock$2$adapted(BlockManager.scala:1707)
	at scala.Option.forall(Option.scala:390)
	at org.apache.spark.storage.BlockManager.replicateBlock(BlockManager.scala:1707)
	at org.apache.spark.storage.BlockManagerDecommissioner.migrateBlock(BlockManagerDecommissioner.scala:356)
	at org.apache.spark.storage.BlockManagerDecommissioner.$anonfun$decommissionRddCacheBlocks$3(BlockManagerDecommissioner.scala:340)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.storage.BlockManagerDecommissioner.decommissionRddCacheBlocks(BlockManagerDecommissioner.scala:339)
	at org.apache.spark.storage.BlockManagerDecommissioner$$anon$1.run(BlockManagerDecommissioner.scala:214)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.net.UnknownHostException: remote
	at java.base/java.net.InetAddress$CachedAddresses.get(Unknown Source)
	at java.base/java.net.InetAddress.getAllByName0(Unknown Source)
	at java.base/java.net.InetAddress.getAllByName(Unknown Source)
	at java.base/java.net.InetAddress.getAllByName(Unknown Source)
	at java.base/java.net.InetAddress.getByName(Unknown Source)
	at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)
	at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)
	at io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)
	at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)
	at io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)
	at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)
	at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:605)
	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)
	at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:990)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:516)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:429)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/23 18:54;ukby1234;spark-error.log;https://issues.apache.org/jira/secure/attachment/13061620/spark-error.log",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Aug 25 05:20:22 UTC 2023,,,,,,,,,,"0|z1jdcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/23 20:59;ignitetcbot;User 'ukby1234' has created a pull request for this issue:
https://github.com/apache/spark/pull/42155;;;","25/Aug/23 05:20;dongjoon;Issue resolved by pull request 42155
[https://github.com/apache/spark/pull/42155];;;",,,,,,,,,,,,,
It's impossible to get column by index if names are not unique,SPARK-44545,13544825,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,alexey.dmitriev,alexey.dmitriev,25/Jul/23 14:18,25/Jul/23 16:25,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"So, I have a dataframe with non-unique columns names:

 
{code:java}
df = spark_session.createDataFrame([[1,2,3], [4,5,6]], ['a', 'a', 'c']) {code}
 

It works fine.

 

Now I try to get a column with non-unique name by index
{code:java}
df[0]
{code}
Expectation: It returns first of the columns

Note, the [doc|[https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.__getitem__.html]] doesn't mention non-unique name as a precondition.

 

Actual result:

It throws exception:

 
{noformat}
AnalysisException                         Traceback (most recent call last)
Cell In[71], line 1
----> 1 df[0]

File /usr/local/spark/python/pyspark/sql/dataframe.py:2935, in DataFrame.__getitem__(self, item)
   2933     return self.select(*item)
   2934 elif isinstance(item, int):
-> 2935     jc = self._jdf.apply(self.columns[item])
   2936     return Column(jc)
   2937 else:

File /usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)
   1316 command = proto.CALL_COMMAND_NAME +\
   1317     self.command_header +\
   1318     args_command +\
   1319     proto.END_COMMAND_PART
   1321 answer = self.gateway_client.send_command(command)
-> 1322 return_value = get_return_value(
   1323     answer, self.gateway_client, self.target_id, self.name)
   1325 for temp_arg in temp_args:
   1326     if hasattr(temp_arg, ""_detach""):

File /usr/local/spark/python/pyspark/errors/exceptions/captured.py:175, in capture_sql_exception.<locals>.deco(*a, **kw)
    171 converted = convert_exception(e.java_exception)
    172 if not isinstance(converted, UnknownException):
    173     # Hide where the exception came from that shows a non-Pythonic
    174     # JVM exception message.
--> 175     raise converted from None
    176 else:
    177     raise

AnalysisException: [AMBIGUOUS_REFERENCE] Reference `a` is ambiguous, could be: [`a`, `a`].{noformat}
 ","I have python 3.11, pyspark 3.4.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-25 14:18:10.0,,,,,,,,,,"0|z1jd7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Connect DataFrame does not allow to add custom instance attributes and check for it,SPARK-44528,13544702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,grundprinzip-db,grundprinzip-db,grundprinzip-db,24/Jul/23 20:32,26/Jul/23 23:54,30/Oct/23 17:26,26/Jul/23 23:54,3.4.1,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Connect,,,,,0,,,,,"```
df = spark.range(10)
df._test = 10

assert(hasattr(df, ""_test""))
assert(!hasattr(df, ""_test_no""))
```

Treats `df._test` like a column",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 26 23:54:23 UTC 2023,,,,,,,,,,"0|z1jcg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/23 23:54;gurwls223;Issue resolved by pull request 42132
[https://github.com/apache/spark/pull/42132];;;",,,,,,,,,,,,,,
SparkConnectServerUtils generated incorrect parameters for jars,SPARK-44519,13544590,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,24/Jul/23 05:55,24/Jul/23 11:40,30/Oct/23 17:26,24/Jul/23 11:40,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,SQL,,,,,0,,,,,SparkConnectServerUtils generate multiple --jars parameters. It will cause the bug that doesn't find out the class.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jul 24 11:40:00 UTC 2023,,,,,,,,,,"0|z1jbr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/23 11:40;LuciferYang;Issue resolved by pull request 42121
[https://github.com/apache/spark/pull/42121];;;",,,,,,,,,,,,,,
first operator should respect the nullability of child expression as well as ignoreNulls option,SPARK-44517,13544575,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,codingcat,codingcat,24/Jul/23 02:14,05/Aug/23 16:51,30/Oct/23 17:26,,3.2.0,3.2.1,3.2.2,3.2.3,3.2.4,3.3.0,3.3.1,3.3.2,3.4.0,3.4.1,,,,,,,,,,,,,,SQL,,,,,0,,,,,"I found the following problem when using Spark recently:

 
{code:java}
// code placeholder
import spark.implicits._

val s = Seq((1.2, ""s"", 2.2)).toDF(""v1"", ""v2"", ""v3"")

val schema = StructType(Seq(StructField(""v1"", DoubleType, nullable = false),StructField(""v2"", StringType, nullable = true),StructField(""v3"", DoubleType, nullable = false)))

val df = spark.createDataFrame(s.rdd, schema)val inputDF = 

val inputDF = df.dropDuplicates(""v3"")

spark.sql(""CREATE TABLE local.db.table (\n v1 DOUBLE NOT NULL,\n v2 STRING, v3 DOUBLE NOT NULL)"")

inputDF.write.mode(""overwrite"").format(""iceberg"").save(""local.db.table"") {code}
 

 

when I use the above code to write to iceberg (i guess Delta Lake will have the same problem) , I got very confusing exception


{code:java}
Exception in thread ""main"" java.lang.IllegalArgumentException: Cannot write incompatible dataset to table with schema:

table 

{  1: v1: required double  2: v2: optional string  3: v3: required double}

Provided schema:

table {  1: v1: optional double  2: v2: optional string  3: v3: required double} {code}

basically it complains that we have v1 as the nullable column in our `inputDF` above which is not allowed since we created table with the v1 as not nullable. The confusion comes from that,  if we check the schema with printSchema() of inputDF, v1 is not nullable
{noformat}
root 
|-- v1: double (nullable = false) 
|-- v2: string (nullable = true) 
|-- v3: double (nullable = false){noformat}
Clearly, something changed the v1's nullability unexpectedly!

 

After some debugging I found that the key is that dropDuplicates(""v3""). In optimization phase, we have ReplaceDeduplicateWithAggregate to replace the Deduplicate with aggregate on v3 and run first() over all other columns. However, first() operator has hard coded nullable as always ""true"" which is the source of changed nullability of v1

 

this is a very confusing behavior of Spark, and probably no one really noticed as we do not care too much without the new table formats like delta lake and iceberg which can make nullability check correctly. Nowadays, we users adopt them more and more, this is surfaced up

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Aug 05 16:51:43 UTC 2023,,,,,,,,,,"0|z1jbns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/23 16:51;hiveqa;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/42117;;;",,,,,,,,,,,,,,
Upgrade snappy-java to 1.1.10.3,SPARK-44513,13544550,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,panbingkun,panbingkun,panbingkun,23/Jul/23 12:56,31/Jul/23 16:08,30/Oct/23 17:26,24/Jul/23 03:04,3.4.1,,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jul 24 03:04:12 UTC 2023,,,,,,,,,,"0|z1jbi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/23 03:04;Qin Yao;Issue resolved by pull request 42113
[https://github.com/apache/spark/pull/42113];;;",,,,,,,,,,,,,,
dataset.sort.select.write.partitionBy sorts wrong column,SPARK-44512,13544542,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,,,leeyc0,leeyc0,23/Jul/23 06:05,25/Jul/23 11:36,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,Optimizer,SQL,,,,0,correctness,,,,"(In this example the dataset is of type Tuple3, and the columns are named _1, _2 and _3)

 

I found -then when AQE is enabled,- that the following code does not produce sorted output (.drop() also have the same problem), unless spark.sql.optimizer.plannedWrite.enabled is set to false.

After further investigation, spark actually sorted wrong column in the following code.

{{dataset.sort(""_1"")}}
{{.select(""_2"", ""_3"")}}
{{.write()}}
{{.partitionBy(""_2"")}}
{{.text(""output"");}}

 
(the following workaround is no longer necessary)
-However, if I insert an identity mapper between select and write, the output would be sorted as expected.-

-{{dataset = dataset.sort(""_1"")}}-
-{{.select(""_2"", ""_3"");}}-
-{{dataset.map((MapFunction<Row, Row>) row -> row, dataset.encoder())}}-
-{{.write()}}-
-{{{}.{}}}{{{}partitionBy(""_2""){}}}-
-{{.text(""output"")}}-

Below is the complete code that reproduces the problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/23 08:19;leeyc0;Test-Details-for-Query-0.png;https://issues.apache.org/jira/secure/attachment/13061596/Test-Details-for-Query-0.png","25/Jul/23 08:20;leeyc0;Test-Details-for-Query-1.png;https://issues.apache.org/jira/secure/attachment/13061597/Test-Details-for-Query-1.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 25 08:37:00 UTC 2023,,,,,,,,,,"0|z1jbgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/23 06:06;leeyc0;Here is the [gist|https://gist.github.com/leeyc0/2bdab65901fe5754c471832acdc00890] that reproduces the issue.

To compile: javac Test.java && jar cvf Test.jar Test.class
bug reproduce: spark-submit --class Test Test.jar
no bug: spark-submit --conf spark.sql.optimizer.plannedWrite.enabled=false --class Test Test.jar

(the following commands are either invalid or no longer necessary)
-no bug if workaround is enabled: spark-submit --class Test Test.jar workaround-
-no bug too if AQE is disabled: spark-submit --conf spark.sql.adaptive.enabled=false --class Test Test.jar (3 output files in each partition key)-

 ;;;","24/Jul/23 14:37;leeyc0;After further test by running on my production data, I found that disabling AQE actually still does not produce sorted result.;;;","24/Jul/23 23:28;leeyc0;After reading SPARK-41914, I found that setting spark.sql.optimizer.plannedWrite.enabled=false (while leaving AQE enabled) seems produces a sorted output as expected, even without the workaround.

(And I found that this option cannot be set in code directly. It must be set in spark-submit. This config option is also undocumented.);;;","25/Jul/23 08:20;leeyc0;After inspecting the SQL plan, below are the differences

spark.sql.optimizer.plannedWrite.enabled=false (correct result)
 !Test-Details-for-Query-0.png! 

spark.sql.optimizer.plannedWrite.enabled=true (incorrect result)
 !Test-Details-for-Query-1.png! 

It appears spark sorted incorrect column if spark.sql.optimizer.plannedWrite.enabled=true
(it should sort _1, but it actually sorted _2 instead);;;","25/Jul/23 08:37;leeyc0;bumping to blocker because I believe this is a potentially very serious issue in the query planner, which may affect other queries

(sort() then select(), but the sorting column is not in select(), then query planner would use wrong column to sort);;;",,,,,,,,,,
Maintenance task should clean up loaded providers on stop error,SPARK-44504,13544342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,anishshri-db,anishshri-db,anishshri-db,20/Jul/23 20:47,21/Jul/23 04:47,30/Oct/23 17:26,21/Jul/23 04:46,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Structured Streaming,,,,,0,,,,,Maintenance task should clean up loaded providers on stop error,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jul 21 04:46:22 UTC 2023,,,,,,,,,,"0|z1ja80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/23 20:57;anishshri-db;Sent PR here: [https://github.com/apache/spark/pull/42098]

 

cc - [~kabhwan] ;;;","21/Jul/23 04:46;kabhwan;Issue resolved by pull request 42098
[https://github.com/apache/spark/pull/42098];;;",,,,,,,,,,,,,
parse_url treats key as regular expression,SPARK-44500,13544306,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,revans2,revans2,20/Jul/23 14:36,24/Jul/23 23:41,30/Oct/23 17:26,,3.2.0,3.3.0,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"To be clear I am not 100% sure that this is a bug. It might be a feature, but I don't see anywhere that it is used as a feature. If it is a feature it really should be documented, because there are pitfalls. If it is a bug it should be fixed because it is really confusing and it is simple to shoot yourself in the foot.

```scala
> val urls = Seq(""http://foo/bar?abc=BAD&a.c=GOOD"", ""http://foo/bar?a.c=GOOD&abc=BAD"").toDF
> urls.selectExpr(""parse_url(value, 'QUERY', 'a.c')"").show(false)

+----------------------------+
|parse_url(value, QUERY, a.c)|
+----------------------------+
|BAD                         |
|GOOD                        |
+----------------------------+

> urls.selectExpr(""parse_url(value, 'QUERY', 'a[c')"").show(false)
java.util.regex.PatternSyntaxException: Unclosed character class near index 15
(&|^)a[c=([^&]*)
               ^
  at java.util.regex.Pattern.error(Pattern.java:1969)
  at java.util.regex.Pattern.clazz(Pattern.java:2562)
  at java.util.regex.Pattern.sequence(Pattern.java:2077)
  at java.util.regex.Pattern.expr(Pattern.java:2010)
  at java.util.regex.Pattern.compile(Pattern.java:1702)
  at java.util.regex.Pattern.<init>(Pattern.java:1352)
  at java.util.regex.Pattern.compile(Pattern.java:1028)

```

The simple fix is to quote the key when making the pattern.

```scala
  private def getPattern(key: UTF8String): Pattern = {
    Pattern.compile(REGEXPREFIX + Pattern.quote(key.toString) + REGEXSUBFIX)
  }
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jul 24 23:41:37 UTC 2023,,,,,,,,,,"0|z1ja00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/23 23:41;planga82;[~jan.chou.wu@gmail.com] What do you think?;;;",,,,,,,,,,,,,,
FileSourceScanExec OutputPartitioning for non bucketed scan,SPARK-44499,13544302,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tushar.mahale,tushar.mahale,20/Jul/23 13:42,20/Jul/23 13:42,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"FileSourceScanExec.outputPartitioning currently is calculated for bucketed scan only and for non-bucketed scan, we return UnknownPartitioning(0). This may result into unnecessary empty tasks creation, based on the SQLConf defaultParallelism setting even though the actual file may have very low number of partitions.

We need to also calculate and set the number of output partitions correctly for non-bucketed scan.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-20 13:42:21.0,,,,,,,,,,"0|z1j9z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K8s-it test failed,SPARK-44494,13544244,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,20/Jul/23 07:51,17/Aug/23 07:31,30/Oct/23 17:26,20/Jul/23 10:22,4.0.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Kubernetes,Tests,,,,0,,,,,"* [https://github.com/apache/spark/actions/runs/5607397734/jobs/10258527838]

{code:java}
[info] - PVs with local hostpath storage on statefulsets *** FAILED *** (3 minutes, 11 seconds)
3786[info]   The code passed to eventually never returned normally. Attempted 7921 times over 3.0001059888166663 minutes. Last failure message: ""++ id -u
3787[info]   + myuid=185
3788[info]   ++ id -g
3789[info]   + mygid=0
3790[info]   + set +e
3791[info]   ++ getent passwd 185
3792[info]   + uidentry=
3793[info]   + set -e
3794[info]   + '[' -z '' ']'
3795[info]   + '[' -w /etc/passwd ']'
3796[info]   + echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false'
3797[info]   + '[' -z /opt/java/openjdk ']'
3798[info]   + SPARK_CLASSPATH=':/opt/spark/jars/*'
3799[info]   + grep SPARK_JAVA_OPT_
3800[info]   + sort -t_ -k4 -n
3801[info]   + sed 's/[^=]*=\(.*\)/\1/g'
3802[info]   + env
3803[info]   ++ command -v readarray
3804[info]   + '[' readarray ']'
3805[info]   + readarray -t SPARK_EXECUTOR_JAVA_OPTS
3806[info]   + '[' -n '' ']'
3807[info]   + '[' -z ']'
3808[info]   + '[' -z ']'
3809[info]   + '[' -n '' ']'
3810[info]   + '[' -z ']'
3811[info]   + '[' -z x ']'
3812[info]   + SPARK_CLASSPATH='/opt/spark/conf::/opt/spark/jars/*'
3813[info]   + SPARK_CLASSPATH='/opt/spark/conf::/opt/spark/jars/*:/opt/spark/work-dir'
3814[info]   + case ""$1"" in
3815[info]   + shift 1
3816[info]   + CMD=(""$SPARK_HOME/bin/spark-submit"" --conf ""spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS"" --conf ""spark.executorEnv.SPARK_DRIVER_POD_IP=$SPARK_DRIVER_BIND_ADDRESS"" --deploy-mode client ""$@"")
3817[info]   + exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=10.244.0.45 --conf spark.executorEnv.SPARK_DRIVER_POD_IP=10.244.0.45 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.examples.MiniReadWriteTest local:///opt/spark/examples/jars/spark-examples_2.12-4.0.0-SNAPSHOT.jar /opt/spark/pv-tests/tmp3727659354473892032.txt
3818[info]   Files local:///opt/spark/examples/jars/spark-examples_2.12-4.0.0-SNAPSHOT.jar from /opt/spark/examples/jars/spark-examples_2.12-4.0.0-SNAPSHOT.jar to /opt/spark/work-dir/spark-examples_2.12-4.0.0-SNAPSHOT.jar
3819[info]   23/07/20 06:15:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
3820[info]   Performing local word count from /opt/spark/pv-tests/tmp3727659354473892032.txt
3821[info]   File contents are List(test PVs)
3822[info]   Creating SparkSession
3823[info]   23/07/20 06:15:15 INFO SparkContext: Running Spark version 4.0.0-SNAPSHOT
3824[info]   23/07/20 06:15:15 INFO SparkContext: OS info Linux, 5.15.0-1041-azure, amd64
3825[info]   23/07/20 06:15:15 INFO SparkContext: Java version 17.0.7
3826[info]   23/07/20 06:15:15 INFO ResourceUtils: ==============================================================
3827[info]   23/07/20 06:15:15 INFO ResourceUtils: No custom resources configured for spark.driver.
3828[info]   23/07/20 06:15:15 INFO ResourceUtils: ==============================================================
3829[info]   23/07/20 06:15:15 INFO SparkContext: Submitted application: Mini Read Write Test
3830[info]   23/07/20 06:15:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0) {code}
The tests in the past two days have failed",,,,,,,,,,,,,,,,,,,,,,SPARK-44495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 26 03:25:47 UTC 2023,,,,,,,,,,"0|z1j9m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/23 07:52;LuciferYang;cc [~yikunkero]  Do you have any suggestions?

 ;;;","20/Jul/23 07:56;LuciferYang;It seems that the test started to fail after Minikube upgrading to 1.31.0, before is v1.30.1

 ;;;","20/Jul/23 10:22;dongjoon;Issue resolved by pull request 42091
[https://github.com/apache/spark/pull/42091];;;","26/Jul/23 03:25;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/42162;;;",,,,,,,,,,,
KubernetesSuite report NPE when not set spark.kubernetes.test.unpackSparkDir,SPARK-44487,13544200,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,20/Jul/23 01:56,21/Jul/23 01:48,30/Oct/23 17:26,21/Jul/23 01:48,3.4.1,,,,,,,,,,,,,,,,,,,4.0.0,,,,Kubernetes,Tests,,,,0,,,,,"KubernetesSuite report NPE when not set spark.kubernetes.test.unpackSparkDir

 

Exception encountered when invoking run on a nested suite.
java.lang.NullPointerException
    at sun.nio.fs.UnixPath.normalizeAndCheck(UnixPath.java:77)
    at sun.nio.fs.UnixPath.<init>(UnixPath.java:71)
    at sun.nio.fs.UnixFileSystem.getPath(UnixFileSystem.java:281)
    at java.nio.file.Paths.get(Paths.java:84)
    at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.$anonfun$beforeAll$4(KubernetesSuite.scala:164)
    at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.$anonfun$beforeAll$4$adapted(KubernetesSuite.scala:163)
    at scala.collection.LinearSeqOptimized.find(LinearSeqOptimized.scala:115)
    at scala.collection.LinearSeqOptimized.find$(LinearSeqOptimized.scala:112)
    at scala.collection.immutable.List.find(List.scala:91)
    at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.beforeAll(KubernetesSuite.scala:163)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jul 21 01:48:03 UTC 2023,,,,,,,,,,"0|z1j9cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/23 01:48;gurwls223;Issue resolved by pull request 42081
[https://github.com/apache/spark/pull/42081];;;",,,,,,,,,,,,,,
Executor decommission causes stage failure,SPARK-44478,13544013,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,dhuett,dhuett,18/Jul/23 18:05,18/Jul/23 18:05,30/Oct/23 17:26,,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,,,,,Scheduler,,,,,0,,,,,"During spark execution, save fails due to executor decommissioning. Issue not present in 3.3.0

Sample error:

 
{code:java}
An error occurred while calling o8948.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Authorized committer (attemptNumber=0, stage=170, partition=233) failed; but task commit success, data duplication may happen. reason=ExecutorLostFailure(1,false,Some(Executor decommission: Executor 1 is decommissioned.))
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageFailed$1(DAGScheduler.scala:1199)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageFailed$1$adapted(DAGScheduler.scala:1199)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleStageFailed(DAGScheduler.scala:1199)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2981)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
        at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
        at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
        at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
        at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
        at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
        at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
        at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
        at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
        at jdk.internal.reflect.GeneratedMethodAccessor497.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.base/java.lang.reflect.Method.invoke(Unknown Source)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Unknown Source)
{code}
 

 

This occurred while running our production k8s spark jobs (spark 3.3.0) in a duplicate test environment, with the only change being the image used was spark 3.4.0 and 3.4.1, and the only changes in jar versions were the requisite dependencies. 

Current workaround is to retry the job, but this can cause substantial slowdowns if it occurs during a longer job.  Possibly related to https://issues.apache.org/jira/browse/SPARK-44389 ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-18 18:05:19.0,,,,,,,,,,"0|z1j86w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckAnalysis uses error subclass as an error class,SPARK-44477,13544006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,18/Jul/23 17:41,21/Jul/23 00:45,30/Oct/23 17:26,21/Jul/23 00:45,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,SQL,,,,,0,,,,,"{{CheckAnalysis}} treats {{TYPE_CHECK_FAILURE_WITH_HINT}} as an error class, but it is instead an error subclass of {{{}DATATYPE_MISMATCH{}}}.
{noformat}
spark-sql (default)> select bitmap_count(12);
[INTERNAL_ERROR] Cannot find main error class 'TYPE_CHECK_FAILURE_WITH_HINT'
org.apache.spark.SparkException: [INTERNAL_ERROR] Cannot find main error class 'TYPE_CHECK_FAILURE_WITH_HINT'
at org.apache.spark.SparkException$.internalError(SparkException.scala:83)
at org.apache.spark.SparkException$.internalError(SparkException.scala:87)
at org.apache.spark.ErrorClassesJsonReader.$anonfun$getMessageTemplate$1(ErrorClassesJSONReader.scala:68)
at scala.collection.immutable.HashMap$HashMap1.getOrElse0(HashMap.scala:361)
at scala.collection.immutable.HashMap$HashTrieMap.getOrElse0(HashMap.scala:594)
at scala.collection.immutable.HashMap$HashTrieMap.getOrElse0(HashMap.scala:589)
at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:73)
{noformat}
This issue only occurs when an expression uses {{TypeCheckResult.TypeCheckFailure}} to indicate input type check failure. {{TypeCheckResult.TypeCheckFailure}} appears to be deprecated in favor of {{{}TypeCheckResult.DataTypeMismatch{}}}, but recently two expressions were added that use {{{}TypeCheckResult.TypeCheckFailure{}}}: {{BitmapCount}} and {{{}BitmapOrAgg{}}}.

{{BitmapCount}} and {{BitmapOrAgg}} should probably be fixed to use {{{}TypeCheckResult.DataTypeMismatch{}}}. Regardless, the code in {{CheckAnalysis}} that handles {{TypeCheckResult.TypeCheckFailure}} should be corrected (or removed).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jul 21 00:45:13 UTC 2023,,,,,,,,,,"0|z1j85c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/23 17:56;bersprockets;PR here: https://github.com/apache/spark/pull/42064;;;","21/Jul/23 00:45;gurwls223;Issue resolved by pull request 42064
[https://github.com/apache/spark/pull/42064];;;",,,,,,,,,,,,,
JobArtifactSet is populated with all artifacts if it is not associated with an artifact,SPARK-44476,13543963,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,18/Jul/23 14:34,19/Jul/23 00:37,30/Oct/23 17:26,19/Jul/23 00:37,3.5.0,4.0.0,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Connect,,,,,0,,,,,"Consider each artifact type - files/jars/archives. For each artifact type, the following bug exists:
 # Initialise a `JobArtifactState` with no artifacts added to it.
 # Create a  `JobArtifactSet` from the `JobArtifactState`.
 # Add an artifact with the same active `JobArtifactState`.
 # Create another `JobArtifactSet`

In the current behaviour, the set created in step 2 contains all the artifacts (through `sc.allAddedFiles` for example) while step 3 contains only the single artifact added in step 3.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 19 00:37:32 UTC 2023,,,,,,,,,,"0|z1j7vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/23 00:37;gurwls223;Issue resolved by pull request 42062
[https://github.com/apache/spark/pull/42062];;;",,,,,,,,,,,,,,
Overwriting the same partition of a partitioned table multiple times with empty data yields non-idempotent results,SPARK-44473,13543899,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ychris,ychris,18/Jul/23 08:03,30/Oct/23 00:18,30/Oct/23 17:26,,3.1.3,3.2.4,3.3.2,3.4.1,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,pull-request-available,,,," 

Preparation:
Create a simple partition table using spark version 3.x, for example:

 
{code:java}
spark-sql> create table test1 (a int) partitioned by (dt string);
Time taken: 0.219 seconds{code}
 

 
 * Overwrite a new partition with empty data, and you can see that the partition information and the corresponding HDFS path are generated , for example:

{code:java}

spark-sql> insert overwrite table test1 partition(dt='20230702') select 2 where 1 <> 1;
Time taken: 0.992 seconds
spark-sql> dfs -ls /user/hive/warehouse/test1;
Found 2 items
-rw-r--r-- 2 hadoop hadoop 0 2023-07-18 14:41 /user/hive/warehouse/test1/_SUCCESS
drwxrwxrwx- hadoop hadoop 0 2023-07-18 14:41 /user/hive/warehouse/test1/dt=20230702
spark-sql> show partitions test1;
dt=20230702
Time taken: 0.162 seconds, Fetched 1 row(s)
{code}
 * When re-running the insert overwrite statement, you can see that the HDFS path corresponding to this partition does not exist.

 
{code:java}
spark-sql> insert overwrite table test1 partition(dt='20230702') select 2 where 1 <> 1;
Time taken: 0.706 seconds
spark-sql> dfs -ls /user/hive/warehouse/test1;
Found 1 items
-rw-r--r--   2 hadoop hadoop          0 2023-07-18 14:45 /user/hive/warehouse/test1/_SUCCESS
spark-sql> show partitions test1;
dt=20230702
Time taken: 0.183 seconds, Fetched 1 row(s){code}
For subsequent tasks that need to use this HDFS path, an exception that the path does not exist will be thrown, which caused us trouble.

 

I was expecting to execute the same statement multiple times to get the same result, {*}not non-idempotent{*}. thanks.",spark : 3.x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,,,,,2023-07-18 08:03:07.0,,,,,,,,,,"0|z1j7hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude configs starting with SPARK_DRIVER_PREFIX and SPARK_EXECUTOR_PREFIX from modifiedConfigs,SPARK-44466,13543869,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,18/Jul/23 02:38,25/Jul/23 02:46,30/Oct/23 17:26,25/Jul/23 02:46,3.4.1,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,SQL,,,,,0,,,,,"Should not include this value: 
!screenshot-1.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/23 02:38;yumwang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13061380/screenshot-1.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 25 02:46:31 UTC 2023,,,,,,,,,,"0|z1j7aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/23 02:48;yumwang;https://github.com/apache/spark/pull/42049;;;","25/Jul/23 02:46;yumwang;Issue resolved by pull request 42049
[https://github.com/apache/spark/pull/42049];;;",,,,,,,,,,,,,
Fix applyInPandasWithStatePythonRunner to output rows that have Null as first column value,SPARK-44464,13543839,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,siying,siying,siying,17/Jul/23 20:00,21/Jul/23 05:11,30/Oct/23 17:26,19/Jul/23 01:19,3.3.3,,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,,,Structured Streaming,,,,,0,,,,,"The current implementation of {{ApplyInPandasWithStatePythonRunner}} cannot deal with outputs where the first column of the row is {{{}null{}}}, as it cannot distinguish the case where the column is null, or the field is filled as the number of data records are smaller than state records. It causes incorrect results for the former case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 19 01:19:32 UTC 2023,,,,,,,,,,"0|z1j748:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/23 22:18;siying;PR created: [https://github.com/apache/spark/pull/42046] CC [~kabhwan] ;;;","19/Jul/23 01:19;kabhwan;Issue resolved via [https://github.com/apache/spark/pull/42046]

 ;;;",,,,,,,,,,,,,
ThreadLocal not being copied to child thread when child thread is reused from pool ,SPARK-44458,13543774,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kapilks_ms,kapilks_ms,17/Jul/23 11:26,18/Jul/23 04:30,30/Oct/23 17:26,,3.1.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Currently setting the Spark configuration using the statement `spark.conf.set(""spark.sql.caseSensitive"", ""true"")` and subsequently executing a Spark action in a separate thread. An intermittent observation reveals that occasionally the configuration set in the main thread fails to propagate to the child thread

Steps to repro: [Spark SQL configs can't get propagated properly to a new thread in Spark 3.1 issue · GitHub|https://gist.github.com/t-rufang/25341b3678e5d7c74e3a209457fce0e9]

 

This is limitation due to ThreadLocal not being copied to child thread when child thread is reused from pool and only done instead in thread creation or when idle thread has expired KeepAlive in Executor

Refer:  

[https://users.scala-lang.org/t/future-executioncontext-and-threadlocal/7675/2] 

[https://www.stevenskelton.ca/threadlocal-variables-scala-futures/] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-17 11:26:36.0,,,,,,,,,,"0|z1j6ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make ArrowEncoderSuite pass  Java 17 daily test ,SPARK-44457,13543750,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,17/Jul/23 09:36,27/Jul/23 00:19,30/Oct/23 17:26,27/Jul/23 00:19,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Connect,Tests,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 27 00:19:02 UTC 2023,,,,,,,,,,"0|z1j6kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/23 00:19;srowen;Resolved by https://github.com/apache/spark/pull/42039;;;",,,,,,,,,,,,,,
SHOW CREATE TABLE does not quote identifiers with special characters,SPARK-44455,13543721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,runyao,runyao,runyao,17/Jul/23 06:32,24/Jul/23 22:20,30/Oct/23 17:26,24/Jul/23 22:20,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"Create a table with special characters:

```

CREATE CATALOG `a_catalog-with+special^chars`; CREATE SCHEMA `a_catalog-with+special^chars`.`a_schema-with+special^chars`; CREATE TABLE `a_catalog-with+special^chars`.`a_schema-with+special^chars`.`table1` ( id int, feat1 varchar(255), CONSTRAINT pk PRIMARY KEY (id,feat1) );

```

Then run SHOW CREATE TABLE:

```

SHOW CREATE TABLE `a_catalog-with+special^chars`.`a_schema-with+special^chars`.`table1`;

```

The response is:

```

createtab_stmt ""CREATE TABLE a_catalog-with+special^chars.a_schema-with+special^chars.table1 ( id INT NOT NULL, feat1 VARCHAR(255) NOT NULL, CONSTRAINT pk PRIMARY KEY (id, feat1)) USING delta TBLPROPERTIES ( 'delta.minReaderVersion' = '1', 'delta.minWriterVersion' = '2') ""

```

As you can see, the table name in the response is not properly escaped with backticks. As a result, if a user copies and pastes this create table command to recreate the table, it will fail:

{{[INVALID_IDENTIFIER] The identifier a_catalog-with is invalid. Please, consider quoting it with back-quotes as `a_catalog-with`.(line 1, pos 22)}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jul 24 22:20:34 UTC 2023,,,,,,,,,,"0|z1j6e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/23 06:39;runyao;Fix in https://github.com/apache/spark/pull/42034;;;","24/Jul/23 22:20;Gengliang.Wang;Issue resolved by pull request 42034
[https://github.com/apache/spark/pull/42034];;;",,,,,,,,,,,,,
Wrong results for dense_rank() <= k from InferWindowGroupLimit and DenseRankLimitIterator,SPARK-44448,13543701,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jchen5,jchen5,jchen5,16/Jul/23 22:46,19/Jul/23 01:13,30/Oct/23 17:26,19/Jul/23 01:13,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"Top-k filters on a dense_rank() window function return wrong results, due to a bug in optimization InferWindowGroupLimit, specifically in the code for DenseRankLimitIterator, introduced in https://issues.apache.org/jira/browse/SPARK-37099.

Repro:
{code:java}
create or replace temp view t1 (p, o) as values (1, 1), (1, 1), (1, 2), (2, 1), (2, 1), (2, 2);

select * from (select *, dense_rank() over (partition by p order by o) as rnk from t1) where rnk = 1;{code}
Spark result:
{code:java}
[1,1,1]
[1,1,1]
[2,1,1]{code}
Correct result:
{code:java}
[1,1,1]
[1,1,1]
[2,1,1]
[2,1,1]{code}
 

The bug is in {{{}DenseRankLimitIterator{}}}, it fails to reset state properly when transitioning from one window partition to the next. {{reset}} only resets {{{}rank = 0{}}}, what it is missing is to reset {{{}currentRankRow = null{}}}. This means that when processing the second and later window partitions, the rank incorrectly gets incremented based on comparing the ordering of the last row of the previous partition to the first row of the new partition.

This means that a dense_rank window func that has more than one window partition and more than one row with dense_rank = 1 in the second or later partitions can give wrong results when optimized.

({{{}RankLimitIterator{}}} narrowly avoids this bug by happenstance, the first row in the new partition will try to increment rank, but increment it by the value of count which is 0, so it happens to work by accident).

Unfortunately, tests for the optimization only had a single row per rank, so did not catch the bug as the bug requires multiple rows per rank.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 19 01:13:40 UTC 2023,,,,,,,,,,"0|z1j69k:",9223372036854775807,,,,,,,,,,,,,3.5.0,,,,,,,,,,"17/Jul/23 01:13;yumwang;cc [~beliefer];;;","17/Jul/23 01:21;jchen5;Fix PR: https://github.com/apache/spark/pull/42026;;;","18/Jul/23 03:22;snoot;User 'jchen5' has created a pull request for this issue:
https://github.com/apache/spark/pull/42026;;;","18/Jul/23 03:23;snoot;User 'jchen5' has created a pull request for this issue:
https://github.com/apache/spark/pull/42026;;;","18/Jul/23 03:30;snoot;User 'jchen5' has created a pull request for this issue:
https://github.com/apache/spark/pull/42042;;;","18/Jul/23 03:31;snoot;User 'jchen5' has created a pull request for this issue:
https://github.com/apache/spark/pull/42042;;;","19/Jul/23 01:13;cloud_fan;Issue resolved by pull request 42026
[https://github.com/apache/spark/pull/42026];;;",,,,,,,,
ALTER COLUMN to increase VARCHAR size fails,SPARK-44437,13543598,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dxiaoitron,dxiaoitron,14/Jul/23 18:35,14/Jul/23 18:35,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"I tried to expand a VARCHAR column by running the following commands but ran into an error when doing so.

 
{code:java}
spark.sql(""DROP TABLE IF EXISTS default.test_table"")

spark.sql(""CREATE TABLE default.test_table (test_col VARCHAR(10))"")

spark.sql(""ALTER TABLE default.test_table ALTER COLUMN test_col TYPE VARCHAR(25)"") {code}
 

The error I get:

pyspark.sql.utils.AnalysisException: ALTER TABLE CHANGE COLUMN is not supported for changing column 'test_col' with type 'VarcharType(10)' to 'test_col' with type 'VarcharType(25)'

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-14 18:35:13.0,,,,,,,,,,"0|z1j5mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckOverflowInTableInsert does not handle cases where the cast was moved by the optimizer,SPARK-44420,13543506,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,olaky,olaky,14/Jul/23 07:10,14/Jul/23 07:10,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Optimizer,,,,,0,,,,,"CheckOverflowInTableInsert is supposed to rewrite error messages that come from overflow exceptions on casts that are implicitly inserted for INSERTs to match the output schema. This does not work in all cases, for example when the cast is moved by the optimizer
 
{code:java}
CREATE TABLE source(value BIGINT) USING parquet
INSERT INTO source values (${Long.MaxValue.toString})
CREATE TABLE target(value INT) USING parquet
INSERT INTO target SELECT CASE WHEN value < 0 THEN 0 ELSE value END FROM source{code}
In this case, the expression will no nothing because it only becomes effective if the direct child is a cast ([matching code|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala#L2186])",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-14 07:10:58.0,,,,,,,,,,"0|z1j528:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Driver and executors are not transferring from standby master to Alive Master.,SPARK-44417,13543499,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,anandc,anandc,14/Jul/23 05:24,14/Jul/23 05:26,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Deploy,,,,,0,,,,,"Issue with the Spark driver and executors transferring from Standby Master to Alive Master.

Spark is deployed with Zookeeper as cluster manager. Ideally if standby node(server) goes down it should transfer applications running on standby node to alive node.
Application transformations are killed in the Standby Master.
  # Just consider we have two Application drivers were running on the Standby Master server.
 # If we forcefully shutdown, This should trigger the failover scenario and application should move to Alive node. But, The Standby Master node both the application drivers were not transferring to Active master instead those were killing.

Check attachment for reference.
 
Steps to reproduce
 * Deploy Spark HA cluster with Zookeeper
 * Deploy multiple application until we have drivers and executor assigned on standby node.
 * Now reboot the server
 * Check if all the applications were moving to Active Master. 
 
 ","RedHat linux 

Zookeeper HA Cluster",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/23 05:26;anandc;Issue_steps.docx;https://issues.apache.org/jira/secure/attachment/13061333/Issue_steps.docx",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-14 05:24:04.0,,,,,,,,,,"0|z1j50o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame depending on temp view fail after the view is dropped,SPARK-44406,13543371,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,podongfeng,podongfeng,13/Jul/23 04:02,24/Oct/23 00:17,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Connect,,,,,0,pull-request-available,,,,"In vanilla Spark:


{code:java}
In [1]: df = spark.createDataFrame([(1, 4), (2, 4), (3, 6)], [""A"", ""B""])

In [2]: df.createOrReplaceTempView(""t"")

In [3]: df2 = spark.sql(""select * from t"")

In [4]: df2.show()
+---+---+                                                                       
|  A|  B|
+---+---+
|  1|  4|
|  2|  4|
|  3|  6|
+---+---+


In [5]: spark.catalog.dropTempView(""t"")
Out[5]: True

In [6]: df2.show()
+---+---+
|  A|  B|
+---+---+
|  1|  4|
|  2|  4|
|  3|  6|
+---+---+
{code}


In Spark Connect:


{code:java}
In [1]: df = spark.createDataFrame([(1, 4), (2, 4), (3, 6)], [""A"", ""B""])

In [2]: df.createOrReplaceTempView(""t"")

In [3]: df2 = spark.sql(""select * from t"")

In [4]: df2.show()
+---+---+
|  A|  B|
+---+---+
|  1|  4|
|  2|  4|
|  3|  6|
+---+---+


In [5]: spark.catalog.dropTempView(""t"")
Out[5]: True

In [6]: df2.show()
23/07/13 11:57:18 ERROR SparkConnectService: Error during: execute. UserId: ruifeng.zheng. SessionId: 1fc234fd-07da-4ad0-9ec5-2d818cef6033.
org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `t` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [t], [], false


{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 13 14:30:40 UTC 2023,,,,,,,,,,"0|z1j48g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/23 14:30;ggintegration;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/41986;;;",,,,,,,,,,,,,,
Update table function arguments to require parentheses around identifier after the TABLE keyword,SPARK-44395,13543312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dtenedor,dtenedor,dtenedor,12/Jul/23 17:27,13/Jul/23 21:05,30/Oct/23 17:26,13/Jul/23 21:05,4.0.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"Per the SQL standard, `TABLE identifier` should actually be passed as `TABLE(identifier)`. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 13 21:05:09 UTC 2023,,,,,,,,,,"0|z1j3vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/23 17:33;ci-cassandra.apache.org;User 'dtenedor' has created a pull request for this issue:
https://github.com/apache/spark/pull/41965;;;","13/Jul/23 21:05;ueshin;Issue resolved by pull request 41965
https://github.com/apache/spark/pull/41965;;;",,,,,,,,,,,,,
`url_decode` can fail w/ an internal error,SPARK-44391,13543303,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,12/Jul/23 15:54,14/Jul/23 05:38,30/Oct/23 17:26,13/Jul/23 09:17,3.5.0,,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,,,SQL,,,,,0,,,,,"The function *url_decode* can fail with the error:


{code:java}
org.apache.spark.SparkException: [INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. 
at org.apache.spark.SparkException$.internalError(SparkException.scala:89) 
at org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:871) 
...
Caused by: java.lang.AssertionError: assertion failed: Incorrect number of children
        at scala.Predef$.assert(Predef.scala:223)
        at org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:391)
        at org.apache.spark.sql.catalyst.analysis.TypeCoercionBase$ImplicitTypeCasts$$anonfun$11.applyOrElse(TypeCoercion.scala:707)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 13 09:17:44 UTC 2023,,,,,,,,,,"0|z1j3tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/23 09:17;githubbot;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/41954;;;","13/Jul/23 09:17;maxgekk;Issue resolved by pull request 41954
[https://github.com/apache/spark/pull/41954];;;",,,,,,,,,,,,,
Using an updated instance of ScalarUserDefinedFunction causes protobuf cast failures on server,SPARK-44388,13543296,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,12/Jul/23 15:13,13/Jul/23 09:55,30/Oct/23 17:26,13/Jul/23 09:55,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"When running the following code-
{code:java}
class A(x: Int) { def get = x * 7 }
val myUdf = udf((x: Int) => new A(x).get)
val modifiedUdf = myUdf.withName(""myUdf"")
spark.range(5).select(modifiedUdf(col(""id""))).as[Int].collect(){code}
which modifies the original myUdf instance through the `withName` method causes the following error to occur during execution:
{noformat}
java.lang.ClassCastException: org.apache.spark.connect.proto.ScalarScalaUDF cannot be cast to com.google.protobuf.MessageLite
    at com.google.protobuf.GeneratedMessageLite$SerializedForm.readResolve(GeneratedMessageLite.java:1462)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1274)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2285)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
    at org.apache.spark.util.Utils$.deserialize(Utils.scala:148){noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 13 09:55:21 UTC 2023,,,,,,,,,,"0|z1j3rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/23 09:03;githubbot;User 'vicennial' has created a pull request for this issue:
https://github.com/apache/spark/pull/41959;;;","13/Jul/23 09:55;gurwls223;Issue resolved by pull request 41959
[https://github.com/apache/spark/pull/41959];;;",,,,,,,,,,,,,
Fix the trim logic did't handle ASCII control characters correctly ,SPARK-44383,13543244,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,kwafor,kwafor,12/Jul/23 09:31,12/Jul/23 09:50,30/Oct/23 17:26,12/Jul/23 09:46,3.1.1,3.4.0,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,,,0,,,,,"The trim logic in Cast expression introduced in [#29375|https://github.com/apache/spark/pull/29375] trim ASCII control characters unexpectly.[!https://user-images.githubusercontent.com/25627922/244651442-ca6a2fb1-2143-4264-84d1-70b6bb755ec7.png!|https://user-images.githubusercontent.com/25627922/244651442-ca6a2fb1-2143-4264-84d1-70b6bb755ec7.png]
And hive
[!https://user-images.githubusercontent.com/25627922/244651942-017aaa4a-133e-4396-9694-79f03f027bbe.png!|https://user-images.githubusercontent.com/25627922/244651942-017aaa4a-133e-4396-9694-79f03f027bbe.png]",,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32559,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 12 09:46:26 UTC 2023,,,,,,,,,,"0|z1j3g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/23 09:46;yao;Issue resolved by https://github.com/apache/spark/pull/41535;;;",,,,,,,,,,,,,,
Jobs that have join & have .rdd calls get executed 2x when AQE is enabled.,SPARK-44378,13543172,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,priyankar,priyankar,11/Jul/23 16:35,12/Jul/23 19:26,30/Oct/23 17:26,,3.1.2,,,,,,,,,,,,,,,,,,,,,,,Spark Submit,,,,,0,aqe,,,,"We have a few spark scala jobs that are currently running in production. Most jobs typically use Dataset, Dataframes. There is a small code in our custom library code, that makes rdd calls example to check if the dataframe is empty: df.rdd.getNumPartitions == 0

When I enable aqe for these jobs, this .rdd is converted into a separate job of it's own and the entire dag is executed 2x, taking 2x more time. This does not happen when AQE is disabled. Why does this happen and what is the best way to fix the issue?

 

Sample code to reproduce the issue:

 

 
{code:java}
import org.apache.spark.sql._ 
  case class Record(
    id: Int,
    name: String
 )
 
    val partCount = 4
    val input1 = (0 until 100).map(part => Record(part, ""a""))
 
    val input2 = (100 until 110).map(part => Record(part, ""c""))
 
    implicit val enc: Encoder[Record] = Encoders.product[Record]
 
    val ds1 = spark.createDataset(
      spark.sparkContext
        .parallelize(input1, partCount)
    )
 
    va


l ds2 = spark.createDataset(
      spark.sparkContext
        .parallelize(input2, partCount)
    )
 
    val ds3 = ds1.join(ds2, Seq(""id""))
    val l = ds3.count()
 
    val incomingPartitions = ds3.rdd.getNumPartitions
    log.info(s""Num partitions ${incomingPartitions}"")
  {code}
 

Spark UI for the same job with AQE,  !Screenshot 2023-07-11 at 9.36.14 AM.png!

 

Spark UI for the same job without AQE:

 

!Screenshot 2023-07-11 at 9.36.19 AM.png!

 

This is causing unexpected regression in our jobs when we try to enable AQE for our jobs in production. We use spark 3.1 in production, but I can see the same behavior in spark 3.2 from the console as well

 

!image2.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/23 16:37;priyankar;Screenshot 2023-07-11 at 9.36.14 AM.png;https://issues.apache.org/jira/secure/attachment/13061243/Screenshot+2023-07-11+at+9.36.14+AM.png","11/Jul/23 16:38;priyankar;Screenshot 2023-07-11 at 9.36.19 AM.png;https://issues.apache.org/jira/secure/attachment/13061244/Screenshot+2023-07-11+at+9.36.19+AM.png","11/Jul/23 16:41;priyankar;image2.png;https://issues.apache.org/jira/secure/attachment/13061245/image2.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Scala,,,,2023-07-11 16:35:22.0,,,,,,,,,,"0|z1j308:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build using maven is broken using 2.13 and Java 11 and Java 17,SPARK-44376,13543164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,eejbyfeldt,eejbyfeldt,11/Jul/23 14:50,15/Sep/23 06:46,30/Oct/23 17:26,07/Aug/23 02:15,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Build,,,,,0,pull-request-available,,,,"Fails with
```
$ ./build/mvn compile -Pscala-2.13 -Djava.version=11 -X
...
[WARNING] [Warn] : [deprecation @  | origin= | version=] -target is deprecated: Use -release instead to compile against the correct platform API.
[ERROR] [Error] : target platform version 8 is older than the release version 11
[WARNING] one warning found
[ERROR] one error found
...
```
if setting the `java.version` property or
```
$ ./build/mvn compile -Pscala-2.13
...
[WARNING] [Warn] : [deprecation @  | origin= | version=] -target is deprecated: Use -release instead to compile against the correct platform API.
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala:71: not found: value sun
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:26: not found: object sun
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:27: not found: object sun
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:206: not found: type DirectBuffer
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:210: not found: type Unsafe
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:212: not found: type Unsafe
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:213: not found: type DirectBuffer
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:216: not found: type DirectBuffer
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:236: not found: type DirectBuffer
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:26: Unused import
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:27: Unused import
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala:452: not found: value sun
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: not found: object sun
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type SignalHandler
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type Signal
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:83: not found: type Signal
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: type SignalHandler
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: value Signal
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:114: not found: type Signal
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:116: not found: value Signal
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:128: not found: value Signal
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: Unused import
[ERROR] [Error] /home/eejbyfeldt/dev/apache/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: Unused import
[WARNING] one warning found
[ERROR] 23 errors found
...
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 07 02:15:27 UTC 2023,,,,,,,,,,"0|z1j2yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/23 16:57;srowen;Did you run dev/change-scala-version.sh 2.13 ?;;;","07/Aug/23 02:15;LuciferYang;Issue resolved by pull request 42364
[https://github.com/apache/spark/pull/42364];;;",,,,,,,,,,,,,
Migrate Buf remote generation alpha to remote plugins,SPARK-44370,13543106,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,11/Jul/23 06:17,12/Jul/23 04:45,30/Oct/23 17:26,12/Jul/23 04:45,3.4.1,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,Buf unsupported remote generation alpha at now. Please refer [https://buf.build/docs/migration-guides/migrate-remote-generation-alpha/] . We should migrate Buf remote generation alpha to remote plugins by follow the guide.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 12 04:45:11 UTC 2023,,,,,,,,,,"0|z1j2lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/23 04:26;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41933;;;","12/Jul/23 04:45;gurwls223;Issue resolved by pull request 41933
[https://github.com/apache/spark/pull/41933];;;",,,,,,,,,,,,,
Cannot create dataframe with CharType/VarcharType column,SPARK-44354,13543024,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,KaiRoesner,KaiRoesner,10/Jul/23 14:35,11/Jul/23 09:38,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,SQL,,,,0,,,,,"When trying to create a dataframe with a CharType or VarcharType column like so:
{code}
from datetime import date
from decimal import Decimal
from pyspark.sql import SparkSession
from pyspark.sql.types import *

data = [
  (1, 'abc', Decimal(3.142), date(2023, 1, 1)),
  (2, 'bcd', Decimal(1.414), date(2023, 1, 2)),
  (3, 'cde', Decimal(2.718), date(2023, 1, 3))]

schema = StructType([
  StructField('INT', IntegerType()),
  StructField('STR', CharType(3)),
  StructField('DEC', DecimalType(4, 3)),
  StructField('DAT', DateType())])

spark = SparkSession.builder.appName('data-types').getOrCreate()
df = spark.createDataFrame(data, schema)
df.show()
{code}
a {{java.lang.IllegalStateException}} is thrown [here|https://github.com/apache/spark/blob/85e252e8503534009f4fb5ea005d44c9eda31447/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala#L168].

Excerpt from the logs:
{code}
py4j.protocol.Py4JJavaError: An error occurred while calling o24.applySchemaToPythonRDD.
: java.lang.IllegalStateException: [BUG] logical plan should not have output of char/varchar type: LogicalRDD [INT#0, STR#1, DEC#2, DAT#3], false

        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:168)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)
        at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
        at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)
        at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
        at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)
        at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
        at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
        at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
        at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
        at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
        at org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:571)
        at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:804)
        at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:789)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:829)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 11 09:35:39 UTC 2023,,,,,,,,,,"0|z1j23c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/23 09:35;KaiRoesner;PS: I tried to work around the exception by using `StringType()` in the schema and then doing 
{code}
df.withColumn('STR', col('STR').cast(CharType(3)))
{code}
That got me a
{code}
WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type.
{code}
So now I'm wondering whether `CharType()` is supported as column data type at all...;;;",,,,,,,,,,,,,,
spark3-shell errors org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table <hive_table_name>. Permission denied: user [AD user] does not have [SELECT] privilege on [<database>/<hive table>] when reads hive view ,SPARK-44339,13542861,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,amarrocks85,amarrocks85,08/Jul/23 01:39,09/Jul/23 02:22,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Spark Shell,Spark Submit,,,,0,,,,,"*Problem statement* 

A hive view is created using beeline to restrict the users from accessing the original hive table since the data contains sensitive information. 

For illustration purpose, let's consider a sensitive table as emp_db.employee with columns id, name, salary created through beeline by user '{*}userA{*}'

 
{code:java}
create external table emp_db.employee (id int, name string, salary double) location '<hdfs_path>'{code}
 

A view is created using beeline by the same user '{*}userA{*}'

 
{code:java}
ate view empview_db.emp_v  as select id,name from emp_db.employee' {code}
 

From Ranger UI, we define a policy under Hadoop SQL Policies that will let '{*}userB{*}' to access database - empview_db  and table - emp_v with SELECT permission.

 

*Steps to replicate* 
 # ssh to edge node where beeline is available using *userB*
 # Try executing following queries
 ## select * from emp_db.employee  *;*
 ## desc formatted empview_db.emp_v;
 ## Above queries works fine without any issues.
 # Now, try using spark3-shell using *userB* 
{code:java}
# spark3-shell --deploy-mode client
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/07/08 01:24:09 WARN HiveConf: HiveConf of name hive.masking.algo does not exist
Spark context Web UI available at http://xxxxxxx:4040
Spark context available as 'sc' (master = yarn, app id = application_xxx_xxx).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0.3.3.7180.0-274
      /_/
         
Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)
Type in expressions to have them evaluated.
Type :help for more information.scala> spark.table(""empview_db.emp_v"").schema
23/07/08 01:24:30 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic
Hive Session ID = b1e3c813-aea9-40da-9012-949e82d4205e
org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table employee. Permission denied: user [userB] does not have [SELECT] privilege on [emp_db/employee]
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:110)
  at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:877)
  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:146)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireTableExists(SessionCatalog.scala:224)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:514)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:500)
  at org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:66)
  at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:311)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1206)
  at scala.Option.orElse(Option.scala:447)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1205)
  at scala.Option.orElse(Option.scala:447)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1197)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1068)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)
  at org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:208)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)
  at org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:208)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  at scala.collection.immutable.List.foldLeft(List.scala:91)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
  at scala.collection.immutable.List.foreach(List.scala:431)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveViews$2(Analyzer.scala:1012)
  at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveViews$1(Analyzer.scala:1012)
  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withAnalysisContext(Analyzer.scala:166)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1004)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1020)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.$anonfun$applyOrElse$47(Analyzer.scala:1068)
  at scala.Option.map(Option.scala:230)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1068)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  at scala.collection.immutable.List.foldLeft(List.scala:91)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
  at scala.collection.immutable.List.foreach(List.scala:431)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)
  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:186)
  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:511)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:186)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:185)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
  at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:607)
  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:600)
  ... 47 elided
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table employee. Permission denied: user [userB] does not have [SELECT] privilege on [emp_db/employee]
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1462)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1411)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1391)
  at org.apache.spark.sql.hive.client.Shim_v0_12.getTable(HiveShim.scala:639)
  at org.apache.spark.sql.hive.client.HiveClientImpl.getRawTableOption(HiveClientImpl.scala:429)
  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$tableExists$1(HiveClientImpl.scala:444)
  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:321)
  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:248)
  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:247)
  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:301)
  at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:444)
  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$tableExists$1(HiveExternalCatalog.scala:877)
  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:101)
  ... 151 more
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Permission denied: user [userB] does not have [SELECT] privilege on [emp_db/employee]
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result$get_table_req_resultStandardScheme.read(ThriftHiveMetastore.java)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result$get_table_req_resultStandardScheme.read(ThriftHiveMetastore.java)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result.read(ThriftHiveMetastore.java)
  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table_req(ThriftHiveMetastore.java:2378)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table_req(ThriftHiveMetastore.java:2365)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:2047)
  at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:206)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:213)
  at com.sun.proxy.$Proxy48.getTable(Unknown Source)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:3514)
  at com.sun.proxy.$Proxy48.getTable(Unknown Source)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1453)
  ... 165 more

{code}

*Expected behavior* - we want spark to behave just like beeline where SELECT * from <view-name> and DESC formatted <view-name> on view works fine without any errors. 

The CDP 7.1.7 documentation link [https://docs.cloudera.com/cdp-private-cloud-base/7.1.7/developing-spark-applications/topics/spark-interaction-with-hive-views.html?]  describes 'Interacting Hive Views'. However, the explanation doesn't fit well with the behavior we see from spark3-shell for hive views.

Looking forward for feedback and inputs that may unblock my use case. Please let me know if  you need any further information. 

 ","CDP 7.1.7 Ranger, kerberized and hadoop impersonation enabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jul 09 02:22:00 UTC 2023,,,,,,,,,,"0|z1j13k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/23 02:22;yumwang;It seems it's cloudera spark issue.;;;",,,,,,,,,,,,,,
Fix the sorting error of Executor ID Column on Executors UI Page,SPARK-44332,13542765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,07/Jul/23 06:31,11/Jul/23 00:16,30/Oct/23 17:26,11/Jul/23 00:16,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,Web UI,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 11 00:16:08 UTC 2023,,,,,,,,,,"0|z1j0i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/23 09:04;githubbot;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41887;;;","11/Jul/23 00:16;srowen;Issue resolved by pull request 41887
[https://github.com/apache/spark/pull/41887];;;",,,,,,,,,,,,,
Scala None shows up as null for Aggregator BUF or OUT  ,SPARK-44323,13542727,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,koert,koert,06/Jul/23 17:08,09/Jul/23 15:35,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"when doing an upgrade from spark 3.3.1 to spark 3.4.1 we suddenly started getting null pointer exceptions in Aggregators (classes extending org.apache.spark.sql.expressions.Aggregator) that use scala Option for BUF and/or OUT. basically None is now showing up as null.

after adding a simple test case and doing a binary search on commits we landed on SPARK-37829 being the cause.

we observed the issue at first with NPE inside Aggregator.merge because None was null. i am having a hard time replicating that in a spark unit test, but i did manage to get a None become null in the output. simple test that now fails:

 
{code:java}
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/DatasetAggregatorSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/DatasetAggregatorSuite.scala
index e9daa825dd4..a1959d7065d 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/DatasetAggregatorSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DatasetAggregatorSuite.scala
@@ -228,6 +228,16 @@ case class FooAgg(s: Int) extends Aggregator[Row, Int, Int] {
   def outputEncoder: Encoder[Int] = Encoders.scalaInt
 }
 
+object OptionStringAgg extends Aggregator[Option[String], Option[String], Option[String]] {
+  override def zero: Option[String] = None
+  override def reduce(b: Option[String], a: Option[String]): Option[String] = merge(b, a)
+  override def finish(reduction: Option[String]): Option[String] = reduction
+  override def merge(b1: Option[String], b2: Option[String]): Option[String] =
+    b1.map{ b1v => b2.map{ b2v => b1v ++ b2v }.getOrElse(b1v) }.orElse(b2)
+  override def bufferEncoder: Encoder[Option[String]] = ExpressionEncoder()
+  override def outputEncoder: Encoder[Option[String]] = ExpressionEncoder()
+}
+
 class DatasetAggregatorSuite extends QueryTest with SharedSparkSession {
   import testImplicits._
 
@@ -432,4 +442,15 @@ class DatasetAggregatorSuite extends QueryTest with SharedSparkSession {
     val agg = df.select(mode(col(""a""))).as[String]
     checkDataset(agg, ""3"")
   }
+
+  test(""typed aggregation: option string"") {
+    val ds = Seq((1, Some(""a"")), (1, None), (1, Some(""c"")), (2, None)).toDS()
+
+    checkDataset(
+      ds.groupByKey(_._1).mapValues(_._2).agg(
+        OptionStringAgg.toColumn
+      ),
+      (1, Some(""ac"")), (2, None)
+    )
+  }
 }
 {code}",,,,,,,,,,,,,,,,,,,,SPARK-37829,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jul 09 15:35:37 UTC 2023,,,,,,,,,,"0|z1j09s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/23 18:28;koert;i think the issue is that Nones inside Tuples now become nulls.

so its the usage of nullSafe inside the childrenDeserializers for tuples introduced in [https://github.com/apache/spark/pull/40755];;;","09/Jul/23 15:35;koert;not sure why pullreq isnt getting linked automatically but its here:

https://github.com/apache/spark/pull/41903;;;",,,,,,,,,,,,,
Generated column expression validation fails if there is a char/varchar column anywhere in the schema,SPARK-44313,13542601,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allison-portis,allison-portis,allison-portis,05/Jul/23 22:37,06/Jul/23 02:27,30/Oct/23 17:26,06/Jul/23 02:27,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,,,SQL,,,,,0,,,,,"When validating generated column expressions, this call https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/GeneratedColumn.scala#L123 to checkAnalysis fails when there are char or varchar columns anywhere in the schema.

 

For example, this query will fail
{code:java}
CREATE TABLE default.example (
    name VARCHAR(64),
    tstamp TIMESTAMP,
    tstamp_date DATE GENERATED ALWAYS AS (CAST(tstamp as DATE))
){code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,SPARK-41290,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 06 02:27:18 UTC 2023,,,,,,,,,,"0|z1izi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/23 02:27;Qin Yao;Issue resolved by pull request 41868
[https://github.com/apache/spark/pull/41868];;;",,,,,,,,,,,,,,
UDF should support function taking value classes,SPARK-44311,13542539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,05/Jul/23 12:46,01/Aug/23 14:52,30/Oct/23 17:26,01/Aug/23 14:50,3.4.1,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"Running the following code in a spark 
```
final case class ValueClass(a: Int) extends AnyVal
final case class Wrapper(v: ValueClass)

val f = udf((a: ValueClass) => a.a > 0)

spark.createDataset(Seq(Wrapper(ValueClass(1)))).filter(f(col(""v""))).show()
```

fails with
```
java.lang.ClassCastException: class org.apache.spark.sql.types.IntegerType$ cannot be cast to class org.apache.spark.sql.types.StructType (org.apache.spark.sql.types.IntegerType$ and org.apache.spark.sql.types.StructType are in unnamed module of loader 'app')
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$42$$anonfun$applyOrElse$218.$anonfun$applyOrElse$220(Analyzer.scala:3241)
  at scala.Option.map(Option.scala:242)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$42$$anonfun$applyOrElse$218.$anonfun$applyOrElse$219(Analyzer.scala:3239)
  at scala.collection.immutable.List.map(List.scala:246)
  at scala.collection.immutable.List.map(List.scala:79)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$42$$anonfun$applyOrElse$218.applyOrElse(Analyzer.scala:3237)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$42$$anonfun$applyOrElse$218.applyOrElse(Analyzer.scala:3234)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:566)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:566)
```",,,,,,,,,,,,,,,,,,,,,,,,,SPARK-44621,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-05 12:46:17.0,,,,,,,,,,"0|z1iz4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 3.0.1 functions.scala -> posexplode_outer API not flattening data,SPARK-44308,13542483,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,csanghvi,csanghvi,05/Jul/23 06:34,05/Jul/23 06:34,30/Oct/23 17:26,,3.0.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,SQL,,,,0,,,,,"Spark 3.x API functions.scala -> posexplode_outer to flatten the array column value doesn't work as expected when the table is created with ""collection.delim"" set to non default value.

This used to work as expected in Spark 2.4.5

 

Use the below DDL to create a hive table 

CREATE EXTERNAL TABLE `testnorm2`(
`enquiryuid` string,
`rulestriggered` array<string>)
ROW FORMAT SERDE
'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
'collection.delim'=';',
'field.delim'='|',
'line.delim'='\n',
'serialization.format'='|')
STORED AS INPUTFORMAT
'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat';

 

And fill up with the table with array values.

The below statements fill up the table with sample data.

 

INSERT INTO testnorm2 SELECT 'A', array('a','b');
INSERT INTO testnorm2 SELECT 'B', array('e','f','g','h');
INSERT INTO testnorm2 SELECT 'C', array();
INSERT INTO testnorm2 SELECT 'D', array('');
INSERT INTO testnorm2 SELECT 'E', array('','');
INSERT INTO testnorm2 SELECT 'F', array('','1','2');
INSERT INTO testnorm2 SELECT 'G', array(null);
INSERT INTO testnorm2 SELECT 'H', array(null,'');
INSERT INTO testnorm2 SELECT 'I', array(null,'4','5','6');
INSERT INTO testnorm2 SELECT 'G', array("""");

 


Open Spark Shell (in spark 3.0.1)

and run below scala code statements


val df = spark.sql(""select * from testnorm2"");

 

the df.show () gives this output in both cases(spark 2.4 and spark 3.0.1).

+----------+--------------+
|enquiryuid|data          |
+----------+--------------+
|         I|   [, 4, 5, 6]|
|         F|      [, 1, 2]|
|         B|  [e, f, g, h]|
|         A|        [a, b]|
|         H|          [, ]|
|         E|          [, ]|
|         G|          null|
|         G|            []|
|         D|            []|
|         C|            []|
+----------+--------------+


val explodeDF = df.select($""id"",(posexplode_outer($""data""));

 

on doing this there is a difference in output for 2.4 and spark 3.0.1

on 2.4.x the output is

+----------+----+----+
|enquiryuid| pos| col|
+----------+----+----+
|         I|   0|null|
|         I|   1|   4|
|         I|   2|   5|
|         I|   3|   6|
|         F|   0|    |
|         F|   1|   1|
|         F|   2|   2|
|         B|   0|   e|
|         B|   1|   f|
|         B|   2|   g|
|         B|   3|   h|
|         A|   0|   a|
|         A|   1|   b|
|         H|   0|null|
|         H|   1|    |
|         E|   0|    |
|         E|   1|    |
|         G|null|null|
|         G|null|null|
|         D|null|null|
+----------+----+----+

Whereas  in 3.x the output is
+----------+----+--------+
|enquiryuid| pos|     col|
+----------+----+--------+
|         I|   0|\N,4,5,6|
|         F|   0|    ,1,2|
|         1|   0|     a,b|
|         C|null|    null|
|         G|null|    null|
|         B|   0| e,f,g,h|
|         H|   0|     \N,|
|         G|null|    null|
|         E|   0|       ,|
|         D|null|    null|
+----------+----+--------+


The array in column 2 is not getting flattened in the case of spark 3.0.1 but in spark 2.4.5 it gets flattened.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-05 06:34:42.0,,,,,,,,,,"0|z1iysg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bloom filter is not added for left outer join if the left side table is smaller than broadcast threshold.,SPARK-44307,13542476,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,maheshk114,maheshk114,05/Jul/23 05:11,28/Oct/23 11:49,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,Optimizer,,,,,0,pull-request-available,,,,"In case of left outer join, even if the left side table is small enough to be broadcasted, shuffle join is used. This is because of the property of the left outer join. If the left side is broadcasted in left outer join, the result generated will be wrong. But this is not taken care of in bloom filter. While injecting the bloom filter, if lest side is smaller than broadcast threshold, bloom filter is not added. It assumes that the left side will be broadcast and there is no need for a bloom filter. This causes bloom filter optimization to be missed in case of left outer join with small left side and huge right-side table.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 06 11:50:31 UTC 2023,,,,,,,,,,"0|z1iyqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/23 11:50;awsthni;User 'maheshk114' has created a pull request for this issue:
https://github.com/apache/spark/pull/41860;;;",,,,,,,,,,,,,,
SparkConnectArtifactManager#cleanUpResources deletes all artifacts instead of session-specific artifacts,SPARK-44300,13542425,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,04/Jul/23 13:01,05/Jul/23 03:06,30/Oct/23 17:26,05/Jul/23 03:06,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"_SparkConnectArtifactManager#cleanUpResources_ deletes all resources instead of session-specific resources. 

This method is triggered through the _userSessionMapping_ cache when an entry is removed ([code|https://github.com/apache/spark/blob/b02ea4cd370ce6a066561dfde9d517ea70805a2b/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectService.scala#L304]). Once triggered, further artifact transfers and existing artifact usage would fail. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 05 03:06:44 UTC 2023,,,,,,,,,,"0|z1iyg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/23 03:06;gurwls223;Issue resolved by pull request 41854
[https://github.com/apache/spark/pull/41854];;;",,,,,,,,,,,,,,
Make `ClassLoaderIsolationSuite` test pass with Scala 2.13,SPARK-44297,13542394,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,04/Jul/23 09:18,04/Jul/23 14:37,30/Oct/23 17:26,04/Jul/23 14:37,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 04 14:37:23 UTC 2023,,,,,,,,,,"0|z1iy9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/23 09:22;githubbot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/41852;;;","04/Jul/23 14:37;LuciferYang;Issue resolved by pull request 41852
[https://github.com/apache/spark/pull/41852];;;",,,,,,,,,,,,,
HeapHistogram column shows unexpectedly w/ select-all-box,SPARK-44294,13542374,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,04/Jul/23 07:49,05/Jul/23 07:42,30/Oct/23 17:26,05/Jul/23 07:41,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Web UI,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 05 07:41:35 UTC 2023,,,,,,,,,,"0|z1iy4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/23 07:41;gurwls223;Issue resolved by pull request 41847
[https://github.com/apache/spark/pull/41847];;;",,,,,,,,,,,,,,
Task failures during custom JAR fetch in executors,SPARK-44293,13542369,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,04/Jul/23 06:46,04/Jul/23 23:42,30/Oct/23 17:26,04/Jul/23 23:42,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"When attempting to use a custom JAR in a Spark Connect session, the tasks fail due to the following error:
{code:java}
23/07/03 17:00:15 INFO Executor: Fetching spark://ip-10-110-22-170.us-west-2.compute.internal:43743/artifacts/d9548b02-ff3b-4278-ab52-aef5d1fc724e//home/venkata.gudesa/spark/artifacts/spark-d6141194-c487-40fd-ba40-444d922808ea/d9548b02-ff3b-4278-ab52-aef5d1fc724e/jars/TestHelloV2.jar with timestamp 0 23/07/03 17:00:15 ERROR Executor: Exception in task 6.0 in stage 4.0 (TID 55) java.lang.RuntimeException: Stream '/artifacts/d9548b02-ff3b-4278-ab52-aef5d1fc724e//home/venkata.gudesa/spark/artifacts/spark-d6141194-c487-40fd-ba40-444d922808ea/d9548b02-ff3b-4278-ab52-aef5d1fc724e/jars/TestHelloV2.jar' was not found.     at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:260)     at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)     at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)     at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
 {code}
 

*Root Cause: The URI for the JAR file is invalid.* (Instead of the URI being in the form of {_}/artifacts/<sessionUUID>/jars/<JAR>{_}, its instead \{_}/artifacts/<sessionUUID>/<absolutePath>{_})",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 04 23:42:09 UTC 2023,,,,,,,,,,"0|z1iy3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/23 23:42;gurwls223;Issue resolved by pull request 41844
[https://github.com/apache/spark/pull/41844];;;",,,,,,,,,,,,,,
[CONNECT][SCALA] range query returns incorrect schema,SPARK-44291,13542365,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nija,nija,nija,04/Jul/23 05:58,05/Jul/23 19:13,30/Oct/23 17:26,05/Jul/23 19:13,3.4.1,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"The following code on Spark Connect produces the following output

Code:

 
{code:java}
val df = spark.range(3)

df.show()
df.printSchema(){code}
 

Output:
{code:java}
+---+
| id|
+---+
|  0|
|  1|
|  2|
+---+

root
 |-- value: long (nullable = true) {code}
The mismatch is that one shows the column as ""id"" while the other shows this as ""value"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-07-04 05:58:14.0,,,,,,,,,,"0|z1iy2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Path Inconsistency when Operating statCache within Yarn Client,SPARK-44272,13542229,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,shuwang,shuwang,shuwang,02/Jul/23 15:17,19/Jul/23 07:24,30/Oct/23 17:26,19/Jul/23 07:21,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Spark Submit,,,,,0,,,,,"The *addResource* from *ClientDistributedCacheManager* can add *FileStatus* to 

*statCache* when it is not yet cached. Also, there is a subtle bug from *isPublic* from 

*getVisibility* method. *uri.getPath()* will not retain URI information like 

scheme, host, etc. So, the *uri* passed to checkPermissionOfOther will differ from the original {*}uri{*}.

For example, if uri is ""file:/foo.invalid.com:8080/tmp/testing"", then 
{code:java}
uri.getPath -> /foo.invalid.com:8080/tmp/testing
uri.toString -> file:/foo.invalid.com:8080/tmp/testing{code}
The consequence of this bug is that we will *double RPC calls* when the resources are remote, which is unnecessary. We see nontrivial overhead when checking those resources from our HDFS, especially when HDFS is overloaded. 

 

Ref: related code within *ClientDistributedCacheManager*
{code:java}
def addResource(...) {
    val destStatus = statCache.getOrElse(destPath.toUri(), fs.getFileStatus(destPath))
val visibility = getVisibility(conf, destPath.toUri(), statCache)
}
private[yarn] def getVisibility() {
isPublic(conf, uri, statCache)
}
private def isPublic(conf: Configuration, uri: URI, statCache: Map[URI, FileStatus]): Boolean = {
val current = new Path(uri.getPath()) // Should not use getPath
checkPermissionOfOther(fs, uri, FsAction.READ, statCache)
}
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 19 07:21:01 UTC 2023,,,,,,,,,,"0|z1ix8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/23 07:21;mridulm80;Issue resolved by pull request 41821
[https://github.com/apache/spark/pull/41821];;;",,,,,,,,,,,,,,
Potential memory leak when temp views created from DF created by structured streaming,SPARK-44253,13542000,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,attilapiros,attilapiros,29/Jun/23 20:22,29/Jun/23 20:41,30/Oct/23 17:26,,2.4.8,3.0.3,3.1.3,3.2.4,3.3.2,3.4.0,3.4.1,,,,,,,,,,,,,,,,,Structured Streaming,,,,,0,,,,,"If the user registers a temporary view from a dataframe created by Structured Streaming and tries to drop the temporary view via his original SparkSession then memory will be leaking.

The reason is Structured streaming has its own SparkSession (as a clone of the original SparkSession, for details see https://issues.apache.org/jira/browse/SPARK-26586 and [https://github.com/apache/spark/blob/branch-3.4/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala#L193-L194]) the created temporary view belongs the cloned SparkSession and the dropping of the temporary view must be done via the cloned SparkSession.

Example for the {*}memory leak{*}:
{noformat}
streamingDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>
  val view = s“tempView_$batchId” 
  batchDF.createOrReplaceTempView(view)
  ...
  spark.catalog.dropTempView(view)
}
{noformat}
*Workaround* (the _dropTempView_ must be called on SparkSession accessed from dataframe created by streaming):
{noformat}
streamingDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>
  val view = s“tempView_$batchId” 
  batchDF.createOrReplaceTempView(view)
  ...
  batchDF.sparkSession.catalog.dropTempView(view)
 }
{noformat}
h4. Example heap dump

The SparkSession with the leak:

!1.png|width=807,height=120!

The two SparkSession instances where the first one was is the original SparkSession created by the user and the second is the clone:
!2.png|width=813,height=157!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 20:22;attilapiros;1.png;https://issues.apache.org/jira/secure/attachment/13060978/1.png","29/Jun/23 20:23;attilapiros;2.png;https://issues.apache.org/jira/secure/attachment/13060979/2.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-29 20:22:46.0,,,,,,,,,,"0|z1ivts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential for incorrect results or NPE when full outer USING join has null key value,SPARK-44251,13541996,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,29/Jun/23 19:53,11/Jul/23 03:22,30/Oct/23 17:26,11/Jul/23 03:21,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,SQL,,,,,0,correctness,,,,"The following query produces incorrect results:
{noformat}
create or replace temp view v1 as values (1, 2), (null, 7) as (c1, c2);
create or replace temp view v2 as values (2, 3) as (c1, c2);

select explode(array(c1)) as x
from v1
full outer join v2
using (c1);

-1   <== should be null
1
2
{noformat}
The following query fails with a {{NullPointerException}}:
{noformat}
create or replace temp view v1 as values ('1', 2), (null, 7) as (c1, c2);
create or replace temp view v2 as values ('2', 3) as (c1, c2);

select explode(array(c1)) as x
from v1
full outer join v2
using (c1);

23/06/25 17:06:39 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID 11)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.smj_consumeFullOuterJoinRow_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.wholestagecodegen_findNextJoinRows_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
...
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 11 03:21:22 UTC 2023,,,,,,,,,,"0|z1ivsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 19:57;bersprockets;This is similar to, but not quite the same as SPARK-43718, and the fix will be similar too.

I will make a PR shortly.
 ;;;","30/Jun/23 17:17;bersprockets;PR can be found here: https://github.com/apache/spark/pull/41809;;;","11/Jul/23 03:21;yumwang;Issue resolved by pull request 41809
[https://github.com/apache/spark/pull/41809];;;",,,,,,,,,,,,
Kafka Source v2 should return preferred locations,SPARK-44248,13541987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,siying,siying,siying,29/Jun/23 18:21,29/Jun/23 21:50,30/Oct/23 17:26,29/Jun/23 21:50,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Structured Streaming,,,,,0,,,,,"DSv2 Kafka streaming source seems to miss setting the preferred location, which may destroy the purpose of cache for Kafka consumer (connection) & fetched data.

For DSv1, we have set the preferred location in RDD.

For DSv2, we should provide the info. in input partition, but we don't add the information into KafkaBatchInputPartition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jun 29 21:50:39 UTC 2023,,,,,,,,,,"0|z1ivqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 21:50;kabhwan;Issue resolved by pull request 41790
[https://github.com/apache/spark/pull/41790];;;",,,,,,,,,,,,,,
pyspark.sql.dataframe doctests can behave differently,SPARK-44245,13541919,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cdkrot,cdkrot,cdkrot,29/Jun/23 09:57,04/Jul/23 23:49,30/Oct/23 17:26,04/Jul/23 23:49,3.4.1,,,,,,,,,,,,,,,,,,,3.5.0,,,,PySpark,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 04 23:49:10 UTC 2023,,,,,,,,,,"0|z1ivbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/23 23:49;gurwls223;Issue resolved by pull request 41787
[https://github.com/apache/spark/pull/41787];;;",,,,,,,,,,,,,,
Spark job submission failed because Xmx string is available on one parameter provided into spark.driver.extraJavaOptions,SPARK-44242,13541901,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nfraison.datadog,nfraison.datadog,nfraison.datadog,29/Jun/23 08:16,12/Aug/23 00:09,30/Oct/23 17:26,12/Aug/23 00:09,3.3.2,3.4.1,,,,,,,,,,,,,,,,,,4.0.0,,,,Spark Submit,,,,,0,,,,,"The spark-submit command failed if Xmx string is found on any parameters provided to spark.driver.extraJavaOptions.

For ex. running this spark-submit command line
{code:java}
./bin/spark-submit --class org.apache.spark.examples.SparkPi --conf ""spark.driver.extraJavaOptions=-Dtest=Xmx""  examples/jars/spark-examples_2.12-3.4.1.jar 100{code}
failed due to
{code:java}
Error: Not allowed to specify max heap(Xmx) memory settings through java options (was -Dtest=Xmx). Use the corresponding --driver-memory or spark.driver.memory configuration instead.{code}
The check performed in [https://github.com/apache/spark/blob/master/launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java#L314] seems to broad",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Aug 12 00:09:27 UTC 2023,,,,,,,,,,"0|z1iv88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/23 00:09;mridulm80;Issue resolved by pull request 41806
[https://github.com/apache/spark/pull/41806];;;",,,,,,,,,,,,,,
Set io.connectionTimeout/connectionCreationTimeout to zero or negative will cause executor incessantes cons/destructions,SPARK-44241,13541900,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,29/Jun/23 08:03,23/Aug/23 00:41,30/Oct/23 17:26,30/Jun/23 10:34,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Spark Core,,,,,0,,,,,"{code:java}
2023-06-28 14:57:23 CST Bootstrap WARN - Failed to set channel option 'CONNECT_TIMEOUT_MILLIS' with value '-1000' for channel '[id: 0xf4b54a73]'
java.lang.IllegalArgumentException: connectTimeoutMillis : -1000 (expected: >= 0)
	at io.netty.util.internal.ObjectUtil.checkPositiveOrZero(ObjectUtil.java:144) ~[netty-common-4.1.74.Final.jar:4.1.74.Final] {code}",,,,,,,,,,,,,,,,,,,,,SPARK-44920,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 30 10:34:59 UTC 2023,,,,,,,,,,"0|z1iv80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 09:23;githubbot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/41785;;;","30/Jun/23 10:34;Qin Yao;Issue resolved by pull request 41785
[https://github.com/apache/spark/pull/41785];;;",,,,,,,,,,,,,
Setting the topKSortFallbackThreshold value may lead to inaccurate results,SPARK-44240,13541884,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,dzcxzl,dzcxzl,29/Jun/23 05:13,29/Jun/23 16:16,30/Oct/23 17:26,,2.4.0,3.0.0,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,," 
{code:java}
set spark.sql.execution.topKSortFallbackThreshold=10000;
SELECT min(id) FROM ( SELECT id FROM range(999999999) ORDER BY id LIMIT 10000) a; {code}
 

If GlobalLimitExec is not the final operator and has a sort operator, shuffle read does not guarantee the order, which leads to the limit read data that may be random.

TakeOrderedAndProjectExec has ordering, so there is no such problem.

 

!topKSortFallbackThreshold.png!
{code:java}
set spark.sql.execution.topKSortFallbackThreshold=10000;
select min(id) from (select  id  from range(999999999) order by id desc limit 10000) a; {code}
!topKSortFallbackThresholdDesc.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 05:14;dzcxzl;topKSortFallbackThreshold.png;https://issues.apache.org/jira/secure/attachment/13060953/topKSortFallbackThreshold.png","29/Jun/23 16:14;dzcxzl;topKSortFallbackThresholdDesc.png;https://issues.apache.org/jira/secure/attachment/13060975/topKSortFallbackThresholdDesc.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-29 05:13:46.0,,,,,,,,,,"0|z1iv4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Even `spark.sql.codegen.factoryMode` is NO_CODEGEN, the WholeStageCodegen also will be generated.",SPARK-44236,13541871,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,29/Jun/23 02:28,08/Aug/23 09:46,30/Oct/23 17:26,08/Aug/23 09:46,3.4.1,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"The `spark.sql.codegen.factoryMode` is NO_CODEGEN, but Spark always generate WholeStageCodegen plan when set `spark.sql.codegen.wholeStage` to `true`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 08 09:46:39 UTC 2023,,,,,,,,,,"0|z1iv1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 09:46;cloud_fan;Issue resolved by pull request 41779
[https://github.com/apache/spark/pull/41779];;;",,,,,,,,,,,,,,
dropDuplicates prevents correct metadata caching,SPARK-44226,13541834,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,darren.cheung2,darren.cheung2,28/Jun/23 17:39,28/Jun/23 18:02,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,PySpark,Spark Core,SQL,,,0,,,,,"{quote}given a df, applying dropDuplicates (dD) AND after writing+reading it to s3 (cache), the metadata is deleted

applying just dD or just cache preserves metadata

however, the issue is fixed by recreating the df after dD

{{spark.createDataFrame(dedupedPositiveUsersDf.rdd, dedupedPositiveUsersDf.schema)}}

this leads to the conclusion that there is some issue with dD (affects the df in some unknown way that prevents metadata caching)

 

Example below:


{{{}val dfWithVector = spark.createDataFrame({}}}{{{}spark.sparkContext.parallelize(denseData),{}}}{{{}StructType(schema){}}}{{{}){}}}
{quote}
{quote}{{println(""metadata before:"" + dfWithVector.schema(""features"").metadata)}}
{quote}
{quote}{{var dfDroppedDuplicates = dfWithVector.dropDuplicates(""id"")}}
{quote}
{quote}{{println(""metadata after duplicates dropped:"" + droppedDuplicatesWithOverwrittenSchema.schema(""features"").metadata)}}
{quote}
{quote}{{dfDroppedDuplicates.write.mode(""overwrite"").parquet(outputLocation)}}
{quote}
{quote}{{val dfRead = spark.read.parquet(outputLocation)}}
{quote}
{quote}{{{}println(""metadata after caching: "" + dfRead.schema(""features"").metadata){}}}{{{{}}{}}}
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-28 17:39:03.0,,,,,,,,,,"0|z1iutc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table drop with Purge statement is not deleting the _temporary folder,SPARK-44224,13541797,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dasarianil,dasarianil,28/Jun/23 13:30,28/Jun/23 13:30,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"When saveAsTable is failed for some reason and `_temporary` folder created is not deleted with Drop table with purge statement. 

On high level , out data process look like below. 
 # Read data from external store using load 
 # Drop the table with purge just in case table exist already
 # Write Dataframe to hive using dataframe.write.saveMode(Overwrite).saveAsTable(""my_table"")

When Step 3 is failed for some reason and job is restarted because of yarm maxAttempts, Step is not deleting the _temporary folder created by Step 3 that is causing job to fail with below exception

org.apache.spark.sql.AnalysisException: Can not create the managed table(""my_table""). The associated location('<hdfs path>') already exists.

This is never been a case in Spark2 because of ""spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation"" config which is removed in Spark 3.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-28 13:30:58.0,,,,,,,,,,"0|z1iul4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client receives zero number of chunks in merge meta response which doesn't trigger fallback to unmerged blocks,SPARK-44215,13541609,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csingh,csingh,csingh,27/Jun/23 16:55,06/Jul/23 01:52,30/Oct/23 17:26,04/Jul/23 23:48,3.2.0,,,,,,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Shuffle,,,,,0,,,,,"We still see instances of the server returning 0 {{numChunks}} in {{mergedMetaResponse}} which causes the executor to fail with {{ArithmeticException}}. 
{code}
java.lang.ArithmeticException: / by zero
	at org.apache.spark.storage.PushBasedFetchHelper.createChunkBlockInfosFromMetaResponse(PushBasedFetchHelper.scala:128)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1047)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:90)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
{code}
Here the executor doesn't fallback to fetch un-merged blocks and this also doesn't result in a {{FetchFailure}}. So, the application fails.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 06 01:52:31 UTC 2023,,,,,,,,,,"0|z1itfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/23 05:00;csingh;PR to backport the change to 3.3
https://github.com/apache/spark/pull/41859;;;","06/Jul/23 01:52;mridulm80;Issue resolved by pull request 41762
https://github.com/apache/spark/pull/41762;;;",,,,,,,,,,,,,
CTAS missing the child info on UI when groupSQLSubExecutionEnabled is enabled,SPARK-44213,13541605,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,yumwang,yumwang,27/Jun/23 16:36,27/Jun/23 16:39,30/Oct/23 17:26,,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"{code:sql}
create table tbl using parquet as select t1.id from range(10) as t1 join range(100) as t2 on t1.id = t2.id;
{code}
Enabled:
 !enabled.png! 
Disabled:
 !screenshot-1.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/23 16:36;yumwang;enabled.png;https://issues.apache.org/jira/secure/attachment/13060897/enabled.png","27/Jun/23 16:36;yumwang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13060898/screenshot-1.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 27 16:39:25 UTC 2023,,,,,,,,,,"0|z1iteg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/23 16:37;yumwang;cc [~linhongliu-db];;;","27/Jun/23 16:39;yumwang;Related issue ticket: SPARK-41752.;;;",,,,,,,,,,,,,
Upgrade netty dependencies to 4.1.94.Final due to CVE-2023-34462,SPARK-44212,13541597,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,raulcd,raulcd,27/Jun/23 15:47,14/Jul/23 10:32,30/Oct/23 17:26,,1.4.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Hi,

On the Apache Arrow project we have noticed that our nightly integration tests with spark started failing lately. With some investigation I've noticed that we are defining a different version of the Java netty dependencies. We upgraded to 4.1.94.Final due to the CVE on the title: [https://github.com/advisories/GHSA-6mjq-h674-j845]

Our PR upgrading the version: [https://github.com/apache/arrow/issues/36209]

I have opened  an issue on the Apache Arrow repository to try and fix something else on our side but I was wondering if you would want to update the version to solve the CVE.

 

Thanks

Raúl",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jul 14 10:32:44 UTC 2023,,,,,,,,,,"0|z1itco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/23 16:23;kiszk;[https://github.com/apache/spark/pull/41681#pullrequestreview-1496876723|http://example.com] is discussing the upgrade of netty.;;;","14/Jul/23 10:32;raulcd;An email was sent to the [dev@spark.apache.org|mailto:dev@spark.apache.org] ML referencing this issue: [https://lists.apache.org/thread/ndmj3ht85j2g40n8clfh92ny6qqbvd09]
There is a long discussion also on the Apache Arrow GitHub here: [https://github.com/apache/arrow/issues/36332]

 ;;;",,,,,,,,,,,,,
Where Clause throwing Resolved attribute(s) _metadata#398 missing from ... error,SPARK-44207,13541541,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,davidxuhz,davidxuhz,27/Jun/23 10:23,29/Jun/23 04:38,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"i have 2 data frames called lt and rt, both with same schema and only 1 row, generated separately by our own curation logic, all the columns are either String, boolean or Timestamp, i am trying to compare them, and i am running a join on two like this 

var joinedDF = lt.join(rt, ""Id"")

after that, i am trying to compare them by schema fist and then by  each column, how many % of rows are same,

code is kindof like this

for (column <- lt.schema) {
     if (rt.columns.contains(column.name) &&
     column.dataType == rt.schema(column.name).dataType) {

      var matchCount = joinedCount
      if (column.dataType.typeName == ""string"") {
             matchCount = joinedDF.where((lt(column.name) <=> rt(column.name))).count}

else

.....

 

on the last line where i am running a where clause, it is throwing an error called AnalysisException Resolved attribute(s) _metadata#398 missing from ...., i don't even have this _metadata column anywhere in my dataframe at all

and i searched online people are saying it is a problem of join, i tried to change the colunm names in rt and joinedDF, both doesn't work, same error is still thrown, can anybody help here",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,Scala,,,,2023-06-27 10:23:48.0,,,,,,,,,,"0|z1it0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing recordHiveCall for getPartitionNames,SPARK-44204,13541495,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chengpan,chengpan,chengpan,27/Jun/23 03:25,27/Jun/23 08:42,30/Oct/23 17:26,27/Jun/23 08:42,3.3.2,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 27 08:42:52 UTC 2023,,,,,,,,,,"0|z1isqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/23 08:42;LuciferYang;Issue resolved by pull request 41756
[https://github.com/apache/spark/pull/41756];;;",,,,,,,,,,,,,,
CacheManager refreshes the fileIndex unnecessarily,SPARK-44199,13541463,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vihangk1,vihangk1,vihangk1,26/Jun/23 18:30,03/Jul/23 23:08,30/Oct/23 17:26,03/Jul/23 23:08,3.4.1,,,,,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,,,0,,,,,"The CacheManager on this line [https://github.com/apache/spark/blob/680ca2e56f2c8fc759743ad6755f6e3b1a19c629/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala#L372] uses a prefix based matching to decide which file index needs to be refreshed. However, that can be incorrect if the users have paths which are not subdirectories but share prefixes. For example, in the function below:

 
{code:java}
  private def refreshFileIndexIfNecessary(
      fileIndex: FileIndex,
      fs: FileSystem,
      qualifiedPath: Path): Boolean = {
    val prefixToInvalidate = qualifiedPath.toString
    val needToRefresh = fileIndex.rootPaths
      .map(_.makeQualified(fs.getUri, fs.getWorkingDirectory).toString)
      .exists(_.startsWith(prefixToInvalidate))
    if (needToRefresh) fileIndex.refresh()
    needToRefresh
  } {code}
{{If the prefixToInvalidate is s3://bucket/mypath/table_dir and the file index has one of the root paths as s3://bucket/mypath/table_dir_2/part=1, then the needToRefresh will be true and the file index gets refreshed unnecessarily. This is not just wasted CPU cycles but can cause query failures as well, if there are access restrictions to the path being refreshed.}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jun 29 16:52:55 UTC 2023,,,,,,,,,,"0|z1isjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 16:52;ignitetcbot;User 'vihangk1' has created a pull request for this issue:
https://github.com/apache/spark/pull/41749;;;",,,,,,,,,,,,,,
SparkR test failed with R 4.3.0,SPARK-44191,13541402,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,LuciferYang,LuciferYang,26/Jun/23 12:55,27/Jun/23 02:28,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SparkR,,,,,0,,,,,"run

 
{code:java}
build/sbt package -Psparkr
R/install-dev.sh; R/run-tests.sh {code}
 

 

 
{code:java}
══ Failed ══════════════════════════════════════════════════════════════════════
── 1. Error ('test_streaming.R:190:3'): PartitionBy ────────────────────────────
Error: Error in awaitTermination : streaming query error - Wrong basePath /var/folders/2w/_w3fqbdn7pl8n4z2q6p6zcj80000gn/T/RtmpDbPxOv/sparkr-testcf965edf703d.parquet for the root path: file:/var/folders/2w/_w3fqbdn7pl8n4z2q6p6zcj80000gn/T/RtmpBFGJln/sparkr-testd75a4d1118fd.parquet/part-00000-bca8678d-70e2-42ef-abaf-5d5b56d46d62-c000.snappy.parquet
=== Streaming Query ===
Identifier: [id = 5a6a5714-ea05-473d-bdef-d30db92c8ef6, runId = bcfc36cd-bd4c-4b4a-9693-8ecafa70129a]
Current Committed Offsets: {}
Current Available Offsets: {FileStreamSource[file:/var/folders/2w/_w3fqbdn7pl8n4z2q6p6zcj80000gn/T/RtmpDbPxOv/sparkr-testcf965edf703d.parquet]: {""logOffset"":0}}


Current State: ACTIVE
Thread State: RUNNABLE


Logical Plan:
WriteToMicroBatchDataSourceV1 FileSink[/var/folders/2w/_w3fqbdn7pl8n4z2q6p6zcj80000gn/T//RtmpDbPxOv/sparkr-testcf961f3f32b4.text], 5a6a5714-ea05-473d-bdef-d30db92c8ef6, [path=/var/folders/2w/_w3fqbdn7pl8n4z2q6p6zcj80000gn/T//RtmpDbPxOv/sparkr-testcf961f3f32b4.text, checkpointLocation=append], Append
+- StreamingExecutionRelation FileStreamSource[file:/var/folders/2w/_w3fqbdn7pl8n4z2q6p6zcj80000gn/T/RtmpDbPxOv/sparkr-testcf965edf703d.parquet], [name#53978, age#53979, count#53980]


Backtrace:
    ▆
 1. ├─SparkR::awaitTermination(q, 5 * 1000) at test_streaming.R:190:2
 2. └─SparkR::awaitTermination(q, 5 * 1000)
 3.   └─SparkR:::handledCallJMethod(x@ssq, ""awaitTermination"", as.integer(timeout))
 4.     └─base::tryCatch(...)
 5.       └─base (local) tryCatchList(expr, classes, parentenv, handlers)
 6.         └─base (local) tryCatchOne(expr, names, parentenv, handlers[[1L]])
 7.           └─value[[3L]](cond)
 8.             └─SparkR:::captureJVMException(e, method)


══ DONE ════════════════════════════════════════════════════════════════════════
Error: Test failures{code}
 

ENV:

 
{code:java}
R --version
R version 4.3.0 (2023-04-21) -- ""Already Tomorrow""
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: aarch64-apple-darwin20 (64-bit)


R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under the terms of the
GNU General Public License versions 2 or 3.
For more information about these matters see
https://www.gnu.org/licenses/.


java -version
openjdk version ""1.8.0_372""
OpenJDK Runtime Environment (Zulu 8.70.0.23-CA-macos-aarch64) (build 1.8.0_372-b07)
OpenJDK 64-Bit Server VM (Zulu 8.70.0.23-CA-macos-aarch64) (build 25.372-b07, mixed mode) {code}
 

 

Run on M2 Max/MacOs 13.4.1

 ",,,,,,,,,,,,,,,,,,,,,,,,,SPARK-43447,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 27 02:28:11 UTC 2023,,,,,,,,,,"0|z1is5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/23 17:42;dongjoon;This is tested via SPARK-43447 on May 11. Maybe, something is change, [~LuciferYang] ?;;;","26/Jun/23 17:46;dongjoon;BTW, please note that Github Action Spark R has been running with R 4.3.0 successfully.
 * [https://github.com/apache/spark/actions/runs/5374330588/jobs/9749600282]

{code:java}
* using R version 4.3.0 (2023-04-21)
* using platform: x86_64-pc-linux-gnu (64-bit)
* R was compiled by
    gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
    GNU Fortran (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
* running under: Ubuntu 20.04.5 LTS {code};;;","27/Jun/23 02:28;LuciferYang;[~dongjoon] Thanks for helping to check this. Maybe there is some problem with my local environment.

 ;;;",,,,,,,,,,,,
Inconsistent path qualifying between catalog and data operations,SPARK-44185,13541343,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,26/Jun/23 03:49,03/Jul/23 09:27,30/Oct/23 17:26,03/Jul/23 09:27,3.2.4,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"For example
 * CREATE TABLE statement with relative LOCATION will infer schema from files from the directory relative to the current working directory and store the directory relative to the warehouse path. 
 * CTAS statement with relative LOCATION cannot assert empty root path as it checks the wrong path it will finally use.
 * DataframeWriter does not qualify the path before checking",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jul 03 09:27:20 UTC 2023,,,,,,,,,,"0|z1irso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/23 09:27;yumwang;Issue resolved by pull request https://github.com/apache/spark/pull/41733;;;",,,,,,,,,,,,,,
Remove a wrong doc about ARROW_PRE_0_15_IPC_FORMAT,SPARK-44184,13541334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,25/Jun/23 22:21,26/Jun/23 01:53,30/Oct/23 17:26,26/Jun/23 01:53,3.0.3,3.1.3,3.2.4,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Documentation,PySpark,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jun 26 01:53:57 UTC 2023,,,,,,,,,,"0|z1irqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/23 01:53;dongjoon;Issue resolved by pull request 41730
[https://github.com/apache/spark/pull/41730];;;",,,,,,,,,,,,,,
DistributionAndOrderingUtils should apply ResolveTimeZone,SPARK-44180,13541305,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,25/Jun/23 12:31,14/Jul/23 01:26,30/Oct/23 17:26,14/Jul/23 01:26,3.4.1,,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jul 14 01:26:11 UTC 2023,,,,,,,,,,"0|z1irk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/23 03:39;snoot;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/41725;;;","14/Jul/23 01:26;cloud_fan;Issue resolved by pull request 41725
[https://github.com/apache/spark/pull/41725];;;",,,,,,,,,,,,,
Enable dynamicPartitionOverwrite in SaveAsHiveFile for insert overwrite,SPARK-44166,13541243,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,pralabhkumar,pralabhkumar,24/Jun/23 11:53,24/Jun/23 12:08,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Currently in InsertIntoHiveTable.scala , there is no way to pass dynamicPartitionOverwrite to true , when calling  saveAsHiveFile . When dynamicPartitioOverwrite is true , spark will use  built-in FileCommitProtocol instead of Hadoop FileOutputCommitter , which is more performant. 

 

Here is the solution . 

When inserting overwrite into Hive table

 

Current code 

 
{code:java}
val writtenParts = saveAsHiveFile(
  sparkSession = sparkSession,
  plan = child,
  hadoopConf = hadoopConf,
  fileFormat = fileFormat,
  outputLocation = tmpLocation.toString,
  partitionAttributes = partitionColumns,
  bucketSpec = bucketSpec,
  options = options)
       {code}
 

 

Proposed code.  

enableDynamicPartitionOverwrite 
{code:java}
val USE_FILECOMMITPROTOCOL_DYNAMIC_PARTITION_OVERWRITE =
    buildConf(""spark.sql.hive.filecommit.dynamicPartitionOverwrite""){code}
 
{code:java}
 val enableDynamicPartitionOverwrite =
      SQLConf.get.getConf(HiveUtils.USE_FILECOMMITPROTOCOL_DYNAMIC_PARTITION_OVERWRITE)
    logWarning(s""enableDynamicPartitionOverwrite: $enableDynamicPartitionOverwrite""){code}
 

 

Now if enableDynamicPartitionOverwrite is true and numDynamicPartitions > 0 and overwrite is true , pass dynamicPartitionOverwrite true. 

 
{code:java}
val writtenParts = saveAsHiveFile( sparkSession = sparkSession, plan = child, hadoopConf = hadoopConf, fileFormat = fileFormat, outputLocation = tmpLocation.toString, partitionAttributes = partitionColumns, bucketSpec = bucketSpec, options = options, dynamicPartitionOverwrite =
        enableDynamicPartitionOverwrite && numDynamicPartitions > 0 && overwrite)       {code}
 

 

In saveAs File 
{code:java}
val committer = FileCommitProtocol.instantiate(
      sparkSession.sessionState.conf.fileCommitProtocolClass,
      jobId = java.util.UUID.randomUUID().toString,
      outputPath = outputLocation,
      dynamicPartitionOverwrite = dynamicPartitionOverwrite) {code}
This will internal call  with dynamicPartitionOverwrite value true. 

 
{code:java}
class SQLHadoopMapReduceCommitProtocol(
    jobId: String,
    path: String,
    dynamicPartitionOverwrite: Boolean = false)
  extends HadoopMapReduceCommitProtocol(jobId, path, dynamicPartitionOverwrite) {code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-24 11:53:16.0,,,,,,,,,,"0|z1ir6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row as UDF inputs causes encoder errors,SPARK-44161,13541218,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhenli,zhenli,zhenli,23/Jun/23 21:20,28/Jun/23 01:07,30/Oct/23 17:26,28/Jun/23 01:07,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,Ensure row inputs to udfs can be handled correctly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-23 21:20:29.0,,,,,,,,,,"0|z1ir14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unused `spark.kubernetes.executor.lostCheck.maxAttempts`,SPARK-44158,13541205,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,23/Jun/23 18:08,23/Jun/23 20:49,30/Oct/23 17:26,23/Jun/23 20:49,2.4.8,3.0.3,3.1.3,3.2.4,3.3.3,3.4.1,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Kubernetes,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24248,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 23 20:49:57 UTC 2023,,,,,,,,,,"0|z1iqy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/23 20:49;dongjoon;Issue resolved by pull request 41713
[https://github.com/apache/spark/pull/41713];;;",,,,,,,,,,,,,,
Outdated JARs in PySpark package,SPARK-44157,13541187,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,adriangonz,adriangonz,23/Jun/23 15:03,23/Jun/23 15:03,30/Oct/23 17:26,,3.4.1,,,,,,,,,,,,,,,,,,,,,,,Build,PySpark,,,,0,pyspark,,,,"The JARs which ship embedded within PySpark's package in PyPi don't seem aligned with the deps specified in Spark's own `pom.xml`.

For example, in Spark's `pom.xml`, `protobuf-java` is set to `3.21.12`:

[https://github.com/apache/spark/blob/6b1ff22dde1ead51cbf370be6e48a802daae58b6/pom.xml#L127]

However, if we look at the JARs embedded within PySpark tarball, the version of `protobuf-java` is `2.5.0` (i.e. `..../site-packages/pyspark/jars/protobuf-java-2.5.0.jar`). Same seems to apply to all other dependencies.

This introduces a set of CVEs which are fixed on upstream Spark, but are still present in PySpark (e.g. `CVE-2022-3509`, `CVE-2021-22569`, ` CVE-2015-5237` and a few others). As well as potentially introduce a source of conflict whenever there's a breaking change on these deps.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-23 15:03:32.0,,,,,,,,,,"0|z1iqu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortAggregation slows down dropDuplicates(),SPARK-44156,13541186,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,emanuelvelzi,emanuelvelzi,23/Jun/23 15:00,23/Jun/23 15:09,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"TL;DR: SortAggregate makes dropDuplicates slower than HashAggregate.

How to make Spark to use HashAggregate over SortAggregate? 

----------------------

We have a Spark cluster running on Kubernetes with the following configurations:
 * Spark v3.3.2
 * Hadoop 3.3.4
 * Java 17

We are running a simple job on a dataset (~6GBi) with almost 600 columns, many of which contain null values. The job involves the following steps:
 # Load data from S3.
 # Apply dropDuplicates().
 # Save the deduplicated data back to S3 using magic committers.

One of the columns is of type ""map"". When we run dropDuplicates() without specifying any parameters (i.e., using all columns), it throws an error:

 
{noformat}
Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Cannot have map type columns in DataFrame which calls set operations(intersect, except, etc.), but the type of column my_column is map<string,array<struct<ci:string,co:string,cur:string,Getaways_Zone:string,Getaways_ID:string>>>;{noformat}
 

To overcome this issue, we used ""dropDuplicates(id)"" by specifying an identifier column.

However, the performance of this method was {*}much worse than expected{*}, taking around 30 minutes.

As an alternative approach, we tested converting the ""map"" column to JSON, applying dropDuplicates() without parameters, and then converting the column back to ""map"" format:

 
{code:java}
DataType t = ds.schema().apply(""my_column"").dataType();
ds = ds.withColumn(""my_column"", functions.to_json(ds.col(""my_column"")));
ds = ds.dropDuplicates();
ds = ds.withColumn(""my_column"", functions.from_json(ds.col(""my_column""),t)); {code}
 

Surprisingly, this approach {*}significantly improved the performance{*}, reducing the execution time to 7 minutes.

The only noticeable difference was in the execution plan. In the *slower* case, the execution plan involved {*}SortAggregate{*}, while in the *faster* case, it involved {*}HashAggregate{*}.

 
{noformat}
== Physical Plan [slow case] == 
Execute InsertIntoHadoopFsRelationCommand (13)
+- AdaptiveSparkPlan (12)
   +- == Final Plan ==
      Coalesce (8)
      +- SortAggregate (7)
         +- Sort (6)
            +- ShuffleQueryStage (5), Statistics(sizeInBytes=141.3 GiB, rowCount=1.25E+7)
               +- Exchange (4)
                  +- SortAggregate (3)
                     +- Sort (2)
                        +- Scan parquet  (1)
   +- == Initial Plan ==
      Coalesce (11)
      +- SortAggregate (10)
         +- Sort (9)
            +- Exchange (4)
               +- SortAggregate (3)
                  +- Sort (2)
                     +- Scan parquet  (1)
{noformat}
{noformat}
== Physical Plan [fast case] ==
Execute InsertIntoHadoopFsRelationCommand (11)
+- AdaptiveSparkPlan (10)
   +- == Final Plan ==
      Coalesce (7)
      +- HashAggregate (6)
         +- ShuffleQueryStage (5), Statistics(sizeInBytes=81.6 GiB, rowCount=1.25E+7)
            +- Exchange (4)
               +- HashAggregate (3)
                  +- Project (2)
                     +- Scan parquet  (1)
   +- == Initial Plan ==
      Coalesce (9)
      +- HashAggregate (8)
         +- Exchange (4)
            +- HashAggregate (3)
               +- Project (2)
                  +- Scan parquet  (1)
{noformat}
 

Based on this observation, we concluded that the difference in performance is related to {*}SortAggregate vs. HashAggregate{*}.

Is this line of thinking correct? How we can to enforce the use of HashAggregate instead of SortAggregate {*}even using one colum to deduplicate{*}?

*The final result is somewhat counterintuitive* because deduplicating using only one column should theoretically be faster, as it provides a simpler way to compare rows and determine duplicates.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-23 15:00:28.0,,,,,,,,,,"0|z1iqu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Adding a dev utility to improve error messages based on LLM,SPARK-44155,13541141,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,itholic,itholic,23/Jun/23 08:52,23/Jun/23 09:16,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Adding a utility function to assist with error message improvement tasks.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 23 09:16:51 UTC 2023,,,,,,,,,,"0|z1iqk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/23 09:16;githubbot;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/41711;;;",,,,,,,,,,,,,,
"Upgrade to spark 3.4.0 from 3.3.2 gives Exception in thread ""main"" java.nio.file.NoSuchFileException: , although jar is present in the location",SPARK-44152,13541104,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,gurwls223,gurwls223,23/Jun/23 02:30,24/Jul/23 16:06,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,," 
I have a spark application that is deployed using k8s and it is of version 3.3.2 Recently there were some vulneabilities in spark 3.3.2

I changed my dockerfile to download 3.4.0 instead of 3.3.2 and also my application jar is built on spark 3.4.0

However while deploying, I get this error

        

*{{Exception in thread ""main"" java.nio.file.NoSuchFileException: <path>/spark-assembly-1.0.jar}}*

 

I have this in deployment.yaml of the app

 

*mainApplicationFile: ""local:///<path>/spark-assembly-1.0.jar""*

 

 

 

 

and I have not changed anything related to that. I see that some code has changed in spark 3.4.0 core's source code regarding jar location.

Has it really changed the functionality ? Is there anyone who is facing same issue as me ? Should the path be specified in a different way?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jul 24 16:05:57 UTC 2023,,,,,,,,,,"0|z1iqbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/23 06:41;gurwls223;This is from https://issues.apache.org/jira/browse/SPARK-44135. I made a mistake in JIRA number so manually switched both JIRAs.;;;","23/Jun/23 08:39;Hande;[~gurwls223] 

Is this an issue spark 3.4.0 ? at least I am facing this issue, with all other constraints remaining .;;;","03/Jul/23 13:52;iainm;I hit this same issue. I believe the the change in behaviour was introduced in:
{quote}SPARK-43540 - Add working directory into classpath on the driver in K8S cluster mode
{quote}
Our docker file was overriding the working directory of the base spark image
{code:java}
FROM apache/spark:3.4.1@sha256:a1dd2487a97fb5e35c5a5b409e830b501a92919029c62f9a559b13c4f5c50f63 as image
WORKDIR /spark-jars
COPY --from=build /...../target/scala-2.12/my-spark.jar /spark-jars/ {code}
Changing it to this solved the problem:
{code:java}
FROM apache/spark:3.4.1@sha256:a1dd2487a97fb5e35c5a5b409e830b501a92919029c62f9a559b13c4f5c50f63 as image
USER root
RUN mkdir /spark-jars
USER spark
COPY --from=build /...../target/scala-2.12/my-spark.jar /spark-jars/ {code}
So probably not an issue.. Is there any documentation/guidance to follow??

 ;;;","03/Jul/23 13:56;Hande; Hi [~iainm]  Yes, I spent probably bit longer to understand what is happening, because it did not sound like a permission issue looking at the error. Because it just says jar not found. Probably adding more detailed documentation helps especially migrating from 3.3.2 to 3.4.0;;;","24/Jul/23 12:44;sdehaes;We are seeing this issue too, the problem seems to be this PR: [https://github.com/apache/spark/pull/37417]

When building an image we copy the jar's into the workdir location, however now when the job is running spark will remove everything in that workdir location.
Resulting in this error. I am not sure on how to continue, what would be the best location to copy the assembly jar?;;;","24/Jul/23 16:05;Hande;Hello [~sdehaes] 

It should work if you copy jar to 

 

/usr/local/bin folder of your docker container

 

. It worked for us;;;",,,,,,,,,
Validate NO_BROADCAST_AND_REPLICATION hint is never set on both sides,SPARK-44148,13541082,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,aokolnychyi,aokolnychyi,22/Jun/23 20:02,22/Jun/23 20:02,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"See [here|https://github.com/apache/spark/pull/41499#discussion_r1227029401].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-22 20:02:21.0,,,,,,,,,,"0|z1iq6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Callback prior to execution,SPARK-44145,13541068,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jgauthier,jgauthier,jgauthier,22/Jun/23 17:10,13/Jul/23 10:21,30/Oct/23 17:26,13/Jul/23 10:21,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"Commands are eagerly executed after analysis phase, while other queries are executed after planning planning. Users of Spark need to understand time spent prior to execution. Currently, they need to understand the difference between these 2 modes. Add a callback after query planning is completed that can be used for such use.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 13 10:21:58 UTC 2023,,,,,,,,,,"0|z1iq3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/23 16:52;ci-cassandra.apache.org;User 'jdesjean' has created a pull request for this issue:
https://github.com/apache/spark/pull/41748;;;","13/Jul/23 10:21;gurwls223;Issue resolved by pull request 41748
[https://github.com/apache/spark/pull/41748];;;",,,,,,,,,,,,,
"Utility to convert python types to spark types compares Python ""type"" object rather than user's ""tpe"" for categorical data types",SPARK-44142,13541026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tedjenks,tedjenks,tedjenks,22/Jun/23 12:05,23/Jun/23 00:11,30/Oct/23 17:26,23/Jun/23 00:09,3.4.0,,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,PySpark,,,,,0,,,,,"In the typehints utility that converts python types to spark types, the line:
{code:java}
    # categorical types
    elif isinstance(tpe, CategoricalDtype) or (isinstance(tpe, str) and type == ""category""):
        return types.LongType() {code}
uses Python's 'type' keyword in the comparison. Hence, it will always be false. Here, the user's type is actually stored in the variable 'tpe'.

 

 

See line [here|https://github.com/apache/spark/blob/1b4048bf62dddae7d324c4b12aa409a1bd456dc5/python/pyspark/pandas/typedef/typehints.py#LL217C7-L217C7].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 23 00:09:52 UTC 2023,,,,,,,,,,"0|z1ipuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/23 12:09;tedjenks;https://github.com/apache/spark/pull/41697;;;","23/Jun/23 00:09;gurwls223;Issue resolved by pull request 41697
[https://github.com/apache/spark/pull/41697];;;",,,,,,,,,,,,,
StateManager may get materialized in executor instead of driver in FlatMapGroupsWithStateExec,SPARK-44136,13540939,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dlgaobo,dlgaobo,dlgaobo,21/Jun/23 21:08,22/Jun/23 00:26,30/Oct/23 17:26,22/Jun/23 00:26,3.3.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Structured Streaming,,,,,0,,,,,StateManager may get materialized in executor instead of driver in FlatMapGroupsWithStateExec because of a previous change https://issues.apache.org/jira/browse/SPARK-40411,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jun 22 00:26:59 UTC 2023,,,,,,,,,,"0|z1ipbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/23 23:07;kabhwan;PR: https://github.com/apache/spark/pull/41693;;;","22/Jun/23 00:26;gurwls223;Issue resolved by pull request 41693
[https://github.com/apache/spark/pull/41693];;;",,,,,,,,,,,,,
Can't set resources (GPU/FPGA) to 0 when they are set to positive value in spark-defaults.conf,SPARK-44134,13540908,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,21/Jun/23 14:14,25/Jun/23 17:43,30/Oct/23 17:26,23/Jun/23 00:41,3.2.0,,,,,,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Spark Core,,,,,0,,,,,"With resource aware scheduling, if you specify a default value in the spark-defaults.conf, a user can't override that to set it to 0.

Meaning spark-defaults.conf has something like:
{{spark.executor.resource.\{resourceName}.amount=1}}

{{spark.task.resource.\{resourceName}.amount}} =1

If the user tries to override when submitting an application with {{{}spark.executor.resource.\{resourceName}.amount{}}}=0 and {{spark.task.resource.\{resourceName}.amount}} =0, it gives the user an error:

 
{code:java}
23/06/21 09:12:57 ERROR Main: Failed to initialize Spark session.
org.apache.spark.SparkException: No executor resource configs were not specified for the following task configs: gpu
        at org.apache.spark.resource.ResourceProfile.calculateTasksAndLimitingResource(ResourceProfile.scala:206)
        at org.apache.spark.resource.ResourceProfile.$anonfun$limitingResource$1(ResourceProfile.scala:139)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.resource.ResourceProfile.limitingResource(ResourceProfile.scala:138)
        at org.apache.spark.resource.ResourceProfileManager.addResourceProfile(ResourceProfileManager.scala:95)
        at org.apache.spark.resource.ResourceProfileManager.<init>(ResourceProfileManager.scala:49)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:455)
        at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2704)
        at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953){code}
This used to work, my guess is this may have gotten broken with the stage level scheduling feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jun 25 17:43:25 UTC 2023,,,,,,,,,,"0|z1ip4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/23 14:14;tgraves;I'm working on a fix for this;;;","23/Jun/23 00:41;dongjoon;Issue resolved by pull request 41703
[https://github.com/apache/spark/pull/41703];;;","25/Jun/23 17:43;dongjoon;Oh, thank you for updating the fix versions, [~tgraves].;;;",,,,,,,,,,,,
nesting full outer joins confuses code generator,SPARK-44132,13540853,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,steven.aerts,steven.aerts,steven.aerts,21/Jun/23 07:29,07/Aug/23 23:09,30/Oct/23 17:26,07/Aug/23 23:09,3.3.0,3.4.0,3.5.0,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,SQL,,,,,1,,,,,"We are seeing issues with the code generator when querying java bean encoded data with 2 nested joins.
{code:java}
dsA.join(dsB, seq(""id""), ""full_outer"").join(dsC, seq(""id""), ""full_outer""); {code}
will generate invalid code in the code generator.  And can depending on the data used generate stack traces like:
{code:java}
 Caused by: java.lang.NullPointerException
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.smj_consumeFullOuterJoinRow_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.wholestagecodegen_findNextJoinRows_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
{code}
Or:
{code:java}
 Caused by: java.lang.AssertionError: index (2) should < 2
        at org.apache.spark.sql.catalyst.expressions.UnsafeRow.assertIndexIsValid(UnsafeRow.java:118)
        at org.apache.spark.sql.catalyst.expressions.UnsafeRow.isNullAt(UnsafeRow.java:315)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.smj_consumeFullOuterJoinRow_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
{code}
When we look at the generated code we see that the code generator seems to be mixing up parameters.  For example:
{code:java}
if (smj_leftOutputRow_0 != null) {                          //<==== null check for wrong/left parameter
  boolean smj_isNull_12 = smj_rightOutputRow_0.isNullAt(1); //<==== causes NPE on right parameter here{code}
It is as if the the nesting of 2 full outer joins is confusing the code generator and as such generating invalid code.

There is one other strange thing.  We found this issue when using data sets which were using the java bean encoder.  We tried to reproduce this in the spark shell or using scala case classes but were unable to do so. 

We made a reproduction scenario as unit tests (one for each of the stacktrace above) on the spark code base and made it available as a [pull request|https://github.com/apache/spark/pull/41688] to this case.",We verified the existence of this bug from spark 3.3 until spark 3.5.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 07 23:09:34 UTC 2023,,,,,,,,,,"0|z1ios8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/23 01:40;bersprockets;You may have this figured out already, but in case not, here's a clue.

You can replicate the NPE in {{spark-shell}} as follows:
{noformat}
val dsA = Seq((1, 1)).toDF(""id"", ""a"")
val dsB = Seq((2, 2)).toDF(""id"", ""a"")
val dsC = Seq((3, 3)).toDF(""id"", ""a"")

val joined = dsA.join(dsB, Stream(""id""), ""full_outer"").join(dsC, Stream(""id""), ""full_outer"");
joined.collectAsList
{noformat}

I think its because the join column sequence {{idSeq}} (in your unit test) is provided as a {{Stream}}. {{toSeq}} in {{JavaConverters}} returns a Stream:
{noformat}
scala> scala.collection.JavaConverters.collectionAsScalaIterableConverter(
            Collections.singletonList(""id"")
    ).asScala.toSeq;
     |      | res2: Seq[String] = Stream(id, ?)

scala> 
{noformat}
This seems to a bug in the handling of the join columns, but only in the case where it's provided as a {{Stream}} (see similar bugs SPARK-38308, SPARK-38528, SPARK-38221, SPARK-26680).;;;","22/Jun/23 04:13;steven.aerts;[~bersprockets] I did not think of that.

Are you already working on a fix?  If not, I can give it a go.  With your hint, I now know where to look for.

Thanks;;;","22/Jun/23 04:17;bersprockets;[~steven.aerts] Go for it!;;;","23/Jun/23 12:59;steven.aerts;[https://github.com/apache/spark/pull/41712] was created with a proposed fix and reproduction scenario for this problem.
Let me know if you prefer to update this Jira ticket, as it is still referring to the BeanEncoder which had nothing to do with it.;;;","24/Jun/23 04:37;snoot;User 'steven-aerts' has created a pull request for this issue:
https://github.com/apache/spark/pull/41712;;;","07/Aug/23 23:09;gurwls223;Issue resolved by pull request 41712
[https://github.com/apache/spark/pull/41712];;;",,,,,,,,,
Revert SPARK-44129 after branch-3.5,SPARK-44130,13540832,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dongjoon,dongjoon,21/Jun/23 01:31,21/Jun/23 01:32,30/Oct/23 17:26,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,Project Infra,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-44129,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-21 01:31:43.0,,,,,,,,,,"0|z1ionk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Use ""3.5.0"" for `master` branch until creating `branch-3.5`",SPARK-44129,13540831,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,dongjoon,dongjoon,21/Jun/23 01:29,21/Jun/23 02:20,30/Oct/23 17:26,21/Jun/23 02:20,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Project Infra,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-44130,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jun 21 02:20:39 UTC 2023,,,,,,,,,,"0|z1ionc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/23 01:47;gurwls223;Issue resolved by pull request 41682
[https://github.com/apache/spark/pull/41682];;;","21/Jun/23 02:13;gurwls223;Reverted in https://github.com/apache/spark/commit/5a55061df25a9f9c1c35c272b1563705d957eb84;;;","21/Jun/23 02:20;dongjoon;Issue resolved by pull request 41684
[https://github.com/apache/spark/pull/41684];;;",,,,,,,,,,,,
Migration shuffle to decommissioned executor should not count as block failure,SPARK-44126,13540821,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,20/Jun/23 22:53,26/Sep/23 02:53,30/Oct/23 17:26,26/Sep/23 02:53,3.4.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,Spark Core,,,,,0,pull-request-available,,,,"When shuffle migration to decommissioned executor, the below exception is thrown:

{code:java}
org.apache.spark.SparkException: Exception thrown in awaitResult:     at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)    at org.apache.spark.network.BlockTransferService.uploadBlockSync(BlockTransferService.scala:122)    at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.$anonfun$run$5(BlockManagerDecommissioner.scala:127)    at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.$anonfun$run$5$adapted(BlockManagerDecommissioner.scala:118)    at scala.collection.immutable.List.foreach(List.scala:431)    at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.run(BlockManagerDecommissioner.scala:118)    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)    at java.base/java.lang.Thread.run(Thread.java:829)Caused by: java.lang.RuntimeException: org.apache.spark.storage.BlockSavedOnDecommissionedBlockManagerException: Block shuffle_2_6429_0.data cannot be saved on decommissioned executor    at org.apache.spark.errors.SparkCoreErrors$.cannotSaveBlockOnDecommissionedExecutorError(SparkCoreErrors.scala:238)    at org.apache.spark.storage.BlockManager.checkShouldStore(BlockManager.scala:277)    at org.apache.spark.storage.BlockManager.putBlockDataAsStream(BlockManager.scala:741)    at org.apache.spark.network.netty.NettyBlockRpcServer.receiveStream(NettyBlockRpcServer.scala:174)    at org.apache.spark.network.server.AbstractAuthRpcHandler.receiveStream(AbstractAuthRpcHandler.java:78)    at org.apache.spark.network.server.TransportRequestHandler.processStreamUpload(TransportRequestHandler.java:202)    at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:115)    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)    at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)    at org.apache.spark.network.crypto.TransportCipher$DecryptionHandler.channelRead(TransportCipher.java:190)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)    at java.base/java.lang.Thread.run(Thread.java:829)
{code}

Then this count as block migration failure, which should not.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 26 02:53:27 UTC 2023,,,,,,,,,,"0|z1iol4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/23 02:53;wuyi;Issue resolved by pull request 41905
[https://github.com/apache/spark/pull/41905];;;",,,,,,,,,,,,,,
Clear persistent objects before SQL analyzer golden file tests,SPARK-44123,13540814,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dtenedor,dtenedor,20/Jun/23 21:19,20/Jun/23 21:19,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"For example, SQL golden file test output in files like 
{code:java}
sql/core/src/test/resources/sql-tests/analyzer-results/hll.sql.out {code}
should not fail with ""already exists"" errors like this:

 
{code:java}
-- !query
CREATE TABLE t1 USING JSON AS VALUES (0), (1), (2), (2), (2), (3), (4) as tab(col)
-- !query analysisorg.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-20 21:19:29.0,,,,,,,,,,"0|z1iojk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cannot parse Type from german ""umlaut""",SPARK-44108,13540776,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jomach,jomach,20/Jun/23 14:52,20/Jun/23 18:25,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Hello all, 

 

I have a client that has a column named : bfzgtäeil

Spark cannot handle this. My test: 

 
{code:java}
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
import org.scalatest.funsuite.AnyFunSuite

class HiveTest extends AnyFunSuite {

  test(""test that Spark does not cut columns with ä"") {
    val data = ""bfzugtäeil:string""
    CatalystSqlParser.parseDataType(data)
  }

} {code}
I debugged it and I'm deep on the  org.antlr.v4.runtime.Lexer class. 

Any ideas ? 

 
{code:java}
== SQL ==bfzugtäeil:string------^^^
	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseDataType(ParseDriver.scala:41)	at com.deutschebahn.zod.fvdl.commons.spark.app.captured.HiveTest2.$anonfun$new$1(HiveTest2.scala:13) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-20 14:52:50.0,,,,,,,,,,"0|z1iob4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to create spark session when master URL set to k8s in spark 3.3.0,SPARK-44102,13540691,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,shamim_er123,shamim_er123,20/Jun/23 05:56,20/Jun/23 06:05,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Getting below error while creating Spark Session using JAVA 

 

Error in executing job: 
org.apache.spark.SparkException: Could not parse Master URL: 'k8s://[https://kubernetes.default.svc.cluster.local:443|https://kubernetes.default.svc.cluster.local/]'
    at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2982)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:563)
    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2704)
    at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953)
    at scala.Option.getOrElse(Option.scala:189)

 

This was working fine with spark version 3.0.1, after upgrading spark version to 3.3.0. its starting throwing above error. 

 

Below are the Dependency version we used.

Spark Core 3.3.0

Spark Kubernetes Jar 3.3.0

io.fabrics dependency with version 5.12.2

 

We are not using spark-submit rather submitting spark application using java code  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-20 05:56:52.0,,,,,,,,,,"0|z1ins8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Apache Arrow to 12.0.1,SPARK-44094,13540511,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,18/Jun/23 08:43,18/Jun/23 20:27,30/Oct/23 17:26,18/Jun/23 20:27,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jun 18 20:27:48 UTC 2023,,,,,,,,,,"0|z1imog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/23 20:27;dongjoon;Issue resolved by pull request 41650
[https://github.com/apache/spark/pull/41650];;;",,,,,,,,,,,,,,
Introduce withResourceTypes to `ResourceRequestTestHelper` to restore `resourceTypes` as default value after testing,SPARK-44091,13540506,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,18/Jun/23 05:14,28/Aug/23 04:37,30/Oct/23 17:26,28/Aug/23 04:35,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,4.0.0,,,Tests,YARN,,,,0,,,,,"When I run 

```
build/sbt ""yarn/test"" -Pyarn -Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest
```

The YarnClusterSuite will have some test failures as follows:


```
[info] - run Spark in yarn-client mode *** FAILED *** (3 seconds, 123 milliseconds)
[info]   FAILED did not equal FINISHED (stdout/stderr was not captured) (BaseYarnClusterSuite.scala:238)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
[info]   at org.apache.spark.deploy.yarn.BaseYarnClusterSuite.checkResult(BaseYarnClusterSuite.scala:238)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite.testBasicYarnApp(YarnClusterSuite.scala:350)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite.$anonfun$new$1(YarnClusterSuite.scala:95)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.apache.spark.deploy.yarn.BaseYarnClusterSuite.$anonfun$test$1(BaseYarnClusterSuite.scala:77)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:221)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:67)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:67)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:67)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:67)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:750)
Exception in thread ""LocalizerRunner for container_1687064827340_0001_01_000001"" org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException
  | => yat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:339)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:1289)
Caused by: java.lang.InterruptedException
  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1223)
  at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)
  at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:339)
  at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:331)
  ... 1 more
```

But when I run

```
build/sbt ""yarn/testOnly org.apache.spark.deploy.yarn.YarnClusterSuite"" -Pyarn
```

All test passed.

and if I re-ignore this case, `build/sbt ""yarn/test"" -Pyarn -Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest` also passed.

I see GA not failed, and I am using Java8 + MacOS 13.4 + M2 Max (also test MacOS + M1, also failed) for test",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 28 04:35:46 UTC 2023,,,,,,,,,,"0|z1imnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/23 03:45;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/41673;;;","28/Aug/23 04:35;dongjoon;Issue resolved by pull request 41673
[https://github.com/apache/spark/pull/41673];;;",,,,,,,,,,,,,
pyspark sql,SPARK-44087,13540487,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,ericsyh,ericsyh,17/Jun/23 13:49,18/Jun/23 20:02,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"h3. Background

I am new to Spark and I am trying to use PySpark to read data from a Kafka topic but got some errors that can't search to get the answer. 
h3. How to reproduce?
 # Follow the [https://kafka.apache.org/quickstart] to start the kafka broker server.
 # Follow the [https://spark.apache.org/docs/latest/index.html#running-the-examples-and-shell] to start the PySpark shell
 # Follow the [https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html] to write the pyspark code and use spark-submit to submit the python code. 

My python code
{code:java}
from pyspark.sql import SparkSession 

spark = SparkSession.Builder().appName('Kafka').master('local').config(""spark.driver.bindAddress"",""127.0.0.1"").getOrCreate()

df = spark.readStream.format(""kafka"").option(""kafka.bootstrap.servers"", ""127.0.0.1:9092"").option(""subscribe"", ""quickstart-events"").option(""startingOffsets"", ""learliest"").load()

df.selectExpr(""CAST(key AS STRING)"", ""CAST(value AS STRING)"")

df.show(){code}
How I submit the job
{code:java}
bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.0 kafka.py{code}
And the error I got 
{code:java}
23/06/17 21:47:16 ERROR Inbox: Ignoring error
java.lang.NullPointerException: Cannot invoke ""org.apache.spark.storage.BlockManagerId.executorId()"" because ""idWithoutTopologyInfo"" is null
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    at java.base/java.lang.Thread.run(Thread.java:1623)
23/06/17 21:47:16 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult:
    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:577)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    at java.base/java.lang.Thread.run(Thread.java:1623)
Caused by: java.lang.NullPointerException: Cannot invoke ""org.apache.spark.storage.BlockManagerId.executorId()"" because ""idWithoutTopologyInfo"" is null
    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
    ... 3 more
^CERROR:root:KeyboardInterrupt while sending command.
Traceback (most recent call last):
  File ""/Users/ericsyh/code/spark-3.4.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ericsyh/code/spark-3.4.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py"", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py"", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Traceback (most recent call last):
  File ""/Users/ericsyh/code/PySpark-Learning/kafka.py"", line 7, in <module>
    getOrCreate()
  File ""/Users/ericsyh/code/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/session.py"", line 477, in getOrCreate
  File ""/Users/ericsyh/code/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py"", line 512, in getOrCreate
  File ""/Users/ericsyh/code/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py"", line 200, in __init__
  File ""/Users/ericsyh/code/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py"", line 287, in _do_init
  File ""/Users/ericsyh/code/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py"", line 417, in _initialize_context
  File ""/Users/ericsyh/code/spark-3.4.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1586, in __call__
23/06/17 21:47:21 INFO DiskBlockManager: Shutdown hook called
  File ""/Users/ericsyh/code/spark-3.4.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1038, in send_command
  File ""/Users/ericsyh/code/spark-3.4.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py"", line 511, in send_command
  File ""/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py"", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^{code}","Kafka: kafka_2.13-3.5.0

Spark: spark-3.4.0-bin-hadoop3

Spark-kafka-driver: org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/23 01:56;ericsyh;kafka.py;https://issues.apache.org/jira/secure/attachment/13059183/kafka.py",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jun 18 20:02:07 UTC 2023,,,,,,,,,,"0|z1imj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/23 20:02;bjornjorgensen;Hi, we do have a mailing list for [Q and A|https://spark.apache.org/community.html] like this. 

;;;",,,,,,,,,,,,,,
Dynamic allocation pending tasks should not include finished ones,SPARK-44084,13540464,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,warrenzhu25,warrenzhu25,17/Jun/23 00:09,17/Jun/23 00:09,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-17 00:09:25.0,,,,,,,,,,"0|z1ime0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate operator does not update reference set properly,SPARK-44082,13540446,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,amaliujia,amaliujia,amaliujia,16/Jun/23 19:18,06/Oct/23 00:17,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,pull-request-available,,,,"Before 
```
== Optimized Logical Plan ==
Project [col1#2, col2#19]
+- Generate replicaterows(sum#17L, col1#2, col2#3), [2], false, [col1#2, col2#3]
   +- Filter (isnotnull(sum#17L) AND (sum#17L > 0))
      +- Aggregate [col1#2, col2#19], [col1#2, col2#19, sum(vcol#14L) AS sum#17L]
         +- Union false, false
            :- Aggregate [col1#2], [1 AS vcol#14L, col1#2, first(col2#3, false) AS col2#19]
            :  +- LogicalRDD [col1#2, col2#3], false
            +- Project [-1 AS vcol#15L, col1#8, col2#9]
               +- LogicalRDD [col1#8, col2#9], false
```

Couldn't find col2#3 in [col1#2,col2#19,sum#17L]

after 
```
== Optimized Logical Plan ==
Project [col1#2, col2#19]
+- Generate replicaterows(sum#17L, col1#2, col2#19), [2], false, [col1#2, col2#19]
   +- Filter (isnotnull(sum#17L) AND (sum#17L > 0))
      +- Aggregate [col1#2, col2#19], [col1#2, col2#19, sum(vcol#14L) AS sum#17L]
         +- Union false, false
            :- Aggregate [col1#2], [1 AS vcol#14L, col1#2, first(col2#3, false) AS col2#19]
            :  +- LogicalRDD [col1#2, col2#3], false
            +- Project [-1 AS vcol#15L, col1#8, col2#9]
               +- LogicalRDD [col1#8, col2#9], false
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jun 26 16:52:38 UTC 2023,,,,,,,,,,"0|z1imaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/23 16:52;ignitetcbot;User 'amaliujia' has created a pull request for this issue:
https://github.com/apache/spark/pull/41633;;;",,,,,,,,,,,,,,
Json reader crashes when a different schema is present,SPARK-44079,13540407,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,charlottevdscheun,charlottevdscheun,16/Jun/23 13:20,29/Jun/23 13:38,30/Oct/23 17:26,29/Jun/23 06:28,3.4.0,,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,,,SQL,,,,,0,,,,,"When using pyspark 3.4, we noticed that when reading a json file with a corrupted record the reader crashes. In pyspark 3.3 this worked fine.

{*}Code{*}:
{code:java}
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
import json


data = """"""[{""a"": ""incorrect"", ""b"": ""correct""}]""""""
schema = StructType([StructField('a', IntegerType(), True), StructField('b', StringType(), True), StructField('_corrupt_record', StringType(), True)])


spark.read.option(""mode"", ""PERMISSIVE"").option(""multiline"",""true"").schema(schema).json(spark.sparkContext.parallelize([data])).show(truncate=False){code}
*Used packages:*
 * Pyspark==3.4.0
 * python==3.10.0
 * delta-spark==2.4.0

 
spark_jars=(
  ""org.apache.spark:spark-avro_2.12:3.4.0""
  "",io.delta:delta-core_2.12:2.4.0""
  "",com.databricks:spark-xml_2.12:0.16.0""
)
 

{*}Expected behaviour{*}:
|a|b|_corrupt_record|
|null|null|[\\{""a"": ""incorrect"", ""b"": ""correct""}]|

 

{*}Actual behaviour{*}:
{code:java}
 
*** py4j.protocol.Py4JJavaError: An error occurred while calling o104.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 2.0 failed 1 times, most recent failure: Lost task 4.0 in stage 2.0 (TID 9) (charlottesmbp2.home executor driver): java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:201)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getAs(rows.scala:35)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get$(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.get(rows.scala:195)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$2(FailureSafeParser.scala:47)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$1(FailureSafeParser.scala:47)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:64)
        at org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:431)
        at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
        at org.apache.spark.scheduler.Task.run(Task.scala:139)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.base/java.lang.Thread.run(Thread.java:1589)
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
        at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
        at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
        at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
        at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
        at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
        at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
        at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
        at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
        at java.base/java.lang.reflect.Method.invoke(Method.java:578)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:1589)
Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:201)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getAs(rows.scala:35)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get$(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.get(rows.scala:195)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$2(FailureSafeParser.scala:47)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$1(FailureSafeParser.scala:47)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:64)
        at org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:431)
        at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
        at org.apache.spark.scheduler.Task.run(Task.scala:139)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        ... 1 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jun 29 06:28:28 UTC 2023,,,,,,,,,,"0|z1im28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/23 09:15;githubbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41662;;;","29/Jun/23 06:28;maxgekk;Issue resolved by pull request 41662
[https://github.com/apache/spark/pull/41662];;;",,,,,,,,,,,,,
Session Configs were not getting honored in RDDs,SPARK-44077,13540308,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kapilks_ms,kapilks_ms,16/Jun/23 05:03,26/Sep/23 00:17,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,pull-request-available,,,,"When calling SQLConf.get on executors, the configs are read from the local properties on the TaskContext. The local properties are populated driver-side when scheduling the job, using the properties found in sparkContext.localProperties. For RDD actions, local properties were not getting populated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-16 05:03:29.0,,,,,,,,,,"0|z1ilg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`Logging plan changes for execution` test failed,SPARK-44074,13540299,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,16/Jun/23 03:12,29/Sep/23 18:08,30/Oct/23 17:26,20/Jun/23 04:40,3.3.4,3.4.2,3.5.0,,,,,,,,,,,,,,,,,3.3.4,3.4.2,3.5.0,,SQL,Tests,,,,0,pull-request-available,,,,"run {{build/sbt clean ""sql/test"" -Dtest.exclude.tags=org.apache.spark.tags.ExtendedSQLTest,org.apache.spark.tags.SlowSQLTest}}

{{}}
{code:java}
2023-06-15T19:58:34.4105460Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32mQueryExecutionSuite:�[0m�[0m
2023-06-15T19:58:34.5395268Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to a file (77 milliseconds)�[0m�[0m
2023-06-15T19:58:34.5856902Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to an existing file (49 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6099849Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to non-existing folder (25 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6136467Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info by invalid path (4 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6425071Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to a file - explainMode=formatted (28 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7084916Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- limit number of fields by sql config (66 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7432299Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- check maximum fields restriction (34 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7554546Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- toString() exception/error handling (11 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7621424Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- SPARK-28346: clone the query plan between different stages (6 milliseconds)�[0m�[0m
2023-06-15T19:58:34.8001412Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[31m- Logging plan changes for execution *** FAILED *** (12 milliseconds)�[0m�[0m
2023-06-15T19:58:34.8007977Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[31m  testAppender.loggingEvents.exists(((x$10: org.apache.logging.log4j.core.LogEvent) => x$10.getMessage().getFormattedMessage().contains(expectedMsg))) was false (QueryExecutionSuite.scala:232)�[0m�[0m 

{code}
 

but run {{build/sbt ""sql/testOnly *QueryExecutionSuite""}} not this issue, need to investigate. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 20 04:40:37 UTC 2023,,,,,,,,,,"0|z1ile8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/23 03:57;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/41663;;;","20/Jun/23 04:40;dongjoon;Issue resolved by pull request 41663
[https://github.com/apache/spark/pull/41663];;;",,,,,,,,,,,,,
maven test ReplSuite failed,SPARK-44069,13540237,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,LuciferYang,LuciferYang,15/Jun/23 13:20,15/Jun/23 15:23,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,https://github.com/LuciferYang/spark/actions/runs/5274544416/jobs/9541917589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-15 13:20:38.0,,,,,,,,,,"0|z1il0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven test `ProductAggSuite` aborted,SPARK-44064,13540138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,15/Jun/23 05:43,25/Jun/23 23:52,30/Oct/23 17:26,25/Jun/23 23:52,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"run 

 
{code:java}
 ./build/mvn  -DskipTests -Pyarn -Pmesos -Pkubernetes -Pvolcano -Phive -Phive-thriftserver -Phadoop-cloud -Pspark-ganglia-lgpl  clean install
 build/mvn test -pl sql/catalyst     {code}
aborted

 
{code:java}
ProductAggSuite:
*** RUN ABORTED ***
  java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$
  at org.apache.spark.sql.catalyst.expressions.codegen.JavaCode$.variable(javaCode.scala:64)
  at org.apache.spark.sql.catalyst.expressions.codegen.JavaCode$.isNullVariable(javaCode.scala:77)
  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:200)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:196)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.$anonfun$create$1(GenerateSafeProjection.scala:156)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:153)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:39)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1369) {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jun 25 23:52:11 UTC 2023,,,,,,,,,,"0|z1ikeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/23 04:17;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/41654;;;","25/Jun/23 23:52;dongjoon;Issue resolved by pull request 41654
[https://github.com/apache/spark/pull/41654];;;",,,,,,,,,,,,,
Revert SPARK-44047,SPARK-44063,13540128,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Won't Fix,,panbingkun,panbingkun,15/Jun/23 03:09,15/Jun/23 03:25,30/Oct/23 17:26,15/Jun/23 03:25,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Build,Connect,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jun 15 03:25:03 UTC 2023,,,,,,,,,,"0|z1ikc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/23 03:25;panbingkun;It's my local environmental issue.;;;",,,,,,,,,,,,,,
Remove deprecated API usage in HiveShim.scala,SPARK-44058,13540085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,amanraj2520,amanraj2520,14/Jun/23 18:05,11/Jul/23 18:35,30/Oct/23 17:26,11/Jul/23 18:35,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Submit,,,,,0,,,,,"Spark's HiveShim.scala calls this particular method in Hive :
createPartitionMethod.invoke(
hive,
table,
spec,
location,
params, // partParams
null, // inputFormat
null, // outputFormat
-1: JInteger, // numBuckets
null, // cols
null, // serializationLib
null, // serdeParams
null, // bucketCols
null) // sortCols
}
 
We do not have any such implementation of createPartition in Hive. We only have this definition :
public Partition createPartition(Table tbl, Map<String, String> partSpec) throws HiveException {
    try

{       org.apache.hadoop.hive.metastore.api.Partition part =           Partition.createMetaPartitionObject(tbl, partSpec, null);       AcidUtils.TableSnapshot tableSnapshot = AcidUtils.getTableSnapshot(conf, tbl);       part.setWriteId(tableSnapshot != null ? tableSnapshot.getWriteId() : 0);       return new Partition(tbl, getMSC().add_partition(part));     }

catch (Exception e)

{       LOG.error(StringUtils.stringifyException(e));       throw new HiveException(e);     }

  }
*The 12 parameter implementation was removed in HIVE-5951*

 

The issue is that this 12 parameter implementation of createPartition method was added in Hive-0.12 and then was removed in Hive-0.13. When Hive 0.12 was used in Spark, SPARK-15334 commit in Spark added this 12 parameters implementation. But after Hive migrated to newer APIs somehow this was not changed in Spark OSS and it looks to us like a Bug from the Spark end.

 

We need to migrate to the newest implementation of Hive createPartition method otherwise this flow can break",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 20 03:57:42 UTC 2023,,,,,,,,,,"0|z1ik2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/23 00:54;yumwang;For Hive 0.13 and later, we use https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala#L753-L768.;;;","15/Jun/23 04:11;amanraj2520;[~yumwang] In that case this function of createPartition is not required right?;;;","15/Jun/23 05:29;yumwang;This is used to connect Hive metastore 0.12.;;;","20/Jun/23 03:57;snoot;User 'amanraj2520' has created a pull request for this issue:
https://github.com/apache/spark/pull/41602;;;",,,,,,,,,,,
Update ORC to 1.8.4,SPARK-44053,13540006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Guiyankuang,Guiyankuang,Guiyankuang,14/Jun/23 08:25,14/Jun/23 17:56,30/Oct/23 17:26,14/Jun/23 16:37,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jun 14 17:56:09 UTC 2023,,,,,,,,,,"0|z1ijl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/23 08:43;Guiyankuang;Our plan is to
spark 3.4.1 upgrade to ORC 1.8.4
spark 3.5.0 upgrade to ORC 1.9.0
So I set the affected version to 3.4.1

[~yumwang]  :);;;","14/Jun/23 16:37;dongjoon;Issue resolved by pull request 41593
[https://github.com/apache/spark/pull/41593];;;","14/Jun/23 17:56;dongjoon;Apache ORC 1.9.0 PR will arrive soon in this month.;;;",,,,,,,,,,,,
Unable to Mount ConfigMap in Driver Pod - ConfigMap Creation Issue,SPARK-44050,13539967,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,harshwardhan.dodiya,harshwardhan.dodiya,14/Jun/23 05:36,07/Aug/23 20:40,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,Kubernetes,Spark Submit,,,,0,,,,,"Dear Spark community,

I am facing an issue related to mounting a ConfigMap in the driver pod of my Spark application. Upon investigation, I realized that the problem is caused by the ConfigMap not being created successfully.

*Problem Description:*
When attempting to mount the ConfigMap in the driver pod, I encounter consistent failures and my pod stays in containerCreating state. Upon further investigation, I discovered that the ConfigMap does not exist in the Kubernetes cluster, which results in the driver pod's inability to access the required configuration data.

*Additional Information:*

I would like to highlight that this issue is not a frequent occurrence. It has been observed randomly, affecting the mounting of the ConfigMap in the driver pod only approximately 5% of the time. This intermittent behavior adds complexity to the troubleshooting process, as it is challenging to reproduce consistently.

*Error Message:*

when describing driver pod (kubectl describe pod pod_name)  get the below error.

""ConfigMap '<configmap-name>' not found.""

*To Reproduce:*

1. Download spark 3.3.1 from [https://spark.apache.org/downloads.html]

2. create an image with ""bin/docker-image-tool.sh""

3. Submit on spark-client via bash command by passing all the details and configurations.

4. Randomly in some of the driver pod we can observe this issue.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/23 05:37;harshwardhan.dodiya;image-2023-06-14-11-07-36-960.png;https://issues.apache.org/jira/secure/attachment/13059050/image-2023-06-14-11-07-36-960.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Aug 07 20:40:51 UTC 2023,,,,,,,,,,"0|z1ijcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/23 19:47;bjornjorgensen;[~harshwardhan.dodiya] Have a look at SPARK-40065 

Update spark from 3.3.1 to 3.3.2 or 3.4.0 ;;;","07/Aug/23 20:40;holden;Ah interesting, it sounds like the fix would be to retry config map creation on failure. ;;;",,,,,,,,,,,,,
Incorrect result after count distinct,SPARK-44040,13539864,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yumwang,boltonidze,boltonidze,13/Jun/23 13:42,16/Jun/23 04:07,30/Oct/23 17:26,16/Jun/23 04:07,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,Spark Core,,,,,0,,,,,"When i try to call count after distinct function for Decimal null field, spark return incorrect result starting from spark 3.4.0.
A minimal example to reproduce:

import org.apache.spark.sql.types._
import org.apache.spark.sql.\{Column, DataFrame, Dataset, Row, SparkSession}
import org.apache.spark.sql.types.\{StringType, StructField, StructType}
val schema = StructType( Array(
StructField(""money"", DecimalType(38,6), true),
StructField(""reference_id"", StringType, true)
))

val payDf = spark.createDataFrame(sc.emptyRDD[Row], schema)

val aggDf = payDf.agg(sum(""money"").as(""money"")).withColumn(""name"", lit(""df1""))
val aggDf1 = payDf.agg(sum(""money"").as(""money"")).withColumn(""name"", lit(""df2""))
val unionDF: DataFrame = aggDf.union(aggDf1)
unionDF.select(""money"").distinct.show // return correct result
unionDF.select(""money"").distinct.count // return 2 instead of 1
unionDF.select(""money"").distinct.count == 1 // return false


This block of code returns some assertion error and after that an incorrect count (in spark 3.2.1 everything works fine and i get correct result = 1):

*scala> unionDF.select(""money"").distinct.show // return correct result*
java.lang.AssertionError: assertion failed:
Decimal$DecimalIsFractional
while compiling: <console>
during phase: globalPhase=terminal, enteringPhase=jvm
library version: version 2.12.17
compiler version: version 2.12.17
reconstructed args: -classpath /Users/aleksandrov/.ivy2/jars/org.apache.spark_spark-connect_2.12-3.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-storage-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar:/Users/aleksandrov/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar -Yrepl-class-based -Yrepl-outdir /private/var/folders/qj/_dn4xbp14jn37qmdk7ylyfwc0000gr/T/spark-f37bb154-75f3-4db7-aea8-3c4363377bd8/repl-350f37a1-1df1-4816-bd62-97929c60a6c1

last tree to typer: TypeTree(class Byte)
tree position: line 6 of <console>
tree tpe: Byte
symbol: (final abstract) class Byte in package scala
symbol definition: final abstract class Byte extends (a ClassSymbol)
symbol package: scala
symbol owners: class Byte
call site: constructor $eval in object $eval in package $line19

== Source file context for tree position ==

3
4object $eval {
5lazyval $result = $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res0
6lazyval $print: {_}root{_}.java.lang.String = {
7 $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw
8
9""""
at scala.reflect.internal.SymbolTable.throwAssertionError(SymbolTable.scala:185)
at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1525)
at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
at scala.reflect.internal.Symbols$Symbol.flatOwnerInfo(Symbols.scala:2353)
at scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala:3346)
at scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:3348)
at scala.reflect.internal.Symbols$ModuleClassSymbol.sourceModule(Symbols.scala:3487)
at scala.reflect.internal.Symbols.$anonfun$forEachRelevantSymbols$1$adapted(Symbols.scala:3802)
at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
at scala.reflect.internal.Symbols.markFlagsCompleted(Symbols.scala:3799)
at scala.reflect.internal.Symbols.markFlagsCompleted$(Symbols.scala:3805)
at scala.reflect.internal.SymbolTable.markFlagsCompleted(SymbolTable.scala:28)
at scala.reflect.internal.pickling.UnPickler$Scan.finishSym$1(UnPickler.scala:324)
at scala.reflect.internal.pickling.UnPickler$Scan.readSymbol(UnPickler.scala:342)
at scala.reflect.internal.pickling.UnPickler$Scan.readSymbolRef(UnPickler.scala:645)
at scala.reflect.internal.pickling.UnPickler$Scan.readType(UnPickler.scala:413)
at scala.reflect.internal.pickling.UnPickler$Scan.$anonfun$readSymbol$10(UnPickler.scala:357)
at scala.reflect.internal.pickling.UnPickler$Scan.at(UnPickler.scala:188)
at scala.reflect.internal.pickling.UnPickler$Scan.readSymbol(UnPickler.scala:357)
at scala.reflect.internal.pickling.UnPickler$Scan.$anonfun$run$1(UnPickler.scala:96)
at scala.reflect.internal.pickling.UnPickler$Scan.run(UnPickler.scala:88)
at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:47)
at scala.tools.nsc.symtab.classfile.ClassfileParser.unpickleOrParseInnerClasses(ClassfileParser.scala:1173)
at scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfileParser.scala:467)
at scala.tools.nsc.symtab.classfile.ClassfileParser.$anonfun$parse$2(ClassfileParser.scala:160)
at scala.tools.nsc.symtab.classfile.ClassfileParser.$anonfun$parse$1(ClassfileParser.scala:146)
at scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.scala:129)
at scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoaders.scala:343)
at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:250)
at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scala:269)
at scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:1104)
at scala.reflect.internal.Symbols$Symbol.toOption(Symbols.scala:2609)
at scala.tools.nsc.interpreter.IMain.translateSimpleResource(IMain.scala:340)
at scala.tools.nsc.interpreter.IMain$TranslatingClassLoader.findAbstractFile(IMain.scala:354)
at scala.reflect.internal.util.AbstractFileClassLoader.findResource(AbstractFileClassLoader.scala:76)
at java.base/java.lang.ClassLoader.getResource(ClassLoader.java:1401)
at java.base/java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1737)
at scala.reflect.internal.util.RichClassLoader$.classAsStream$extension(ScalaClassLoader.scala:89)
at scala.reflect.internal.util.RichClassLoader$.classBytes$extension(ScalaClassLoader.scala:81)
at scala.reflect.internal.util.ScalaClassLoader.classBytes(ScalaClassLoader.scala:131)
at scala.reflect.internal.util.ScalaClassLoader.classBytes$(ScalaClassLoader.scala:131)
at scala.reflect.internal.util.AbstractFileClassLoader.classBytes(AbstractFileClassLoader.scala:41)
at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:70)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:576)
at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:75)
at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:317)
at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8895)
at org.codehaus.janino.UnitCompiler.reclassifyName(UnitCompiler.java:9115)
at org.codehaus.janino.UnitCompiler.reclassifyName(UnitCompiler.java:8806)
at org.codehaus.janino.UnitCompiler.reclassify(UnitCompiler.java:8667)
at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7194)
at org.codehaus.janino.UnitCompiler.access$18100(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$26.visitAmbiguousName(UnitCompiler.java:6785)
at org.codehaus.janino.UnitCompiler$26.visitAmbiguousName(UnitCompiler.java:6784)
at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4603)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6784)
at org.codehaus.janino.UnitCompiler.access$15100(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$25.visitLvalue(UnitCompiler.java:6745)
at org.codehaus.janino.UnitCompiler$25.visitLvalue(UnitCompiler.java:6742)
at org.codehaus.janino.Java$Lvalue.accept(Java.java:4528)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6742)
at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$23.visitRvalue(UnitCompiler.java:6690)
at org.codehaus.janino.UnitCompiler$23.visitRvalue(UnitCompiler.java:6681)
at org.codehaus.janino.Java$Rvalue.accept(Java.java:4495)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6681)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9392)
at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7486)
at org.codehaus.janino.UnitCompiler.access$16100(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$25.visitMethodInvocation(UnitCompiler.java:6756)
at org.codehaus.janino.UnitCompiler$25.visitMethodInvocation(UnitCompiler.java:6742)
at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5470)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6742)
at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9590)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9475)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9391)
at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5232)
at org.codehaus.janino.UnitCompiler.access$9300(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4735)
at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4711)
at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5470)
at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4711)
at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5854)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:4101)
at org.codehaus.janino.UnitCompiler.access$6300(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:4057)
at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:4040)
at org.codehaus.janino.Java$Assignment.accept(Java.java:4864)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:4040)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2523)
at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1580)
at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:3209)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1646)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1579)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1575)
at org.codehaus.janino.Java$Block.accept(Java.java:3115)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2659)
at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1581)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$IfStatement.accept(Java.java:3284)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1646)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1579)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1575)
at org.codehaus.janino.Java$Block.accept(Java.java:3115)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2637)
at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1581)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$IfStatement.accept(Java.java:3284)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1646)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1579)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1575)
at org.codehaus.janino.Java$Block.accept(Java.java:3115)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2001)
at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1584)
at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3389)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3658)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3329)
at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1447)
at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1420)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:829)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1026)
at org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$3.visitMemberClassDeclaration(UnitCompiler.java:425)
at org.codehaus.janino.UnitCompiler$3.visitMemberClassDeclaration(UnitCompiler.java:418)
at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1533)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:418)
at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1397)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:864)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:442)
at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$3.visitPackageMemberClassDeclaration(UnitCompiler.java:422)
at org.codehaus.janino.UnitCompiler$3.visitPackageMemberClassDeclaration(UnitCompiler.java:418)
at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1688)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:418)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:392)
at org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$2.visitCompilationUnit(UnitCompiler.java:363)
at org.codehaus.janino.UnitCompiler$2.visitCompilationUnit(UnitCompiler.java:361)
at org.codehaus.janino.Java$CompilationUnit.accept(Java.java:371)
at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:361)
at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:264)
at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:294)
at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:288)
at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:267)
at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:82)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1496)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1586)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1583)
at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1443)
at org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:726)
at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:725)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:699)
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
at scala.collection.TraversableLike.map(TraversableLike.scala:286)
at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
at scala.collection.AbstractTraversable.map(Traversable.scala:108)
at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:699)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)
at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)
at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:47)
at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:140)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)
at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:181)
at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:181)
at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:183)
at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:266)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:264)
at scala.collection.Iterator.foreach(Iterator.scala:943)
at scala.collection.Iterator.foreach$(Iterator.scala:943)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
at scala.collection.IterableLike.foreach(IterableLike.scala:74)
at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:264)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:381)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)
at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
at org.apache.spark.sql.Dataset.show(Dataset.scala:809)
at org.apache.spark.sql.Dataset.show(Dataset.scala:768)
at org.apache.spark.sql.Dataset.show(Dataset.scala:777)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:33)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:35)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:37)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:39)
at $line19.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:41)
at $line19.$read$$iw$$iw$$iw$$iw.<init>(<console>:43)
at $line19.$read$$iw$$iw$$iw.<init>(<console>:45)
at $line19.$read$$iw$$iw.<init>(<console>:47)
at $line19.$read$$iw.<init>(<console>:49)
at $line19.$read.<init>(<console>:51)
at $line19.$read$.<init>(<console>:55)
at $line19.$read$.<clinit>(<console>)
at $line19.$eval$.$print$lzycompute(<console>:7)
at $line19.$eval$.$print(<console>:6)
at $line19.$eval.$print(<console>)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
at org.apache.spark.repl.Main$.doMain(Main.scala:78)
at org.apache.spark.repl.Main$.main(Main.scala:58)
at org.apache.spark.repl.Main.main(Main.scala)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1020)
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
error: error while loading Decimal, class file '/Users/aleksandrov/Projects/apache/spark-3.4.0-bin-hadoop3/jars/spark-catalyst_2.12-3.4.0.jar(org/apache/spark/sql/types/Decimal.class)' is broken
(class java.lang.RuntimeException/error reading Scala signature of Decimal.class: assertion failed:
Decimal$DecimalIsFractional
while compiling: <console>
during phase: globalPhase=terminal, enteringPhase=jvm
library version: version 2.12.17
compiler version: version 2.12.17
reconstructed args: -classpath /Users/aleksandrov/.ivy2/jars/org.apache.spark_spark-connect_2.12-3.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-storage-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar:/Users/aleksandrov/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar -Yrepl-class-based -Yrepl-outdir /private/var/folders/qj/_dn4xbp14jn37qmdk7ylyfwc0000gr/T/spark-f37bb154-75f3-4db7-aea8-3c4363377bd8/repl-350f37a1-1df1-4816-bd62-97929c60a6c1

last tree to typer: TypeTree(class Byte)
tree position: line 6 of <console>
tree tpe: Byte
symbol: (final abstract) class Byte in package scala
symbol definition: final abstract class Byte extends (a ClassSymbol)
symbol package: scala
symbol owners: class Byte
call site: constructor $eval in object $eval in package $line19

== Source file context for tree position ==

3
4object $eval {
5lazyval $result = res0
6lazyval $print: {_}root{_}.java.lang.String = {
7 $iw
8
9"""" )
+-----+
|money|

+-----+
|null|

+-----+

*scala> unionDF.select(""money"").distinct.count // return 2 instead of 1*
res1: Long = 2 

*scala> unionDF.select(""money"").distinct.count == 1 // return False*
res2: Boolean = false",,,,,,,,,,,,,,,,,,,,,,,,,SPARK-38162,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 16 04:07:40 UTC 2023,,,,,,,,,,"0|z1iipk:",9223372036854775807,,,,,,,,,,,,,3.4.1,,,,,,,,,,"13/Jun/23 16:15;yumwang;Thanks for reporting this bug. We will fix it soon.;;;","13/Jun/23 16:29;bersprockets;It seems this can be reproduced in {{spark-sql}} as well.

Interestingly, turning off AQE seems to fix the issue (for both the above dataframe version and the below SQL version):
{noformat}
spark-sql (default)> create or replace temp view v1 as
select 1 as c1 limit 0;
Time taken: 0.959 seconds
spark-sql (default)> create or replace temp view agg1 as
select sum(c1) as c1, ""agg1"" as name
from v1;
Time taken: 0.16 seconds
spark-sql (default)> create or replace temp view agg2 as
select sum(c1) as c1, ""agg2"" as name
from v1;
Time taken: 0.035 seconds
spark-sql (default)> create or replace temp view union1 as
select * from agg1
union
select * from agg2;
Time taken: 0.088 seconds
spark-sql (default)> -- the following incorrectly produces 2 rows
select distinct c1 from union1;
NULL
NULL
Time taken: 1.649 seconds, Fetched 2 row(s)
spark-sql (default)> set spark.sql.adaptive.enabled=false;
spark.sql.adaptive.enabled	false
Time taken: 0.019 seconds, Fetched 1 row(s)
spark-sql (default)> -- the following correctly produces 1 row
select distinct c1 from union1;
NULL
Time taken: 1.372 seconds, Fetched 1 row(s)
spark-sql (default)> 
{noformat};;;","13/Jun/23 16:35;yumwang;https://github.com/apache/spark/pull/41576;;;","14/Jun/23 09:16;githubbot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/41576;;;","16/Jun/23 04:07;yumwang;Issue resolved by pull request 41576
[https://github.com/apache/spark/pull/41576];;;",,,,,,,,,,
Support list-like for binary ops,SPARK-44033,13539766,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,itholic,itholic,13/Jun/23 02:31,18/Sep/23 01:07,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Pandas API on Spark,,,,,0,pull-request-available,,,,"We should fix the error below:
{code:java}
>>> pser = pd.Series([1, 2, 3, 4, 5, 6], name=""x"")
>>> psser = ps.from_pandas(pser)
>>> other = [np.nan, 1, 3, 4, np.nan, 6]
>>> psser <= other
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/haejoon.lee/Desktop/git_store/spark/python/pyspark/pandas/base.py"", line 412, in __le__
    return self._dtype_op.le(self, other)
  File ""/Users/haejoon.lee/Desktop/git_store/spark/python/pyspark/pandas/data_type_ops/num_ops.py"", line 242, in le
    _sanitize_list_like(right)
  File ""/Users/haejoon.lee/Desktop/git_store/spark/python/pyspark/pandas/data_type_ops/base.py"", line 199, in _sanitize_list_like
    raise TypeError(""The operation can not be applied to %s."" % type(operand).__name__)
TypeError: The operation can not be applied to list.{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 18 00:30:17 UTC 2023,,,,,,,,,,"0|z1ii3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/23 23:18;gdhuper;[~itholic] I am interested in working on this. If I understand this issue correctly, we are essentially trying to add support for (binary ops)[[spark/python/pyspark/pandas/data_type_ops/complex_ops.py at master · apache/spark (github.com)|https://github.com/apache/spark/blob/master/python/pyspark/pandas/data_type_ops/complex_ops.py]]  for lists / arrays . Would you be provide more context on this? I am new to this codebase. Thanks ;;;","22/Jul/23 04:18;itholic;Thanks for your interesting on this issue, [~gdhuper] .

 > we are essentially trying to add support for binary ops for lists / arrays

Exactly. When you try the example in the description with removing `_sanitize_list_like(right)` from `data_type_ops/num_ops.py`, it will be:

 
{code:java}
>>> pser = pd.Series([1, 2, 3, 4, 5, 6], name=""x"")
>>> psser = ps.from_pandas(pser)
>>> other = [np.nan, 1, 3, 4, np.nan, 6]
>>> psser <= other
Traceback (most recent call last):
...
pyspark.errors.exceptions.captured.SparkRuntimeException: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '[NaN, 1, 3, 4, NaN, 6]' of class java.util.ArrayList. {code}
We should make this work, and add a test into proper places, such as https://github.com/apache/spark/blob/master/python/pyspark/pandas/tests/data_type_ops/test_num_ops.py.

 ;;;","23/Jul/23 03:22;gdhuper;[~itholic] I was able to reproduce the errors you mentioned above (both with `_sanitize_list_like()` and without it).

 
{code:java}
pyspark.errors.exceptions.captured.SparkRuntimeException: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '[2, 1, 3, 4, 8, 6]' of class java.util.ArrayList. {code}
 

As I was tracing the callstack, I came across this code: [https://github.com/apache/spark/blob/515dfc166a95ecc8decce0f0cd99e06fe395f94f/python/pyspark/sql/column.py#L277C1-L277C1]

and tried to implement it but I still get the same error.  

Would you be able to provide some guidance on this?
 * So I tried the same repro steps with just pandas library where I tried comparing a pandas Series with a plain python list and it works. From my understanding, it looks like we just need to add a shim for the binary ops and expose this functionality through spark.
 * Do we need to implement other binary ops just like this: [https://github.com/apache/spark/blob/46440a4a542148bc05b8c0f80d1860e6380efdb6/python/pyspark/pandas/data_type_ops/base.py#L394C1-L394C1] ?;;;","24/Jul/23 03:06;itholic;> Do we need to implement other binary ops just like this

 

Correct, basically we try to reuse the PySpark functions from JVM side internally like [https://github.com/apache/spark/blob/515dfc166a95ecc8decce0f0cd99e06fe395f94f/python/pyspark/sql/column.py#L277C1-L277C1.]

But in case we can't leverage it like this case, we should manually implement this in Python code like [https://github.com/apache/spark/blob/46440a4a542148bc05b8c0f80d1860e6380efdb6/python/pyspark/pandas/data_type_ops/base.py#L394C1-L394C1.];;;","17/Sep/23 21:30;gdhuper;Hi [~itholic],

Apologies for the delay on this. But I just pushed a draft PR with some changes. Would you be able to take a look and provide any feedback? Thanks 

Link to the PR: https://github.com/apache/spark/pull/42962;;;","18/Sep/23 00:30;itholic;No worries! Let me take a look soon. Thanks for working on this, [~gdhuper] .;;;",,,,,,,,,
CSV Table Read Error with CharType(length) column,SPARK-44025,13539599,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,camper42,camper42,12/Jun/23 04:09,06/Oct/23 00:17,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,pull-request-available,,,,"Problem:
 # read a CSV format table
 # table has a `CharType(length)` column
 # read table failed with Exception:  `org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 4 times, most recent failure: Lost task 0.3 in stage 36.0 (TID 72) (10.113.9.208 executor 11): java.lang.IllegalArgumentException: requirement failed: requiredSchema (struct<name:string,age:int,job:string>) should be the subset of dataSchema (struct<name:string,age:int,job:string>).`

 

reproduce with official image:
 # {{docker run -it apache/spark:v3.4.0 /opt/spark/bin/spark-sql}}
 # {{CREATE TABLE csv_bug (name STRING, age INT, job CHAR(4)) USING CSV OPTIONS ('header' = 'true', 'sep' = ';') LOCATION ""/opt/spark/examples/src/main/resources/people.csv"";}}
 # SELECT * FROM csv_bug;
 # ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.IllegalArgumentException: requirement failed: requiredSchema (struct<name:string,age:int,job:string>) should be the subset of dataSchema (struct<name:string,age:int,job:string>).",{{apache/spark:v3.4.0 image}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 16 03:54:36 UTC 2023,,,,,,,,,,"0|z1ih2o:",9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,,,"13/Jun/23 03:12;panbingkun;[~cloud_fan] Can I try to fix it?;;;","16/Jun/23 03:54;snoot;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41564;;;",,,,,,,,,,,,,
Unable to deserialize broadcasted map statuses when executor decommissioned,SPARK-44019,13539510,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,david.klinberg,david.klinberg,10/Jun/23 09:24,05/Sep/23 04:06,30/Oct/23 17:26,05/Sep/23 04:06,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,,,,,,,,,,Shuffle,,,,,0,decommision,MapOutput,,,"during examination of graceful executor decommission at high rate of interruptions, jobs occasionally abort on the exception(which isn't reproducd when decommission is disabled):
{code:java}
org.apache.spark.shuffle.MetadataFetchFailedException: Unable to deserialize broadcasted map statuses for shuffle xx: java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_yyy_pieceZ of broadcast_yyy{code}
This exceptions are reproduced from reducer tasks while multiple decommission are in progress or finished which most of the time causes the entire stage to fail due to its highly reproduced.

This seems to be related to `updateMapOuput` broadcast invalidation which destroys and creates those broadcast variables on each shuffle migration([reference|https://github.com/apache/spark/blob/v3.1.0-rc1/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L134]).

*Example sequence of events from driver:*
{code:java}
2023-06-03 18:41:17:798 INFO TorrentBroadcast:61 Destroying Broadcast(171) (from updateMapOutput at BlockManagerMasterEndpoint.scala:639)
...
2023-06-03 18:41:17:811 INFO BlockManagerInfo:61 Removed broadcast_171_piece0 on xxx_host_xxx:7079 in memory (size: 4.0 MiB, free: 10.4 GiB)
...
2023-06-03 18:41:17:841 WARN TaskSetManager:73 Lost task 6.0 in stage 124.2 (TID xxxxx) (x.x.x.x executor xxxx): FetchFailed(null, shuffleId=xx, mapIndex=-1, mapId=-1, reduceId=-1, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Unable to deserialize broadcasted map statuses for shuffle xx: java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_171_piece0 of broadcast_171{{}}{code}
 ","Spark version: 3.3.1
running on Kubernetes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 05 04:06:47 UTC 2023,,,,,,,,,,"0|z1igiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/23 13:17;eejbyfeldt;I think this is a duplicate of https://issues.apache.org/jira/browse/SPARK-38101;;;","05/Sep/23 04:06;david.klinberg;Right, Thanks for notifying.
Closing this issue.;;;","05/Sep/23 04:06;david.klinberg;Duplicate with https://issues.apache.org/jira/browse/SPARK-38101;;;",,,,,,,,,,,,
Artifacts with name as an absolute path may overwrite other files ,SPARK-44016,13539422,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,09/Jun/23 12:23,13/Jun/23 17:05,30/Oct/23 17:26,13/Jun/23 17:05,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"In `SparkConnectAddArtifactsHandler`, an artifact being moved to a staging location may overwrite another file when the `name`/`path` of the artifact is an `absolute` path. 

This happens when the [stagedPath|https://github.com/apache/spark/blob/master/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala#L172] is being computed with the help of the `.resolve(...)` method where the `resolve` method returns the `other` path (in this case, the name of the artifact) if the `other` path is an absolute path.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-09 12:23:42.0,,,,,,,,,,"0|z1ifzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python StreamingQueryProgress rowsPerSecond type fix,SPARK-44010,13539351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,WweiL,WweiL,WweiL,09/Jun/23 01:51,09/Jun/23 04:11,30/Oct/23 17:26,09/Jun/23 04:11,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,PySpark,Structured Streaming,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 09 04:11:47 UTC 2023,,,,,,,,,,"0|z1ifjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/23 04:11;gurwls223;Issue resolved by pull request 41522
[https://github.com/apache/spark/pull/41522];;;",,,,,,,,,,,,,,
Unresolved hint cause query failure,SPARK-44007,13539230,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,blackpig,blackpig,08/Jun/23 09:19,09/Jun/23 06:00,30/Oct/23 17:26,09/Jun/23 06:00,3.3.1,3.4.0,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"After the Resolve Hints Rules are completed, immediately remove unknown Hints to *avoid query errors caused by Unresolved Hints.*

Query error: 
{code:java}
// create t0,t1
CREATE TABLE t0(c0 bigint) USING PARQUET 
CREATE TABLE t1(c1 bigint) USING PARQUET

// query with unknown hint
 with w0 as (select * from t0),
 w1 as (select c0 from w0 group by c0),
 w2 as (select  /*+ userHint(t1) */  c1 from t1 ),
 w3 as (
 select w2.c1, w0.c0, w1.c0
 from w2 
 join w0 on w2.c1 = w0.c0
 join w1 on w2.c1 = w1.c0
 )
 select * from w3;

23/06/08 17:25:23 WARN HintErrorLogger: Unrecognized hint: userHint(t1)
[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `w2`.`c1` cannot be resolved. Did you mean one of the following? [`w2`.`c1`, `w0`.`c0`].; line 8 pos 11;
'WithCTE
:- CTERelationDef 4, false
:  +- SubqueryAlias w0
:     +- Project [c0#0L]
:        +- SubqueryAlias spark_catalog.default.t0
:           +- Relation spark_catalog.default.t0[c0#0L] parquet
:- CTERelationDef 5, false
:  +- SubqueryAlias w1
:     +- Aggregate [c0#0L], [c0#0L]
:        +- SubqueryAlias w0
:           +- CTERelationRef 4, true, [c0#0L]
:- CTERelationDef 6, false
:  +- SubqueryAlias w2
:     +- Project [c1#1L]
:        +- SubqueryAlias spark_catalog.default.t1
:           +- Relation spark_catalog.default.t1[c1#1L] parquet
:- 'CTERelationDef 7, false
:  +- 'SubqueryAlias w3
:     +- 'Project ['w2.c1, 'w0.c0, 'w1.c0]
:        +- 'Join Inner, ('w2.c1 = 'w1.c0)
:           :- Join Inner, (c1#1L = c0#0L)
:           :  :- SubqueryAlias w2
:           :  :  +- CTERelationRef 6, true, [c1#1L]
:           :  +- SubqueryAlias w0
:           :     +- CTERelationRef 4, true, [c0#0L]
:           +- SubqueryAlias w1
:              +- CTERelationRef 5, true, [c0#0L]
+- 'Project [*]
   +- 'SubqueryAlias w3
      +- 'CTERelationRef 7, false


// query without unknown hint
 with w0 as (select * from t0),
 w1 as (select * from w0 group by c0),
 w2 as (select c1 from t1 ),
 w3 as (
 select w2.c1, w0.c0, w1.c0
 from w2 
 join w0 on w2.c1 = w0.c0
 join w1 on w2.c1 = w1.c0
 )
 select * from w3;  
Time taken: 12.666 seconds


{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-08 09:19:52.0,,,,,,,,,,"0|z1ieso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicPartitionDataSingleWriter is being starved by Parquet MemoryManager,SPARK-44003,13539171,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ravwojdyla,ravwojdyla,08/Jun/23 02:47,11/Jun/23 04:00,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,1,,,,,"We have a pyspark job that writes to a partitioned parquet dataset via:

{code:python}
df.write.parquet(
  path=path,
  compression=""snappy"",
  mode=""overwrite"",
  partitionBy=""year"",
)
{code}

In this specific production case we partition by 28 distinct years, so 28 directories, each directory with 200 part files, total of 5.6K files. This particular job runs on a single dedicated and ephemeral VM. We have noticed that most of the time the VM is far from being saturated and the job is very slow. It's not IO or CPU bound. Here's an [annotated VM utilization graph |https://gist.githubusercontent.com/ravwojdyla/e468bace2bc899f86348dee067173270/raw/03cfb383d49ad43adaec2eaa3d9cbf0a3c9b8c0b/VM_util.png]. The blue line is CPU, and turquoise is memory. This graph doesn't show IO, but we have also monitored that, and it also was not saturated. On the labels:
 * {{BQ}}, you can ignore this
 * {{SPARK~1}} spark computes some data
 * {{SPARK~2}} is 1st slow period
 * {{SPARK~3}} is 2nd slow period

We took two 10 minute JFR profiles, those are marked {{P-1}} and {{P-2}} in the graph above. So {{P-1}} is solely in {{SPARK~2}}, and {{P-2}} is partially in {{SPARK~2}} but mostly in {{SPARK~3}}. Here's the [{{P-1}}|https://gist.githubusercontent.com/ravwojdyla/e468bace2bc899f86348dee067173270/raw/98c107ebd28608da55d84d13b3aa6eaf25b3c854/p1.png] profile, and here's [{{P-2}}|https://gist.githubusercontent.com/ravwojdyla/e468bace2bc899f86348dee067173270/raw/98c107ebd28608da55d84d13b3aa6eaf25b3c854/p2.png] profile.

The picture is a bit more clear when we look at the locks, here's the [report|https://gist.githubusercontent.com/ravwojdyla/e468bace2bc899f86348dee067173270/raw/c0f1fb78ac9d5f90a3106b4b43a3a7b27700f66a/locks.png]. We see that the threads were blocked on locks for a total of 20.5h, mostly/specifically on the global {{org.apache.parquet.hadoop.MemoryManager}}, which has two synchronized methods: {{addWriter}} and {{removeWriter}}. From [parquet-mr GH src|https://github.com/apache/parquet-mr/blob/9d80330ae4948787ac0bf4e4b0d990917f106440/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/MemoryManager.java#L77-L98]:

{code:java}
  /**
   * Add a new writer and its memory allocation to the memory manager.
   * @param writer the new created writer
   * @param allocation the requested buffer size
   */
  synchronized void addWriter(InternalParquetRecordWriter<?> writer, Long allocation) {
    Long oldValue = writerList.get(writer);
    if (oldValue == null) {
      writerList.put(writer, allocation);
    } else {
      throw new IllegalArgumentException(""[BUG] The Parquet Memory Manager should not add an "" +
          ""instance of InternalParquetRecordWriter more than once. The Manager already contains "" +
          ""the writer: "" + writer);
    }
    updateAllocation();
  }

  /**
   * Remove the given writer from the memory manager.
   * @param writer the writer that has been closed
   */
  synchronized void removeWriter(InternalParquetRecordWriter<?> writer) {
    writerList.remove(writer);
    if (!writerList.isEmpty()) {
      updateAllocation();
    }
  }
{code}

During the 10 minute profiling session all worker threads were mostly waiting on this lock. The implementation of the writer comes from [Spark src|https://github.com/apache/spark/blob/0ed48feab65f2d86f5dda3e16bd53f2f795f5bc5/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatDataWriter.scala#L339]:

{code:java}
/**
 * Dynamic partition writer with single writer, meaning only one writer is opened at any time for
 * writing. The records to be written are required to be sorted on partition and/or bucket
 * column(s) before writing.
 */
class DynamicPartitionDataSingleWriter(
{code}

It appears that a combination of large number of writers created via Spark's {{DynamicPartitionDataSingleWriter}} and the {{MemoryManager}} synchronization bottleneck drastically reduces the performance by starving the writer threads. We have validated that by removing the partitioning the job is much faster and fully utilizes the VM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jun 08 12:36:25 UTC 2023,,,,,,,,,,"0|z1iefk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/23 02:52;ravwojdyla;And here's the [lock report|https://gist.githubusercontent.com/ravwojdyla/e468bace2bc899f86348dee067173270/raw/c0f1fb78ac9d5f90a3106b4b43a3a7b27700f66a/no_partition.png] for the same job after we have removed the partitioning by year. We can see that now we wait only a total of about 1 minute vs 20h.;;;","08/Jun/23 03:06;ravwojdyla;I believe {{MemoryManager}} has too strict synchronization mechanism. {{MemoryManager}} was introduced in 2014 in [apache/parquet-mr@23db4eb|https://github.com/apache/parquet-mr/commit/23db4eb88aa018da25563586bab322e7c1867ad5]. On the other hand maybe Spark's {{DynamicPartitionDataSingleWriter}} could be a bit more mindful wrt to the {{MemoryManager}}.  What do you think?;;;","08/Jun/23 12:36;ravwojdyla;Another question: is there a strict requirement to have a global/singleton instance of {{MemoryManager}}?

;;;",,,,,,,,,,,,
SparkConnectArtifactStatusesHandler freezes on second request,SPARK-44002,13539145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,07/Jun/23 20:56,07/Jun/23 23:47,30/Oct/23 17:26,07/Jun/23 23:47,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,SparkConnectArtifactStatusesHandler freezes on the second request when a cache exists. Need to free a lock on the block.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jun 07 23:47:21 UTC 2023,,,,,,,,,,"0|z1ie9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/23 23:47;gurwls223;Issue resolved by pull request 41500
[https://github.com/apache/spark/pull/41500];;;",,,,,,,,,,,,,,
Data is still fetched even though result was returned,SPARK-43999,13539108,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kamil_kliczbor,kamil_kliczbor,07/Jun/23 14:25,27/Jun/23 03:01,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,1,,,,,"h2. Short problem description:

I have two tables:
 * tab1 is empty
 * tab6 has milions of records
 * when Spark returns results due to empty database table tab1, it still asks for the tab6 data

When I create the query that uses LEFT JOIN, the results are returned immediately, however under the hood the huge table is requested to return the results anyway.
h2. Repro:
h3. Prepare the MSSQL server database 

1. Install SQLExpress (in my case MSSQL2012, but can be any version) as a named instance SQL2012.
2. Download and install [SSMS|https://learn.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver16] (or any other tool) and run the following Query
{code:sql}
USE [master]
GO

CREATE DATABASE QueueSlots
GO

CREATE LOGIN [spark] WITH PASSWORD=N'spark', DEFAULT_DATABASE=[master], CHECK_EXPIRATION=OFF, CHECK_POLICY=OFF
GO

USE [QueueSlots]
GO

CREATE USER [spark] FOR LOGIN [spark] WITH DEFAULT_SCHEMA=[dbo]
GO
{code}
3. Then create the tables and fill the tab6 with the data:
{code:sql}
CREATE TABLE tab1 (Id INT, Name NVARCHAR(50))
CREATE TABLE tab6 (Id INT, Name NVARCHAR(50))

insert into tab6
select o1.object_id as Id , o1.name as Name
from sys.objects as o1 
cross join sys.objects as o2
cross join sys.objects as o3
cross join sys.objects as o4
-- it might be required to increase the numer of the cross joins to increase the number of the rows, approximately 1 mln is enough - select should take several seconds
{code}
h3. Prepare Spark
 # Download mssql jdbc driver in version 12.2.0
 # Run spark-shell2.cmd with the settings -cp ""<your_path>/lib/sqljdbc/12.2/mssql-jdbc-12.2.0.jre8.jar""

h3. Create temporary views on Spark
{code:java}
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

sqlContext.sql(""""""
CREATE TEMPORARY VIEW tab1
USING org.apache.spark.sql.jdbc
OPTIONS (
  driver 'com.microsoft.sqlserver.jdbc.SQLServerDriver',
  url 'jdbc:sqlserver://;serverName=localhost;instanceName=sql2012;databaseName=QueueSlots;encrypt=true;trustServerCertificate=true;',  
  dbtable 'dbo.Tab1',
  user 'spark',
  password 'spark'
)
"""""")

sqlContext.sql(""""""
CREATE TEMPORARY VIEW tab6
USING org.apache.spark.sql.jdbc
OPTIONS (
  driver 'com.microsoft.sqlserver.jdbc.SQLServerDriver',
  url 'jdbc:sqlserver://;serverName=localhost;instanceName=sql2012;databaseName=QueueSlots;encrypt=true;trustServerCertificate=true;',  
  dbtable 'dbo.Tab6',
  user 'spark',
  password 'spark'
)
"""""")
{code}
h3. Enable SQL Server Profiler tracing
 # Go to SSMS and open Sql Server Profiler (Tools -> Sql Server Profiler). Create new trace to the ""QueueSlots"" database. Use filtering options to see only queries issued for that database (Events Selection tab -> check Show all events and Show all columns, then click Column Filters -> DatabaseName like QueueSlots).
 # Run the trace

h3. Run the query in Spark console
 # Run the following query
{code:java}
sqlContext.sql(""""""
SELECT t1.Id, t1.Name, t6.Name
  FROM tab1 as t1
  LEFT OUTER JOIN tab6 AS t6 ON t6.Id = t1.Id
"""""").show
{code}

The results are returned immediately as:
{code:java}
+---+----+----+
| Id|Name|Name|
+---+----+----+
+---+----+----+

[Stage 63:>                                                         (0 + 1) / 1]
{code}
h3. {color:#00875a}Expected{color}

As the results are returned immediately for empty table, another sources are not queried.
h3. {color:#de350b}Given:{color}

The table6 is requested to return the data even though it is not being used and it is CPU and IO consuming operation.

 

 ",Production,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/23 14:32;kamil_kliczbor;Profiler.PNG;https://issues.apache.org/jira/secure/attachment/13058857/Profiler.PNG",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 27 03:01:19 UTC 2023,,,,,,,,,,"0|z1ie1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/23 14:36;bysza;I'm experiencing the same issue. Outstanding queries to temporary (external) views are not cancelled even thou the main query result was already returned.;;;","16/Jun/23 08:52;kamil_kliczbor;I checked against the version 3.4.0 and the problem still exists.;;;","26/Jun/23 07:33;fanjia;The reason are when AQE on, the small empty table alway return faster when join on two table. When get the left result, the AQE optimizer will use stats to reOptimize plan, so the right table result will be unnecessary. The result return, but the right table query stage will not be cancel or force finish at now. ;;;","26/Jun/23 08:23;kamil_kliczbor;[~fanjia] , thanks for the feedback. When do you plan to fix this behaviour?;;;","26/Jun/23 08:33;fanjia;> When do you plan to fix this behaviour?

 

I'm doing this.;;;","27/Jun/23 03:01;fanjia;https://github.com/apache/spark/pull/41755;;;",,,,,,,,,
Use the value of spark.eventLog.compression.codec set by user when write compact file,SPARK-43991,13539015,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,shuyouZZ,shuyouZZ,07/Jun/23 07:27,07/Oct/23 00:17,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,Web UI,,,,0,pull-request-available,,,,"Currently, if enable rolling log in SHS, only {{originalFilePath}} is used to determine the path of compact file.
{code:java}
override val logPath: String = originalFilePath.toUri.toString + EventLogFileWriter.COMPACTED
{code}
If the user set {{spark.eventLog.compression.codec}} in sparkConf and it is different from the default value of spark conf, when the log compact logic is triggered, the old event log file will be compacted and use the default value of spark conf.
{code:java}
protected val compressionCodec =
    if (shouldCompress) {
      Some(CompressionCodec.createCodec(sparkConf, sparkConf.get(EVENT_LOG_COMPRESSION_CODEC)))
    } else {
      None
    }

private[history] val compressionCodecName = compressionCodec.map { c =>
    CompressionCodec.getShortName(c.getClass.getName)
  }
{code}
However, The compression codec used by EventLogFileReader to read log is split from the log path, this will lead to EventLogFileReader can not read the compacted log file normally.
{code:java}
def codecName(log: Path): Option[String] = {
    // Compression codec is encoded as an extension, e.g. app_123.lzf
    // Since we sanitize the app ID to not include periods, it is safe to split on it
    val logName = log.getName.stripSuffix(COMPACTED).stripSuffix(IN_PROGRESS)
    logName.split(""\\."").tail.lastOption
  }
{code}
So we should override the {{shouldCompress}} and {{compressionCodec}} variable in class {{{}CompactedEventLogFileWriter{}}}, use the compression codec set by the user.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 04 16:48:26 UTC 2023,,,,,,,,,,"0|z1idgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/23 16:48;hudson;User 'shuyouZZ' has created a pull request for this issue:
https://github.com/apache/spark/pull/41491;;;",,,,,,,,,,,,,,
Spark protobuf enums.as.ints raises exception on repeated enum types,SPARK-43985,13538947,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,justaparth,justaparth,justaparth,06/Jun/23 15:16,07/Jun/23 03:06,30/Oct/23 17:26,07/Jun/23 03:06,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Protobuf,,,,,0,,,,,"For repeated enum types, the `enums.as.ints` being enabled currently raises an exception when trying to deserialize repeated enum fields. We should fix this behavior so that repeated enum fields work correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jun 07 03:06:29 UTC 2023,,,,,,,,,,"0|z1id1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/23 03:06;gurwls223;Issue resolved by pull request 41481
[https://github.com/apache/spark/pull/41481];;;",,,,,,,,,,,,,,
bad case of connect-jvm-client-mima-check,SPARK-43977,13538852,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,06/Jun/23 03:46,06/Jun/23 14:46,30/Oct/23 17:26,06/Jun/23 14:46,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Tests,,,,,0,,,,,"run 

```

build/sbt ""protobuf/clean""

dev/connect-jvm-client-mima-check

```
{code:java}
Using SPARK_LOCAL_IP=localhost
Using SPARK_LOCAL_IP=localhost
Do connect-client-jvm module mima check ...
Failed to find the jar: spark-protobuf-assembly(.*).jar or spark-protobuf(.*)3.5.0-SNAPSHOT.jar inside folder: /Users/yangjie01/SourceCode/git/spark-mine-sbt/connector/protobuf/target. This file can be generated by similar to the following command: build/sbt package|assembly
finish connect-client-jvm module mima check ...
connect-client-jvm module mima check passed.
 {code}
The check result is wrong,  there are both error messages and checks successful

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 06 14:46:34 UTC 2023,,,,,,,,,,"0|z1icgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/23 03:50;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/41473;;;","06/Jun/23 14:46;LuciferYang;Issue resolved by pull request 41473
[https://github.com/apache/spark/pull/41473];;;",,,,,,,,,,,,,
Handle the case where modifiedConfigs doesn't exist in event logs,SPARK-43976,13538850,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,06/Jun/23 03:08,06/Jun/23 16:35,30/Oct/23 17:26,06/Jun/23 16:35,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 06 16:35:40 UTC 2023,,,,,,,,,,"0|z1icg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/23 16:35;dongjoon;Issue resolved by pull request 41472
[https://github.com/apache/spark/pull/41472];;;",,,,,,,,,,,,,,
Structured Streaming UI should display failed queries correctly,SPARK-43973,13538832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,05/Jun/23 22:39,06/Jun/23 05:08,30/Oct/23 17:26,06/Jun/23 05:08,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Web UI,,,,,0,,,,,"The Structured Streaming UI is designed to be able to show a query's status (active/finished/failed) and if failed, the error message.
Due to a bug in the implementation, the error message in {{QueryTerminatedEvent}} isn't being tracked by the UI data, so in turn the UI always shows failed queries as ""finished"".

Example:
{code:scala}
implicit val ctx = spark.sqlContext
import org.apache.spark.sql.execution.streaming.MemoryStream

spark.conf.set(""spark.sql.ansi.enabled"", ""true"")

val inputData = MemoryStream[(Int, Int)]

val df = inputData.toDF().selectExpr(""_1 / _2 as a"")

inputData.addData((1, 2), (3, 4), (5, 6), (7, 0))
val testQuery = df.writeStream.format(""memory"").queryName(""kristest"").outputMode(""append"").start
testQuery.processAllAvailable()
{code}

Here we intentionally fail a query, but the Spark UI's Structured Streaming tab would show this as ""FINISHED"" without any errors, which is wrong.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 06 05:08:03 UTC 2023,,,,,,,,,,"0|z1icc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/23 05:08;Gengliang.Wang;Issue resolved by pull request 41468
[https://github.com/apache/spark/pull/41468];;;",,,,,,,,,,,,,,
Tests never succeed on pyspark 3.4.0 (work OK on pyspark 3.3.2),SPARK-43972,13538825,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jamet,jamet,05/Jun/23 20:32,19/Jun/23 17:38,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"I have a project that uses pyspark. The tests have always run fine on pyspark versions prior to pyspark 3.4.0 but now fail on that version (which was released on 2023-04-13).

My project is configured to use the latest available version of pyspark:
{code:json}
dependencies = [
  ""pyspark"",
  ""faker""
]
{code}
[https://github.com/jamiekt/jstark/blob/c1629cee4e4b8fb0b4471f6fc2941f1b0a99a4bf/pyproject.toml#L26-L29]

The tests are run using GitHub Actions. An example of the failing tests is at [https://github.com/jamiekt/jstark/actions/runs/4977164046], you can see there that the tests are run upon various combinations of OS & python version, they are all cancelled after running for over 5 hours.

If I [pin the version of pyspark to 3.3.2|https://github.com/jamiekt/jstark/commit/5fd7115d3719a7d6ef2547e8e35feb3ed76ee99f] then the tests all succeed in ~10 minutes, see [https://github.com/jamiekt/jstark/actions/runs/5061332947] for such a successful run.
----
This can be reproduced by cloning the repository and running only one test. The project uses hatch for managing environments and dependencies so you would need that installed ({{{}pipx install hatch{}}}/{{{}brew install hatch{}}}). I have reproduced the problem on python3.10.

Reproduce the problem by running these commands:
{code:bash}
# force use of python3.10
export HATCH_PYTHON=/path/to/python3.10
git clone https://github.com/jamiekt/jstark.git
cd jstark
# following command will create a virtualenv & install all dependencies, including pyspark 3.4.0
hatch run pytest -k test_basketweeks_by_product_and_customer
{code}
On my machine this never completes. I need to CTRL+C to crash out of it. I consider this to be equivalent behaviour to the tests that fail in the GitHub Actions pipeline after 6 hours.

Now let's checkout the branch which pins pyspark to 3.3.2 and run the same thing (the hatch environment will get rebuilt with pyspark 3.3.2)
{code:bash}
git checkout try-pyspark3-3-2
hatch run pytest -k test_basketweeks_by_product_and_customer
{code}
this time it succeeds in ~31seconds:
{code:bash}
➜  hatch run pytest -k test_basketweeks_by_product_and_customer
================================================================================================== test session starts ======================================================================================================================
platform darwin -- Python 3.10.10, pytest-7.3.1, pluggy-1.0.0
rootdir: /private/tmp/jstark
plugins: Faker-18.9.0, cov-4.0.0
collected 79 items / 78 deselected / 1 selected

tests/test_grocery_retailer_feature_generator.py .                                                                                                                                                                                                                       [100%]

================================================================= 1 passed, 78 deselected in 31.30s =================
{code}
That particular test constructs a very very complex pyspark dataframe which I suspect might be contributing to the problem, however the issue here is that it works on pyspark 3.3.2 but not on pyspark 3.4.0.","Sorry, not sure what I'm supposed to put in this section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,,,,2023-06-05 20:32:34.0,,,,,,,,,,"0|z1icao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Improve error messages: CANNOT_DECODE_URL, CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE, CANNOT_PARSE_DECIMAL, CANNOT_READ_FILE_FOOTER, CANNOT_RECOGNIZE_HIVE_TYPE.",SPARK-43962,13538726,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,05/Jun/23 04:34,06/Jun/23 07:25,30/Oct/23 17:26,06/Jun/23 07:25,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,Improve error message for usability.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 06 07:25:36 UTC 2023,,,,,,,,,,"0|z1iboo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/23 07:25;maxgekk;Issue resolved by pull request 41455
[https://github.com/apache/spark/pull/41455];;;",,,,,,,,,,,,,,
DataFrameConversionTestsMixin is not tested properly.,SPARK-43960,13538693,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,04/Jun/23 09:51,04/Jun/23 23:34,30/Oct/23 17:26,04/Jun/23 23:34,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Pandas API on Spark,Tests,,,,0,,,,,DataFrameConversionTestsMixin is not tested properly. We should fix it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jun 04 23:34:20 UTC 2023,,,,,,,,,,"0|z1ibhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/23 23:34;gurwls223;Issue resolved by pull request 41450
[https://github.com/apache/spark/pull/41450];;;",,,,,,,,,,,,,,
"Fix the bug doesn't display column's sql for Percentile[Cont|Disc]",SPARK-43956,13538665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,03/Jun/23 05:13,04/Jun/23 00:31,30/Oct/23 17:26,03/Jun/23 05:20,3.3.0,,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,,,0,,,,,"Last year, I committed Percentile[Cont|Disc] functions for Spark SQL.
Recently, I found the sql method of Percentile[Cont|Disc] doesn't display column's sql suitably.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jun 04 00:31:30 UTC 2023,,,,,,,,,,"0|z1ibb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/23 06:58;beliefer;resolved by https://github.com/apache/spark/pull/41436;;;","04/Jun/23 00:31;dongjoon;This is backported to branch-3.3 via https://github.com/apache/spark/pull/41446;;;",,,,,,,,,,,,,
RocksDB state store can become corrupt on task retries,SPARK-43951,13538603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kimahriman,kimahriman,02/Jun/23 13:07,02/Jun/23 13:09,30/Oct/23 17:26,02/Jun/23 13:09,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"A couple of our streaming jobs have failed since upgrading to Spark 3.4 with an error such as:

org.rocksdb.RocksDBException: Mismatch in unique ID on table file ###. Expected: [###,###} Actual\{###,###} in file ..../MANIFEST-####

This is due to the change from [https://github.com/facebook/rocksdb/commit/6de7081cf37169989e289a4801187097f0c50fae] that enabled unique ID checks by default, and I finally tracked down the exact sequence of steps that leads to this failure in the way RocksDB state store is used.
 # A task fails after uploading the checkpoint to HDFS. Lets say it uploaded 11.zip to version 11 of the table, but the task failed before it could finish after successfully uploading the checkpoint.
 # The same task is retried and goes back to load version 10 of the table as expected.
 # Cleanup/maintenance is called for this partition, which looks in HDFS for persisted versions and sees up through version 11 since that zip file was successfully uploaded on the previous task.
 # As part of resolving what SST files are part of each table version, versionToRocksDBFiles.put(version, newResolvedFiles) is called for version 11 with its SST files that were uploaded in the first failed task.
 # The second attempt at the task commits and goes to sync its checkpoint to HDFS.
 # versionToRocksDBFiles contains the SST files to upload from step 4, and these files are considered ""the same"" as what's in the local working dir because the name and file size match.
 # No SST files are uploaded because they matched above, but in reality the unique ID inside the SST files is different (presumably this is just randomly generated and inserted into each SST file?), it just doesn't affect the size.
 # A new METADATA file is uploaded which has the new unique IDs listed inside.
 # When version 11 of the table is read during the next batch, the unique IDs in the METADATA file don't match the unique IDS in the SST files, which causes the exception.

 

This is basically a ticking time bomb for anyone using RocksDB. Thoughts on possible fixes would be:
 * Disable unique ID verification. I don't currently see a binding for this in the RocksDB java wrapper, so that would probably have to be added first.
 * Disable checking if files are already uploaded with the same size, and just always upload SST files no matter what.
 * Update the ""same file"" check to also be able to do some kind of CRC comparison or something like that.
 * Update the mainteance/cleanup to not update the versionToRocksDBFiles map.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 02 13:09:07 UTC 2023,,,,,,,,,,"0|z1iaxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/23 13:09;kimahriman;Of course as soon as I finish figuring all this out I found https://github.com/apache/spark/pull/41089;;;",,,,,,,,,,,,,,
Upgrade Cloudpickle to 2.2.1,SPARK-43949,13538540,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,02/Jun/23 08:16,02/Jun/23 10:40,30/Oct/23 17:26,02/Jun/23 10:40,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,PySpark,,,,,0,,,,,Cloudpickle 2.2.1 has a fix for named tuple issue (https://github.com/cloudpipe/cloudpickle/issues/460). PySpark relies on namedtuple heavily especially for RDD. We should upgrade and fix it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 02 10:40:53 UTC 2023,,,,,,,,,,"0|z1iajc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/23 10:40;gurwls223;Fixed in https://github.com/apache/spark/pull/41433;;;",,,,,,,,,,,,,,
Incorrect SparkException when missing config in resources in Stage-Level Scheduling,SPARK-43947,13538530,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,jlaskowski,jlaskowski,02/Jun/23 07:31,02/Jun/23 07:32,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Scheduler,,,,,0,,,,,"[ResourceUtils.listResourceIds|https://github.com/apache/spark/blob/807abf9c53ee8c1c7ef69646ebd8a266f60d5580/core/src/main/scala/org/apache/spark/resource/ResourceUtils.scala#L152-L155] can throw an exception for any missing config, not just `amount`.

{code:scala}
      val index = key.indexOf('.')
      if (index < 0) {
        throw new SparkException(s""You must specify an amount config for resource: $key "" +
          s""config: $componentName.$RESOURCE_PREFIX.$key"")
      }
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-02 07:31:56.0,,,,,,,,,,"0|z1iah4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add rule to remove unused CTEDef,SPARK-43946,13538529,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jinhai-cloud,jinhai-cloud,02/Jun/23 07:24,02/Jun/23 07:56,30/Oct/23 17:26,,3.2.4,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"{code:java}
// code placeholder
with t1 as (
  select rand() c3
),
t2 as (select * from t1)
select c3 from t1 where c3 > 0 {code}
{code:java}
// code placeholder
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InlineCTE ===
 WithCTE                                               WithCTE
 :- CTERelationDef 0, false                            :- CTERelationDef 0, false
 :  +- Project [rand(3418873542988342437) AS c3#236]   :  +- Project [rand(3418873542988342437) AS c3#236]
 :     +- OneRowRelation                               :     +- OneRowRelation
!:- CTERelationDef 1, false                            +- Project [c3#236]
!:  +- Project [c3#236]                                   +- Filter (c3#236 > cast(0 as double))
!:     +- CTERelationRef 0, true, [c3#236]                   +- CTERelationRef 0, true, [c3#236]
!+- Project [c3#236]                                   
!   +- Filter (c3#236 > cast(0 as double))             
!      +- CTERelationRef 0, true, [c3#236]             
 {code}
When the above query applies the inlineCTE rule, inline is not possible because the refCount of CTERelationDef 0 is equal to 2.

However, according to the optimized logicalplan, the plan can be further optimized because the refCount of CTERelationDef 0 is equal to 1.

Therefore, we can add the rule *RemoveUnusedCTEDef* to delete the unreferenced CTERelationDef to prevent the refCount from being miscalculated
{code:java}
// code placeholder
Project [c3#236]
+- Filter (c3#236 > cast(0 as double))
   +- Project [rand(-7871530451581327544) AS c3#236]
      +- OneRowRelation {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 02 07:56:06 UTC 2023,,,,,,,,,,"0|z1iagw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/23 07:56;jinhai-cloud;[~cloud_fan], Can you take a look at this issue for me?;;;",,,,,,,,,,,,,,
Fix bug for `SQLQueryTestSuite` when run on local env,SPARK-43945,13538527,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,02/Jun/23 07:02,02/Jun/23 16:36,30/Oct/23 17:26,02/Jun/23 16:36,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 02 16:36:24 UTC 2023,,,,,,,,,,"0|z1iagg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/23 07:05;hudson;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41431;;;","02/Jun/23 16:36;maxgekk;Issue resolved by pull request 41431
[https://github.com/apache/spark/pull/41431];;;",,,,,,,,,,,,,
Fix bug for toSQLId,SPARK-43936,13538498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,02/Jun/23 02:31,02/Jun/23 05:38,30/Oct/23 17:26,02/Jun/23 05:38,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"After SPARK-43910, {{__auto_generated_subquery_name}} from ids in errors should remove, but when the type of {{parts}} is ArrayBuffer, match will fail. causing unexpected behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 02 05:38:58 UTC 2023,,,,,,,,,,"0|z1iaa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/23 02:39;panbingkun;h4. Hi，[~maxgekk] 

Do you have time to help review this PR?;;;","02/Jun/23 04:44;snoot;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41430;;;","02/Jun/23 05:38;LuciferYang;Issue resolved by pull request 41430
[https://github.com/apache/spark/pull/41430];;;",,,,,,,,,,,,
Cannot CREATE VIEW despite columns explicitly aliased ,SPARK-43918,13538450,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,srielau,srielau,01/Jun/23 17:14,01/Jun/23 17:14,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"spark.sql(""CREATE VIEW v(c) AS SELECT b AS c FROM (SELECT (SELECT 1)) AS T(b)"").show()

org.apache.spark.sql.AnalysisException: Not allowed to create a permanent view `spark_catalog`.`default`.`v` without explicitly assigning an alias for expression c.

The problem seems to be the scalar subquery (SELECT 1) not being aliased.
But that shouldn't matter. ""AS X should be the backstop. In fact, AS T(b) should have been sufficient.
Not to speak of the fact that the column is named c in the view header. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-01 17:14:29.0,,,,,,,,,,"0|z1i9zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect SparkException for Stage-Level Scheduling in local mode,SPARK-43912,13538375,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,jlaskowski,jlaskowski,01/Jun/23 08:55,01/Jun/23 08:55,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Scheduler,,,,,0,,,,,"While in `local[*]` mode, the following `SparkException` is thrown:

```text
org.apache.spark.SparkException: TaskResourceProfiles are only supported for Standalone cluster for now when dynamic allocation is disabled.
  at org.apache.spark.resource.ResourceProfileManager.isSupported(ResourceProfileManager.scala:71)
  at org.apache.spark.resource.ResourceProfileManager.addResourceProfile(ResourceProfileManager.scala:126)
  at org.apache.spark.rdd.RDD.withResources(RDD.scala:1802)
  ... 42 elided
```

This happens for the following snippet:

```scala
val rdd = sc.range(0, 9)

import org.apache.spark.resource.ResourceProfileBuilder
val rpb = new ResourceProfileBuilder
val rp1 = rpb.build()

rdd.withResources(rp1)
```","```text
scala> println(spark.version)
3.4.0

scala> println(sc.master)
local[*]
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-06-01 08:55:35.0,,,,,,,,,,"0|z1i9jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add check to see if column name is legal for __dir__,SPARK-43889,13538161,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jarviscao,jarviscao,jarviscao,30/May/23 20:55,31/May/23 07:00,30/Oct/23 17:26,31/May/23 07:00,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,PySpark,,,,,0,,,,,"In SPARK-43270, we add a support for the autocomplete suggest for {{df.|}} , which also suggest column_name for dataframe. 

However, we found out later that some dataframe can have column which has illegal variable name (e.g: name?1, name 1, 2name etc.) These variable name should be filtered out and this will be consistent with pandas behavior now",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 31 07:00:03 UTC 2023,,,,,,,,,,"0|z1i87s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/23 22:16;jarviscao;Submit a pr about this https://github.com/apache/spark/pull/41393;;;","31/May/23 03:47;snoot;User 'BeishaoCao-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/41393;;;","31/May/23 07:00;gurwls223;Issue resolved by pull request 41393
[https://github.com/apache/spark/pull/41393];;;",,,,,,,,,,,,
Reduce function doesn't infer Decimal type precision correctly,SPARK-43887,13538146,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jrosseventus,jrosseventus,30/May/23 18:47,30/May/23 18:49,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"A working reduce function example

 
{code:java}
select reduce(arrays, array(0, 0), (acc,x) -> array(get(acc,0)+get(x,0), get(acc,1)+get(x,1))) from (select array(array(1, 2), array(1000, 2000)) as arrays){code}
 

produces 
{code:java}
[1001,2002]{code}
 

But, if you introduce decimals, it breaks:
{code:java}
select reduce(arrays, array(0, 0), (acc,x) -> array(get(acc,0)+get(x,0), get(acc,1)+get(x,1))) from (select array(array(1, 2), array(1000.00, 2000)) as arrays); {code}
produces:
{code:java}
[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve ""reduce(arrays, array(0, 0), lambdafunction(array((get(namedlambdavariable(), 0) + get(namedlambdavariable(), 0)), (get(namedlambdavariable(), 1) + get(namedlambdavariable(), 1))), namedlambdavariable(), namedlambdavariable()), lambdafunction(namedlambdavariable(), namedlambdavariable()))"" due to data type mismatch: Parameter 3 requires the ""ARRAY<INT>"" type, however ""lambdafunction(array((get(namedlambdavariable(), 0) + get(namedlambdavariable(), 0)), (get(namedlambdavariable(), 1) + get(namedlambdavariable(), 1))), namedlambdavariable(), namedlambdavariable())"" has the type ""ARRAY<DECIMAL(13,2)>"".; line 1 pos 0; Project [reduce(arrays#6, array(0, 0), lambdafunction(array((cast(get(lambda acc#7, 0) as decimal(10,0)) + get(lambda x#8, 0)), (cast(get(lambda acc#7, 1) as decimal(10,0)) + get(lambda x#8, 1))), lambda acc#7, lambda x#8, false), lambdafunction(lambda id#9, lambda id#9, false)) AS reduce(arrays, array(0, 0), lambdafunction(array((get(namedlambdavariable(), 0) + get(namedlambdavariable(), 0)), (get(namedlambdavariable(), 1) + get(namedlambdavariable(), 1))), namedlambdavariable(), namedlambdavariable()), lambdafunction(namedlambdavariable(), namedlambdavariable()))#10] +- SubqueryAlias __auto_generated_subquery_name    +- Project [array(cast(array(1, 2) as array<decimal(12,2)>), array(cast(1000.00 as decimal(12,2)), cast(2000 as decimal(12,2)))) AS arrays#6]       +- OneRowRelation
 {code}
We've reproduced the issue even when trying to specify the decimal cast of all inputs in the array and the types in the accumulator. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-30 18:47:37.0,,,,,,,,,,"0|z1i84g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS Command Nodes Prevent Some Optimizer Rules From Running,SPARK-43883,13538138,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tedjenks,tedjenks,30/May/23 16:49,30/May/23 17:35,30/Oct/23 17:26,,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"The changes introduced to resolve SPARK-41713 in [https://github.com/apache/spark/pull/39220] modified the CTAS commands from having a `DataWritingCommand` trait to a `LeafRunnableCommand` trait. The `DataWritingCommand` trait extends `UnaryCommand`, and has children set to the value of query in the CTAS command. This means that when `transform` is called to traverse the tree with the CTAS command at the root, the entire query is traversed. `LeafRunnableCommand` has a `LeafLike` trait which explicitly sets the value of children to `Nil`. This means that when `transform` is called on the command, no children are found and the query is unaffected by the rule.

In practice, this means that optimizer rules that rely on `transform` (such as `BooleanSimplification`) to traverse the tree do not work with a CTAS. 

This can be demonstrated with a simple query in spark-shell. Without the CTAS we can run a command with an easily simplified boolean expression (`id == 9 && id == 9`) and see it gets optimized out:

!Working - No Create Table.png|width=883,height=342!

With a CTAS, the optimisation does not get applied (as we can see from the `AND` still present in the optimized and physical plans):

!Not Working - Create Table.png|width=885,height=524!

This works in 3.2.0 which had the old CTAS implementation:

!Working - 3.2.0.png|width=885,height=345!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/23 16:49;tedjenks;Not Working - Create Table.png;https://issues.apache.org/jira/secure/attachment/13058651/Not+Working+-+Create+Table.png","30/May/23 16:49;tedjenks;Working - 3.2.0.png;https://issues.apache.org/jira/secure/attachment/13058652/Working+-+3.2.0.png","30/May/23 16:49;tedjenks;Working - No Create Table.png;https://issues.apache.org/jira/secure/attachment/13058653/Working+-+No+Create+Table.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 30 17:34:36 UTC 2023,,,,,,,,,,"0|z1i82o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/23 17:34;tedjenks;Working on a fix in https://github.com/apache/spark/pull/41386;;;",,,,,,,,,,,,,,
spark cluster deploy mode cannot initialize metastore java.sql.SQLException: No suitable driver found for jdbc:mysql,SPARK-43865,13537990,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,pin_zhang,pin_zhang,29/May/23 12:19,01/Jun/23 07:33,30/Oct/23 17:26,01/Jun/23 07:33,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"1. Test with JDK 11 + SPARK340
object BugHS {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.set(""javax.jdo.option.ConnectionURL"",""jdbc:mysql://mysql:3306/hive_ms_spark3?useSSL=false"")
    conf.set(""javax.jdo.option.ConnectionDriverName"",""com.mysql.jdbc.Driver"")
    conf.set(""javax.jdo.option.ConnectionUserName"",""**"")
    conf.set(""javax.jdo.option.ConnectionPassword"",""**"")
    conf.set(""spark.sql.hive.thriftServer.singleSession"",""false"")
    conf.set(""spark.sql.warehouse.dir"",""hdfs://hadoop/warehouse_spark3"")
    import org.apache.spark.sql.SparkSession
    val spark = SparkSession
      .builder()
      .appName(""Test"").config(conf).enableHiveSupport()
      .getOrCreate()
    HiveThriftServer2.startWithContext(spark.sqlContext)
    spark.sql(""create table IF NOT EXISTS test2 (id int) USING parquet"")
  }
}
2. Submit in cluster mode
   a. spark_config.properties 
            spark.master=spark://master:6066
            spark.jars=hdfs://hadoop/tmp/test_bug/mysql-connector-java-5.1.47.jar
            spark.master.rest.enabled=true
   b. spark-submit2.cmd --deploy-mode cluster  --properties-file spark_config.properties  --class com.test.BugHS ""hdfs://hadoop/tmp/test_bug/bug_classloader.jar""  

3.  Meet ""No suitable driver found"" exception, caused by classloader is different for driver in spark.jars and metastore jar in JDK 11
java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://mysql:3306/hive_ms_spark3?useSSL=false, username = root. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
java.sql.SQLException: No suitable driver found for jdbc:mysql://mysql:3306/hive_ms_spark3?useSSL=false
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:702)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)
	at jdk.internal.reflect.GeneratedConstructorAccessor77.newInstance(Unknown Source)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jun 01 06:36:31 UTC 2023,,,,,,,,,,"0|z1i760:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/23 02:24;yumwang;Could you add the [JDBC driver for MySQL|https://mvnrepository.com/artifact/com.mysql/mysql-connector-j] to classpath?;;;","01/Jun/23 06:36;pin_zhang;It's not convinient to upload jar to all worker nodes, because the jars is dymanically issued accoding to configuration.;;;",,,,,,,,,,,,,
History Server delete event log of running Spark applicaiton,SPARK-43861,13537969,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,shengkui,shengkui,29/May/23 06:53,29/May/23 06:53,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"When enable Spark history cleaner:

{noformat}
spark.history.fs.cleaner.enabled=true
spark.history.fs.cleaner.interval=1d
spark.history.fs.cleaner.maxAge=7d
spark.history.fs.cleaner.maxNum=10
{noformat}

Spark will delete history for running Spark applications, following is some message picked from the spark log:

{noformat}
153 23/05/26 14:19:43 INFO FsHistoryProvider: Parsing /tmp/spark-events/local-1685081895370 to re-build UI...
154 23/05/26 14:19:43 INFO FsHistoryProvider: Finished parsing /tmp/spark-events/local-1685081895370
155 23/05/28 13:09:53 INFO FsHistoryProvider: Deleting expired event log for app-20230524141002-0000
156 23/05/29 13:09:53 INFO FsHistoryProvider: Deleting expired event log for local-1684999095807
157 23/05/29 13:09:53 INFO FsHistoryProvider: Deleting expired event log for app-20230524171730-0000.inprogress
158 23/05/29 13:34:24 INFO ApplicationCache: Failed to load application attempt app-20230524171730-0000/
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-29 06:53:48.0,,,,,,,,,,"0|z1i71c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web UI WholeStageCodegen duration is 0 ms,SPARK-43848,13537951,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,luckywind,luckywind,29/May/23 02:52,29/May/23 03:06,30/Oct/23 17:26,,3.2.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"In the Web UI when I’m running a query with limit clause, the duration of WholeStageCodegen operator is always 0 ms.
the corresponding feature is SPARK-13916, commitid: 76958d820f57d23e3cbb5b7205c680a5daea0499 . durationMs update only when we iterate the last row of partition, but when we only iterate a few rows, the duration will always be 0 ms

below  code will repetition the issue:

spark.sql(""use tpcds1g"")
spark.sql(""""""
select i_item_sk from item
limit 100
"""""").collect",spark local mode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/23 02:53;luckywind;0ms.jpg;https://issues.apache.org/jira/secure/attachment/13058596/0ms.jpg",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,,,,2023-05-29 02:52:15.0,,,,,,,,,,"0|z1i6xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saving an AVRO file with Scala 2.13 results in NoClassDefFoundError,SPARK-43843,13537937,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,bersprockets,bersprockets,28/May/23 21:37,29/May/23 00:47,30/Oct/23 17:26,29/May/23 00:47,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"I launched spark-shell as so:
{noformat}
bin/spark-shell --driver-memory 8g --jars `find . -name ""spark-avro*.jar"" | grep -v test | head -1`
{noformat}
I got the below error trying to create an AVRO file:
{noformat}
scala> val df = Seq((1, 2), (3, 4)).toDF(""a"", ""b"")
val df = Seq((1, 2), (3, 4)).toDF(""a"", ""b"")
val df: org.apache.spark.sql.DataFrame = [a: int, b: int]

scala> df.write.mode(""overwrite"").format(""avro"").save(""avro_file"")
df.write.mode(""overwrite"").format(""avro"").save(""avro_file"")
java.lang.NoClassDefFoundError: scala/collection/immutable/StringOps
  at org.apache.spark.sql.avro.AvroFileFormat.supportFieldName(AvroFileFormat.scala:160)
  at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$checkFieldNames$1(DataSourceUtils.scala:75)
  at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$checkFieldNames$1$adapted(DataSourceUtils.scala:74)
  at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
  at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:105)
  at org.apache.spark.sql.execution.datasources.DataSourceUtils$.checkFieldNames(DataSourceUtils.scala:74)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:120)
...
scala> 
{noformat}","Scala version 2.13.8 (Java HotSpot(TM) 64-Bit Server VM, Java 11.0.12)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon May 29 00:47:31 UTC 2023,,,,,,,,,,"0|z1i6u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/May/23 00:47;bersprockets;Nevermind, I had an old {{spark-avro_2.12-3.5.0-SNAPSHOT.jar}} laying about in my {{work}} directory which the find in my {{--jars}} value found first.;;;",,,,,,,,,,,,,,
Upgrade `gcs-connector` to 2.2.14,SPARK-43842,13537936,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,28/May/23 21:25,29/May/23 00:19,30/Oct/23 17:26,29/May/23 00:19,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon May 29 00:19:35 UTC 2023,,,,,,,,,,"0|z1i6u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/May/23 00:19;gurwls223;Issue resolved by pull request 41352
[https://github.com/apache/spark/pull/41352];;;",,,,,,,,,,,,,,
Non-existent column in projection of full outer join with USING results in StringIndexOutOfBoundsException,SPARK-43841,13537934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,28/May/23 18:48,29/May/23 07:05,30/Oct/23 17:26,29/May/23 07:05,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"The following query throws a {{StringIndexOutOfBoundsException}}:
{noformat}
with v1 as (
 select * from values (1, 2) as (c1, c2)
),
v2 as (
  select * from values (2, 3) as (c1, c2)
)
select v1.c1, v1.c2, v2.c1, v2.c2, b
from v1
full outer join v2
using (c1);
{noformat}
The query should fail anyway, since {{b}} refers to a non-existent column. But it should fail with a helpful error message, not with a {{StringIndexOutOfBoundsException}}.

The issue seems to be in {{StringUtils#orderSuggestedIdentifiersBySimilarity}}. {{orderSuggestedIdentifiersBySimilarity}} assumes that a list of candidate attributes with a mix of prefixes will never have an attribute name with an empty prefix. But in this case it does ({{c1}} from the {{coalesce}} has no prefix, since it is not associated with any relation or subquery):
{noformat}
+- 'Project [c1#5, c2#6, c1#7, c2#8, 'b]
   +- Project [coalesce(c1#5, c1#7) AS c1#9, c2#6, c2#8] <== c1#9 has no prefix, unlike c2#6 (v1.c2) or c2#8 (v2.c2)
      +- Join FullOuter, (c1#5 = c1#7)
         :- SubqueryAlias v1
         :  +- CTERelationRef 0, true, [c1#5, c2#6]
         +- SubqueryAlias v2
            +- CTERelationRef 1, true, [c1#7, c2#8]
{noformat}
Because of this, {{orderSuggestedIdentifiersBySimilarity}} returns a sorted list of suggestions like this:
{noformat}
ArrayBuffer(.c1, v1.c2, v2.c2)
{noformat}
{{UnresolvedAttribute.parseAttributeName}} chokes on an attribute name that starts with a namespace separator ('.').
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon May 29 07:05:07 UTC 2023,,,,,,,,,,"0|z1i6tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/May/23 23:38;bersprockets;PR at https://github.com/apache/spark/pull/41353;;;","29/May/23 07:05;maxgekk;Issue resolved by pull request 41353
[https://github.com/apache/spark/pull/41353];;;",,,,,,,,,,,,,
Subquery on single table with having clause can't be optimized,SPARK-43838,13537913,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,28/May/23 05:28,20/Jul/23 05:08,30/Oct/23 17:26,20/Jul/23 02:17,3.4.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,,,,,"Eg:
{code:java}
sql(""create view t(c1, c2) as values (0, 1), (0, 2), (1, 2)"")

sql(""select c1, c2, (select count(*) cnt from t t2 where t1.c1 = t2.c1 "" +
""having cnt = 0) from t t1"").show() {code}
The error will throw:
{code:java}
[PLAN_VALIDATION_FAILED_RULE_IN_BATCH] Rule org.apache.spark.sql.catalyst.optimizer.RewriteCorrelatedScalarSubquery in batch Operator Optimization before Inferring Filters generated an invalid plan: The plan becomes unresolved: 'Project [toprettystring(c1#224, Some(America/Los_Angeles)) AS toprettystring(c1)#238, toprettystring(c2#225, Some(America/Los_Angeles)) AS toprettystring(c2)#239, toprettystring(cnt#246L, Some(America/Los_Angeles)) AS toprettystring(scalarsubquery(c1))#240]
+- 'Project [c1#224, c2#225, CASE WHEN isnull(alwaysTrue#245) THEN 0 WHEN NOT (cnt#222L = 0) THEN null ELSE cnt#222L END AS cnt#246L]
   +- 'Join LeftOuter, (c1#224 = c1#224#244)
      :- Project [col1#226 AS c1#224, col2#227 AS c2#225]
      :  +- LocalRelation [col1#226, col2#227]
      +- Project [cnt#222L, c1#224#244, cnt#222L, c1#224, true AS alwaysTrue#245]
         +- Project [cnt#222L, c1#224 AS c1#224#244, cnt#222L, c1#224]
            +- Aggregate [c1#224], [count(1) AS cnt#222L, c1#224]
               +- Project [col1#228 AS c1#224]
                  +- LocalRelation [col1#228, col2#229]The previous plan: Project [toprettystring(c1#224, Some(America/Los_Angeles)) AS toprettystring(c1)#238, toprettystring(c2#225, Some(America/Los_Angeles)) AS toprettystring(c2)#239, toprettystring(scalar-subquery#223 [c1#224 && (c1#224 = c1#224#244)], Some(America/Los_Angeles)) AS toprettystring(scalarsubquery(c1))#240]
:  +- Project [cnt#222L, c1#224 AS c1#224#244]
:     +- Filter (cnt#222L = 0)
:        +- Aggregate [c1#224], [count(1) AS cnt#222L, c1#224]
:           +- Project [col1#228 AS c1#224]
:              +- LocalRelation [col1#228, col2#229]
+- Project [col1#226 AS c1#224, col2#227 AS c2#225]
   +- LocalRelation [col1#226, col2#227] {code}
 

The reason are when execute subquery decorrelation, the fields in the subquery but not in having clause are wrongly pull up. This problem only occurs when there contain having clause.

 ",,,,,,,,,,,,SPARK-43778,,,,,SPARK-43778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 20 02:17:10 UTC 2023,,,,,,,,,,"0|z1i6ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/May/23 04:43;fanjia;https://github.com/apache/spark/pull/41347;;;","20/Jul/23 02:17;cloud_fan;Issue resolved by pull request 41347
[https://github.com/apache/spark/pull/41347];;;",,,,,,,,,,,,,
Spark Glue job introduces duplicates while writing a dataframe as file to S3,SPARK-43818,13537840,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ukant,ukant,26/May/23 18:11,27/May/23 03:17,30/Oct/23 17:26,,3.1.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"We have AWS Glue (Spark) based ETL framework which processes the data through multiple hops and finally write the dataframe in S3 bucket as parquet files with snappy compression. We have used this framework to process and write data to S3 for 1000+ tables/files and it works fine. But for two of the tables - in memory data frame contains correct records but when data frame gets persisted in S3 as file, it introduces duplicate entries and as the total count remains same duplicate cause missing records as well.

{*}Data Points{*}:
 # This happens only for large(wider tables + millions of rows)
 # When this happens we notice stage failures and retry succeeding but causing duplicates/missing records

{*}Code Steps{*}:
|Steps Information|Dataframe|Query / Operation /Action|
|Query Raw DB & get no of partition ( to  loop one by one)| |select distinct partition_0 FROM  <raw db >.<table name>|
|Raw DF Query|raw|select SCHDWKID_REF, TASK_REF, LIFECYCLE_REF, TASK_DESC, WHOSE_ENT_NAME, WHOSE_INST_REF, WHOSE_INST_CDE, STENDDAT_STRTDT, STENDDAT_ENDDAT, AGENT_ENT_NAME, AGENT_INST_REF, AGENT_INST_CDE, AGENT_CODE, LOCATION_ENT_NAME, LOCATION_INST_REF, LOCATION_INST_CDE, CASEID_NUMBER, FACE_AMT, TAAR_AMT, AUTH_AMT, TRANSFER_YORN_ENCODE, TRANSFER_YORN_DECODE, TRANSFER_YORN_ELMREF, CASE_YORN_ENCODE, CASE_YORN_DECODE, CASE_YORN_ELMREF, CHANGEID_REF, CNTRCTID_REF, CNTRCTID_NUMBER, KTKDSCID_REF, KWNOFFID_REF, KWNOFFID_CODE, USERID_REF, USERID_CODE, WQUEUEID_REF, WQUEUEID_CODE, STATUS_REF, STATUS_CODE, STATUS_ASAT, LASTUPD_USER, LASTUPD_TERMNO, LASTUPD_PROG, LASTUPD_INFTIM, KWQPRIID_REF, KWQPRIID_CODE, INSURED_NAME, AGENT_NAME, EDM_INGESTED_AT, EDM_INGEST_TIME, PARTITION_0, DELTA_IND, TRANSACT_SEQ from RAW_ORACLE_ORAP12_NYLDPROD60CL.SCHEDULED_WORK where partition_0= '20230428'|
|Structured  DF Query|structured|SELECT * FROM RL_LAKE_ORACLE_ORAP12_NYLDPROD60CL.SCHEDULED_WORK WHERE part_num > 0 |
| | | |
|Merge DF Generated By joining raw & structured on nks|df_merge|df_merge = structured.join(raw,keys,how='fullouter')|
|action column will be added to
 Merge Df|df_merge|df_merge = df_merge.withColumn(""action"", fn.when((((df_merge['structured.EDH_RECORD_STATUS_IN'] == 'A') \| (df_merge['structured.EDH_RECORD_STATUS_IN'] == 'D')) & ( df_merge['raw.chksum'].isNull()) & (~ df_merge['structured.CHKSUM'].isNull())) , ""NOACTION"")
                                                .when((df_merge['structured.CHKSUM'].isNull()) & (df_merge['raw.delta_ind']!= 'D'), ""INSERT"")
                                                .when((df_merge['structured.CHKSUM'] != df_merge['raw.chksum']) & (~ df_merge['structured.CHKSUM'].isNull()) & (df_merge['structured.EDH_RECORD_STATUS_IN'] == 'A') & ((df_merge['raw.delta_ind'] == 'U') \| (df_merge['raw.delta_ind'] == 'I')), ""UPDATE"")
                                                .when(((df_merge['raw.delta_ind']== 'D') & (df_merge['structured.EDH_RECORD_STATUS_IN'] == 'A')) , ""DELETE"")
                                                .when(((df_merge['raw.delta_ind']== 'D') & (df_merge['structured.EDH_RECORD_STATUS_IN'] == 'D') ) , ""DELETECOPY"")
                                                .when(((df_merge['raw.delta_ind']== 'I') & (df_merge['structured.EDH_RECORD_STATUS_IN'] == 'D') & (~ df_merge['raw.chksum'].isNull()) & (~ df_merge['structured.CHKSUM'].isNull())) , ""DELETEREINSERT"")
                                                .when(((df_merge['raw.delta_ind']== 'D') & (df_merge['structured.CHKSUM'].isNull())) , ""DELETEABSENT"")
                                                .when((df_merge['structured.CHKSUM'] == df_merge['raw.chksum']), ""NOCHANGE""))|
| | | |
|No Action df will be derived from merge df|df_noaction|df_noaction = df_merge.select(keys + ['structured.' + x.upper() for x in structured_cols_list if x.upper() not in keys]).where((df_merge.action == 'NOACTION') \| (df_merge.action == 'NOCHANGE'))|
|Delete Copy DF will be derived|df_dcopy|df_dcopy = df_merge.select(keys + ['structured.' + x.upper() for x in structured_cols_list if x.upper() not in keys]).where(df_merge.action == 'DELETECOPY')|
|Delete Absent df will be derived|df_dabs|df_dabs = df_merge.select(keys + ['raw.' + x.upper() for x in raw_cols_list if x.upper() not in keys]).where(df_merge.action == 'DELETEABSENT')|
|insert df will be derived|df_insert|df_insert = df_merge.select(keys + ['raw.' + x.upper() for x in raw_cols_list if x.upper() not in keys]).where(df_merge.action == 'INSERT')|
|Outdated Df will be derived , records from structured where we have updates|df_outdated|df_outdated = df_merge.select(keys + ['structured.' + x.upper() for x in structured_cols_list if x.upper() not in keys]).where(df_merge.action == 'UPDATE')|
|Deleted Records will be derived (Inactive)|df_delrecIn|df_delrecIn = df_merge.select(keys + ['structured.' + x.upper() for x in structured_cols_list if x.upper() not in keys]).where(df_merge.action == 'DELETE')|
|Deleted  Records will be derived (Active)|df_delrecAc|df_delrecAc = df_merge.select(keys + ['structured.' + x.upper() for x in structured_cols_list if x.upper() not in keys]).where(df_merge.action == 'DELETE')|
|Deleted & Re inserted Records will be derived (Inactive)|df_delReInsertInactive|df_delReInsertInactive = df_merge.select(keys + ['structured.' + x.upper() for x in structured_cols_list if x.upper() not in keys]).where(df_merge.action == 'DELETEREINSERT')|
|Deleted & Re inserted Records will be derived (active)|df_delReInsertActive|df_delReInsertActive = df_merge.select(keys + ['raw.' + x.upper() for x in raw_cols_list if x.upper() not in keys]).where(df_merge.action == 'DELETEREINSERT')|
|Updated Records df|df_update|df_update = df_merge.select(keys + ['raw.' + x.upper() for x in raw_cols_list if x.upper() not in keys]).where(df_merge.action == 'UPDATE')|
| | | |
|Updated df active df by unioning all active df generated previously|df_active|df_active = df_insert.unionAll(df_update).unionAll(df_delrecAc).unionAll(df_delReInsertActive).unionAll(df_dabs)|
|We will cachec df into Memory|df_active|df_active=df_active.cache()|
|Writing df_active df to debug tables by storing in df_active_before_index |df_active_before_index |df_active_before_index = df_active|
|adding sk column values to active df|df_active|df_active=dfZipWithIndex(df_active,skey_last_val,sk_col_nme,'part_num1',int(part_cnt),load_type)|
|We will cache df into Memory|df_active|df_active = df_active.cache()|
|created new df with active df from no action & delete copy df|df_active2|df_active2=df_noaction.unionAll(df_dcopy)|
|union of previously generated active2 & active|df_active|df_active=df_active.unionAll(df_active2)|
|Updated inactive df |df_inactive_final|df_inactive_final = df_outdated.unionAll(df_delrecIn).unionAll(df_delReInsertInactive)|
|Adding to debug tables |df_merge,df_active_final,df_inactive_final, raw,structured, df_active2,df_active_before_index|These debug table steps were added to troubleshoot duplicate/missing records issue. The tables(external) can be queried.|
|Debug tables we can query|scheduled_work_active_bindex, 
scheduled_work_active , scheduled_work_active2 ,scheduled_work_before_write_active , scheduled_work_inactive , scheduled_work_merge, scheduled_work_raw ,scheduled_work_structured |These debug table steps were added to troubleshoot duplicate/missing records issue. The tables(external) can be queried.|
|change audit data types of active & inactive df | |change_audit_datatype(fhz_df,structured_audit_col_names)|
|active data set  null replaced with blanks|df|df=df.fillna('')|
|active data set column renaming | |conf_df=df_col_rename(df, raw_all_col_n_partcol, structured_all_col_names)|
|For Varchar columns trimming function is applied on active data set| |conf_df = conf_df.withColumn(i[0], fn.trim(conf_df[i[0]]))|
|Active dataset column renaming | |df = df.withColumnRenamed(""PART_NUM"",""part_num"")|
|repartitioning dataset based on dpus|df|df=df.repartition(no_of_dpu)|
|Dropping unused columns from active dataset df|df| |
|delete partitions| |del_partition|
|delete active files | |del_File(bucket_name,del_path, part_col_cnt)|
|write active data set(which is same as the data writing to s3 structured path) to debug tables|df (debug table name = scheduled_work_before_write_active)|create_temp_table(df_dict, final_tbl, tbl_path, str_dir_path,'ldap')|
|Writing active df to structured layer|df |wrt_df|",Production,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-26 18:11:20.0,,,,,,,,,,"0|z1i68o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Corrupts Data In-Transit for High Volume (> 20 TB/hr) of Data,SPARK-43816,13537837,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,saiallu2020,saiallu2020,26/May/23 17:43,26/Jun/23 14:18,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,correctness,,,,"h1. Bug Context

Hello! I would like to report a bug that my team noticed while we were using Spark (please see the Environment section to see our exact setup).

The application we built is meant to convert a large number of JSON files (JSON Lines format) and write them to a Delta table. The JSON files are located in an Azure Data Lake Gen 2 +without+ hierarchical namespacing. The Delta table is in an Azure Data Lake Gen 2 +with+ hierarchical namespacing.

We have a PySpark notebook in our Synapse Analytics workspace which reads the JSON files into a DataFrame and then writes them to the Delta table. It uses batch processing.

The JSON files have {+}no corrupt records{+}, we checked them thoroughly. And there are no code flaws in our PySpark notebook, we also checked that.

Our code reads 15 TB of JSON files (each file is about 400 MB in size) into our PySpark DataFrame in the following way.
{code:java}
originalDF = (  
spark.read
    .schema(originDataSchema)
    .option(""pathGlobFilter"", DESIRED_FILE_PATTERN)
    .option(""mode"", ""PERMISSIVE"")
    .option(""columnNameOfCorruptRecord"", ""DiscoveredCorruptRecords"")
    .option(""badRecordsPath"", BAD_RECORDS_PATH)
    .json(ORIGIN_FILES_PATH)
) {code}
To read this data and then write it to a Delta table takes about 37 minutes.

The problem that we noticed is that as the data is read into the PySpark DataFrame, a small percent of it becomes corrupted. Only about 1 in 10 million records become corrupted. This is just a made-up example to illustrate the point:
{code:java}
// The original JSON record looks like this
{ ""Name"": ""Robert"", ""Email"": ""bob@gmail.com"", ""Nickname"": ""Bob"" }

// When we look in the PySpark DataFrame we see this (for a small percent of records)
{ ""Name"": ""Robertbob@"", ""Email"": ""gmail.com"", ""Nickname"": ""Bob"" }{code}
 

Essentially, the spark.read() has some deserialization problem that only emerges for high data throughput (> 20 TB/hr).

When we tried using a smaller dataset (1/4 the size), it didn't show any signs of corruption.

When we use the same exact code and then parse just one JSON file which contains the record mentioned above, everything works perfectly fine.

The spark.read() corruption is also not deterministic. If we re-run the 20 TB/hr test, we still see corruption but in different records.

 
h1. Our Temporary Solution

What we noticed is that the ""spark.sql.files.maxPartitionBytes"" was by default set to 128 MB. This meant that for the average JSON files we were reading - which was 400 MB - Spark was making four calls to the Azure Data Lake and fetching a [byte range|https://learn.microsoft.com/en-us/rest/api/storageservices/get-file#:~:text=Range-,Optional.%20Returns%20file%20data%20only%20from%20the%20specified%20byte%20range.,-x%2Dms%2Drange] (i.e. the 1st call got bytes 0-128MB, the 2nd call got bytes 128MB-256MB, etc.).

We increased ""spark.sql.files.maxPartitionBytes"" to a large number (1 GB) and that made the data corruption problem go away.

 
h1. How We Think You Can Fix This

From my understanding, when Spark makes a call for a byte range, it will often ""cut off"" the data in the middle of a JSON record. Our JSON files are in the JSON Lines format and they contain thousands of lines, each with a JSON record. So calling a byte range from 0 - 128MB will most likely mean that the cutoff point is right in the middle of a JSON record.

Spark seems to have some code logic which handles this by only processing the ""full lines"" that are received. But this logic seems to be failing a small percent of the time. Specifically, we have about 50,000 JSON files, that means ~200,000 byte range calls are being made. And spark.read() is creating about 150 corrupt records.

So we think you should look at the Spark code which is doing this ""cut off"" handling for byte ranges and see if there's something missing there. Or something in the deserialization logic of spark.read().

Again, this bug only emerges for high volumes of data transfer (> 20 TB/hr). This could be a ""race condition"" or some kind of performance-related bug.","We are using Azure Synapse Analytics. Within that, we have provisioned a Spark Pool with 101 nodes. 100 nodes are used for the executors and 1 node is for the driver. Each node is what Synapse Analytics calls a ""Memory Optimized Medium size node"". This means each node has 8 vCores and 64 GB memory. The Spark Pool does not do dynamic allocation of executors (101 nodes are created at the start and present throughout the Spark job). Synapse has something called ""Intelligent Cache,"" but we disabled it (set to 0%). The nodes all use Spark 3.3.1.5.2-90111858. If you need details on any specific Spark settings, I can get that for you. Mostly we are just using the defaults.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,,,,Mon Jun 26 14:18:01 UTC 2023,,,,,,,,,,"0|z1i680:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/23 02:40;yumwang;You can also set another config: spark.sql.files.maxPartitionNum. https://github.com/apache/spark/pull/41545.;;;","20/Jun/23 19:00;saiallu2020;Thank you for solving this Yuming, I really appreciate your help!;;;","22/Jun/23 15:17;saiallu2020;Hello Yuming, could you please explain a bit more why this new setting will prevent data corruption from happening in the future?;;;","25/Jun/23 01:17;yumwang;[~saiallu2020] This is similar to spark.sql.files.maxPartitionBytes.;;;","26/Jun/23 14:18;saiallu2020;Hello [~yumwang], I reached out to your email. Would you mind sharing with our team more about your process of identifying the Shuffle Service issue and a detailed explanation of the solution? We are just really curious to learn more technical details, as this is a very interesting issue.;;;",,,,,,,,,,
Spark cannot construct the DecimalType in CatalystTypeConverters.convertToCatalyst() API,SPARK-43814,13537783,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,Jk_Self,Jk_Self,26/May/23 09:11,01/Jun/23 01:07,30/Oct/23 17:26,,3.2.2,3.3.2,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,," 

When constructing the DecimalType in CatalystTypeConverters.convertToCatalyst(), spark throw following exception:

 
{code:java}
Decimal scale (18) cannot be greater than precision (1).
org.apache.spark.sql.AnalysisException: Decimal scale (18) cannot be greater than precision (1).
        at org.apache.spark.sql.errors.QueryCompilationErrors$.decimalCannotGreaterThanPrecisionError(QueryCompilationErrors.scala:1671)
        at org.apache.spark.sql.types.DecimalType.<init>(DecimalType.scala:48)
        at org.apache.spark.sql.catalyst.CatalystTypeConverters$.convertToCatalyst(CatalystTypeConverters.scala:518)
        at org.apache.spark.sql.DataFrameFunctionsSuite.$anonfun$new$712(DataFrameFunctionsSuite.scala:3714){code}
 

 

This issue can be reproduced by the following case:

 
{code:java}
  val expression = Literal.default(DecimalType.SYSTEM_DEFAULT)
  val schema = StructType(
    StructField(""a"", IntegerType, nullable = true) :: Nil)
  val empData = Seq(Row(1))
  val df = spark.createDataFrame(spark.sparkContext.parallelize(empData), schema)
  val resultDF = df.select(Column(expression))
  val result = resultDF.collect().head.get(0)
  CatalystTypeConverters.convertToCatalyst(result)

{code}
 

It seems that the reason for the failure is that the value of precision is not set when the Decimal.toJavaBigDecimal() method is called. However, Java BigDecimal only provides an interface for modifying scale and does not provide an interface for modifying Precision.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-26 09:11:09.0,,,,,,,,,,"0|z1i5w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unbase64 and unhex codegen are invalid with failOnError,SPARK-43802,13537722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,25/May/23 21:45,27/May/23 02:31,30/Oct/23 17:26,26/May/23 17:37,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,,,0,,,,,"to_binary with hex and base64 generate invalid codegen:

{{spark.range(5).selectExpr('to_binary(base64(cast(id as binary)), ""BASE64"")').show()}}

results in

{{Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: Unknown variable or type ""BASE64""}}

because this is the generated code:



/* 107 */         if (!org.apache.spark.sql.catalyst.expressions.UnBase64.isValidBase64(project_value_1)) {

/* 108 */           throw QueryExecutionErrors.invalidInputInConversionError(

/* 109 */             ((org.apache.spark.sql.types.BinaryType$) references[1] /* to */),

/* 110 */             project_value_1,

/* 111 */             BASE64,

/* 112 */             ""try_to_binary"");

/* 113 */         }",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat May 27 02:31:03 UTC 2023,,,,,,,,,,"0|z1i5io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/May/23 17:37;maxgekk;Issue resolved by pull request 41317
[https://github.com/apache/spark/pull/41317];;;","27/May/23 02:31;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/41334;;;",,,,,,,,,,,,,
Streaming ForeachWriter can't accept custom user defined class,SPARK-43796,13537693,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,WweiL,WweiL,25/May/23 17:30,16/Aug/23 19:24,30/Oct/23 17:26,16/Aug/23 19:24,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,Structured Streaming,,,,0,,,,,"[https://github.com/apache/spark/pull/41129]

The last example in the PR description doesn't work with current REPL implementation. 

Code:

 
{code:java}
import org.apache.spark.sql.{ForeachWriter, Row} 
import java.io._ 
val filePath = ""/home/wei.liu/test_foreach/output-custom"" 
case class MyTestClass(value: Int) {
      override def toString: String = value.toString
}
val writer = new ForeachWriter[MyTestClass] {
    var fileWriter: FileWriter = _
    def open(partitionId: Long, version: Long): Boolean = {
      fileWriter = new FileWriter(filePath, true)
      true
    }
    def process(row: MyTestClass): Unit = {
      fileWriter.write(row.toString)
      fileWriter.write(""\n"")
    }
    def close(errorOrNull: Throwable): Unit = {
      fileWriter.close()
    }
}
val df = spark.readStream .format(""rate"") .option(""rowsPerSecond"", ""10"") .load()
val query = df .selectExpr(""CAST(value AS INT)"") .as[MyTestClass] .writeStream .foreach(writer) .outputMode(""update"") .start()
{code}
Error:
{code:java}
23/05/24 19:17:31 ERROR Utils: Aborting task
java.lang.NoClassDefFoundError: Could not initialize class ammonite.$sess.cmd4$
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:35)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:30)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.catalyst.encoders.OuterScopes$.$anonfun$getOuterScope$1(OuterScopes.scala:59)
	at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.$anonfun$doGenCode$1(objects.scala:598)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.doGenCode(objects.scala:598)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:201)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:196)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.$anonfun$create$1(GenerateSafeProjection.scala:156)
	at scala.collection.immutable.List.map(List.scala:293)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:153)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:39)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1369)
	at org.apache.spark.sql.catalyst.expressions.SafeProjection$.createCodeGeneratedObject(Projection.scala:171)
	at org.apache.spark.sql.catalyst.expressions.SafeProjection$.createCodeGeneratedObject(Projection.scala:168)
	at org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:51)
	at org.apache.spark.sql.catalyst.expressions.SafeProjection$.create(Projection.scala:194)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:173)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:166)
	at org.apache.spark.sql.execution.streaming.sources.ForeachDataWriter.write(ForeachWriterTable.scala:147)
	at org.apache.spark.sql.execution.streaming.sources.ForeachDataWriter.write(ForeachWriterTable.scala:132)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:493)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1521)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1487)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748) {code}
This issue is similar to SPARK-43198

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 16 19:24:07 UTC 2023,,,,,,,,,,"0|z1i5c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/23 01:05;hvanhovell;This is caused by two different issues:
- SPARK-44794: We do not properly propagate the needed JobArtifactState to the thread that executes the stream.
- SPARK-44799: Classes defined in Ammonite need an outer pointer. The current mechanism assumes a REPL is present, and this fails. We are fixing this by propagating an outer scope with the encoder.
;;;","16/Aug/23 19:24;hvanhovell;This has been fixed.;;;",,,,,,,,,,,,,
IllegalStateException when cogrouping two datasets derived from the same source,SPARK-43781,13537559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,mrry,mrry,24/May/23 22:58,11/Aug/23 02:28,30/Oct/23 17:26,11/Aug/23 02:19,3.3.1,3.4.0,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,1,,,,,"Attempting to {{cogroup}} two datasets derived from the same source dataset yields an {{IllegalStateException}} when the query is executed.

Minimal reproducer:
{code:java}
StructType inputType = DataTypes.createStructType(
    new StructField[]{
        DataTypes.createStructField(""id"", DataTypes.LongType, false),
        DataTypes.createStructField(""type"", DataTypes.StringType, false)
    }
);

StructType keyType = DataTypes.createStructType(
    new StructField[]{
        DataTypes.createStructField(""id"", DataTypes.LongType, false)
    }
);

List<Row> inputRows = new ArrayList<>();
inputRows.add(RowFactory.create(1L, ""foo""));
inputRows.add(RowFactory.create(1L, ""bar""));
inputRows.add(RowFactory.create(2L, ""foo""));
Dataset<Row> input = sparkSession.createDataFrame(inputRows, inputType);

KeyValueGroupedDataset<Row, Row> fooGroups = input
    .filter(""type = 'foo'"")
    .groupBy(""id"")
    .as(RowEncoder.apply(keyType), RowEncoder.apply(inputType));

KeyValueGroupedDataset<Row, Row> barGroups = input
    .filter(""type = 'bar'"")
    .groupBy(""id"")
    .as(RowEncoder.apply(keyType), RowEncoder.apply(inputType));

Dataset<Row> result = fooGroups.cogroup(
    barGroups,
    (CoGroupFunction<Row, Row, Row, Row>) (row, iterator, iterator1) -> new ArrayList<Row>().iterator(),
    RowEncoder.apply(inputType));

result.explain();
result.show();{code}
Explain output (note mismatch in column IDs between Sort/Exchagne and LocalTableScan on the first input to the CoGroup):
{code:java}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- SerializeFromObject [validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, id), LongType, false) AS id#37L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, type), StringType, false), true, false, true) AS type#38]
   +- CoGroup org.apache.spark.sql.KeyValueGroupedDataset$$Lambda$1478/1869116781@77856cc5, createexternalrow(id#16L, StructField(id,LongType,false)), createexternalrow(id#16L, type#17.toString, StructField(id,LongType,false), StructField(type,StringType,false)), createexternalrow(id#16L, type#17.toString, StructField(id,LongType,false), StructField(type,StringType,false)), [id#39L], [id#39L], [id#39L, type#40], [id#39L, type#40], obj#36: org.apache.spark.sql.Row
      :- !Sort [id#39L ASC NULLS FIRST], false, 0
      :  +- !Exchange hashpartitioning(id#39L, 2), ENSURE_REQUIREMENTS, [plan_id=19]
      :     +- LocalTableScan [id#16L, type#17]
      +- Sort [id#39L ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(id#39L, 2), ENSURE_REQUIREMENTS, [plan_id=20]
            +- LocalTableScan [id#39L, type#40]{code}
Exception:
{code:java}
java.lang.IllegalStateException: Couldn't find id#39L in [id#16L,type#17]
        at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
        at scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)
        at scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:698)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
        at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1254)
        at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1253)
        at org.apache.spark.sql.catalyst.expressions.BinaryExpression.mapChildren(Expression.scala:608)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:528)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)
        at scala.collection.immutable.List.map(List.scala:246)
        at scala.collection.immutable.List.map(List.scala:79)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)
        at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:160)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.getPartitionKeyExtractor$1(ShuffleExchangeExec.scala:323)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$13(ShuffleExchangeExec.scala:391)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$13$adapted(ShuffleExchangeExec.scala:390)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748) {code}
Other observations:
 * The same code works if I call {{createDataFrame()}} twice and use two separate datasets as input to the cogroup.
 * The real code uses two different filters on the same cached dataset as the two inputs to the cogroup. However, this results in the same exception, and the same apparent error in the physical plan, which looks as follows:
{code:java}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- SerializeFromObject [validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, id), LongType, false) AS id#47L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, type), StringType, false), true, false, true) AS type#48]
   +- CoGroup org.apache.spark.sql.KeyValueGroupedDataset$$Lambda$1526/693211959@7b2e931, createexternalrow(id#16L, StructField(id,LongType,false)), createexternalrow(id#16L, type#17.toString, StructField(id,LongType,false), StructField(type,StringType,false)), createexternalrow(id#16L, type#17.toString, StructField(id,LongType,false), StructField(type,StringType,false)), [id#49L], [id#49L], [id#49L, type#50], [id#49L, type#50], obj#46: org.apache.spark.sql.Row
      :- !Sort [id#49L ASC NULLS FIRST], false, 0
      :  +- !Exchange hashpartitioning(id#49L, 2), ENSURE_REQUIREMENTS, [plan_id=26]
      :     +- Filter (type#17 = foo)
      :        +- InMemoryTableScan [id#16L, type#17], [(type#17 = foo)]
      :              +- InMemoryRelation [id#16L, type#17], StorageLevel(disk, memory, deserialized, 1 replicas)
      :                    +- LocalTableScan [id#16L, type#17]
      +- Sort [id#49L ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(id#49L, 2), ENSURE_REQUIREMENTS, [plan_id=27]
            +- Filter (type#50 = bar)
               +- InMemoryTableScan [id#49L, type#50], [(type#50 = bar)]
                     +- InMemoryRelation [id#49L, type#50], StorageLevel(disk, memory, deserialized, 1 replicas)
                           +- LocalTableScan [id#16L, type#17] {code}

 * The issue doesn't arise if I write the same code in PySpark, using {{{}FlatMapCoGroupsInPandas{}}}.","Reproduces in a unit test, using Spark 3.3.1, the Java API, and a {{local[2]}} SparkSession.",,,,,,,,,,,,,,,,SPARK-42132,,SPARK-42132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jul 12 09:04:06 UTC 2023,,,,,,,,,,"0|z1i4io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/23 09:04;githubbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41554;;;",,,,,,,,,,,,,,
Support correlated columns in join ON conditions,SPARK-43780,13537542,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gubichev,gubichev,gubichev,24/May/23 20:03,15/Aug/23 01:52,30/Oct/23 17:26,15/Aug/23 01:52,3.4.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,,,,,"Subqueries that have joins with outer references in join conditions should be supported.

For example:

 
{code:java}
spark-sql (default)> create view t0(t0a) as values (0), (1), (2);
spark-sql (default)> create view t1(t1a) as values (1), (2), (3);
spark-sql (default)> create view t2(t2a) as values (2), (3), (4);
spark-sql (default)> select * from t0 join lateral (select * from t1 join t2 on t1a = t2a and t1a = t0a);
[UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.CORRELATED_REFERENCE] Unsupported subquery expression: Expressions referencing the outer query are not supported outside of WHERE/HAVING clauses: ""((t1a = t2a) AND (t1a = t0a))"".; line 1 pos 48;
Project [t0a#15, t1a#17, t2a#19]
+- LateralJoin lateral-subquery#14 [t0a#15], Inner
   :  +- SubqueryAlias __auto_generated_subquery_name
   :     +- Project [t1a#17, t2a#19]
   :        +- Join Inner, ((t1a#17 = t2a#19) AND (t1a#17 = outer(t0a#15)))
   :           :- SubqueryAlias spark_catalog.default.t1
   :           :  +- View (`spark_catalog`.`default`.`t1`, [t1a#17])
   :           :     +- Project [cast(col1#18 as int) AS t1a#17]
   :           :        +- LocalRelation [col1#18]
   :           +- SubqueryAlias spark_catalog.default.t2
   :              +- View (`spark_catalog`.`default`.`t2`, [t2a#19])
   :                 +- Project [cast(col1#20 as int) AS t2a#19]
   :                    +- LocalRelation [col1#20]
   +- SubqueryAlias spark_catalog.default.t0
      +- View (`spark_catalog`.`default`.`t0`, [t0a#15])
         +- Project [cast(col1#16 as int) AS t0a#15]
            +- LocalRelation [col1#16] {code}
Here, the subquery has a join with join condition referencing the outer table t0. This should be handled very similarly to how we handle correlation in Filter predicates.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 15 01:52:28 UTC 2023,,,,,,,,,,"0|z1i4ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/23 01:52;cloud_fan;Issue resolved by pull request 41301
[https://github.com/apache/spark/pull/41301];;;",,,,,,,,,,,,,,
RewriteCorrelatedScalarSubquery should handle duplicate attributes,SPARK-43778,13537525,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,gubichev,gubichev,24/May/23 18:17,22/Oct/23 00:19,30/Oct/23 17:26,20/Jul/23 05:08,3.4.0,,,,,,,,,,,,,,,,,,,4.0.0,,,,SQL,,,,,0,pull-request-available,,,,"This is a correctness problem caused by the fact that the decorrelation rule does not dedup join attributes properly. This leads to the join on (c1 = c1), which is simplified to True and the join becomes a cross product.

 

Example query:

 
{code:java}
create view t(c1, c2) as values (0, 1), (0, 2), (1, 2)

select c1, c2, (select count(*) cnt from t t2 where t1.c1 = t2.c1 having cnt = 0) from t t1
-- Correct answer: [(0, 1, null), (0, 2, null), (1, 2, null)]
+---+---+------------------+
|c1 |c2 |scalarsubquery(c1)|
+---+---+------------------+
|0  |1  |null              |
|0  |1  |null              |
|0  |2  |null              |
|0  |2  |null              |
|1  |2  |null              |
|1  |2  |null              |
+---+---+------------------+ {code}
 

 

 ",,,,,,,,,,,SPARK-43838,,,,,SPARK-43838,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 20 05:06:17 UTC 2023,,,,,,,,,,"0|z1i4b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/23 05:06;fanjia;This ticket already fixed by https://issues.apache.org/jira/browse/SPARK-43838. Should we add backport for this? [~cloud_fan] ;;;",,,,,,,,,,,,,,
Coalescing partitions in AQE returns different results with row_number windows. ,SPARK-43777,13537519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,kristopherkane,kristopherkane,24/May/23 17:15,25/May/23 14:46,30/Oct/23 17:26,25/May/23 14:46,3.1.3,3.3.2,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"While updating our code base from 3.1 to 3.3, I had a test fail due to wrong results.  With 3.1, we did not proactively turn on AQE in sbt based tests and noticed the failure due to AQE enabled by default between 3.1 and 3.3

An easily reproducible test: 
{code:java}
    val testDataDf = Seq(
      (1, 1, 0, 0),
      (1, 1, 0, 0),
      (1, 1, 0, 0),
      (1, 0, 0, 1),
      (1, 0, 0, 1),
      (2, 0, 0, 0),
      (2, 0, 1, 0),
      (2, 1, 0, 0),
      (3, 0, 1, 0),
      (3, 0, 1, 0),
    ).toDF(""id"", ""is_attribute1"", ""is_attribute2"", ""is_attribute3"")    

val placeWindowSpec = Window
      .partitionBy(""id"")
      .orderBy($""count"".desc)    

val resultDf: DataFrame = testDataDf
      .select(""id"", ""is_attribute1"", ""is_attribute2"", ""is_attribute3"")
      .withColumn(
        ""place"",
        when($""is_attribute1"" === 1, ""attribute1"")
          .when($""is_attribute2"" === 1, ""attribute2"")
          .when($""is_attribute3"" === 1, ""attribute3"")
          .otherwise(""other"")
      )
      .groupBy(""id"", ""place"")
      .agg(
        functions.count(""*"").as(""count"")
      )
      .withColumn(
        ""rank"",
        row_number().over(placeWindowSpec)
      )    


resultDf.orderBy(""id"", ""place"", ""rank"").show() {code}
 

Various results based on Spark version and AQE settings: 
{code:java}
Spark 3.1
Without AQE
+---+----------+-----+----+
| id|     place|count|rank|
+---+----------+-----+----+
|  1|attribute1|    3|   1|
|  1|attribute3|    2|   2|
|  2|attribute1|    1|   2|
|  2|attribute2|    1|   1|
|  2|     other|    1|   3|
|  3|attribute2|    2|   1|
+---+----------+-----+----+

AQE with defaults
+---+----------+-----+----+
| id|     place|count|rank|
+---+----------+-----+----+
|  1|attribute1|    3|   1|
|  1|attribute3|    2|   2|
|  2|attribute1|    1|   2|
|  2|attribute2|    1|   1|
|  2|     other|    1|   3|
|  3|attribute2|    2|   1|
+---+----------+-----+----+

AQE with .set(""spark.sql.adaptive.coalescePartitions.enabled"", ""false"")
+---+----------+-----+----+
| id|     place|count|rank|
+---+----------+-----+----+
|  1|attribute1|    3|   1|
|  1|attribute3|    2|   2|
|  2|attribute1|    1|   2|
|  2|attribute2|    1|   1|
|  2|     other|    1|   3|
|  3|attribute2|    2|   1|
+---+----------+-----+----+

AQE with .set(""spark.sql.adaptive.coalescePartitions.initialPartitionNum"", ""1"") - Like Spark 3.3 with AQE defaults
+---+----------+-----+----+
| id|     place|count|rank|
+---+----------+-----+----+
|  1|attribute1|    3|   1|
|  1|attribute3|    2|   2|
|  2|attribute1|    1|   3|
|  2|attribute2|    1|   2|
|  2|     other|    1|   1|
|  3|attribute2|    2|   1|

----------------------------------------
Spark 3.3.2
----------------------------------------
AQE with defaults
+---+----------+-----+----+
| id|     place|count|rank|
+---+----------+-----+----+
|  1|attribute1|    3|   1|
|  1|attribute3|    2|   2|
|  2|attribute1|    1|   3|
|  2|attribute2|    1|   2|
|  2|     other|    1|   1|
|  3|attribute2|    2|   1|
+---+----------+-----+----+
--------------------------------------------
AQE with .set(""spark.sql.adaptive.coalescePartitions.enabled"", ""false"") - This matches Spark 3.1
+---+----------+-----+----+
| id|     place|count|rank|
+---+----------+-----+----+
|  1|attribute1|    3|   1|
|  1|attribute3|    2|   2|
|  2|attribute1|    1|   2|
|  2|attribute2|    1|   1|
|  2|     other|    1|   3|
|  3|attribute2|    2|   1|
+---+----------+-----+----+ {code}
As you can see, the 'rank' column of row_number(partition by, order by) returns a different rank for id value 2's three attributes based on how AQE coalesces partitions. ",SBT based Spark project with unit tests running on spark-testing-base. Tested with Spark 3.1.3 and 3.3.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 25 14:46:14 UTC 2023,,,,,,,,,,"0|z1i49s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/23 02:33;ulysses;It acutally is a random event that all the row count of id 2 are 1 so sort by id take no effect. The ordering of shuffle result is not determinate and so is AQE.

If you really need a determinate result, change window sepc to `orderBy($""count"".desc, $""place"")`;;;","25/May/23 14:46;kristopherkane;Yes, a colleague pointed out last night that I had missed that in the window spec.  ;;;",,,,,,,,,,,,,
Fix bug in AvroSuite for 'reading from invalid path throws exception',SPARK-43767,13537414,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,24/May/23 02:10,24/May/23 04:10,30/Oct/23 17:26,24/May/23 04:10,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 24 04:10:39 UTC 2023,,,,,,,,,,"0|z1i3mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/May/23 04:10;dongjoon;Issue resolved by pull request 41289
[https://github.com/apache/spark/pull/41289];;;",,,,,,,,,,,,,,
Incorrect attribute nullability after RewriteCorrelatedScalarSubquery leads to incorrect query results,SPARK-43760,13537399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gubichev,gubichev,gubichev,24/May/23 00:14,01/Jun/23 07:12,30/Oct/23 17:26,31/May/23 00:29,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,,,0,,,,,"The following query:

 
{code:java}
select * from (
 select t1.id c1, (
  select t2.id c from range (1, 2) t2
  where t1.id = t2.id  ) c2
 from range (1, 3) t1 ) t
where t.c2 is not null
-- !query schema
struct<c1:bigint,c2:bigint>
-- !query output
1	1
2	NULL
 {code}
 
should return 1 row, because the second row is supposed to be removed by IsNotNull predicate. However, due to a wrong nullability propagation after subquery decorrelation, the output of the subquery is declared as not-nullable (incorrectly), so the predicate is constant folded into True.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 31 00:29:59 UTC 2023,,,,,,,,,,"0|z1i3j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/May/23 00:29;cloud_fan;Issue resolved by pull request 41287
[https://github.com/apache/spark/pull/41287];;;",,,,,,,,,,,,,,
Expose TimestampNTZType in pyspark.sql.types,SPARK-43759,13537396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,23/May/23 23:59,04/Jul/23 02:40,30/Oct/23 17:26,24/May/23 04:59,3.5.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,PySpark,,,,,0,,,,,{{TimestampNTZType}} is missing in {{__all__}} list in {{pyspark.sql.types}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 24 04:59:38 UTC 2023,,,,,,,,,,"0|z1i3ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/May/23 04:59;podongfeng;Issue resolved by pull request 41286
[https://github.com/apache/spark/pull/41286];;;",,,,,,,,,,,,,,
Upgrade snappy-java to 1.1.10.0,SPARK-43758,13537392,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,23/May/23 22:47,24/May/23 02:51,30/Oct/23 17:26,24/May/23 02:51,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Build,,,,,0,,,,,Update {{snappy-java}} to 1.1.10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 24 02:51:48 UTC 2023,,,,,,,,,,"0|z1i3hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/May/23 02:51;dongjoon;Issue resolved by pull request 41285
[https://github.com/apache/spark/pull/41285];;;",,,,,,,,,,,,,,
Change CheckConnectJvmClientCompatibility to deny list to increase the API check coverage,SPARK-43757,13537385,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhenli,zhenli,zhenli,23/May/23 21:37,28/Jun/23 14:38,30/Oct/23 17:26,28/Jun/23 14:38,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"The current compatibility check only checks selected classes. So when adding a new class, if a developer forgets to add this class into the checklist, then this API is not covered in compatibility tests. Thus we should change this API check to always include all APIs by default.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 23 22:11:56 UTC 2023,,,,,,,,,,"0|z1i3g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/23 22:11;hudson;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/41284;;;","23/May/23 22:11;hudson;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/41284;;;",,,,,,,,,,,,,
Incorrect result of MINUS in spark sql.,SPARK-43753,13537352,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kernelforce,kernelforce,23/May/23 14:35,13/Jun/23 10:14,30/Oct/23 17:26,,2.4.8,3.0.3,3.1.3,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"sql(""""""
with va as (
  select '123' id, 'a' name
   union all
  select '123' id, 'b' name
)
select '123' id, 'a' name from va t where t.name = 'a'
 minus 
select '123' id, 'a' name from va s where s.name = 'b'
"""""").show
+---+----+
| id|name|
+---+----+
|123|   a|
+---+----+

which is expected to be empty result set.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,sql,,,,Tue Jun 13 10:14:12 UTC 2023,,,,,,,,,,"0|z1i38o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/23 10:14;fanjia;Seem already fixed on the master branch.;;;",,,,,,,,,,,,,,
default column value should support v2 write commands,SPARK-43752,13537347,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,cloud_fan,cloud_fan,23/May/23 14:06,15/Sep/23 09:12,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-23 14:06:15.0,,,,,,,,,,"0|z1i37k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Connect scala UDF serialization pulling in unrelated classes not available on server,SPARK-43744,13537298,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhenli,juliuszsompolski,juliuszsompolski,23/May/23 09:18,29/Jul/23 03:00,30/Oct/23 17:26,29/Jul/23 03:00,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,SPARK-43745,,,,"[https://github.com/apache/spark/pull/41487] moved ""interrupt all - background queries, foreground interrupt"" and ""interrupt all - foreground queries, background interrupt"" tests from ClientE2ETestSuite into a new isolated suite SparkSessionE2ESuite to avoid an unexplicable UDF serialization issue.

 

When these tests are moved back to ClientE2ETestSuite and when testing with
{code:java}
build/mvn clean install -DskipTests -Phive
build/mvn test -pl connector/connect/client/jvm -Dtest=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite{code}
 
the tests fails with
{code:java}
23/05/22 15:44:11 ERROR SparkConnectService: Error during: execute. UserId: . SessionId: 0f4013ca-3af9-443b-a0e5-e339a827e0cf.
java.lang.NoClassDefFoundError: org/apache/spark/sql/connect/client/SparkResult
at java.lang.Class.getDeclaredMethods0(Native Method)
at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
at java.lang.Class.getDeclaredMethod(Class.java:2128)
at java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1643)
at java.io.ObjectStreamClass.access$1700(ObjectStreamClass.java:79)
at java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:520)
at java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:494)
at java.security.AccessController.doPrivileged(Native Method)
at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:494)
at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:391)
at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:681)
at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2005)
at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)
at java.io.ObjectInputStream.readClass(ObjectInputStream.java:1815)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1640)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
at org.apache.spark.util.Utils$.deserialize(Utils.scala:148)
at org.apache.spark.sql.connect.planner.SparkConnectPlanner.org$apache$spark$sql$connect$planner$SparkConnectPlanner$$unpackUdf(SparkConnectPlanner.scala:1353)
at org.apache.spark.sql.connect.planner.SparkConnectPlanner$TypedScalaUdf$.apply(SparkConnectPlanner.scala:761)
at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformTypedMapPartitions(SparkConnectPlanner.scala:531)
at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformMapPartitions(SparkConnectPlanner.scala:495)
at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:143)
at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handlePlan(SparkConnectStreamHandler.scala:100)
at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$2(SparkConnectStreamHandler.scala:87)
at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)
at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$1(SparkConnectStreamHandler.scala:53)
at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
at org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:209)
at org.apache.spark.sql.connect.artifact.SparkConnectArtifactManager$.withArtifactClassLoader(SparkConnectArtifactManager.scala:178)
at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handle(SparkConnectStreamHandler.scala:48)
at org.apache.spark.sql.connect.service.SparkConnectService.executePlan(SparkConnectService.scala:166)
at org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:611)
at org.sparkproject.connect.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
at org.sparkproject.connect.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:352)
at org.sparkproject.connect.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:866)
at org.sparkproject.connect.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
at org.sparkproject.connect.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.connect.client.SparkResult
at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
... 56 more{code}
for some reason, the serialization of 
{code:java}
n => { Thread.sleep(30000); n }{code}
closure into an UDF tries to reference SparkResult class on the Spark Connect server... (???)

 

See [https://github.com/apache/spark/pull/41005/files/35e500d4cb72f8d3bee21a7f86ee16cbbc8a936c#r1200551487] and [https://github.com/apache/spark/pull/41487#discussion_r1221390234] thread for some debugging info.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 25 09:12:59 UTC 2023,,,,,,,,,,"0|z1i2wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/23 14:42;juliuszsompolski;[https://github.com/apache/spark/pull/41487] has swept the issue under the rug by moving the tests around to a different suite.

Reopening this to get to the bottom of the issue, as I believe it remains a real issue that is not covered.;;;","25/Jul/23 09:12;githubbot;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/42069;;;","25/Jul/23 09:12;githubbot;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/42069;;;",,,,,,,,,,,,
Port HIVE-12188: DoAs does not work properly in non-kerberos secured HS2,SPARK-43743,13537295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,23/May/23 08:55,23/May/23 10:35,30/Oct/23 17:26,23/May/23 10:34,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,Please see: https://issues.apache.org/jira/browse/HIVE-12188,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-12188,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 23 10:34:23 UTC 2023,,,,,,,,,,"0|z1i2w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/23 09:15;githubbot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/41276;;;","23/May/23 10:34;dongjoon;Issue resolved by pull request 41276
[https://github.com/apache/spark/pull/41276];;;",,,,,,,,,,,,,
refactor default column value resolution,SPARK-43742,13537284,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,23/May/23 07:50,28/May/23 23:28,30/Oct/23 17:26,28/May/23 23:28,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun May 28 23:28:30 UTC 2023,,,,,,,,,,"0|z1i2tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/23 09:04;githubbot;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/41262;;;","28/May/23 23:28;dongjoon;Issue resolved by pull request 41262
[https://github.com/apache/spark/pull/41262];;;",,,,,,,,,,,,,
Handle missing row.excludedInStages field,SPARK-43719,13537224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,22/May/23 22:51,23/May/23 05:17,30/Oct/23 17:26,23/May/23 05:17,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 23 05:17:46 UTC 2023,,,,,,,,,,"0|z1i2g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/23 04:23;snoot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41266;;;","23/May/23 05:17;dongjoon;Issue resolved by pull request 41266
[https://github.com/apache/spark/pull/41266];;;",,,,,,,,,,,,,
References to a specific side's key in a USING join can have wrong nullability,SPARK-43718,13537216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,22/May/23 20:48,23/May/23 04:48,30/Oct/23 17:26,23/May/23 04:48,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,,,0,correctness,,,,"Assume this data:
{noformat}
create or replace temp view t1 as values (1), (2), (3) as (c1);
create or replace temp view t2 as values (2), (3), (4) as (c1);
{noformat}
The following query produces incorrect results:
{noformat}
spark-sql (default)> select explode(array(t1.c1, t2.c1)) as x1
from t1
full outer join t2
using (c1);
1
-1      <== should be null
2
2
3
3
-1      <== should be null
4
Time taken: 0.663 seconds, Fetched 8 row(s)
spark-sql (default)> 
{noformat}
Similar issues occur with right outer join and left outer join.

{{t1.c1}} and {{t2.c1}} have the wrong nullability at the time the array is resolved, so the array's {{containsNull}} value is incorrect.

Queries that don't use arrays also can get wrong results. Assume this data:
{noformat}
create or replace temp view t1 as values (0), (1), (2) as (c1);
create or replace temp view t2 as values (1), (2), (3) as (c1);
create or replace temp view t3 as values (1, 2), (3, 4), (4, 5) as (a, b);
{noformat}
The following query produces incorrect results:
{noformat}
select t1.c1 as t1_c1, t2.c1 as t2_c1, b
from t1
full outer join t2
using (c1),
lateral (
  select b
  from t3
  where a = coalesce(t2.c1, 1)
) lt3;
1	1	2
NULL	3	4
Time taken: 2.395 seconds, Fetched 2 row(s)
spark-sql (default)> 
{noformat}
The result should be the following:
{noformat}
0	NULL	2
1	1	2
NULL	3	4
{noformat}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 23 04:48:12 UTC 2023,,,,,,,,,,"0|z1i2eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/May/23 20:56;bersprockets;I think I have a handle on this. I will submit in a PR in the coming days.;;;","23/May/23 00:46;bersprockets;PR here: https://github.com/apache/spark/pull/41267;;;","23/May/23 04:48;dongjoon;Issue resolved by pull request 41267
[https://github.com/apache/spark/pull/41267];;;",,,,,,,,,,,,
Scala Client Dataset#reduce failed to handle null partitions for scala primitive types,SPARK-43717,13537211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhenli,zhenli,zhenli,22/May/23 19:32,07/Jun/23 06:31,30/Oct/23 17:26,07/Jun/23 06:31,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"Scala client failed with NPE when running:

assert(spark.range(0, 5, 1, 10).as[Long].reduce(_ + _) == 10)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jun 07 06:31:05 UTC 2023,,,,,,,,,,"0|z1i2dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/23 04:22;snoot;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/41264;;;","07/Jun/23 06:31;LuciferYang;Issue resolved by pull request 41264
[https://github.com/apache/spark/pull/41264];;;",,,,,,,,,,,,,
"Maven test failed: `interrupt all - background queries, foreground interrupt` and `interrupt all - foreground queries, background interrupt`",SPARK-43648,13537062,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,22/May/23 07:47,09/Jun/23 11:09,30/Oct/23 17:26,09/Jun/23 11:09,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"run 
{code:java}
build/mvn clean install -DskipTests -Phive
build/mvn test -pl connector/connect/client/jvm -Dtest=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite {code}
`interrupt all - background queries, foreground interrupt` and `interrupt all - foreground queries, background interrupt` failed as follows:
{code:java}
23/05/22 15:44:11 ERROR SparkConnectService: Error during: execute. UserId: . SessionId: 0f4013ca-3af9-443b-a0e5-e339a827e0cf.
java.lang.NoClassDefFoundError: org/apache/spark/sql/connect/client/SparkResult
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.getDeclaredMethod(Class.java:2128)
	at java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1643)
	at java.io.ObjectStreamClass.access$1700(ObjectStreamClass.java:79)
	at java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:520)
	at java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:494)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:494)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:391)
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:681)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2005)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readClass(ObjectInputStream.java:1815)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1640)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
	at org.apache.spark.util.Utils$.deserialize(Utils.scala:148)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.org$apache$spark$sql$connect$planner$SparkConnectPlanner$$unpackUdf(SparkConnectPlanner.scala:1353)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner$TypedScalaUdf$.apply(SparkConnectPlanner.scala:761)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformTypedMapPartitions(SparkConnectPlanner.scala:531)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformMapPartitions(SparkConnectPlanner.scala:495)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:143)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handlePlan(SparkConnectStreamHandler.scala:100)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$2(SparkConnectStreamHandler.scala:87)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$1(SparkConnectStreamHandler.scala:53)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:209)
	at org.apache.spark.sql.connect.artifact.SparkConnectArtifactManager$.withArtifactClassLoader(SparkConnectArtifactManager.scala:178)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handle(SparkConnectStreamHandler.scala:48)
	at org.apache.spark.sql.connect.service.SparkConnectService.executePlan(SparkConnectService.scala:166)
	at org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:611)
	at org.sparkproject.connect.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
	at org.sparkproject.connect.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:352)
	at org.sparkproject.connect.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:866)
	at org.sparkproject.connect.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.sparkproject.connect.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.connect.client.SparkResult
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	... 56 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-43745,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 09 11:09:51 UTC 2023,,,,,,,,,,"0|z1i1g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/23 09:31;juliuszsompolski;Duplicated by https://issues.apache.org/jira/browse/SPARK-43744; closing the duplicate.

See [https://github.com/apache/spark/pull/41005/files/35e500d4cb72f8d3bee21a7f86ee16cbbc8a936c#r1200551487] thread for some debugging info.
It seems it may be related to https://issues.apache.org/jira/browse/SPARK-43227;;;","09/Jun/23 11:09;gurwls223;Issue resolved by pull request 41487
[https://github.com/apache/spark/pull/41487];;;",,,,,,,,,,,,,
Maven test failed in ClientE2ETestSuite/CatalogSuite/StreamingQuerySuite without -Phive,SPARK-43647,13537060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,22/May/23 07:28,26/May/23 01:31,30/Oct/23 17:26,26/May/23 01:31,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,," 
{code:java}
build/mvn clean install -DskipTests
build/mvn test -pl connector/connect/client/jvm{code}
 

13 test failed with similar reasons:

 
{code:java}
- read and write *** FAILED ***
  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated
  at io.grpc.Status.asRuntimeException(Status.java:535)
  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)
  at scala.collection.Iterator.toStream(Iterator.scala:1417)
  at scala.collection.Iterator.toStream$(Iterator.scala:1416)
  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)
  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)
  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)
  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)
  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:489)
  ... {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-43745,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 26 01:31:34 UTC 2023,,,,,,,,,,"0|z1i1fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/May/23 01:31;LuciferYang;Issue resolved by pull request 41282
[https://github.com/apache/spark/pull/41282];;;",,,,,,,,,,,,,,
pos_explode functions accessing encrypted columns when the columns are not used in select ,SPARK-43614,13537020,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,Nageswaran,Nageswaran,22/May/23 05:05,22/May/23 05:08,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,1,,,,,"Hi Team,

Suppose, we have a array of nested field and the nested field contains one encrypted column and the one non encrypted column. 

In an environment where I cannot decrypt the encrypted fields, and I wanted to access the non encrypted column of array of nested fields using `pos_explode` , I get error saying encrypted fields cannot be decrypted. 

Why does we have to access the `encrypted` fields when I am interested only in non encrtyped fields of array of nested fields. 

 

Reproducer to create a encrypted file with one of the fields of array of nested field encrypted. 

Spark 3.4.0 Using Spark-shell - mac
Downloaded the file [parquet-hadoop-1.12.0-tests.jar|https://repo1.maven.org/maven2/org/apache/parquet/parquet-hadoop/1.12.0/parquet-hadoop-1.12.0-tests.jar] and added it to spark-jars folder
 * Array of nested field column name: `nestedArrayCol`
 * Nested field name : `nestedItem`
 * Array of nested field encypted column name : nestedArrayCol.ic

{code:scala}
sc.hadoopConfiguration.set(""parquet.crypto.factory.class"" ,""org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory"")

sc.hadoopConfiguration.set(""parquet.encryption.kms.client.class"" ,""org.apache.parquet.crypto.keytools.mocks.InMemoryKMS"")

sc.hadoopConfiguration.set(""parquet.encryption.key.list"",""key1a: BAECAwQFBgcICQoLDA0ODw==, key2a: BAECAAECAAECAAECAAECAA==, keyz: BAECAAECAAECAAECAAECAA=="")

sc.hadoopConfiguration.set(""parquet.encryption.key.material.store.internally"",""false"")

val encryptedParquetPath = ""/tmp/par_enc_footer_non_encrypted""
val partitionCol = 1
case class nestedItem(ic: Int = 0, sic : Double, pc: Int = 0)
case class SquareItem(int_column: Int, square_int_column : Double, partitionCol: Int, nestedCol :nestedItem, nestedArrayCol : Array[nestedItem])
val dataRange = (1 to 100000).toList
val squares = sc.parallelize(dataRange.map(i => new SquareItem(i, scala.math.pow(i,2), partitionCol,nestedItem(i,i), Array(nestedItem(i,i) ))))
squares.toDS().show()
squares.toDS().write.partitionBy(""partitionCol"").mode(""overwrite"").option(""parquet.encryption.column.keys"", ""key1a:square_int_column,nestedCol.ic,nestedArrayCol.list.element.ic;"").option(""parquet.encryption.plaintext.footer"",true).option(""parquet.encryption.footer.key"", ""keyz"").parquet(encryptedParquetPath)
{code}
 
 When I tried to read the code 

 
{code:scala}
val encryptedParquetPath = ""/tmp/par_enc_footer_non_encrypted""
spark.sqlContext.read.parquet(encryptedParquetPath).createOrReplaceTempView(""test"")

spark.sql(""select int_column, nestedCol.sic, pos,ni, posexplode(nestedArrayCol) as ( pos, ni) from test"").show()
 {code}
I get the below error
{code:java}
org.apache.spark.SparkException: Encountered error while reading file file:///tmp/par_enc_footer_non_encrypted/partitionCol=1/part-00008-4c554a8e-7c43-4695-907d-c442b03b2af6.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:877)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:304)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1623)
Caused by: org.apache.parquet.crypto.ParquetCryptoRuntimeException: [nestedArrayCol, list, element, ic]. Null File Decryptor
	at org.apache.parquet.hadoop.metadata.EncryptedColumnChunkMetaData.decryptIfNeeded(ColumnChunkMetaData.java:610)
	at org.apache.parquet.hadoop.metadata.EncryptedColumnChunkMetaData.getTotalSize(ColumnChunkMetaData.java:666)
	at org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:940)
	at org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:909)
	at org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1016)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:274)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:404)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:321)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)
	... 21 more
23/05/22 10:33:29 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (192.168.68.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///tmp/par_enc_footer_non_encrypted/partitionCol=1/part-00008-4c554a8e-7c43-4695-907d-c442b03b2af6.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:877)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:304)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1623)
Caused by: org.apache.parquet.crypto.ParquetCryptoRuntimeException: [nestedArrayCol, list, element, ic]. Null File Decryptor
	at org.apache.parquet.hadoop.metadata.EncryptedColumnChunkMetaData.decryptIfNeeded(ColumnChunkMetaData.java:610)
	at org.apache.parquet.hadoop.metadata.EncryptedColumnChunkMetaData.getTotalSize(ColumnChunkMetaData.java:666)
	at org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:940)
	at org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:909)
	at org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1016)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:274)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:404)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:321)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)
	... 21 more

23/05/22 10:33:29 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (192.168.68.55 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///tmp/par_enc_footer_non_encrypted/partitionCol=1/part-00008-4c554a8e-7c43-4695-907d-c442b03b2af6.c000.snappy.parquet. Details:
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:877)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:304)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1623)
Caused by: org.apache.parquet.crypto.ParquetCryptoRuntimeException: [nestedArrayCol, list, element, ic]. Null File Decryptor
	at org.apache.parquet.hadoop.metadata.EncryptedColumnChunkMetaData.decryptIfNeeded(ColumnChunkMetaData.java:610)
	at org.apache.parquet.hadoop.metadata.EncryptedColumnChunkMetaData.getTotalSize(ColumnChunkMetaData.java:666)
	at org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:940)
	at org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:909)
	at org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1016)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:274)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:404)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:321)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)
	... 21 more

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
  at scala.Option.foreach(Option.scala:407)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:809)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:768)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:777)
  ... 47 elided
Caused by: org.apache.spark.SparkException: Encountered error while reading file file:///tmp/par_enc_footer_non_encrypted/partitionCol=1/part-00008-4c554a8e-7c43-4695-907d-c442b03b2af6.c000.snappy.parquet. Details:
  at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:877)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:304)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
  at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
  at org.apache.spark.scheduler.Task.run(Task.scala:139)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  at java.base/java.lang.Thread.run(Thread.java:1623)
Caused by: org.apache.parquet.crypto.ParquetCryptoRuntimeException: [nestedArrayCol, list, element, ic]. Null File Decryptor
  at org.apache.parquet.hadoop.metadata.EncryptedColumnChunkMetaData.decryptIfNeeded(ColumnChunkMetaData.java:610)
  at org.apache.parquet.hadoop.metadata.EncryptedColumnChunkMetaData.getTotalSize(ColumnChunkMetaData.java:666)
  at org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:940)
  at org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:909)
  at org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1016)
  at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:274)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:404)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:321)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)
  at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)
  ... 21 more
{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-22 05:05:03.0,,,,,,,,,,"0|z1i16w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Subquery decorrelation rewriteDomainJoins failure from ConstantFolding to isnull,SPARK-43596,13536922,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jchen5,jchen5,19/May/23 23:49,25/May/23 08:43,30/Oct/23 17:26,25/May/23 08:43,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"We can get a decorrelation error because of rewrites that run in between DecorrelateInnerQuery and rewriteDomainJoins, that modify the correlation join conditions. In particular, ConstantFolding can transform `innercol <=> null` to `isnull(innercol)` and then rewriteDomainJoins does not recognize this and throws error Unable to rewrite domain join with conditions: ArrayBuffer(isnull(innercol#280)) because the isnull is not an equality, so it isn't usable for rewriting the domain join.

Can fix by recognizing `isnull(innercol)` as `innercol <=> null` in rewriteDomainJoins.

This area is also fragile in general and other rewrites that run between the two steps of decorrelation could potentially break their assumptions, so we may want to investigate longer-term follow ups for that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 25 08:43:21 UTC 2023,,,,,,,,,,"0|z1i0l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/23 08:43;cloud_fan;Issue resolved by pull request 41265
[https://github.com/apache/spark/pull/41265];;;",,,,,,,,,,,,,,
NoSuchMethodError in Spark 3.4 with JDK8u362 & JDK8u372,SPARK-43592,13536864,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Not A Problem,,shivamkasatt,shivamkasatt,19/May/23 12:47,29/May/23 16:38,30/Oct/23 17:26,29/May/23 16:38,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,java,JDK1.8,jdk11,,"My project was on spark 3.3 with JDK8u362 and I tried updating it to spark 3.4, Official documentation of spark 3.4 says it works with JDK8u362 and above but when I tried upgrading docker base image of spark to JDK8u362 and JDK8u372 it is failing at runtime with below error, For JDK8u362 it throws error for Java.nio.CharBuffer.position method and for JDK8u372 it throws error for java.nio.ByteBuffer.flip method. But when I run with JDK11 image in spark Docker file it works fine. Am I missing anything or how to fix this issue as I want to run it with JDK8.
{code:java}
ava.lang.NoSuchMethodError: java.nio.CharBuffer.position(I)Ljava/nio/CharBuffer;
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.unescapeSQLString(ParserUtils.scala:220) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.string(ParserUtils.scala:95) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$createString$2(AstBuilder.scala:2632) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286) ~[scala-library-2.12.17.jar:?]
        at scala.collection.Iterator.foreach(Iterator.scala:943) ~[scala-library-2.12.17.jar:?]
        at scala.collection.Iterator.foreach$(Iterator.scala:943) ~[scala-library-2.12.17.jar:?]
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1431) ~[scala-library-2.12.17.jar:?]
        at scala.collection.IterableLike.foreach(IterableLike.scala:74) ~[scala-library-2.12.17.jar:?]
        at scala.collection.IterableLike.foreach$(IterableLike.scala:73) ~[scala-library-2.12.17.jar:?]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:56) ~[scala-library-2.12.17.jar:?]
        at scala.collection.TraversableLike.map(TraversableLike.scala:286) ~[scala-library-2.12.17.jar:?]
        at scala.collection.TraversableLike.map$(TraversableLike.scala:279) ~[scala-library-2.12.17.jar:?]
        at scala.collection.AbstractTraversable.map(Traversable.scala:108) ~[scala-library-2.12.17.jar:?]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.createString(AstBuilder.scala:2632) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitStringLiteral$1(AstBuilder.scala:2618) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitStringLiteral(AstBuilder.scala:2618) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitStringLiteral(AstBuilder.scala:58) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$StringLiteralContext.accept(SqlBaseParser.java:19511) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:73) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitConstantDefault(SqlBaseParserBaseVisitor.java:1735) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ConstantDefaultContext.accept(SqlBaseParser.java:18373) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:73) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitValueExpressionDefault(SqlBaseParserBaseVisitor.java:1567) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ValueExpressionDefaultContext.accept(SqlBaseParser.java:17491) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:63) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1630) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$withPredicate$1(AstBuilder.scala:1870) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.withPredicate(AstBuilder.scala:1784) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitPredicated$1(AstBuilder.scala:1768) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:1765) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:58) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext.accept(SqlBaseParser.java:16909) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:73) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitExpression(SqlBaseParserBaseVisitor.java:1518) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ExpressionContext.accept(SqlBaseParser.java:16766) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:63) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1630) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitParenthesizedExpression$1(AstBuilder.scala:2361) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitParenthesizedExpression(AstBuilder.scala:2361) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitParenthesizedExpression(AstBuilder.scala:58) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ParenthesizedExpressionContext.accept(SqlBaseParser.java:18036) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:73) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitValueExpressionDefault(SqlBaseParserBaseVisitor.java:1567) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ValueExpressionDefaultContext.accept(SqlBaseParser.java:17491) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:63) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1630) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitPredicated$1(AstBuilder.scala:1766) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:1765) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:58) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext.accept(SqlBaseParser.java:16909) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:63) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1630) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitLogicalBinary$4(AstBuilder.scala:1694) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at scala.collection.SeqLike.$anonfun$reverseMap$2(SeqLike.scala:295) ~[scala-library-2.12.17.jar:?]
        at scala.collection.SeqLike.reverseMap(SeqLike.scala:294) ~[scala-library-2.12.17.jar:?]
        at scala.collection.SeqLike.reverseMap$(SeqLike.scala:289) ~[scala-library-2.12.17.jar:?]
        at scala.collection.AbstractSeq.reverseMap(Seq.scala:45) ~[scala-library-2.12.17.jar:?]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitLogicalBinary$1(AstBuilder.scala:1694) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitLogicalBinary(AstBuilder.scala:1669) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitLogicalBinary(AstBuilder.scala:58) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$LogicalBinaryContext.accept(SqlBaseParser.java:16958) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:63) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1630) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.withWhereClause(AstBuilder.scala:703) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitCommonSelectQueryClausePlan$2(AstBuilder.scala:807) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$EnhancedLogicalPlan$.optionalMap$extension(ParserUtils.scala:256) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitCommonSelectQueryClausePlan(AstBuilder.scala:807) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$withSelectQuerySpecification$1(AstBuilder.scala:788) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.withSelectQuerySpecification(AstBuilder.scala:776) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitRegularQuerySpecification$1(AstBuilder.scala:668) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:656) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:58) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext.accept(SqlBaseParser.java:10386) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:73) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitQueryPrimaryDefault(SqlBaseParserBaseVisitor.java:902) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryPrimaryDefaultContext.accept(SqlBaseParser.java:9891) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:73) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitQueryTermDefault(SqlBaseParserBaseVisitor.java:888) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryTermDefaultContext.accept(SqlBaseParser.java:9658) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:63) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:114) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$1(AstBuilder.scala:120) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:119) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:58) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept(SqlBaseParser.java:6891) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:73) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitStatementDefault(SqlBaseParserBaseVisitor.java:69) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.SqlBaseParser$StatementDefaultContext.accept(SqlBaseParser.java:1988) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18) ~[antlr4-runtime-4.9.3.jar:4.9.3]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitSingleStatement$1(AstBuilder.scala:80) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:80) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.$anonfun$parsePlan$2(ParseDriver.scala:92) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:160) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.$anonfun$parsePlan$1(ParseDriver.scala:92) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52) ~[spark-sql_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:633) ~[spark-sql_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) ~[spark-catalyst_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:632) ~[spark-sql_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827) ~[spark-sql_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630) ~[spark-sql_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:671) ~[spark-sql_2.12-3.4.0.jar:3.4.0]
        at com.cisco.intersight.cep.AdvisoryProcessorTask.processActions(AdvisoryProcessorTask.java:376) ~[orion.jar:?]
        at com.cisco.intersight.cep.AdvisoryProcessorTask.processAdvisory(AdvisoryProcessorTask.java:329) ~[orion.jar:?]
        at com.cisco.intersight.cep.AdvisoryProcessorTask.processAdvisories(AdvisoryProcessorTask.java:306) ~[orion.jar:?]
        at com.cisco.intersight.cep.AdvisoryProcessorTask.run(AdvisoryProcessorTask.java:286) ~[orion.jar:?]
        at com.cisco.intersight.cep.AdvisoryDriver.main(AdvisoryDriver.java:115) ~[orion.jar:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1020) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) ~[spark-core_2.12-3.4.0.jar:3.4.0]% {code}
Error with JDK8u362
{code:java}
org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;
java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;
        at org.apache.spark.util.io.ChunkedByteBufferOutputStream.toChunkedByteBuffer(ChunkedByteBufferOutputStream.scala:115)
        at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:362)
        at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:160)
        at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)
        at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)
        at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)
        at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1548)
        at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1530)
        at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1535)
        at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1353)
        at org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1334)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2934)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.17.jar:?]
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.17.jar:?]
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.17.jar:?]
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1545) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1353) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1334) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2934) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.4.0.jar:3.4.0]
Caused by: java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;
        at org.apache.spark.util.io.ChunkedByteBufferOutputStream.toChunkedByteBuffer(ChunkedByteBufferOutputStream.scala:115) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:362) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:160) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1548) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1530) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1535) ~[spark-core_2.12-3.4.0.jar:3.4.0]
        ... 6 more 
 {code}","JDK: JDK8u362, JDK8u372
Kubernetes
Spark 3.4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon May 29 16:38:20 UTC 2023,,,,,,,,,,"0|z1i088:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/23 12:48;shivamkasatt;Posted the same on stackoverflow: 
https://stackoverflow.com/questions/76286857/nosuchmethoderror-in-spark-3-4-with-jdk8u362-jdk8u372;;;","19/May/23 14:14;yumwang;Maybe same issue: SPARK-32475?;;;","22/May/23 05:10;shivamkasatt;by maven compiler version is fine & [Spark Documentation|https://spark.apache.org/docs/latest/] says that spark 3.4 works on JDK8u362 and above.;;;","22/May/23 05:19;yumwang;Did you build your project through Java 9+?;;;","22/May/23 05:21;shivamkasatt;yes! when i tried building with JDK11. it works fine;;;","22/May/23 05:26;yumwang;Do you mean build your project through Java 11 and run your project through JDK8u362?;;;","22/May/23 05:30;shivamkasatt;When I use JDK11 image in spark dockerfile and maven target and source are kept 1.8 -> It works fine
When I use JDK8 image in spark dockerfile and maven target and source jdk 1.8 -> it throws error;;;","29/May/23 16:38;srowen;You are building with Java 11 and that causes the problem. target/source does not prevent this kind of issue. You have to build with 8 to run on 8.
The docs are referring to the build we make, but you are making your own build.;;;",,,,,,,
Fix `cannotBroadcastTableOverMaxTableBytesError` to use `bytesToString`,SPARK-43589,13536824,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,19/May/23 06:36,19/May/23 15:35,30/Oct/23 17:26,19/May/23 15:35,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-37321,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 19 09:23:45 UTC 2023,,,,,,,,,,"0|z1hzzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/23 09:22;githubbot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41232;;;","19/May/23 09:23;githubbot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41232;;;",,,,,,,,,,,,,
Spark Connect client cannot read from Hive metastore,SPARK-43585,13536809,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,heywxl,heywxl,19/May/23 03:44,19/May/23 03:45,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"I created a Spark Connect shell in a pod using the following yaml.
{code:java}
apiVersion: v1
kind: Service
metadata:
  name: spark-connect-svc
  namespace: MY_NAMESPACE
spec:
  clusterIP: None
  selector:
    app: spark-connect-pod
    podType: spark-connect-driver


apiVersion: v1
kind: Pod
metadata:
  name: spark-connect-pod
  namespace: realtime-streaming
  labels:
    app: spark-connect-pod
    podType: spark-connect-driver
spec:
  restartPolicy: Never
  containers:
  - command:
    - sh
    - -c
    - /opt/spark/sbin/start-connect-server.sh --master k8s://https://MY_API_SERVER:443 --packages org.apache.spark:spark-connect_2.12:3.4.0 --conf spark.kubernetes.executor.limit.cores=1.0 --conf spark.kubernetes.executor.request.cores=1.0 --conf spark.executor.cores=1 --conf spark.executor.memory=6G --conf spark.kubernetes.container.image=MY_ECR_REPO/spark:3.4-prd  --conf spark.kubernetes.executor.podNamePrefix=spark-connect --num-executors=10 --conf spark.kubernetes.driver.pod.name=spark-connect-pod --conf spark.kubernetes.namespace=MY_NAMESPACE && tail -100f /opt/spark/logs/spark--org.apache.spark.sql.connect.service.SparkConnectServer-1-spark-connect-pod.out
    image: MY_ECR_REPO/spark-py:3.4-prd
    name: spark-connect-pod
 {code}
The Spark Connect server was successfully launched and I can connect to it using pyspark.

 

But when I want to add a Hive metastore config , it won't work.

 
{code:java}
>>> spark = SparkSession.builder.remote(""sc://spark-connect-svc"").config(""spark.hive.metastore.uris"", ""thrift://hive-metastore:9083"").getOrCreate()
>>> spark.sql(""show databases"").show()
+---------+
|namespace|
+---------+
|  default|
+---------+{code}
There're many databases under the hive metastore and I've tested with the local env, all works fine. 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-19 03:44:23.0,,,,,,,,,,"0|z1hzw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade `kubernetes-client` to 6.6.2,SPARK-43581,13536792,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,19/May/23 00:44,19/May/23 02:46,30/Oct/23 17:26,19/May/23 02:46,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Build,Kubernetes,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 19 02:46:30 UTC 2023,,,,,,,,,,"0|z1hzsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/23 02:46;dongjoon;Issue resolved by pull request 41223
[https://github.com/apache/spark/pull/41223];;;",,,,,,,,,,,,,,
"Update ""Supported Pandas API"" page to point out the proper pandas docs",SPARK-43547,13536638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,18/May/23 01:36,18/May/23 03:05,30/Oct/23 17:26,18/May/23 03:05,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Documentation,Pandas API on Spark,,,,0,,,,,[https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/supported_pandas_api.html#supported-pandas-api] not point out the wrong pandas version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 18 03:05:04 UTC 2023,,,,,,,,,,"0|z1hyu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/May/23 03:05;gurwls223;Issue resolved by pull request 41208
[https://github.com/apache/spark/pull/41208];;;",,,,,,,,,,,,,,
Incorrect column resolution on FULL OUTER JOIN with USING,SPARK-43541,13536613,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,17/May/23 19:48,19/May/23 00:25,30/Oct/23 17:26,18/May/23 18:13,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,,,0,,,,,"This was tested on Spark 3.3.2 and Spark 3.4.0.

{code}
Causes [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `aws_dbr_a`.`key` cannot be resolved. Did you mean one of the following? [`key`].; line 4, pos 7
{code}


FULL OUTER JOIN with USING and/or the WHERE seems relevant since I can get the query to work with any of these modifications. 


{code}
# -- FULL OUTER JOIN
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   FULL OUTER JOIN gcp_pro_b USING (key)
                   WHERE aws_dbr_a.key NOT LIKE 'spark.clusterUsageTags.%';
[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `aws_dbr_a`.`key` cannot be resolved. Did you mean one of the following? [`key`].; line 4 pos 7
# -- INNER JOIN
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   JOIN gcp_pro_b USING (key)
                   WHERE aws_dbr_a.key NOT LIKE 'spark.clusterUsageTags.%';
+-----+
| key |
|-----|
| a   |
+-----+
1 row in set
Time: 0.507s
# -- NO Filter
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   FULL OUTER JOIN gcp_pro_b USING (key);
+-----+
| key |
|-----|
| a   |
+-----+
1 row in set
Time: 1.021s
# -- ON instead of USING
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   FULL OUTER JOIN gcp_pro_b ON aws_dbr_a.key = gcp_pro_b.key
                   WHERE aws_dbr_a.key NOT LIKE 'spark.clusterUsageTags.%';
+-----+
| key |
|-----|
| a   |
+-----+
1 row in set
Time: 0.514s
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 19 00:25:37 UTC 2023,,,,,,,,,,"0|z1hyoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/May/23 18:13;maxgekk;Issue resolved by pull request 41204
[https://github.com/apache/spark/pull/41204];;;","18/May/23 22:11;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/41220;;;","19/May/23 00:25;dongjoon;This is backported to branch-3.3 via https://github.com/apache/spark/pull/41221;;;",,,,,,,,,,,,
Spark Homebrew Formulae currently depends on non-officially-supported Java 20,SPARK-43538,13536523,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yumwang,ghislain.fourny,ghislain.fourny,17/May/23 09:42,23/May/23 00:13,30/Oct/23 17:26,23/May/23 00:12,3.2.4,3.3.2,3.4.0,,,,,,,,,,,,,,,,,3.4.0,,,,Java API,,,,,0,,,,,"I am not sure if homebrew-related issues can also be reported here? The Homebrew formulae for apache-spark runs on (latest) openjdk 20.

[https://formulae.brew.sh/formula/apache-spark]

However, Apache Spark is documented to work with Java 8/11/17:

[https://spark.apache.org/docs/latest/]

Is this an overlook, or is Java 20 officially supported, too?

Thanks!","Homebrew (e.g., macOS)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 23 00:12:44 UTC 2023,,,,,,,,,,"0|z1hy4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/May/23 12:00;yumwang;We have not tested on Java 20, because Java 20 is not LTS.;;;","17/May/23 12:03;ghislain.fourny;Thanks, Yuming Wang! Does it mean the apache-spark Homebrew Formulae should then be adapted to openjdk@17 (or 8 or 11) to avoid unpredictable behavior?;;;","17/May/23 12:09;yumwang;Yes. I think so: https://github.com/Homebrew/homebrew-core/pull/131189;;;","23/May/23 00:12;yumwang;Issue resolved by pull request [https://github.com/Homebrew/homebrew-core/pull/131189]. Please reinstall it if you have installed the Spark Homebrew formulae.
;;;",,,,,,,,,,,
Statsd sink reporter reports incorrect counter metrics.,SPARK-43536,13536519,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,abmodi,abmodi,17/May/23 09:30,17/May/23 11:14,30/Oct/23 17:26,,3.1.3,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,1,,,,,"There is a mismatch between the definition of counter metrics between dropwizard (which  is used by spark) and statsD. While Dropwizard interprets counters as cumulative metrics, statsD interprets them as delta metrics. This causes double aggregation in statsd causing inconsistent metrics.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 17 11:14:08 UTC 2023,,,,,,,,,,"0|z1hy3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/May/23 11:14;ignitetcbot;User 'venkateshbalaji99' has created a pull request for this issue:
https://github.com/apache/spark/pull/41199;;;",,,,,,,,,,,,,,
Add log4j-1.2-api and log4j-slf4j2-impl to classpath if active hadoop-provided,SPARK-43534,13536503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yumwang,yumwang,yumwang,17/May/23 07:08,20/May/23 23:45,30/Oct/23 17:26,20/May/23 23:45,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Build,,,,,0,,,,,"Build Spark:
{code:sh}
./dev/make-distribution.sh --name default --tgz -Phive -Phive-thriftserver -Pyarn -Phadoop-provided
tar -zxf spark-3.5.0-SNAPSHOT-bin-default.tgz {code}
Remove the following jars from spark-3.5.0-SNAPSHOT-bin-default:
{noformat}
jars/log4j-1.2-api-2.20.0.jar
jars/log4j-slf4j2-impl-2.20.0.jar
{noformat}
Add a new log4j2.properties to spark-3.5.0-SNAPSHOT-bin-default/conf:
{code:none}
rootLogger.level = info
rootLogger.appenderRef.file.ref = File
rootLogger.appenderRef.stderr.ref = console

appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_ERR
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss,SSS} %p [%t] %c{2}:%L : %m%n

appender.file.type = RollingFile
appender.file.name = File
appender.file.fileName = /tmp/spark/logs/spark.log
appender.file.filePattern = /tmp/spark/logs/spark.%d{yyyyMMdd-HH}.log
appender.file.append = true
appender.file.layout.type = PatternLayout
appender.file.layout.pattern = %d{yy/MM/dd HH:mm:ss,SSS} %p [%t] %c{2}:%L : %m%n
appender.file.policies.type = Policies
appender.file.policies.time.type = TimeBasedTriggeringPolicy
appender.file.policies.time.interval = 1
appender.file.policies.time.modulate = true
appender.file.policies.size.type = SizeBasedTriggeringPolicy
appender.file.policies.size.size = 256M
appender.file.strategy.type = DefaultRolloverStrategy
appender.file.strategy.max = 100
{code}

Start Spark thriftserver:
{code:java}
sbin/start-thriftserver.sh
{code}

Check the log:
{code:sh}
cat /tmp/spark/logs/spark.log
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/23 07:52;yumwang;hadoop log jars.png;https://issues.apache.org/jira/secure/attachment/13058286/hadoop+log+jars.png","17/May/23 07:16;yumwang;log4j-1.2-api-2.20.0.jar;https://issues.apache.org/jira/secure/attachment/13058284/log4j-1.2-api-2.20.0.jar","17/May/23 07:16;yumwang;log4j-slf4j2-impl-2.20.0.jar;https://issues.apache.org/jira/secure/attachment/13058285/log4j-slf4j2-impl-2.20.0.jar",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat May 20 23:45:35 UTC 2023,,,,,,,,,,"0|z1hy08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/May/23 07:52;yumwang;https://github.com/apache/spark/pull/41195;;;","19/May/23 18:43;ignitetcbot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/41195;;;","20/May/23 23:45;srowen;Issue resolved by pull request 41195
[https://github.com/apache/spark/pull/41195];;;",,,,,,,,,,,,
Support general expressions as OPTIONS values ,SPARK-43529,13536464,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dtenedor,dtenedor,dtenedor,16/May/23 22:22,12/Jun/23 04:31,30/Oct/23 17:26,12/Jun/23 04:31,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jun 12 04:31:16 UTC 2023,,,,,,,,,,"0|z1hxrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/23 04:09;snoot;User 'dtenedor' has created a pull request for this issue:
https://github.com/apache/spark/pull/41191;;;","12/Jun/23 04:31;Gengliang.Wang;Issue resolved by pull request 41191
[https://github.com/apache/spark/pull/41191];;;",,,,,,,,,,,,,
Fix catalog.listCatalogs in PySpark,SPARK-43527,13536410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,podongfeng,podongfeng,podongfeng,16/May/23 13:27,16/May/23 23:31,30/Oct/23 17:26,16/May/23 23:31,3.4.0,3.4.1,3.5.0,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,PySpark,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 16 23:31:48 UTC 2023,,,,,,,,,,"0|z1hxfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/May/23 23:31;gurwls223;Issue resolved by pull request 41186
[https://github.com/apache/spark/pull/41186];;;",,,,,,,,,,,,,,
Memory leak in Spark UI,SPARK-43524,13536389,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,aminebag,aminebag,16/May/23 09:36,17/May/23 00:32,30/Oct/23 17:26,17/May/23 00:32,2.4.4,,,,,,,,,,,,,,,,,,,,,,,Web UI,,,,,0,,,,,"We have a distributed Spark application running on Azure HDInsight using Spark version 2.4.4.

After a few days of active processing on our application, we have noticed that the GC CPU time ratio of the driver is close to 100%. We suspected a memory leak. Thus, we have produced a heap dump and analyzed it using Eclipse Memory Analyzer.

Here is some interesting data from the driver's heap dump (heap size is 8 GB):
 * The estimated retained heap size of String objects (~5M instances) is 3.3 GB. It seems that most of these instances correspond to spark events.
 * Spark UI's AppStatusListener instance estimated retained size is 1.1 GB.
 * The number of LiveJob objects with status ""RUNNING"" is 18K, knowing that there shouldn't be more than 16 live running jobs since we use a fixed size thread pool of 16 threads to run spark queries.
 * The number of LiveTask objects is 485K.
 * The AsyncEventQueue instance associated to the AppStatusListener has a value of 854 for dropped events count and a value of 10001 for total events count, knowing that the dropped events counter is reset every minute and that the queue's default capacity is 10000.

We think that there is a memory leak in Spark UI. Here is our analysis of the root cause of this leak:
 * AppStatusListener is notified of Spark events using a bounded queue in AsyncEventQueue.
 * AppStatusListener updates its state (kvstore, liveTasks, liveStages, liveJobs, ...) based on the received events. For example, onTaskStart adds a task to liveTasks map and onTaskEnd removes the task from liveTasks map.
 * When the rate of events is very high, the bounded queue in AsyncEventQueue is full, some events are dropped and don't make it to AppStatusListener.
 * Dropped events that signal the end of a processing unit prevent the state of AppStatusListener from being cleaned. For example, a dropped onTaskEnd event, will prevent the task from being removed from liveTasks map, and the task will remain in the heap until the driver's JVM is stopped.

We were able to confirm our analysis by reducing the capacity of the AsyncEventQueue (spark.scheduler.listenerbus.eventqueue.capacity=10). After having launched many spark queries using this config, we observed that the number of active jobs in Spark UI increased rapidly and remained high even though all submitted queries have completed. We have also noticed that some executor task counters in Spark UI were negative, which confirms that AppStatusListener state does not accurately reflect the reality and that it can be a victim of event drops.

Suggested fix:
There are some limits today on the number of ""dead"" objects in AppStatusListener's maps (for example: spark.ui.retainedJobs). We suggest enforcing another configurable limit on the number of total objects in AppStatusListener's maps and kvstore. This should limit the leak in the case of high events rate, but AppStatusListener stats will remain inaccurate.",,,,,,,,,,,,,,,,,,,SPARK-43523,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-16 09:36:59.0,,,,,,,,,,"0|z1hxaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in Spark UI,SPARK-43523,13536388,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,aminebag,aminebag,16/May/23 09:35,15/Sep/23 00:17,30/Oct/23 17:26,,2.4.4,3.4.0,,,,,,,,,,,,,,,,,,,,,,Web UI,,,,,0,pull-request-available,,,,"We have a distributed Spark application running on Azure HDInsight using Spark version 2.4.4.

After a few days of active processing on our application, we have noticed that the GC CPU time ratio of the driver is close to 100%. We suspected a memory leak. Thus, we have produced a heap dump and analyzed it using Eclipse Memory Analyzer.

Here is some interesting data from the driver's heap dump (heap size is 8 GB):
 * The estimated retained heap size of String objects (~5M instances) is 3.3 GB. It seems that most of these instances correspond to spark events.
 * Spark UI's AppStatusListener instance estimated retained size is 1.1 GB.
 * The number of LiveJob objects with status ""RUNNING"" is 18K, knowing that there shouldn't be more than 16 live running jobs since we use a fixed size thread pool of 16 threads to run spark queries.
 * The number of LiveTask objects is 485K.
 * The AsyncEventQueue instance associated to the AppStatusListener has a value of 854 for dropped events count and a value of 10001 for total events count, knowing that the dropped events counter is reset every minute and that the queue's default capacity is 10000.

We think that there is a memory leak in Spark UI. Here is our analysis of the root cause of this leak:
 * AppStatusListener is notified of Spark events using a bounded queue in AsyncEventQueue.
 * AppStatusListener updates its state (kvstore, liveTasks, liveStages, liveJobs, ...) based on the received events. For example, onTaskStart adds a task to liveTasks map and onTaskEnd removes the task from liveTasks map.
 * When the rate of events is very high, the bounded queue in AsyncEventQueue is full, some events are dropped and don't make it to AppStatusListener.
 * Dropped events that signal the end of a processing unit prevent the state of AppStatusListener from being cleaned. For example, a dropped onTaskEnd event, will prevent the task from being removed from liveTasks map, and the task will remain in the heap until the driver's JVM is stopped.

We were able to confirm our analysis by reducing the capacity of the AsyncEventQueue (spark.scheduler.listenerbus.eventqueue.capacity=10). After having launched many spark queries using this config, we observed that the number of active jobs in Spark UI increased rapidly and remained high even though all submitted queries have completed. We have also noticed that some executor task counters in Spark UI were negative, which confirms that AppStatusListener state does not accurately reflect the reality and that it can be a victim of event drops.

Suggested fix:
There are some limits today on the number of ""dead"" objects in AppStatusListener's maps (for example: spark.ui.retainedJobs). We suggest enforcing another configurable limit on the number of total objects in AppStatusListener's maps and kvstore. This should limit the leak in the case of high events rate, but AppStatusListener stats will remain inaccurate.",,,,,,,,,,,,,,,,,,SPARK-43524,,,,,,,,,,,,"20/May/23 23:21;aminebag;spark_shell_oom.log;https://issues.apache.org/jira/secure/attachment/13058388/spark_shell_oom.log","20/May/23 15:30;aminebag;spark_ui_memory_leak.zip;https://issues.apache.org/jira/secure/attachment/13058384/spark_ui_memory_leak.zip",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jun 05 21:25:08 UTC 2023,,,,,,,,,,"0|z1hxao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/23 12:42;srowen;I don't think that's a memory leak; you're just observing that it saves a lot of state when you ask it to. You can also reduce things like the number of tasks/jobs that it retains info about. You're also on a long-since EOL version of Spark which would not be supported.;;;","19/May/23 21:54;aminebag;Hi [~srowen], thanks for your reply.

 

However, I still think that it is a memory leak, since many jobs, stages and tasks get cumulated in memory and are never released.

For example, all jobs that have a ""RUNNING"" state and that missed the ""onJobEnd"" event (due to event drop from queue) will remain in the heap forever, and keep adding up indefinitely.

The same goes for tasks and stages and that missed the ""onTaskEnd"" and the ""onStageCompleted"" events.

 

As for the limits on the number of retained jobs, tasks and stages, they only apply to units that are considered finished (as shown below). Thus, these limits do not prevent the memory leak. That is why I suggest adding other limits that apply even to units that are still running or pending.
 * [AppStatusListener.scala cleanupJobs (line 1263)|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala#L1263-L1265] :

{code:java}
    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>
      j.info.status != JobExecutionStatus.RUNNING && j.info.status != JobExecutionStatus.UNKNOWN
    }{code}
 * [AppStatusListener.scala cleanupStagesInKVStore (line 1314)|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala#LL1314C1-L1316C1] :

{code:java}
    val stages = KVUtils.viewToSeq(view, countToDelete.toInt) { s =>
      s.info.status != v1.StageStatus.ACTIVE && s.info.status != v1.StageStatus.PENDING
    } {code}
 * [AppStatusListener.scala cleanupTasks (line 1377)|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala#LL1377C1-L1380C1] :

{code:java}
      // Try to delete finished tasks only.
      val toDelete = KVUtils.viewToSeq(view, countToDelete) { t =>
        !live || t.status != TaskState.RUNNING.toString()
      } {code}
 

As for the version, I have only used the Spark version 2.4.4. But I don't think that upgrading to a more recent version will fix the memory leak since the code causing the leak is still there.

 

Regards,;;;","19/May/23 22:14;srowen;You would need to report this versus a supported Spark version to be considered, because any change from this would have to be applied to master. It's entirely possible it is fixed and sounds like you haven't ruled that out. What causes a job to miss events? if that's what's happening then that is the issue not a memory leak. You're talking about memory consumed by things considered 'running'.;;;","19/May/23 22:33;aminebag;Theses units are only considered running from the point of view of Spark UI, while in reality they were already finished a long time ago and the application is totally idle.

Spark UI is not aware that these leaked units are already finished because the queue (AsyncEventQueue) it is using to listen to events (onJobEnd, onTaskEnd, onStageCompleted, ...) in order to update its state is full, and new events are dropped.

As shown below, we try to add the event to the bounded queue using LinkedBlockingQueue::offer. If the queue is full, the event is not inserted and the droppedEvents counter is incremented.

[AsyncEventQueue.scala post (line 152)|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala#LL152C1-L164C1] :
{code:java}
  def post(event: SparkListenerEvent): Unit = {
    if (stopped.get()) {
      return
    }    
    eventCount.incrementAndGet()
    if (eventQueue.offer(event)) {
      return
    }
    eventCount.decrementAndGet()
    droppedEvents.inc()
    droppedEventsCounter.incrementAndGet() {code}
 I will try to reproduce the leak on a more recent version of Spark and provide you with the results.;;;","20/May/23 15:31;aminebag;I have managed to reproduce the memory leak with Spark version 3.4.0 in standalone mode {*}within just 5 minutes of activity{*}.

{color:#0747a6}*Setup :*{color}
 * Cluster :

{code:java}
SPARK_WORKER_MEMORY=1g
SPARK_WORKER_INSTANCES=4
SPARK_WORKER_CORES=32 {code}
 * Application :

{code:java}
spark.scheduler.listenerbus.eventqueue.capacity=10
spark.executor.memory=512m{code}
 * Code submitted using spark-shell :

{code:java}
import Array._
import spark.implicits._
val uuid = udf(() => java.util.UUID.randomUUID().toString)
(1 to 100).foreach(x => sc.parallelize(range(x, 10000)).toDF(""id"")
 .repartition(1000)
 .withColumn(""uuid"", uuid())
 .withColumn(""key"", substring(col(""uuid""), 0, 2))
 .groupBy(""key"")
 .agg(count(""id"").alias(""c""))
 .sort(col(""c"").desc)
 .filter(x => x.getAs[Long](1) % 3 == 0)
 .count()) {code}
{color:#0747a6}*Results :*{color}
 * Logs :

{code:java}
23/05/20 16:44:04 ERROR AsyncEventQueue: Dropping event from queue appStatus. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
23/05/20 16:45:04 WARN AsyncEventQueue: Dropped 2560 events from appStatus since Sat May 20 16:44:04 CEST 2023.
23/05/20 16:46:04 WARN AsyncEventQueue: Dropped 8797 events from appStatus since Sat May 20 16:45:04 CEST 2023.
23/05/20 16:47:04 WARN AsyncEventQueue: Dropped 15909 events from appStatus since Sat May 20 16:46:04 CEST 2023.
23/05/20 16:48:04 WARN AsyncEventQueue: Dropped 20031 events from appStatus since Sat May 20 16:47:04 CEST 2023.{code}
 * Stats in the Spark UI at the end of processing (nothing is running anymore on the application) :
 ** 14 active jobs
 ** 15 active stages
 ** 4 pending stages
 ** -5303 active tasks
 * Heap dump of the driver :
 ** AppStatusListener estimated retained heap size is 95 MB.
 ** LiveTask objects count is 19k.
 ** LiveJob objects with status ""RUNNING"" is 14.

More details can be found in the file attached.;;;","20/May/23 23:16;aminebag;I have managed to induce a java heap space OutOfMemoryError within 3 hours of active processing using the same setup and code while increasing the number of iterations from 100 to 10000.

A heap dump of the driver (Xmx=1g), generated after the memory error, contains 128k LiveTask objects with an estimated retained size of 563 MB.

In my view, this is another strong evidence for the presence of the memory leak.;;;","20/May/23 23:21;aminebag;[^spark_shell_oom.log];;;","29/May/23 16:33;srowen;I still don't think this is a leak; I think you're launching a ton of jobs very quickly and have little driver memory. I am not sure this is realistic, but the cause is indeed stuff getting dropped in the event queue.;;;","29/May/23 19:44;aminebag;Hi [~srowen],

The purpose of the test is to illustrate the memory leak and be able to produce it quickly. It's not meant to be realistic. Regardles, this memory leak is something we have witnessed first hand on our production environment with a real life application : we have a driver with 8 gb of heap and it's memory became gradually full because of the leak within less than a week.

In my test, I have used a driver of only 1 gb of memory to quickly reach an OutOfMemory error. If I had used a driver with more memory (say 64 gb) I would have eventually arrived to the same result but it would have taken much more time.

Also, I'm indeed launching a lot of jobs. But, I'm launching them sequentially and not in parallel, to illustrate the point I'm trying to make here : every job execution leaves behind new objects that can't be garbage collected and that have no utility. I believe this is is by defintion a memory leak.

I'm quoting [this article|https://www.baeldung.com/java-memory-leaks#:~:text=A%20Memory%20Leak%20is%20a,degrades%20system%20performance%20over%20time] that provides a definition that matches exactly the issue I'm describing here : 
{code:java}
A Memory Leak is a situation where there are objects present in the heap that are no longer used, but the garbage collector is unable to remove them from memory, and therefore, they're unnecessarily maintained.
A memory leak is bad because it blocks memory resources and degrades system performance over time. If not dealt with, the application will eventually exhaust its resources, finally terminating with a fatal java.lang.OutOfMemoryError. {code};;;","29/May/23 19:49;srowen;OK, I don't think you've established that but it doesn't really matter - do you have a change to propose?;;;","29/May/23 19:59;aminebag;To contain the leak, I suggest adding a new boolean property that controls whether to apply the currently defined limits (spark.ui.retainedJobs, retainedStages, retainedTasks, ...) only on objects considered ""dead"" by Spark UI (which is the current behavior) or also on objects considered ""live"" by Spark UI.

I'll create the change and let you know.;;;","29/May/23 20:16;srowen;I've got one idea. The issue is indeed memory pressure b/c lots of tasks are queued up. We want the listeners to go faster if possible, but at least the allocation site that actually fails here could be made smarter.  In AppStatusListener, 

{code}
  def activeStages(): Seq[v1.StageData] = {
    liveStages.values.asScala
      .filter(s => Option(s.info).exists(_.submissionTime.isDefined))
      .map(_.toApi())
      .toList
      .sortBy(_.stageId)
  }
{code}

Change .toList to .toArray. This should avoid a slow sort and a copy or two. I'm not sure if that makes a difference but anything to reduce mem pressure and speed up event processing should contribute to avoiding the problem even in extreme setups like this.
;;;","29/May/23 20:36;aminebag;Sure, I'll include that in my change, even though I don't know why would .toArray provide better performance than .toList (I'm not very familiar with Scala).

I agree that speeding up the listener would reduce the severity of the issue, but I don't think that it's sufficient, since we can't guarantee that the AppStatusListener will be able to keep up with the speed of the event producer at the peaks of activity.

That's why I think it's important to provide a way to contain the leak when the AppStatusListener becomes a slow consumer (in addition to any other optimizations).;;;","01/Jun/23 12:22;aminebag;Hello [~srowen],

Here's the change I'm suggesting : https://github.com/apache/spark/pull/41423;;;","05/Jun/23 21:25;ci-cassandra.apache.org;User 'aminebag' has created a pull request for this issue:
https://github.com/apache/spark/pull/41423;;;"
Creating struct column occurs  error 'org.apache.spark.sql.AnalysisException [DATATYPE_MISMATCH.CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING]',SPARK-43522,13536374,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,fanjia,Heedo,Heedo,16/May/23 08:00,18/May/23 06:30,30/Oct/23 17:26,18/May/23 06:30,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,,,0,,,,,"When creating a struct column in Dataframe, the code that ran without problems in version 3.3.1 does not work in version 3.4.0.

 

Example
{code:java}
val testDF = Seq(""a=b,c=d,d=f"").toDF.withColumn(""key_value"", split('value, "","")).withColumn(""map_entry"", transform(col(""key_value""), x => struct(split(x, ""="").getItem(0), split(x, ""="").getItem(1) ) )){code}
 

In 3.3.1

 
{code:java}
 
testDF.show()
+-----------+---------------+--------------------+ 
|      value|      key_value|           map_entry| 
+-----------+---------------+--------------------+ 
|a=b,c=d,d=f|[a=b, c=d, d=f]|[{a, b}, {c, d}, ...| 
+-----------+---------------+--------------------+
 
testDF.printSchema()
root
 |-- value: string (nullable = true)
 |-- key_value: array (nullable = true)
 |    |-- element: string (containsNull = false)
 |-- map_entry: array (nullable = true)
 |    |-- element: struct (containsNull = false)
 |    |    |-- col1: string (nullable = true)
 |    |    |-- col2: string (nullable = true)
{code}
 

 

In 3.4.0

 
{code:java}
org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING] Cannot resolve ""struct(split(namedlambdavariable(), =, -1)[0], split(namedlambdavariable(), =, -1)[1])"" due to data type mismatch: Only foldable `STRING` expressions are allowed to appear at odd position, but they are [""0"", ""1""].;
'Project [value#41, key_value#45, transform(key_value#45, lambdafunction(struct(0, split(lambda x_3#49, =, -1)[0], 1, split(lambda x_3#49, =, -1)[1]), lambda x_3#49, false)) AS map_entry#48]
+- Project [value#41, split(value#41, ,, -1) AS key_value#45]
   +- LocalRelation [value#41]  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:269)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
....
 
{code}
 

However, if you do an alias to struct elements, you can get the same result as the previous version.

 
{code:java}
val testDF = Seq(""a=b,c=d,d=f"").toDF.withColumn(""key_value"", split('value, "","")).withColumn(""map_entry"", transform(col(""key_value""), x => struct(split(x, ""="").getItem(0).as(""col1"") , split(x, ""="").getItem(1).as(""col2"") ) )){code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 18 06:30:19 UTC 2023,,,,,,,,,,"0|z1hx7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/May/23 15:00;fanjia;https://github.com/apache/spark/pull/41187;;;","18/May/23 06:30;cloud_fan;Issue resolved by pull request 41187
[https://github.com/apache/spark/pull/41187];;;",,,,,,,,,,,,,
Unexpected NullPointerException or IllegalArgumentException inside UDFs of ML features caused by certain SQL functions,SPARK-43514,13536327,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,prometheus3375,prometheus3375,15/May/23 22:59,26/Jun/23 00:41,30/Oct/23 17:26,,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,,,,,ML,SQL,,,,0,ml,sql,,,"We designed a function that joins two DFs on common column with some similarity. All next code will be on Scala 2.12.

I've added {{show}} calls for demonstration purposes.

{code:scala}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{HashingTF, MinHashLSH, NGram, RegexTokenizer, MinHashLSHModel}
import org.apache.spark.sql.{DataFrame, Column}

/**
 * Joins two dataframes on a string column using LSH algorithm for similarity computation.
 *
 * The output dataframe has three columns:
 *
 *   - `datasetA`, data from the left dataframe.
 *   - `datasetB`, data from the right dataframe.
 *   - `distance`, abstract distance between values of `joinColumn` from datasets;
 *     the lower `distance` value, the more similar `joinColumn` values are.
 */
def similarityJoin(
  leftDf: DataFrame,
  rightDf: DataFrame,
  joinColumn: String,
  threshold: Double = 0.8,
): DataFrame = {
  leftDf.show(false)
  rightDf.show(false)

  val pipeline = new Pipeline().setStages(Array(
    new RegexTokenizer()
      .setPattern("""")
      .setMinTokenLength(1)
      .setInputCol(joinColumn)
      .setOutputCol(""tokens""),
    new NGram().setN(3).setInputCol(""tokens"").setOutputCol(""ngrams""),
    new HashingTF().setInputCol(""ngrams"").setOutputCol(""vectors""),
    new MinHashLSH().setInputCol(""vectors"").setOutputCol(""lsh""),
  ))

  val model = pipeline.fit(leftDf)

  val storedHashed = model.transform(leftDf)
  val landedHashed = model.transform(rightDf)

  def columnMapper(dsName: String)(c: String): Column = col(s""$dsName.$c"") as c

  val result = model
    .stages
    .last
    .asInstanceOf[MinHashLSHModel]
    .approxSimilarityJoin(storedHashed, landedHashed, threshold, ""distance"")
    .withColumn(""datasetA"", struct(leftDf.columns.map(columnMapper(""datasetA"")).toSeq: _*))
    .withColumn(""datasetB"", struct(rightDf.columns.map(columnMapper(""datasetB"")).toSeq: _*))

  result.show(false)

  result
}
{code}


Now consider such simple example:

{code:scala}
val inputDF1 = Seq("""", null).toDF(""name"").filter(length($""name"") > 2) as ""df1""
val inputDF2 = Seq("""", null).toDF(""name"").filter(length($""name"") > 2) as ""df2""

similarityJoin(inputDF1, inputDF2, ""name"", 0.6)
{code}

This example runs with no errors and outputs 3 empty DFs. Let's add {{distinct}} method to one data frame:

{code:scala}
val inputDF1 = Seq("""", null).toDF(""name"").distinct().filter(length($""name"") > 2) as ""df1""
val inputDF2 = Seq("""", null).toDF(""name"").filter(length($""name"") > 2) as ""df2""

similarityJoin(inputDF1, inputDF2, ""name"", 0.6)
{code}

This example outputs two empty DFs and then fails at {{result.show(false)}}. Error:

{code:none}
org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (LSHModel$$Lambda$3769/0x0000000101804840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => array<struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>).
  ... many elided
Caused by: java.lang.IllegalArgumentException: requirement failed: Must have at least 1 non zero entry.
  at scala.Predef$.require(Predef.scala:281)
  at org.apache.spark.ml.feature.MinHashLSHModel.hashFunction(MinHashLSH.scala:61)
  at org.apache.spark.ml.feature.LSHModel.$anonfun$transform$1(LSH.scala:99)
  ... many more
{code}

----

Now let's take a look on the example which is close to our application code. Define some helper functions:

{code:scala}
import org.apache.spark.sql.functions._


def process1(df: DataFrame): Unit = {
    val companies = df.select($""id"", $""name"")

    val directors = df
            .select(explode($""directors""))
            .select($""col.name"", $""col.id"")
            .dropDuplicates(""id"")

    val toBeMatched1 = companies
            .filter(length($""name"") > 2)
            .select(
                $""name"",
                $""id"" as ""sourceLegalEntityId"",
            )

    val toBeMatched2 = directors
            .filter(length($""name"") > 2)
            .select(
                $""name"",
                $""id"" as ""directorId"",
            )

    similarityJoin(toBeMatched1, toBeMatched2, ""name"", 0.6)
}

def process2(df: DataFrame): Unit = {
    def process_financials(column: Column): Column = {
        transform(
            column,
            x => x.withField(""date"", to_timestamp(x(""date""), ""dd MMM yyyy"")),
        )
    }

    val companies = df.select(
        $""id"",
        $""name"",
        struct(
            process_financials($""financials.balanceSheet"") as ""balanceSheet"",
            process_financials($""financials.capitalAndReserves"") as ""capitalAndReserves"",
        ) as ""financials"",
    )

    val directors = df
            .select(explode($""directors""))
            .select($""col.name"", $""col.id"")
            .dropDuplicates(""id"")

    val toBeMatched1 = companies
            .filter(length($""name"") > 2)
            .select(
                $""name"",
                $""id"" as ""sourceLegalEntityId"",
            )

    val toBeMatched2 = directors
            .filter(length($""name"") > 2)
            .select(
                $""name"",
                $""id"" as ""directorId"",
            )

    similarityJoin(toBeMatched1, toBeMatched2, ""name"", 0.6)
}
{code}

Function {{process2}} does the same job as {{process1}}, but also does some transforms on {{financials}} column before executing similarity join.

Example data frame and its schema:

{code:scala}
import org.apache.spark.sql.types._

val schema = StructType(
    Seq(
        StructField(""id"", StringType),
        StructField(""name"", StringType),
        StructField(
            ""directors"",
            ArrayType(
                StructType(Seq(StructField(""id"", StringType), StructField(""name"", StringType))),
                containsNull = true,
            ),
        ),
        StructField(
            ""financials"",
            StructType(
                Seq(
                    StructField(
                        ""balanceSheet"",
                        ArrayType(
                            StructType(Seq(
                                StructField(""date"", StringType),
                                StructField(""value"", StringType),
                            )
                            ),
                            containsNull = true,
                        ),
                    ),
                    StructField(
                        ""capitalAndReserves"",
                        ArrayType(
                            StructType(Seq(
                                StructField(""date"", StringType),
                                StructField(""value"", StringType),
                            )
                            ),
                            containsNull = true,
                        ),
                    ),
                ),
            ),
        ),
    )
)

val mainDF = (1 to 10)
        .toDF(""data"")
        .withColumn(""data"", lit(null) cast schema)
        .select(""data.*"")
{code}

This code just makes a data frame with 10 rows of null column casted to the specified schema.

Now let's pass {{mainDF}} to previously defined functions and observe results.

Example 1:
{code:scala}
process1(mainDF)
{code}
Outputs three empty DFs, no errors.

Example 2:
{code:scala}
process1(mainDF.distinct())
{code}
Outputs two empty DFs and then fails at {{result.show(false)}}. Error:

{code:none}
org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$3266/0x0000000101620040: (string) => array<string>).
  ... many elided
Caused by: java.lang.NullPointerException
  at org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)
  ... many more
{code}

Example 3:
{code:scala}
process2(mainDF)
{code}
Outputs two empty DFs and then fails at {{result.show(false)}}. Error:

{code:none}
org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$3266/0x0000000101620040: (string) => array<string>).
  ... many elided
Caused by: java.lang.NullPointerException
  at org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)
  ... many more
{code}

Somehow presence of {{distinct}} DF method or {{transform}} (or {{to_timestamp}}) SQL function before executing similarity join causes it to fail on empty input data frames. If these operations are done after join, then no errors are emitted.

---

In the examples above I used trivia data frames for testing, and by design of these examples {{similarityJoin}} gets empty DFs. They are empty because join column is preventively cleared from null values by {{filter(length($""name"") > 2)}}. We had the same issue with {{RegexTokenizer}} raising {{NullPointerException}} with the real data where data frames were not empty after identical filter statement. This is really strange to get {{NullPointerException}} for DFs which do not have null values in join column.

Current workaround: -call {{distinct}} DF method and {{transform}} SQL function after similarity join.- apparently, adding `.cache()` to the source DF resolves the issue.","Scala version: 2.12.17

Test examples were executed inside Zeppelin 0.10.1 with Spark 3.4.0.

Spark 3.3.2 deployed on cluster was used to check the issue on real data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/23 10:54;prometheus3375;2023-05-30 13-47-04.mp4;https://issues.apache.org/jira/secure/attachment/13058642/2023-05-30+13-47-04.mp4","07/Jun/23 17:47;ritikam;Plan.png;https://issues.apache.org/jira/secure/attachment/13058859/Plan.png","23/May/23 00:46;ritikam;Screen Shot 2023-05-22 at 5.39.55 PM.png;https://issues.apache.org/jira/secure/attachment/13058422/Screen+Shot+2023-05-22+at+5.39.55+PM.png","01/Jun/23 06:15;ritikam;Screen Shot 2023-05-31 at 11.14.24 PM.png;https://issues.apache.org/jira/secure/attachment/13058697/Screen+Shot+2023-05-31+at+11.14.24+PM.png","30/May/23 10:55;prometheus3375;Test.scala;https://issues.apache.org/jira/secure/attachment/13058643/Test.scala",,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,,,,Mon Jun 26 00:41:19 UTC 2023,,,,,,,,,,"0|z1hwx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/May/23 17:32;prometheus3375;-We applied ""current workaround"" to application code and this does not solve the issue.-
UPD: issue was resolved in application by calling `.cache()` DF method.;;;","23/May/23 00:47;ritikam;Unable to reproduce the error on 3.4.0

See the attachment. !Screen Shot 2023-05-22 at 5.39.55 PM.png!

 ;;;","30/May/23 10:59;prometheus3375;[~ritikam], as shown in the console, you were using Spark 3.3.0 ({_}Welcome to SPARK version 3.3.0{_}). Here is screen cast with fresh installed Spark 3.4.0 (only custom log config were used).

Scala file used during recording: [^Test.scala]. Note: in this file the result of similarity join has different columns, but this does not affect anything. I've updated the function in the report too.

Screencast: [^2023-05-30 13-47-04.mp4].;;;","01/Jun/23 06:15;ritikam;My SPARK_HOME was still pointing to 3.3.0, once fixed I do see the reported error

 !Screen Shot 2023-05-31 at 11.14.24 PM.png! ;;;","07/Jun/23 19:38;ritikam;Hello please add this to your sparks-defaults.conf and this should solve the problem.


{code:java}
spark.sql.optimizer.excludedRules                    org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation
{code}
;;;","07/Jun/23 20:48;prometheus3375;[~ritikam] is there a way to specify this setting when creating SparkContext (i.e. in code)?

I saw your deleted comment, can you also check 3.3.2 version? We have the issue in this version too.;;;","07/Jun/23 21:35;ritikam;I had deleted the comment because it was incorrect. I could recreate the issue in 3.3.0 as well. Once we exclude the ConvertToLocalRelation rule it works both in 3.3.0 and 3.4.0.

By excluding this rule the Filter clause does not go through optimization rules and it was the Filter clause that was throwing the Null pointer exception.;;;","07/Jun/23 22:34;prometheus3375;We were also having weird issue with data teleportation on cluster. There is a data frame, we are using {{filter}} by some values from ID column and then {{show}} to view its contents. Then we do bunch of selects; ID column is unchanged. We again {{filter}} by ID column and use {{show}}, but now we have some rows missing or some rows added.

We resolved this issue by using {{cache}} method after {{groupBy}} and after {{similarityJoin}} before both {{show}}. Is there a possibility that this optimization rule is also causing this problem?;;;","08/Jun/23 04:03;ritikam;Please provide the code to recreate the issue. ;;;","08/Jun/23 15:36;ritikam;Please refer to Spark Properties section in the documentation to see how to set the excludeRule property on SparkConf
https://spark.apache.org/docs/latest/configuration.html#spark-properties;;;","09/Jun/23 11:21;prometheus3375;[~ritikam], thank you for sharing the link. I've added this rule, and it indeed resolved the issue with NullPointerException, but our code now fails with other exception.

{code:none}
Caused by: org.apache.spark.SparkException: Failed to execute user defined function (LSHModel$$Lambda$1964/0x0000000800d42840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => array<struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.ScalaUDF_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.subExpr_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)
	at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:276)
	at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:275)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: requirement failed: Must have at least 1 non zero entry.
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.ml.feature.MinHashLSHModel.hashFunction(MinHashLSH.scala:61)
	at org.apache.spark.ml.feature.LSHModel.$anonfun$transform$1(LSH.scala:99)
	... 22 more
{code}

Unfortunately, examples provided in the issue do not fail with excluded rule you suggested, and I have not come up with new examples to reproduce. However, we've fixed all the issues (exceptions and data ""teleportation"") already by adding method {{cache}} to some dataframes.;;;","26/Jun/23 00:41;ritikam;Somehow your code is invoking the hashFunction on MinHashLSHModel with Vector containing zero entires

And the following code throws an error
 
{code:java}
 @Since(""2.1.0"")
  override protected[ml] def hashFunction(elems: Vector): Array[Vector] = {
    require(elems.nonZeroIterator.nonEmpty, ""Must have at least 1 non zero entry."")
{code}

But the example you have provided is not invoking the hashFunction

;;;",,,
withColumnRenamed duplicates columns if new column already exists,SPARK-43513,13536317,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,frederik.paradis,frederik.paradis,15/May/23 20:58,11/Jul/23 21:02,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"withColumnRenamed should either replace the column when new column already exists or should specify the specificity in the documentation. See the code below as an example of the current state.

{code:python}
from pyspark.sql import SparkSession

spark = SparkSession.builder.master(""local[1]"").appName(""local-spark-session"").getOrCreate()

df = spark.createDataFrame([(1, 0.5, 0.4), (2, 0.5, 0.8)], [""id"", ""score"", ""test_score""])
r = df.withColumnRenamed(""test_score"", ""score"")
print(r)  # DataFrame[id: bigint, score: double, score: double]

# pyspark.sql.utils.AnalysisException: Reference 'score' is ambiguous, could be: score, score.
print(r.select(""score""))
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 11 21:02:26 UTC 2023,,,,,,,,,,"0|z1hwv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/23 19:06;wenxin;Some observations
1. This is not unique to 3.4.0 release. I'm seeing the same issue when testing the same procedure on PySpark 3.3
2. This issue might not unique to ""withColumnRenamed"" method. More broadly speaking, we do allow creating a pyspark frame with duplicate column names. However, we only allow selecting columns by the ""names"". (Pyspark doesn't have the ""iloc"" dataframe selection as Pandas library does). As a result, when we have a pyspark dataframe with duplicate column names, select with the duplicate column name will always throw the exception.

For example 
{code:java}
>>> df3 = spark.createDataFrame([('Monday',25,27,29,30),('Tuesday',40,38,36,34),('Wednesday',18,20,22,17),('Thursday',25,27,29,19)],['day','temperature','temperature','temperature','temperature'])

>>> df3.show(2)
+-------+-----------+-----------+-----------+-----------+
|    day|temperature|temperature|temperature|temperature|
+-------+-----------+-----------+-----------+-----------+
| Monday|         25|         27|         29|         30|
|Tuesday|         40|         38|         36|         34|
+-------+-----------+-----------+-----------+-----------+

>>> df3.select('temperature')
site-packages/pyspark/sql/utils.py"", line 196, in deco raise converted from None
pyspark.sql.utils.AnalysisException: Reference 'temperature' is ambiguous, could be: temperature, temperature, temperature, temperature. {code}
One known workaround for this is to use dataframe.toDF method to rename the column names, as illustrated here https://www.geeksforgeeks.org/pyspark-dataframe-distinguish-columns-with-duplicated-name/;;;","11/Jul/23 21:02;frederik.paradis;Hi [~wenxin]. Thank you for your comment.

1. Didn't know what to put there so I just put the latest version.
2. I see what you mean. However, it seems counter-intuitive to me to allow to allow columns with the same name and no other ways to differentiate them other than their positions. Especially with the fact that joins do move columns around and that (mostly?) all operations in Spark do not support referring to columns by their positions. Beyond that, I guess it's more of a question of engineering design and vision of the software.;;;",,,,,,,,,,,,,
Spark application hangs when YarnAllocator adds running executors after processing completed containers,SPARK-43510,13536286,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mauzhang,mauzhang,mauzhang,15/May/23 15:35,06/Jun/23 13:30,30/Oct/23 17:26,06/Jun/23 13:30,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,YARN,,,,,0,,,,,"I see application hangs when containers are preempted immediately after allocation as follows.
{code:java}
23/05/14 09:11:33 INFO YarnAllocator: Launching container container_e3812_1684033797982_57865_01_000382 on host hdc42-mcc10-01-0910-4207-015-tess0028.stratus.rno.ebay.com for executor with ID 277 for ResourceProfile Id 0 
23/05/14 09:11:33 WARN YarnAllocator: Cannot find executorId for container: container_e3812_1684033797982_57865_01_000382
23/05/14 09:11:33 INFO YarnAllocator: Completed container container_e3812_1684033797982_57865_01_000382 (state: COMPLETE, exit status: -102)
23/05/14 09:11:33 INFO YarnAllocator: Container container_e3812_1684033797982_57865_01_000382 was preempted.{code}
Note the warning log where YarnAllocator cannot find executorId for the container when processing completed containers. The only plausible cause is YarnAllocator added the running executor after processing completed containers. The former happens in a separate thread after executor launch.

YarnAllocator believes there are still running executors, although they are already lost due to preemption. Hence, the application hangs without any running executors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 19 04:20:15 UTC 2023,,,,,,,,,,"0|z1hwog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/23 04:20;snoot;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/41173;;;",,,,,,,,,,,,,,
The status of free pages in allocatedPages mismatch with pageTable after invoking TaskMemoryManager::cleanUpAllAllocatedMemory,SPARK-43507,13536194,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,qian heng,qian heng,15/May/23 08:38,15/May/23 08:42,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"For TaskMemoryManager,  `allocatePags`  is a Bitmap for tracking free pages in `pageTables`.

But inner TaskMemoryManager::cleanUpAllAllocatedMemory, it fills up `pageTables` with null but not clear up the `allocatePags`. It leads to the status of free pages mismatch between them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-15 08:38:34.0,,,,,,,,,,"0|z1hw40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deserialisation Failure on State Store Schema Evolution (Spark Structured Streaming),SPARK-43503,13536163,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,varunarora279,varunarora279,15/May/23 05:40,18/May/23 02:25,30/Oct/23 17:26,,3.2.1,,,,,,,,,,,,,,,,,,,,,,,Structured Streaming,,,,,0,,,,,"In streaming query, state is persisted in RocksDB using mapGroupsWithState function. We use Encoders.bean to serialise state and store it in State Store. Code snippet :-

 
{code:java}
df
        .groupByKey((MapFunction<Row, String>) event -> event.getAs(""stateGroupingId""), Encoders.STRING())
        .mapGroupsWithState(mapGroupsWithStateFunction, Encoders.bean(StateInfo.class), Encoders.bean(StateOutput.class), GroupStateTimeout.ProcessingTimeTimeout()); {code}

As per the above example, StateInfo bean contains state information which is stored in State store. However, on adding/removing field from StateInfo bean and on re-running query we get deserialisation exception. Is there a way to handle this scenario or to provide custom deserialisation to handle schema evolution.

Exception :-
{code:java}
Stack: [0x000070000cd0a000,0x000070000ce0a000],  sp=0x000070000ce08400,  free space=1017k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
V  [libjvm.dylib+0x57c2b5]  Unsafe_GetLong+0x55
J 8700  sun.misc.Unsafe.getLong(Ljava/lang/Object;J)J (0 bytes) @ 0x000000010fe9e6be [0x000000010fe9e600+0xbe]
j  org.apache.spark.unsafe.Platform.getLong(Ljava/lang/Object;J)J+5
j  org.apache.spark.sql.catalyst.expressions.UnsafeArrayData.pointTo(Ljava/lang/Object;JI)V+2
j  org.apache.spark.sql.catalyst.expressions.UnsafeMapData.pointTo(Ljava/lang/Object;JI)V+187
j  org.apache.spark.sql.catalyst.expressions.UnsafeRow.getMap(I)Lorg/apache/spark/sql/catalyst/expressions/UnsafeMapData;+52
j  org.apache.spark.sql.catalyst.expressions.UnsafeRow.getMap(I)Lorg/apache/spark/sql/catalyst/util/MapData;+2
j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$SpecificSafeProjection;Lorg/apache/spark/sql/catalyst/InternalRow;)Lorg/apache/spark/sql/catalyst/util/ArrayData;+53
j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.StaticInvoke_0$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$SpecificSafeProjection;Lorg/apache/spark/sql/catalyst/InternalRow;)Ljava/util/Map;+14
j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.initializeJavaBean_0_1$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$SpecificSafeProjection;Lorg/apache/spark/sql/catalyst/InternalRow;Lorg/example/streaming/StateInfo;)V+2
j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Ljava/lang/Object;)Ljava/lang/Object;+74
j  org.apache.spark.sql.execution.ObjectOperator$.$anonfun$deserializeRowToObject$1(Lorg/apache/spark/sql/catalyst/expressions/package$Projection;Lorg/apache/spark/sql/catalyst/expressions/Expression;Lorg/apache/spark/sql/catalyst/InternalRow;)Ljava/lang/Object;+2
j  org.apache.spark.sql.execution.ObjectOperator$$$Lambda$2600.apply(Ljava/lang/Object;)Ljava/lang/Object;+12
j  org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelper$StateManagerImplBase.getStateObject(Lorg/apache/spark/sql/catalyst/expressions/UnsafeRow;)Ljava/lang/Object;+9
j  org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelper$StateManagerImplBase.getState(Lorg/apache/spark/sql/execution/streaming/state/StateStore;Lorg/apache/spark/sql/catalyst/expressions/UnsafeRow;)Lorg/apache/spark/sql/execution/streaming/state/FlatMapGroupsWithStateExecHelper$StateData;+16
j  org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$InputProcessor.$anonfun$processNewData$1(Lorg/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$InputProcessor;Lscala/Tuple2;)Lscala/collection/GenTraversableOnce;+45
j  org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$InputProcessor$$Lambda$3237.apply(Ljava/lang/Object;)Ljava/lang/Object;+8
J 5928 C2 scala.collection.Iterator$$anon$11.hasNext()Z (35 bytes) @ 0x000000010e9a6a58 [0x000000010e9a6620+0x438]
j  org.apache.spark.util.CompletionIterator.hasNext()Z+4
j  scala.collection.Iterator$ConcatIterator.hasNext()Z+22
j  org.apache.spark.util.CompletionIterator.hasNext()Z+4
J 2875 C1 scala.collection.Iterator$$anon$10.hasNext()Z (10 bytes) @ 0x000000010f1264cc [0x000000010f1263c0+0x10c]
J 5819 C1 scala.collection.Iterator$$anon$12.hasNext()Z (60 bytes) @ 0x000000010eb53e44 [0x000000010eb53ce0+0x164]
J 2875 C1 scala.collection.Iterator$$anon$10.hasNext()Z (10 bytes) @ 0x000000010f1264cc [0x000000010f1263c0+0x10c]
J 2875 C1 scala.collection.Iterator$$anon$10.hasNext()Z (10 bytes) @ 0x000000010f1264cc [0x000000010f1263c0+0x10c]
J 2875 C1 scala.collection.Iterator$$anon$10.hasNext()Z (10 bytes) @ 0x000000010f1264cc [0x000000010f1263c0+0x10c]
J 2875 C1 scala.collection.Iterator$$anon$10.hasNext()Z (10 bytes) @ 0x000000010f1264cc [0x000000010f1263c0+0x10c]
j  org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(ZILscala/collection/Iterator;)Lscala/collection/Iterator;+199
j  org.apache.spark.sql.execution.SparkPlan$$Lambda$3096.apply(Ljava/lang/Object;)Ljava/lang/Object;+12
j  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(Lscala/Function1;Lorg/apache/spark/TaskContext;ILscala/collection/Iterator;)Lscala/collection/Iterator;+2
j  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(Lscala/Function1;Lorg/apache/spark/TaskContext;Ljava/lang/Object;Lscala/collection/Iterator;)Lscala/collection/Iterator;+7
j  org.apache.spark.rdd.RDD$$Lambda$2500.apply(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+13
j  org.apache.spark.rdd.MapPartitionsRDD.compute(Lorg/apache/spark/Partition;Lorg/apache/spark/TaskContext;)Lscala/collection/Iterator;+27
j  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(Lorg/apache/spark/Partition;Lorg/apache/spark/TaskContext;)Lscala/collection/Iterator;+26
j  org.apache.spark.rdd.RDD.iterator(Lorg/apache/spark/Partition;Lorg/apache/spark/TaskContext;)Lscala/collection/Iterator;+42
j  org.apache.spark.scheduler.ResultTask.runTask(Lorg/apache/spark/TaskContext;)Ljava/lang/Object;+203
j  org.apache.spark.scheduler.Task.run(JILorg/apache/spark/metrics/MetricsSystem;Lscala/collection/immutable/Map;Lscala/Option;)Ljava/lang/Object;+226
j  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Lorg/apache/spark/executor/Executor$TaskRunner;Lscala/runtime/BooleanRef;)Ljava/lang/Object;+36
j  org.apache.spark.executor.Executor$TaskRunner$$Lambda$2458.apply()Ljava/lang/Object;+8
j  org.apache.spark.util.Utils$.tryWithSafeFinally(Lscala/Function0;Lscala/Function0;)Ljava/lang/Object;+4
j  org.apache.spark.executor.Executor$TaskRunner.run()V+443
j  java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V+95
j  java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub
V  [libjvm.dylib+0x2d4795]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x771
V  [libjvm.dylib+0x2d3460]  JavaCalls::call_virtual(JavaValue*, KlassHandle, Symbol*, Symbol*, JavaCallArguments*, Thread*)+0x14a
V  [libjvm.dylib+0x2d367b]  JavaCalls::call_virtual(JavaValue*, Handle, KlassHandle, Symbol*, Symbol*, Thread*)+0x57
V  [libjvm.dylib+0x341cf5]  thread_entry(JavaThread*, Thread*)+0x78
V  [libjvm.dylib+0x564226]  JavaThread::thread_main_inner()+0x82
V  [libjvm.dylib+0x5640eb]  JavaThread::run()+0x19d
V  [libjvm.dylib+0x48f359]  java_start(Thread*)+0xfa
C  [libsystem_pthread.dylib+0x61d3]  _pthread_start+0x7d
C  [libsystem_pthread.dylib+0x1bd3]  thread_start+0xf
C  0x0000000000000000{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-15 05:40:14.0,,,,,,,,,,"0|z1hvxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame.drop should support empty column,SPARK-43502,13536157,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,15/May/23 04:12,16/May/23 04:38,30/Oct/23 17:26,16/May/23 04:38,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,PySpark,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 16 04:38:27 UTC 2023,,,,,,,,,,"0|z1hvw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/May/23 04:38;podongfeng;Issue resolved by pull request 41180
[https://github.com/apache/spark/pull/41180];;;",,,,,,,,,,,,,,
In expression not compatible with EqualTo Expression,SPARK-43491,13536060,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,rememberliu,rememberliu,13/May/23 05:11,17/Oct/23 00:17,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,pull-request-available,,,,"The query results of Spark SQL 3.1.1  and Hive SQL 3.1.0 are inconsistent with same sql. Spark SQL calculates `{{{}0 in ('00')`{}}} as false, which act different from `{{{}=`{}}} keyword, but Hive calculates true. Hive is compatible with the `{{{}in`{}}} keyword in 3.1.0, but SparkSQL does not.

It's better  when dataTypes of elements in `{{{}In`{}}} expression are the same, it should behaviour as same as BinaryComparison like ` {{{}EqualTo`{}}}.

Test SQL:
{code:java}
scala> spark.sql(""select 1 as test where 0 = '00'"").show
+----+
|test|
+----+
|   1|
+----+

scala> spark.sql(""select 1 as test where 0 in ('00')"").show
+----+
|test|
+----+
+----+{code}
 

!image-2023-05-13-13-14-55-853.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/23 05:14;rememberliu;image-2023-05-13-13-14-55-853.png;https://issues.apache.org/jira/secure/attachment/13058055/image-2023-05-13-13-14-55-853.png","13/May/23 05:15;rememberliu;image-2023-05-13-13-15-50-685.png;https://issues.apache.org/jira/secure/attachment/13058056/image-2023-05-13-13-15-50-685.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat May 13 09:09:51 UTC 2023,,,,,,,,,,"0|z1hvg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/May/23 09:09;githubbot;User 'liukuijian8040' has created a pull request for this issue:
https://github.com/apache/spark/pull/41162;;;",,,,,,,,,,,,,,
number of files read is incorrect if it is bucket table,SPARK-43486,13535920,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,yumwang,yumwang,12/May/23 06:43,14/Jun/23 01:33,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,, !screenshot-1.png! ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/23 06:43;yumwang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13058028/screenshot-1.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jun 14 01:33:43 UTC 2023,,,,,,,,,,"0|z1hul4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/May/23 00:08;panbingkun;Can I do it? [~yumwang] ;;;","13/May/23 01:15;yumwang;[~panbingkun] Yes, please.;;;","13/Jun/23 12:00;fanjia;[~panbingkun] Hi, any update for this?;;;","14/Jun/23 00:50;panbingkun;Sorry, I didn't reproduce it.;;;","14/Jun/23 01:33;fanjia;I didn't reproduce it too.:(;;;",,,,,,,,,,
Confused errors from the DATEADD function,SPARK-43485,13535919,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,12/May/23 06:42,15/May/23 11:34,30/Oct/23 17:26,15/May/23 10:55,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"The code example portraits the issue:

{code:sql}
spark-sql (default)> select dateadd('MONTH', 1, date'2023-05-11');
[WRONG_NUM_ARGS.WITHOUT_SUGGESTION] The `dateadd` requires 2 parameters but the actual number is 3. Please, refer to 'https://spark.apache.org/docs/latest/sql-ref-functions.html' for a fix.; line 1 pos 7
{code}

The error says about number of arguments passed to DATEADD but the issue is about the type of the first argument.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon May 15 11:34:06 UTC 2023,,,,,,,,,,"0|z1hukw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/May/23 10:55;maxgekk;Issue resolved by pull request 41143
[https://github.com/apache/spark/pull/41143];;;","15/May/23 11:34;awsthni;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/41143;;;",,,,,,,,,,,,,
Kafka/Kinesis Assembly should not package hadoop-client-runtime,SPARK-43484,13535918,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,12/May/23 06:27,12/May/23 16:18,30/Oct/23 17:26,12/May/23 16:14,3.2.4,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,,,,3.5.0,,,,Build,Structured Streaming,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 12 16:18:34 UTC 2023,,,,,,,,,,"0|z1huko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/May/23 16:14;csun;Issue resolved by pull request 41152
[https://github.com/apache/spark/pull/41152];;;","12/May/23 16:18;hudson;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/41152;;;",,,,,,,,,,,,,
Expression lineage not handling Window and AggregatePart operators,SPARK-43475,13535889,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,rkkorlapati,rkkorlapati,12/May/23 00:41,30/May/23 02:25,30/Oct/23 17:26,30/May/23 02:25,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Optimizer,,,,,0,,,,,Expression lineage logic (findExpressionAndTrackLineageDown) has missing handling for partial aggregation (AggregatePart) and Window operators. This is prohibiting optimizations such as bloom filters which depends on expression lineage,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-12 00:41:49.0,,,,,,,,,,"0|z1hue8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Mexico has changed observation of DST, this breaks timestamp_utc()",SPARK-43472,13535884,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,srielau,srielau,11/May/23 21:52,11/May/23 21:52,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"[https://www.timeanddate.com/time/change/mexico/mexico-city?year=2023#:~:text=Daylight%20Saving%20Time%20(DST)%20Not,was%20on%20October%2030%2C%202022.]

Mexico has stopped observing DST. This results in wrong results for:
from_utc_timestamp([timestamp], 'America/Mexico_City')",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-11 21:52:22.0,,,,,,,,,,"0|z1hud4:",9223372036854775807,,,,,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle missing hadoopProperties and metricsProperties,SPARK-43471,13535866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,qitan,qitan,qitan,11/May/23 20:06,11/May/23 22:30,30/Oct/23 17:26,11/May/23 22:30,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 11 22:30:35 UTC 2023,,,,,,,,,,"0|z1hu94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/23 22:30;dongjoon;Issue resolved by pull request 41145
[https://github.com/apache/spark/pull/41145];;;",,,,,,,,,,,,,,
makeDotNode should not fail when DeterministicLevel is absent,SPARK-43441,13535700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,qitan,qitan,qitan,10/May/23 21:22,11/May/23 04:20,30/Oct/23 17:26,11/May/23 04:20,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 11 04:20:17 UTC 2023,,,,,,,,,,"0|z1ht88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/23 04:20;dongjoon;Issue resolved by pull request 41124
[https://github.com/apache/spark/pull/41124];;;",,,,,,,,,,,,,,
Drop does not work when passed a string with an alias,SPARK-43439,13535681,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,frederik.paradis,frederik.paradis,10/May/23 18:12,16/Jul/23 15:07,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"When passing a string to the drop method, if the string contains an alias, the column is not dropped. However, passing a column object with the same name and alias, it works.
{code:python}
from pyspark.sql import SparkSession
import pyspark.sql.functions as F

spark = SparkSession.builder.master(""local[1]"").appName(""local-spark-session"").getOrCreate()

df = spark.createDataFrame([(1, 10)], [""any"", ""hour""]).alias(""a"")

j = df.drop(""a.hour"")
print(j)  # DataFrame[any: bigint, hour: bigint]

jj = df.drop(F.col(""a.hour""))
print(jj)  # DataFrame[any: bigint]
{code}
 

Related issues:

https://issues.apache.org/jira/browse/SPARK-31123

https://issues.apache.org/jira/browse/SPARK-14759

 ",,,,,,,,,,,,,,,,,,,,,,SPARK-31123,SPARK-14759,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,pyspark,,,,Sun Jul 16 15:07:46 UTC 2023,,,,,,,,,,"0|z1ht40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/23 15:07;frederik.paradis;Just went through the source code and it seems there is a different semantic to drop when passed strings vs Column objects. It seems that the documentation will highlight that in the next version:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2850
https://github.com/apache/spark/blob/master/python/pyspark/sql/dataframe.py#L5144

However, it seems weird to me in Python that the arguments of this function have a different semantic than the select function which under the hood converts everything into Column objects. One suggestion I might do would be to create a drop_column function which would have the same semantic as select.;;;",,,,,,,,,,,,,,
Unsigned integer types are deserialized as signed numeric equivalents,SPARK-43427,13535519,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,justaparth,justaparth,09/May/23 12:37,18/Sep/23 00:17,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Protobuf,,,,,0,pull-request-available,,,,"I'm not sure if ""bug"" is the correct tag for this jira, but i've tagged it like that for now since the behavior seems odd, happy to update to ""improvement"" or something else based on the conversation!

h2. Issue

Protobuf supports unsigned integer types, including `uint32` and `uint64`. When deserializing protobuf values with fields of these types, uint32 is converted to `IntegerType` and uint64 is converted to `LongType` in the resulting spark struct. `IntegerType` and `LongType` are [signed|https://spark.apache.org/docs/latest/sql-ref-datatypes.html] integer types, so this can lead to confusing results.

Namely, if a uint32 value in a stored proto is above 2^31 or a uint64 value is above 2^63, their representation in binary will contain a 1 in the highest bit, which when interpreted as a signed integer will come out as negative (I.e. overflow).

I propose that we deserialize unsigned integer types into a type that can contain them correctly, e.g.
uint32 => `LongType`
uint64 => `Decimal(20, 0)`

h2. Backwards Compatibility / Default Behavior
Should we maintain backwards compatibility and we add an option that allows deserializing these types differently? Or should we change change the default behavior (with an option to go back to the old way)? 

I think by default it makes more sense to deserialize them as the larger types so that it's semantically more correct. However, there may be existing users of this library that would be affected by this behavior change. Though, maybe we can justify the change since the function is tagged as `Experimental` (and spark 3.4.0 was only released very recently).

h2. Precedent
I believe that unsigned integer types in parquet are deserialized in a similar manner, i.e. put into a larger type so that the unsigned representation natively fits. https://issues.apache.org/jira/browse/SPARK-34817 and https://github.com/apache/spark/pull/31921",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 11 09:02:58 UTC 2023,,,,,,,,,,"0|z1hs40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/23 09:02;ignitetcbot;User 'justaparth' has created a pull request for this issue:
https://github.com/apache/spark/pull/41108;;;",,,,,,,,,,,,,,
Add TimestampNTZType to ColumnarBatchRow,SPARK-43425,13535499,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fokko,fokko,fokko,09/May/23 09:55,11/May/23 00:45,30/Oct/23 17:26,10/May/23 04:31,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 10 04:31:00 UTC 2023,,,,,,,,,,"0|z1hrzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/May/23 04:31;dongjoon;Issue resolved by pull request 41103
[https://github.com/apache/spark/pull/41103];;;",,,,,,,,,,,,,,
Tags are lost on LogicalRelation when adding _metadata,SPARK-43422,13535475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,olaky,olaky,olaky,09/May/23 06:45,10/May/23 08:17,30/Oct/23 17:26,10/May/23 08:17,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Optimizer,,,,,0,,,,,The  AddMetadataColumns does not copy tags for the LogicalRelation when adding metadata output in addMetadataCol,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 10 08:17:22 UTC 2023,,,,,,,,,,"0|z1hru8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/May/23 08:17;gurwls223;Issue resolved by pull request 41104
[https://github.com/apache/spark/pull/41104];;;",,,,,,,,,,,,,,
IN subquery ListQuery has wrong nullability,SPARK-43413,13535421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jchen5,jchen5,jchen5,08/May/23 21:12,16/May/23 01:41,30/Oct/23 17:26,16/May/23 01:41,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"IN subquery expressions are incorrectly marked as non-nullable, even when they are actually nullable. They correctly check the nullability of the left-hand-side, but the right-hand-side of a IN subquery, the ListQuery, is currently defined with nullability = false always. This is incorrect and can lead to incorrect query transformations.

Example: (non_nullable_col IN (select nullable_col)) <=> TRUE . Here the IN expression returns NULL when the nullable_col is null, but our code marks it as non-nullable, and therefore SimplifyBinaryComparison transforms away the <=> TRUE, transforming the expression to non_nullable_col IN (select nullable_col) , which is an incorrect transformation because NULL values of nullable_col now cause the expression to yield NULL instead of FALSE.

This bug can potentially lead to wrong results, but in most cases this doesn't directly cause wrong results end-to-end, because IN subqueries are almost always transformed to semi/anti/existence joins in RewritePredicateSubquery, and this rewrite can also incorrectly discard NULLs, which is another bug. But we can observe it causing wrong behavior in unit tests, and it could easily lead to incorrect query results if there are changes to the surrounding context, so it should be fixed regardless.

This is a long-standing bug that has existed at least since 2016, as long as the ListQuery class has existed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 16 01:41:20 UTC 2023,,,,,,,,,,"0|z1hrig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/23 04:57;ci-cassandra.apache.org;User 'jchen5' has created a pull request for this issue:
https://github.com/apache/spark/pull/41094;;;","16/May/23 01:41;cloud_fan;Issue resolved by pull request 41094
[https://github.com/apache/spark/pull/41094];;;",,,,,,,,,,,,,
Can't union dataframes with # in subcolumn name,SPARK-43411,13535404,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,rraveendran-msft,rraveendran-msft,08/May/23 17:50,08/May/23 19:31,30/Oct/23 17:26,08/May/23 19:31,3.1.2,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"I was using Spark within an Azure Synapse notebook to load dataframes from various storage accounts and union them into a single dataframe, but it seems to fail as the SQL internal to union doesn't handle special characters properly. Here is a code example of what I was running:
{code:java}
val data1 = spark.read.parquet(""abfss://PATH1"")
val data2 = spark.read.parquet(""abfss://PATH2"")
val data3 = spark.read.parquet(""abfss://PATH3"")
val data4 = spark.read.parquet(""abfss://PATH4"")

val data = data1
.unionByName(data2, allowMissingColumns=true)
.unionByName(data3, allowMissingColumns=true)
.unionByName(data4, allowMissingColumns=true)

data.printSchema() {code}
The issue arose due to having a StructType column, e.g. ABC, that has a subcolumn with # in the name, e.g. #XYZ#. This doesn't seem to be a problem outright, as other Spark functions like select work fine:
{code:java}
data1.select(""ABC.#XYZ#"").where(col(""#XYZ#"").isNotNull).show(5, truncate = false) {code}
However, when I ran the earlier snippet with the union statements, I get this error:
{code:java}
org.apache.spark.sql.catalyst.parser.ParseException:
extraneous input '#' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 0)
 
== SQL ==
#XYZ#
^^^ {code}
This seems to indicate the issue is with the implementation of unionByName/union etc. however I'm not familiar enough with the codebase to figure out where this would be an issue (I was able to trace that UnionByName calls on Union which I think is defined here: [spark/basicLogicalOperators.scala at master · apache/spark · GitHub|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala])","* Azure Synapse Notebooks
 * Apache Spark Pool: [Azure Synapse Runtime for Apache Spark 3.1 (EOLA) - Azure Synapse Analytics | Microsoft Learn|https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-3-runtime]
 ** Spark 3.1.2
 ** Ubuntu 18.04
 ** Python 3.8
 ** Scala 2.12.10
 ** Hadoop 3.1.1
 ** Java 1.8.0_282
 ** .NET Core 3.1
 ** .NET for Apache Spark 2.0.0
 ** Delta Lake 1.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon May 08 19:31:03 UTC 2023,,,,,,,,,,"0|z1hreo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/May/23 19:31;rraveendran-msft;Turns out this issue isn't present in later versions of Spark (I just tested on the latest version in Synapse, which is 3.3.1, and it worked). I'm assuming since this version is EOL this will be a wontfix, hence closing this out.;;;",,,,,,,,,,,,,,
Filter current version while reusing sst files for RocksDB state store provider while uploading to DFS to prevent id mismatch,SPARK-43404,13535324,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,anishshri-db,anishshri-db,anishshri-db,08/May/23 03:59,09/Jun/23 11:50,30/Oct/23 17:26,08/May/23 23:29,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Structured Streaming,,,,,0,,,,,Filter current version while reusing sst files for RocksDB state store provider while uploading to DFS to prevent id mismatch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 09 09:13:19 UTC 2023,,,,,,,,,,"0|z1hqww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/May/23 06:10;anishshri-db;PR here: [https://github.com/apache/spark/pull/41089]

 

cc - [~kabhwan] ;;;","08/May/23 23:29;kabhwan;Issue resolved by pull request 41089
[https://github.com/apache/spark/pull/41089];;;","09/Jun/23 09:13;githubbot;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/41530;;;",,,,,,,,,,,,
GET /history/<appId>/1/jobs/ failed: java.lang.IllegalStateException: DB is closed,SPARK-43403,13535322,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,zhouyifan279,zhouyifan279,08/May/23 03:32,11/May/23 03:26,30/Oct/23 17:26,,3.1.2,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,!image-2023-05-08-11-33-13-634.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/23 03:33;zhouyifan279;image-2023-05-08-11-33-13-634.png;https://issues.apache.org/jira/secure/attachment/13057872/image-2023-05-08-11-33-13-634.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 11 03:26:42 UTC 2023,,,,,,,,,,"0|z1hqwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/23 03:26;snoot;User 'zhouyifan279' has created a pull request for this issue:
https://github.com/apache/spark/pull/41105;;;",,,,,,,,,,,,,,
Add Primary Key syntax support,SPARK-43400,13535316,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,melin,melin,08/May/23 01:31,10/May/23 06:26,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"apache paimon and hudi support primary key definitions. It is necessary to support the primary key definition syntax

https://docs.snowflake.com/en/sql-reference/sql/create-table-constraint#constraint-properties

[~gurwls223] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-08 01:31:07.0,,,,,,,,,,"0|z1hqv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor timeout should be max of idleTimeout rddTimeout shuffleTimeout,SPARK-43398,13535310,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,07/May/23 18:12,12/Jun/23 07:42,30/Oct/23 17:26,12/Jun/23 07:42,3.0.0,3.1.3,3.2.4,3.3.2,3.4.0,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,Spark Core,,,,,0,,,,,"When dynamic allocation enabled, Executor timeout should be max of idleTimeout, rddTimeout and shuffleTimeout.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jun 12 07:42:28 UTC 2023,,,,,,,,,,"0|z1hqts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/23 07:42;dongjoon;Issue resolved by pull request 41082
[https://github.com/apache/spark/pull/41082];;;",,,,,,,,,,,,,,
Sequence expression can overflow,SPARK-43393,13535242,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,deepayan_db,deepayan_db,06/May/23 03:04,03/Oct/23 00:17,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,pull-request-available,,,,"Spark has a (long-standing) overflow bug in the {{sequence}} expression.

 

Consider the following operations:

{{spark.sql(""CREATE TABLE foo (l LONG);"")}}
{{spark.sql(s""INSERT INTO foo VALUES (${Long.MaxValue});"")}}
{{spark.sql(""SELECT sequence(0, l) FROM foo;"").collect()}}

 

The result of these operations will be:

{{Array[org.apache.spark.sql.Row] = Array([WrappedArray()])}}

an unintended consequence of overflow.

 

The sequence is applied to values {{0}} and {{Long.MaxValue}} with a step size of {{1}} which uses a length computation defined [here|https://github.com/apache/spark/blob/16411188c7ba6cb19c46a2bd512b2485a4c03e2c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L3451]. In this calculation, with {{{}start = 0{}}}, {{{}stop = Long.MaxValue{}}}, and {{{}step = 1{}}}, the calculated {{len}} overflows to {{{}Long.MinValue{}}}. The computation, in binary looks like:

{{{{0111111111111111111111111111111111111111111111111111111111111111 -}}}}

{{{{0000000000000000000000000000000000000000000000000000000000000000}}}}

{{{{------------------------------------------------------------------      0111111111111111111111111111111111111111111111111111111111111111 /}}}}

{{{{0000000000000000000000000000000000000000000000000000000000000001}}}}

{{{{------------------------------------------------------------------                0111111111111111111111111111111111111111111111111111111111111111 +}}}}

{{{{0000000000000000000000000000000000000000000000000000000000000001}}}}

{{{{------------------------------------------------------------------      1000000000000000000000000000000000000000000000000000000000000000}}}}

The following [check|https://github.com/apache/spark/blob/16411188c7ba6cb19c46a2bd512b2485a4c03e2c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L3454] passes as the negative {{Long.MinValue}} is still {{{}<= MAX_ROUNDED_ARRAY_LENGTH{}}}. The following cast to {{toInt}} uses this representation and [truncates the upper bits|https://github.com/apache/spark/blob/16411188c7ba6cb19c46a2bd512b2485a4c03e2c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L3457] resulting in an empty length of 0.

Other overflows are similarly problematic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat May 06 03:57:09 UTC 2023,,,,,,,,,,"0|z1hqeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/May/23 03:57;deepayan_db;PR [here|https://github.com/apache/spark/pull/41072] but Actions are not running on the forked repository.;;;",,,,,,,,,,,,,,
Sequence expression can overflow,SPARK-43392,13535241,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,deepayan_db,deepayan_db,06/May/23 03:04,07/May/23 02:47,30/Oct/23 17:26,07/May/23 02:47,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Spark has a (long-standing) overflow bug in the {{sequence}} expression.

 

Consider the following operations:

{{spark.sql(""CREATE TABLE foo (l LONG);"")}}
{{spark.sql(s""INSERT INTO foo VALUES (${Long.MaxValue});"")}}
{{spark.sql(""SELECT sequence(0, l) FROM foo;"").collect()}}

 

The result of these operations will be:

{{Array[org.apache.spark.sql.Row] = Array([WrappedArray()])}}

an unintended consequence of overflow.

 

The sequence is applied to values {{0}} and {{Long.MaxValue}} with a step size of {{1}} which uses a length computation defined [here|https://github.com/apache/spark/blob/16411188c7ba6cb19c46a2bd512b2485a4c03e2c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L3451]. In this calculation, with {{{}start = 0{}}}, {{{}stop = Long.MaxValue{}}}, and {{{}step = 1{}}}, the calculated {{len}} overflows to {{{}Long.MinValue{}}}. The computation, in binary looks like:

{{  0111111111111111111111111111111111111111111111111111111111111111}}

{{- 0000000000000000000000000000000000000000000000000000000000000000 }}

{{------------------------------------------------------------------      0111111111111111111111111111111111111111111111111111111111111111}}

{{/ 0000000000000000000000000000000000000000000000000000000000000001}}

{{------------------------------------------------------------------      0111111111111111111111111111111111111111111111111111111111111111}}

{{+ 0000000000000000000000000000000000000000000000000000000000000001}}

{{------------------------------------------------------------------      1000000000000000000000000000000000000000000000000000000000000000}}

{{}}

The following [check|https://github.com/apache/spark/blob/16411188c7ba6cb19c46a2bd512b2485a4c03e2c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L3454] passes as the negative {{Long.MinValue}} is still {{{}<= MAX_ROUNDED_ARRAY_LENGTH{}}}. The following cast to {{toInt}} uses this representation and [truncates the upper bits|https://github.com/apache/spark/blob/16411188c7ba6cb19c46a2bd512b2485a4c03e2c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L3457] resulting in an empty length of 0.{{{}{}}}

Other overflows are similarly problematic.{{{}{}}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun May 07 02:47:21 UTC 2023,,,,,,,,,,"0|z1hqeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/May/23 02:47;fanjia;Duplicate with SPARK-43393;;;",,,,,,,,,,,,,,
Idle connection should not be closed when closeIdleConnection is disabled,SPARK-43391,13535226,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,warrenzhu25,warrenzhu25,06/May/23 00:06,06/May/23 00:24,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,Spark will close idle connection when there're outstanding requests but no traffic for at least {{requestTimeoutMs}} even with closeIdleConnection is disabled.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-05-06 00:06:28.0,,,,,,,,,,"0|z1hqb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Latest docker Spark image has critical CVE,SPARK-43388,13535208,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,mahiki,mahiki,05/May/23 18:44,29/May/23 16:34,30/Oct/23 17:26,29/May/23 16:34,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Docker,,,,,0,,,,,"I pulled the latest spark 3.4.0 image from dockerhub, on 2023-04-28 and found after scanning on docker desktop that there are several critical CVE found (see screenshot).

It seems like some changes to github actions are needed to rebuild with updated dependencies on a regular cadence.

 

Notes:

Original project issue: https://issues.apache.org/jira/browse/SPARK-40513

[https://hub.docker.com/r/apache/spark/tags]

https://github.com/apache/spark-docker/actions

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/23 18:44;mahiki;spark-docker.CVE-everywhere.png;https://issues.apache.org/jira/secure/attachment/13057859/spark-docker.CVE-everywhere.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 19 14:45:10 UTC 2023,,,,,,,,,,"0|z1hq74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/May/23 01:32;yikunkero;Thanks for report.
- The critical issue is related to `jackson-mapper-asl` and `snakeyaml` java library which should be resolved in 3.4.1 (future release) also cc [~LuciferYang]
- Rebuild with updated dependencies can't resolved the issue until the spark update the deps pom.;;;","19/May/23 14:45;srowen;Generally speaking - please also make an argument that these affect Spark when reporting. (But this one is already updated, yes);;;",,,,,,,,,,,,,
Improve list of suggested column/attributes in `UNRESOLVED_COLUMN.WITH_SUGGESTION` error class,SPARK-43386,13535197,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vli-databricks,vli-databricks,vli-databricks,05/May/23 17:13,12/May/23 05:39,30/Oct/23 17:26,12/May/23 05:39,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,Match the style of unresolved column/attribute when sorting list of suggested columns. If an unresolved column name is single-part identifier - use same style for suggested columns.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 12 05:39:03 UTC 2023,,,,,,,,,,"0|z1hq4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/May/23 09:12;githubbot;User 'vitaliili-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/41038;;;","10/May/23 09:13;githubbot;User 'vitaliili-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/41038;;;","12/May/23 05:39;maxgekk;Issue resolved by pull request 41038
[https://github.com/apache/spark/pull/41038];;;",,,,,,,,,,,,
Fix Avro data type conversion issues to avoid producing incorrect results,SPARK-43380,13535060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zeruibao,zeruibao,zeruibao,04/May/23 23:01,30/Oct/23 13:39,30/Oct/23 17:26,02/Jun/23 22:10,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,pull-request-available,,,,"We found the following issues with open-source Avro:
 * Interval types can be read as date or timestamp types that would lead to wildly different results
 * Decimal types can be read with lower precision, that leads to data being read as {{null}} instead of suggesting that a wider decimal format should be provided",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jun 02 22:10:27 UTC 2023,,,,,,,,,,"0|z1hpa8:",9223372036854775807,,,,,,,,,,,,,3.5.0,,,,,,,,,,"05/May/23 03:32;snoot;User 'zeruibao' has created a pull request for this issue:
https://github.com/apache/spark/pull/41052;;;","02/Jun/23 22:10;Gengliang.Wang;Issue resolved by pull request 41052
[https://github.com/apache/spark/pull/41052];;;",,,,,,,,,,,,,
SerializerHelper.deserializeFromChunkedBuffer leaks deserialization streams,SPARK-43378,13534986,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,04/May/23 12:59,06/Jun/23 07:04,30/Oct/23 17:26,05/May/23 00:34,3.4.0,3.4.1,3.5.0,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Spark Core,,,,,0,,,,,The method SerializerHelper.deserializeFromChunkedBuffer leaks serializations stream. This can lead to huge performance regressions when using kryo serializer as the spark application can become bottlenecked on the driver creating expensive kryo objects that are then leaked as part of the deserialization stream,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 05 00:34:57 UTC 2023,,,,,,,,,,"0|z1hots:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/May/23 00:34;srowen;Resolved by https://github.com/apache/spark/pull/41049;;;",,,,,,,,,,,,,,
Revert [SPARK-39203][SQL] Rewrite table location to absolute URI based on database URI,SPARK-43373,13534904,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,04/May/23 02:56,04/May/23 02:57,30/Oct/23 17:26,04/May/23 02:56,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 04 02:57:10 UTC 2023,,,,,,,,,,"0|z1hobk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/May/23 02:57;cloud_fan;https://github.com/apache/spark/pull/40871;;;",,,,,,,,,,,,,,
Spark Driver Bind Address is off-by-one,SPARK-43366,13534897,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,derektbrown,derektbrown,04/May/23 02:47,26/May/23 20:04,30/Oct/23 17:26,,3.3.3,,,,,,,,,,,,,,,,,,,,,,,Block Manager,,,,,0,,,,,"I have the following environment variable set in my driver pod configuration:


{code:java}
SPARK_DRIVER_BIND_ADDRESS=10.244.0.53{code}
However, I see an off-by-one IP address being referred to in the Spark logs:


{code:java}
23/05/04 02:37:03 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.0.54:53140) with ID 1,  ResourceProfileId 0
23/05/04 02:37:03 INFO BlockManagerMasterEndpoint: Registering block manager 10.244.0.54:32805 with 413.9 MiB RAM, BlockManagerId(1, 10.244.0.54, 32805, None){code}

I am not sure why this might be the case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 26 20:04:24 UTC 2023,,,,,,,,,,"0|z1hoa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/23 14:46;srowen;-Was the original port in use? it'll try the next one then- EDIT: this makes no sense;;;","26/May/23 19:54;derektbrown;[~srowen] the issue isn't with the port; the issue is with the IP address. The ports are both 32805.;;;","26/May/23 20:04;srowen;Ack yeah, nevermind. Reading too fast without coffee. That I don't know, except to say that's not going to be controlled by the _driver_ IP. Block manager would be tied to _executors_. Is that IP an executor?;;;",,,,,,,,,,,,
DELETE from Hive table result in INTERNAL error,SPARK-43359,13534872,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,srielau,srielau,03/May/23 18:43,16/May/23 18:21,30/Oct/23 17:26,16/May/23 18:21,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,,,0,,,,,"spark-sql (default)> CREATE TABLE T1(c1 INT);
spark-sql (default)> DELETE FROM T1 WHERE c1 = 1;
[INTERNAL_ERROR] Unexpected table relation: HiveTableRelation [`spark_catalog`.`default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [c1#3], Partition Cols: []]

org.apache.spark.SparkException: [INTERNAL_ERROR] Unexpected table relation: HiveTableRelation [`spark_catalog`.`default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [c1#3], Partition Cols: []]
	at org.apache.spark.SparkException$.internalError(SparkException.scala:77)
	at org.apache.spark.SparkException$.internalError(SparkException.scala:81)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:310)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)
	at org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 16 18:21:53 UTC 2023,,,,,,,,,,"0|z1ho4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/May/23 12:24;panbingkun;Let me fix it.;;;","16/May/23 18:21;dongjoon;Issue resolved by pull request 41172
[https://github.com/apache/spark/pull/41172];;;",,,,,,,,,,,,,
Spark AWS Glue date partition push down broken,SPARK-43357,13534836,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sdehaes,sdehaes,sdehaes,03/May/23 14:03,10/May/23 07:13,30/Oct/23 17:26,09/May/23 13:06,3.1.0,3.1.1,3.1.2,3.1.3,3.2.0,3.2.1,3.2.2,3.2.3,3.2.4,3.3.0,3.3.1,3.3.2,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"When using the following project: [https://github.com/awslabs/aws-glue-data-catalog-client-for-apache-hive-metastore]
To have glue supported as as a hive metastore for spark there is an issue when reading a date-partitioned data set. Writing is fine.
You get the following error: 


{quote}org.apache.hadoop.hive.metastore.api.InvalidObjectException: Unsupported expression '2023 - 05 - 03' (Service: AWSGlue; Status Code: 400; Error Code: InvalidInputException; Request ID: beed68c6-b228-442e-8783-52c25b9d2243; Proxy: null)
{quote}
 

A fix for this is making sure the date passed to glue is quoted",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 10 07:13:49 UTC 2023,,,,,,,,,,"0|z1hnwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/23 03:28;snoot;User 'stijndehaes' has created a pull request for this issue:
https://github.com/apache/spark/pull/41035;;;","09/May/23 13:06;cloud_fan;Issue resolved by pull request 41035
[https://github.com/apache/spark/pull/41035];;;","10/May/23 07:13;sdehaes;Any chance we could backport this to older versions? 3.1, 3.2, 3.4 all have the same issue. I don't know which versions are actively supported?
I am willing to make new PR's to these older versions if needed.;;;",,,,,,,,,,,,
Fix flaky test for `DataFrame` creation,SPARK-43349,13534765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,03/May/23 00:16,03/May/23 10:41,30/Oct/23 17:26,03/May/23 10:41,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Pandas API on Spark,,,,,0,,,,,some test of `test_creation_index` is not working properly in some envs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 03 10:41:23 UTC 2023,,,,,,,,,,"0|z1hngw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/23 10:41;gurwls223;Issue resolved by pull request 41025
[https://github.com/apache/spark/pull/41025];;;",,,,,,,,,,,,,,
Spark Streaming is not able to read a .txt file whose name has [] special character,SPARK-43343,13534735,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,siying,siying,siying,02/May/23 18:28,09/May/23 03:55,30/Oct/23 17:26,09/May/23 03:55,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Structured Streaming,,,,,0,,,,,"* For example, If a directory contains a following file:
/path/abc[123]
and users would load spark.readStream.format(""text"").load(""/path"") as stream input. It throws an exception, saying no matching path /path/abc[123]. Spark thinks abc[123] is a regex that only matches file named abc1, abc2 and abc3.

 * Upon investigation this is due to how we [getBatch|https://github.com/databricks/runtime/blob/3af402d23620a0952e151d96c3184d2233217c87/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala#L269] in the FileStreamSource. In `FileStreamSource` we already check file pattern matching and find all match file names. However, in DataSource we check for glob characters again and try to expend it [here|https://github.com/databricks/runtime/blob/3af402d23620a0952e151d96c3184d2233217c87/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L274].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 09 03:55:44 UTC 2023,,,,,,,,,,"0|z1hna8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/23 03:55;kabhwan;Issue resolved by pull request 41022
[https://github.com/apache/spark/pull/41022];;;",,,,,,,,,,,,,,
Revert SPARK-39006 Show a directional error message for executor PVC dynamic allocation failure,SPARK-43342,13534704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dcoliversun,ofrenkel,ofrenkel,02/May/23 15:24,07/May/23 01:27,30/Oct/23 17:26,07/May/23 01:27,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Kubernetes,,,,,1,,,,,"When using static PVC with Spark 3.4, spark PI example fails with the error below. Previous versions of Spark worked well.
{code:java}
23/04/26 13:22:02 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes for ResourceProfile Id: 0, target: 5, known: 0, sharedSlotFromPendingPods: 2147483647. 23/04/26 13:22:02 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script 23/04/26 13:22:02 ERROR ExecutorPodsSnapshotsStoreImpl: Going to stop due to IllegalArgumentException java.lang.IllegalArgumentException: PVC ClaimName: a1pvc should contain OnDemand or SPARK_EXECUTOR_ID when requiring multiple executors         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.checkPVCClaimName(MountVolumesFeatureStep.scala:135)         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.$anonfun$constructVolumes$4(MountVolumesFeatureStep.scala:75)         at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)         at scala.collection.Iterator.foreach(Iterator.scala:943)         at scala.collection.Iterator.foreach$(Iterator.scala:943)         at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)         at scala.collection.IterableLike.foreach(IterableLike.scala:74)         at scala.collection.IterableLike.foreach$(IterableLike.scala:73)         at scala.collection.AbstractIterable.foreach(Iterable.scala:56)         at scala.collection.TraversableLike.map(TraversableLike.scala:286)         at scala.collection.TraversableLike.map$(TraversableLike.scala:279)         at scala.collection.AbstractTraversable.map(Traversable.scala:108)         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.constructVolumes(MountVolumesFeatureStep.scala:58)         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.configurePod(MountVolumesFeatureStep.scala:35)         at org.apache.spark.scheduler.cluster.k8s.KubernetesExecutorBuilder.$anonfun$buildFromFeatures$5(KubernetesExecutorBuilder.scala:83)         at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)         at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)         at scala.collection.immutable.List.foldLeft(List.scala:91)         at org.apache.spark.scheduler.cluster.k8s.KubernetesExecutorBuilder.buildFromFeatures(KubernetesExecutorBuilder.scala:82)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:430)         at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:417)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:370)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:363)         at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)         at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)         at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:363)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:134)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:134)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:143)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:131)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl.$anonfun$addSubscriber$1(ExecutorPodsSnapshotsStoreImpl.scala:85)         at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)         at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)         at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)         at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)         at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)         at java.base/java.lang.Thread.run(Thread.java:833)   {code}
How to reproduce:
 # Create statically provisioned PV, for example nfs PV: [https://kubernetes.io/docs/concepts/storage/volumes/#nfs]
 # Create PVC that binds to PV above.
 # Run Spark PI example: $SPARK_HOME/bin/spark-submit --master k8s://kubernetes.default.svc --properties-file spark.properties $SPARK_HOME/examples/src/main/python/pi.py 10

spark.properties contents:
{code:java}
spark.executor.instances=5
spark.kubernetes.executor.volumes.persistentVolumeClaim.nfs1.mount.path=/isilon/mnts
spark.kubernetes.executor.volumes.persistentVolumeClaim.nfs1.mount.readOnly=false
spark.kubernetes.executor.volumes.persistentVolumeClaim.nfs1.options.claimName=a1pvc
spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs1.options.claimName=a1pvc
spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs1.mount.readOnly=false
spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs1.mount.path=/isilon/mnts {code}",,,,,,,,,,,,,,,,,,,,SPARK-39006,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun May 07 01:27:11 UTC 2023,,,,,,,,,,"0|z1hn3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/May/23 02:17;dcoliversun;[~dongjoon] [~yikunkero] It seems like a regression caused by [SPARK-39006|https://issues.apache.org/jira/browse/SPARK-39006], please assign to me;;;","04/May/23 03:41;dongjoon;Thank you for pinging me, [~dcoliversun] . BTW, Apache Spark community has a community rule to set the `Assignee` when committers merge a proper PR.

Please make a PR first.;;;","04/May/23 03:42;dongjoon;BTW, thank you for reporting, [~ofrenkel] .;;;","05/May/23 04:17;snoot;User 'dcoliversun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41057;;;","05/May/23 04:18;snoot;User 'dcoliversun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41057;;;","07/May/23 01:27;dongjoon;Issue resolved by pull request 41069
[https://github.com/apache/spark/pull/41069];;;",,,,,,,,,
StructType.toDDL does not pick up on non-nullability of column in nested struct,SPARK-43341,13534700,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,bramboog,bramboog,02/May/23 14:56,03/May/23 09:11,30/Oct/23 17:26,,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"h2. The problem

When converting a StructType instance containing a nested StructType column which in turn contains a column for which {{nullable = false}} to a DDL string using {{{}.toDDL{}}}, the resulting DDL string does not include this non-nullability. For example:
{code:java}
val testschema = StructType(List(
  StructField(""key"", IntegerType, false),
  StructField(""value"", StringType, true),
  StructField(""nestedCols"", StructType(List(
    StructField(""nestedKey"", IntegerType, false),
    StructField(""nestedValue"", StringType, true)
  )), false)
))

println(testschema.toDDL)
println(StructType.fromDDL(testschema.toDDL)){code}
gives:
{code:java}
key INT NOT NULL,value STRING,nestedCols STRUCT<nestedKey: INT, nestedValue: STRING> NOT NULL

StructType(
  StructField(key,IntegerType,false),
  StructField(value,StringType,true),
  StructField(nestedCols,StructType(
    StructField(nestedKey,IntegerType,true),
    StructField(nestedValue,StringType,true)
  ),false)
){code}
 

This is due to the fact that {{StructType.toDDL}} calls {{StructField.toDDL}} for its fields, which in turn calls {{.sql}} for its {{{}dataType{}}}. If {{dataType}} is a {{{}StructType{}}}, the call to {{.sql}} in turn calls {{.sql}} for all the nested fields, and this last method does not include the nullability of the field in its output.
h2. Proposed solution

{{StructField.toDDL}} should call {{dataType.toDDL}} for a {{{}StructType{}}}, since this will include information about nullability of nested columns.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,Converting a StructType with nested non-nullable columns to a DDL string using .toDDL will now actually include the non-nullability in said DDL string.,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,,,,Wed May 03 09:11:57 UTC 2023,,,,,,,,,,"0|z1hn2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/23 09:11;githubbot;User 'BramBoog' has created a pull request for this issue:
https://github.com/apache/spark/pull/41016;;;",,,,,,,,,,,,,,
Handle missing stack-trace field in eventlogs,SPARK-43340,13534698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ahussein,ahussein,ahussein,02/May/23 14:52,05/May/23 23:04,30/Oct/23 17:26,05/May/23 23:04,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,,,0,,,,,"Recently I was testing with some 3.0.2 eventlogs.

The SHS-3.4+ does not interpret failed jobs/ failed SQLs correctly.

Instead it will list them as ""Incomplete/Active"" whereas it should be listed as ""Failed"".

The problem is due to missing fields in eventlogs generated by previous versions. In this case the eventlog does not have ""Stack Trace"" field which causes a NPE
{code:java}
{
   ""Event"":""SparkListenerJobEnd"",
   ""Job ID"":31,
   ""Completion Time"":1616171909785,
   ""Job Result"":{
      ""Result"":""JobFailed"",
      ""Exception"":{
         ""Message"":""Job aborted""
      }
   }
} {code}
*The SHS logfile*
{code:java}
23/05/01 21:57:16 INFO FsHistoryProvider: Parsing file:/tmp/nds_q86_fail_test to re-build UI...
23/05/01 21:57:17 ERROR ReplayListenerBus: Exception parsing Spark event log: file:/tmp/nds_q86_fail_test
java.lang.NullPointerException
    at org.apache.spark.util.JsonProtocol$JsonNodeImplicits.extractElements(JsonProtocol.scala:1589)
    at org.apache.spark.util.JsonProtocol$.stackTraceFromJson(JsonProtocol.scala:1558)
    at org.apache.spark.util.JsonProtocol$.exceptionFromJson(JsonProtocol.scala:1569)
    at org.apache.spark.util.JsonProtocol$.jobResultFromJson(JsonProtocol.scala:1423)
    at org.apache.spark.util.JsonProtocol$.jobEndFromJson(JsonProtocol.scala:967)
    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:878)
    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:865)
    at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:88)
    at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:59)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3(FsHistoryProvider.scala:1140)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3$adapted(FsHistoryProvider.scala:1138)
    at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2786)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1(FsHistoryProvider.scala:1138)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1$adapted(FsHistoryProvider.scala:1136)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.apache.spark.deploy.history.FsHistoryProvider.parseAppEventLogs(FsHistoryProvider.scala:1136)
    at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:1117)
    at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1355)
    at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:345)
    at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:199)
    at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
    at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:134)
    at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
    at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:55)
    at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:51)
    at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
    at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
    at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
    at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
    at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
    at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
    at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
    at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:88)
    at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:100)
    at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:256)
    at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:104)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:503)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
    at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
    at org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
    at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
    at org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
    at org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
    at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
    at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)
    at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    at org.sparkproject.jetty.server.Server.handle(Server.java:516)
    at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
    at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
    at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:479)
    at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
    at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)
    at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
    at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
    at java.lang.Thread.run(Thread.java:750)
23/05/01 21:57:17 ERROR ReplayListenerBus: Malformed line #24368: {""Event"":""SparkListenerJobEnd"",""Job ID"":31,""Completion Time"":1616171909785,""Job Result"":{""Result"":""JobFailed"",""Exception"":
{""Message"":""Job aborted""}
}}
23/05/01 21:57:17 INFO FsHistoryProvider: Finished parsing file:/tmp/nds_q86_fail_test
 
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 05 23:04:37 UTC 2023,,,,,,,,,,"0|z1hn20:",9223372036854775807,,,,,,,,,,,,,3.4.0,3.5.0,,,,,,,,,"02/May/23 15:01;tgraves;Likely related to SPARK-39489;;;","05/May/23 20:10;hudson;User 'amahussein' has created a pull request for this issue:
https://github.com/apache/spark/pull/41050;;;","05/May/23 23:04;dongjoon;Issue resolved by pull request 41050
[https://github.com/apache/spark/pull/41050];;;",,,,,,,,,,,,
LEFT JOIN is treated as INNER JOIN when being in a middle of double join,SPARK-43339,13534686,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,lchistov1987,lchistov1987,02/May/23 12:45,17/Jun/23 14:55,30/Oct/23 17:26,17/Jun/23 05:58,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Optimizer,,,,,0,,,,,"Consider query like

 
{code:java}
SELECT ss_item_sk
       FROM   store_sales
              LEFT OUTER JOIN store_returns
                           ON ( sr_item_sk = ss_item_sk ),
              reason
       WHERE  sr_reason_sk = r_reason_sk
              AND r_reason_desc = 'reason 38'{code}
 

Spark generates following plan:

 
{code:java}
AdaptiveSparkPlan isFinalPlan=false
+- Project [ss_item_sk#2]
   +- BroadcastHashJoin [sr_reason_sk#458], [r_reason_sk#734], Inner, BuildRight, false
      :- Project [ss_item_sk#2, sr_reason_sk#458]
      :  +- BroadcastHashJoin [ss_item_sk#2], [sr_item_sk#452], Inner, BuildRight, false
      :     :- FileScan parquet [ss_item_sk#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/leonid/tpcds-spark-data-no-padding/store_sales], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ss_item_sk:int>
      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#7227]
      :        +- Filter (isnotnull(sr_item_sk#452) AND isnotnull(sr_reason_sk#458))
      :           +- FileScan parquet [sr_item_sk#452,sr_reason_sk#458] Batched: true, DataFilters: [isnotnull(sr_item_sk#452), isnotnull(sr_reason_sk#458)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/leonid/tpcds-spark-data-no-padding/store_returns], PartitionFilters: [], PushedFilters: [IsNotNull(sr_item_sk), IsNotNull(sr_reason_sk)], ReadSchema: struct<sr_item_sk:int,sr_reason_sk:int>
      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#7231]
         +- Project [r_reason_sk#734]
            +- Filter ((isnotnull(r_reason_desc#736) AND (r_reason_desc#736 = reason 38)) AND isnotnull(r_reason_sk#734))
               +- FileScan parquet [r_reason_sk#734,r_reason_desc#736] Batched: true, DataFilters: [isnotnull(r_reason_desc#736), (r_reason_desc#736 = reason 38), isnotnull(r_reason_sk#734)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/leonid/tpcds-spark-data-no-padding/reason], PartitionFilters: [], PushedFilters: [IsNotNull(r_reason_desc), EqualTo(r_reason_desc,reason 38), IsNotNull(r_reason_sk)], ReadSchema: struct<r_reason_sk:int,r_reason_desc:string>
{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Jun 17 14:55:27 UTC 2023,,,,,,,,,,"0|z1hmzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/23 05:58;yumwang;It is optimized by EliminateOuterJoin.;;;","17/Jun/23 13:57;lchistov1987;[~yumwang] 

Sorry, but I don't understand your comment. The generated plan looks wrong to me. ;;;","17/Jun/23 14:55;yumwang;This is not a bug. The final result is correct.

https://github.com/apache/spark/blob/d88633ada5eb73e8876acaa2c2a53b9596f2acdd/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala#L187-L194;;;",,,,,,,,,,,,
Support  modify the SESSION_CATALOG_NAME value,SPARK-43338,13534657,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,melin,melin,02/May/23 07:49,17/Jul/23 09:34,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"{code:java}
private[sql] object CatalogManager {
val SESSION_CATALOG_NAME: String = ""spark_catalog""
}{code}
 
The SESSION_CATALOG_NAME value cannot be modified。

If multiple Hive Metastores exist, the platform manages multiple hms metadata and classifies them by catalogName. A different catalog name is required

[~fanjia] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jul 17 09:34:26 UTC 2023,,,,,,,,,,"0|z1hmsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/23 05:47;yao;Support multiple Hive metastore servers is a big feature, which can not be achieved by making SESSION_CATALOG_NAME variable

 

cc [~cloud_fan] ;;;","09/May/23 06:20;melin;You understand a big feature. Only one hms is accessed in a sparksession. I just want spark_catalog to be modified. For example, if you have two hadoop clusters, there should be two hms. Metadata management platform (similar to databricks unity catalog), the acquisition of the HMS metadata, in order to distinguish the uniqueness, need to add catalogName (tableid: catalogName. SchemaName. TableName). When spark accesses hive tables, it is consistent with the catalogname of tableid instead of spark_catalog。

 ;;;","09/May/23 06:32;yao;Why not just use Catalog V2 API to implement a separate catalog extension with hive support? or using an exist one https://kyuubi.readthedocs.io/en/v1.7.1-rc0/connector/spark/hive.html;;;","09/May/23 07:36;melin;If the same hive database has parquet and hudi table, does HiveTableCatalog support access to hudi table?  not want to register two catalog;;;","22/May/23 12:16;fanjia;`If the same hive database has parquet and hudi table, does HiveTableCatalog support access to hudi table?`

Are you had try this? I'm not sure this change are necessary. `Session_Catalog` just one special catalog for spark.

`If multiple Hive Metastores exist, the platform manages multiple hms metadata and classifies them by catalogName.`

Seem like this requirement very match with Datasource V2, handle different catalog on spark. ;;;","22/May/23 12:24;melin;I don't need to access multiple hms in the same sparksession, I only need to access one of them. Assign each hms a unique catalogname only so that the meta tableId is unique: catalog.database.table.;;;","22/May/23 12:29;fanjia;`Assign each hms a unique catalogname only so that the meta tableId is unique: catalog.database.table.`

 

I think the Datasource V2 can do that. But i didn't verify it.;;;","22/May/23 12:42;melin;kyuubi verified it: [https://kyuubi.readthedocs.io/en/v1.7.1-rc0/connector/spark/hive.html]
 
kyuubi is implemented based on HiveSessionCatalog. If there are huid tables in the hive database, another Hudi catalog needs to be registered. The same hms has two catalognames, which does not meet my requirements.
 ;;;","17/Jul/23 09:34;melin;[~yao]

Would consider setting a custom catalog name?  example:

spark.sql.session.catalog.default.name=spark_catalog;;;",,,,,,
Asc/desc arrow icons for sorting column does not get displayed in the table column,SPARK-43337,13534646,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maytasm,maytasm,maytasm,02/May/23 05:35,05/May/23 19:08,30/Oct/23 17:26,05/May/23 19:08,3.3.0,3.4.0,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,,,Spark Core,Web UI,,,,0,,,,,"The sorting icon is not displayed when the column is clicked to sort by asc/desc.

See attached image: (The index column is sort by asc order by the down arrow is not displayed)

!image (4).png!

This broke due to the upgrade from DataTables from 1.10.20 to 1.10.25. I have confirmed that version 1.13.2 in master branch does not have this problem. 
For reference, in 1.13.2, it looks like the following:
!Screen Shot 2023-05-01 at 10.39.00 PM.png!  ",,,,,,,,,,,,,,,,SPARK-42435,,,,SPARK-38924,,,,,SPARK-31871,,,,,"02/May/23 05:39;maytasm;Screen Shot 2023-05-01 at 10.39.00 PM.png;https://issues.apache.org/jira/secure/attachment/13057762/Screen+Shot+2023-05-01+at+10.39.00+PM.png","02/May/23 05:38;maytasm;image (4).png;https://issues.apache.org/jira/secure/attachment/13057761/image+%284%29.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 05 19:08:36 UTC 2023,,,,,,,,,,"0|z1hmqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/May/23 03:56;snoot;User 'maytasm' has created a pull request for this issue:
https://github.com/apache/spark/pull/41011;;;","05/May/23 19:08;srowen;Issue resolved by pull request 41060
[https://github.com/apache/spark/pull/41060];;;",,,,,,,,,,,,,
Error while serializing ExecutorPeakMetricsDistributions into API JSON response,SPARK-43334,13534633,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tejdeepg,tejdeepg,tejdeepg,01/May/23 21:29,24/May/23 23:25,30/Oct/23 17:26,24/May/23 23:25,3.3.3,,,,,,,,,,,,,,,,,,,3.5.0,,,,Web UI,,,,,0,,,,,"When we try to get the ExecutorPeakMetricsDistributions [through the API|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/api/v1/api.scala#L463] (/stages), there is a possibility of encountering an issue while serializing the StagesData into a JSON if the executor metrics are empty. 

 

The following error is thrown : 
{code:java}
Caused by: com.fasterxml.jackson.databind.JsonMappingException: -1 (through reference chain: scala.c
ollection.immutable.$colon$colon[0]->org.apache.spark.status.api.v1.StageData[""executorMetricsDistri
butions""]->org.apache.spark.status.api.v1.ExecutorMetricsDistributions[""peakMemoryMetrics""])
        at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.jav
a:390)
        at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.jav
a:349) {code}
This happens because the indices for the quartiles are populated incorrectly as -1 since the metrics itself are empty and this leads to this exception being thrown.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed May 24 23:25:46 UTC 2023,,,,,,,,,,"0|z1hmnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/May/23 19:50;hudson;User 'thejdeep' has created a pull request for this issue:
https://github.com/apache/spark/pull/41017;;;","24/May/23 23:25;srowen;Issue resolved by pull request 41017
[https://github.com/apache/spark/pull/41017];;;",,,,,,,,,,,,,
Typo in sql-migration-guide.md,SPARK-43330,13534593,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kpark,kpark,kpark,01/May/23 13:04,02/May/23 00:29,30/Oct/23 17:26,02/May/23 00:29,3.2.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Documentation,,,,,0,,,,,"There is a minor typo in [sql-migration-guide.md|#L154]

 
| - In Spark 3.2, `TRANSFORM` operator can support `ArrayType/MapType/StructType` without Hive SerDe, in this mode, we use `StructsToJosn` to convert `ArrayType/MapType/StructType` column to `STRING` and use `JsonToStructs` to parse `STRING` to `ArrayType/MapType/StructType`. In Spark 3.1, Spark just support case `ArrayType/MapType/StructType` column as `STRING` but can't support parse `STRING` to `ArrayType/MapType/StructType` output columns.|

 

StructsToJosn -> StructsToJson",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 02 00:29:07 UTC 2023,,,,,,,,,,"0|z1hmf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/May/23 13:14;kpark;I have created a PR with a fix - [https://github.com/apache/spark/pull/41004|https://github.com/apache/spark/pull/41003]

(sorry for typo in commit message 😅 ({-}[https://github.com/apache/spark/pull/41003))|https://github.com/apache/spark/pull/41003]{-}

 

 ;;;","02/May/23 00:29;gurwls223;Issue resolved by pull request 41004
[https://github.com/apache/spark/pull/41004];;;",,,,,,,,,,,,,
driver and executors shared same Kubernetes PVC in Spark 3.4+,SPARK-43329,13534580,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dcoliversun,comet,comet,01/May/23 03:25,07/May/23 01:48,30/Oct/23 17:26,07/May/23 01:48,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Kubernetes,,,,,0,,,,,"I able to shared same PVC for spark 3.3. but on Spark 3.4 onward. i get below error.  I would like all the executors and driver to mount the same PVC. Is this a bug ? I don't want to use SPARK_EXECUTOR_ID or OnDemand because otherwise each of the executors will use an unique and separate PVC. 
 
Error message is ""should contain OnDemand or SPARK_EXECUTOR_ID when requiring multiple executors""
 
below is how I enabled it pvc in spark 3.3 and it works, but does not work in Spark 3.4

{code:sh}
spark.kubernetes.driver.volumes.persistentVolumeClaim.rwxpvc.options.claimName=rwxpvc 
--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.rwxpvc.mount.path=/opt/spark/work-dir 
--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.rwxpvc.options.claimName=rwxpvc 
--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.rwxpvc.mount.path=/opt/spark/work-dir 
 
{code}


 
 
 ",,,,,,,,,,,,,,,,,,,,SPARK-39006,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun May 07 01:48:39 UTC 2023,,,,,,,,,,"0|z1hmc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/May/23 04:18;dcoliversun;[~dongjoon] this ticket is duplicated with SPARK-43342;;;","07/May/23 01:48;dongjoon;Thank you for reporting, [~comet] .

This is fixed by reverting SPARK-39006.

Here is the reverting commit.

[https://github.com/apache/spark/commit/3ba1fa3678a4fcc0aaba8abb0d4312e8fb42efba];;;",,,,,,,,,,,,,
Trigger `committer.setupJob` before plan execute in `FileFormatWriter`,SPARK-43327,13534549,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Zing,Zing,Zing,30/Apr/23 07:11,22/Aug/23 03:08,30/Oct/23 17:26,22/Aug/23 03:08,3.2.3,,,,,,,,,,,,,,,,,,,3.3.4,,,,SQL,,,,,0,,,,,"In this jira, the case where `outputOrdering` might not work if AQE is enabled has been resolved.

https://issues.apache.org/jira/browse/SPARK-40588

However, since it materializes the AQE plan in advance (triggers getFinalPhysicalPlan) , it may cause the committer.setupJob(job) to not execute When `AdaptiveSparkPlanExec#getFinalPhysicalPlan()` is executed with an error.

Normally this step should be executed after committer.setupJob(job).

This may eventually result in the insertoverwrite directory being deleted.

 
{code:java}
import org.apache.hadoop.fs.{FileSystem, Path}

import org.apache.spark.sql.QueryTest
import org.apache.spark.sql.catalyst.TableIdentifier

sql(""CREATE TABLE IF NOT EXISTS spark32_overwrite(amt1 int) STORED AS ORC"")
sql(""CREATE TABLE IF NOT EXISTS spark32_overwrite2(amt1 long) STORED AS ORC"")
sql(""INSERT OVERWRITE TABLE spark32_overwrite2 select 6000044164"")
sql(""set spark.sql.ansi.enabled=true"")

val loc =
  spark.sessionState.catalog.getTableMetadata(TableIdentifier(""spark32_overwrite"")).location

val fs = FileSystem.get(loc, spark.sparkContext.hadoopConfiguration)
println(""Location exists: "" + fs.exists(new Path(loc)))

try {
  sql(""INSERT OVERWRITE TABLE spark32_overwrite select amt1 from "" +
    ""(select cast(amt1 as int) as amt1 from spark32_overwrite2 distribute by amt1)"")
} finally {
  println(""Location exists: "" + fs.exists(new Path(loc)))
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Aug 22 03:08:26 UTC 2023,,,,,,,,,,"0|z1hm5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/23 14:22;Zing;pr : https://github.com/apache/spark/pull/41154;;;","22/Aug/23 03:08;cloud_fan;Issue resolved by pull request 41154
[https://github.com/apache/spark/pull/41154];;;",,,,,,,,,,,,,
CoalesceBucketsInJoin not working with SHUFFLE_HASH hint,SPARK-43326,13534531,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,neshkeev,neshkeev,29/Apr/23 13:03,30/Apr/23 09:33,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"h1. NOTICE: related to SPARK-43021
h1. What I did

I define the following code:

{{from pyspark.sql import SparkSession}}

{{spark = (}}
{{  SparkSession}}
{{    .builder}}
{{    .appName(""Bucketing"")}}
{{    .master(""local[4]"")}}
{{    .config(""spark.sql.bucketing.coalesceBucketsInJoin.enabled"", True)}}
{{    .config(""spark.sql.autoBroadcastJoinThreshold"", ""-1"")}}
{{    .getOrCreate()}}
{{)}}

{{# AQE prevents CoalesceBucketsInJoin in 3.3.2 in 3.3.2}}
{{spark.conf.set(""spark.sql.adaptive.enabled"", False)}}

{{df1 = spark.range(0, 100)}}
{{df2 = spark.range(0, 100, 2)}}

{{df1.write.bucketBy(4, ""id"").mode(""overwrite"").saveAsTable(""t1"")}}
{{df2.write.bucketBy(2, ""id"").mode(""overwrite"").saveAsTable(""t2"")}}

{{t1 = spark.table(""t1"")}}
{{t2 = spark.table(""t2"")}}

{{t2.join(t1.hint(""SHUFFLE_HASH""), ""id"").explain()}}

{{== Physical Plan ==}}
{{*(3) Project [id#23L|#23L]}}
{{+- *(3) ShuffledHashJoin [id#23L|#23L], [id#21L|#21L], Inner, BuildRight}}
{{:- Exchange hashpartitioning(id#23L, 4), ENSURE_REQUIREMENTS, [plan_id=667]}}
{{: +- *(1) Filter isnotnull(id#23L)}}
{{: +- *(1) ColumnarToRow}}
{{: +- FileScan parquet spark_catalog.default.t2[id#23L|#23L] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(id#23L)|#23L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/notebooks/spark-warehouse/t2|file:///home/jovyan/notebooks/spark-warehouse/t2], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>}}
{{+- *(2) Filter isnotnull(id#21L)}}
{{+- *(2) ColumnarToRow}}
{{+- FileScan parquet spark_catalog.default.t1[id#21L|#21L] Batched: true, Bucketed: true, DataFilters: [isnotnull(id#21L)|#21L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/notebooks/spark-warehouse/t1|file:///home/jovyan/notebooks/spark-warehouse/t1], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 4 out of 4}}
h1. What happened

There is an Exchange node in the join plan
h1. What is expected

According to the docs about [{{spark.sql.bucketing.coalesceBucketsInJoin.enabled}}|https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration] there should not be any  Exchange/Shuffle nodes in the plan (the number of buckets of t1 is divided by the number of buckets of t2), but it doesn't say anything about what should happen if there are hints applied.

h1. Observation

# I checked other hints (MERGE and SHUFFLE_REPLICATE_NL) and they don't yield any Exchange nodes in the plan
# When I apply the hint to the t2 table ({{t2.hint(""SHUFFLE_HASH"").join(t1, ""id"")}}), no exchange happen for any hint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Apr 30 09:33:37 UTC 2023,,,,,,,,,,"0|z1hm1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/23 09:33;Zing;Thank you for the record. I will follow up on this issue~;;;",,,,,,,,,,,,,,
spark reader csv and json support wholetext parameters,SPARK-43318,13534461,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,melin,melin,28/Apr/23 12:23,15/Sep/23 04:25,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"FTPInputStream used by Hadoop FTPFileSystem does not support seek, and spark HadoopFileLinesReader fails to be read. 

Support to read the entire file, and then split lines, avoid reading failure

 

[https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FTPInputStream.java]

 

[~cloud_fan] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 02 02:43:24 UTC 2023,,,,,,,,,,"0|z1hlls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/May/23 02:43;gurwls223;Spark reader CSV supports multiline that will read the whole file?;;;",,,,,,,,,,,,,,
Adding missing default values for MERGE INSERT actions,SPARK-43313,13534373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dtenedor,dtenedor,dtenedor,28/Apr/23 00:08,18/May/23 18:39,30/Oct/23 17:26,04/May/23 20:18,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-38334,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 18 18:39:09 UTC 2023,,,,,,,,,,"0|z1hl2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/May/23 20:18;Gengliang.Wang;Issue resolved by pull request 40996
[https://github.com/apache/spark/pull/40996];;;","18/May/23 18:34;dongjoon;This is reverted from branch-3.4 via [https://github.com/apache/spark/commit/079594ae976b377459ad09d864106734ef65c32d];;;","18/May/23 18:39;dongjoon;To [~dtenedor] , SPARK-38334 was resolved with Fixed Version 3.4.0.
I converted this issue from a subtask to an independent issue. And, add a link to SPARK-38334 instead.
Please proceed new Jira issues (both bugs or improvements) independently or with a new umbrella JIRA.

 ;;;",,,,,,,,,,,,
Dataset.observe is ignored when writing to Kafka with batch query,SPARK-43310,13534337,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,deuberda,deuberda,27/Apr/23 15:30,06/May/23 15:54,30/Oct/23 17:26,,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,,,,,SQL,Structured Streaming,,,,1,,,,,"When writing to Kafka with a batch query, metrics defined with {{Dataset.observe}} are not recorded. 

For example, 
{code:java}
import org.apache.spark.sql.execution.QueryExecution
import org.apache.spark.sql.util.QueryExecutionListener

spark.listenerManager.register(new QueryExecutionListener {
  override def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit = {
    println(qe.observedMetrics)
  }

  override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {
    //pass
  }
})

val df = Seq((""k"", ""v"")).toDF(""key"", ""value"")
val observed = df.observe(""my_observation"", lit(""metric_value"").as(""some_metric""))
observed
  .write
  .format(""kafka"")
  .option(""kafka.bootstrap.servers"", ""host1:port1"")
  .option(""topic"", ""topic1"")
  .save()
{code}
prints {{{}Map(){}}}.",,,,,,,,,,,SPARK-33776,,,,,,,,,,,,,,,,,,,"06/May/23 10:08;fanjia;image-2023-05-06-18-08-41-335.png;https://issues.apache.org/jira/secure/attachment/13057864/image-2023-05-06-18-08-41-335.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat May 06 10:17:17 UTC 2023,,,,,,,,,,"0|z1hkuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/May/23 01:38;fanjia;I'm working on it.;;;","06/May/23 10:17;fanjia;The reason are kafka sink use `SaveIntoDataSourceCommand` to write data. There are two `QueryExecution`,  an `QueryExecution` outside `SaveIntoDataSourceCommand`, and a run-time `QueryExecution` inside `SaveIntoDataSourceCommand`. The metrics tool cannot obtain the internal `QueryExecution`, so it cannot obtain the corresponding metrics. We need to migrate the functionality of Kafka to DataSourceV2.

!image-2023-05-06-18-08-41-335.png|width=626,height=412!;;;",,,,,,,,,,,,,
Cascade failure in Guava cache due to fate-sharing,SPARK-43300,13534241,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liuzq12,liuzq12,liuzq12,27/Apr/23 01:05,16/May/23 01:49,30/Oct/23 17:26,16/May/23 01:49,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,,,0,,,,,"Guava cache is widely used in spark, however, it suffers from fate-sharing behavior: If there are multiple requests trying to access the same key in the {{cache}} at the same time when the key is not in the cache, Guava cache will block all requests and create the object only once. If the creation fails, all requests will fail immediately without retry. So we might see task failure due to irrelevant failure in other queries due to fate sharing.

This fate sharing behavior might lead to unexpected results in some situation.

We can wrap around Guava cache with a KeyLock to synchronize all requests with the same key, so they will run individually and fail as if they come one at a time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 16 01:49:18 UTC 2023,,,,,,,,,,"0|z1hk9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/May/23 01:49;joshrosen;Fixed in https://github.com/apache/spark/pull/40982;;;",,,,,,,,,,,,,,
predict_batch_udf with scalar input fails when batch size consists of a single value,SPARK-43298,13534218,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,leewyang,leewyang,leewyang,26/Apr/23 20:31,27/Apr/23 18:51,30/Oct/23 17:26,27/Apr/23 18:51,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,ML,PySpark,,,,0,,,,,"This is related to SPARK-42250.  For scalar inputs, the predict_batch_udf will fail if the batch size is 1:
{code:java}
import numpy as np
from pyspark.ml.functions import predict_batch_udf
from pyspark.sql.types import DoubleType

df = spark.createDataFrame([[1.0],[2.0]], schema=[""a""])

def make_predict_fn():
    def predict(inputs):
        return inputs
    return predict

identity = predict_batch_udf(make_predict_fn, return_type=DoubleType(), batch_size=1)
preds = df.withColumn(""preds"", identity(""a"")).collect()
{code}
fails with:
{code:java}
  File ""/.../spark/python/pyspark/worker.py"", line 869, in main
    process()
  File ""/.../spark/python/pyspark/worker.py"", line 861, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/.../spark/python/pyspark/sql/pandas/serializers.py"", line 354, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File ""/.../spark/python/pyspark/sql/pandas/serializers.py"", line 86, in dump_stream
    for batch in iterator:
  File ""/.../spark/python/pyspark/sql/pandas/serializers.py"", line 347, in init_stream_yield_batches
    for series in iterator:
  File ""/.../spark/python/pyspark/worker.py"", line 555, in func
    for result_batch, result_type in result_iter:
  File ""/.../spark/python/pyspark/ml/functions.py"", line 818, in predict
    yield _validate_and_transform_prediction_result(
  File ""/.../spark/python/pyspark/ml/functions.py"", line 339, in _validate_and_transform_prediction_result
    if len(preds_array) != num_input_rows:
TypeError: len() of unsized object
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Apr 27 18:51:23 UTC 2023,,,,,,,,,,"0|z1hk48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/23 03:31;snoot;User 'leewyang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40967;;;","27/Apr/23 18:51;gurwls223;Issue resolved by pull request 40967
[https://github.com/apache/spark/pull/40967];;;",,,,,,,,,,,,,
__qualified_access_only should be ignored in normal columns,SPARK-43293,13534167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,26/Apr/23 14:25,27/Apr/23 02:55,30/Oct/23 17:26,27/Apr/23 02:55,3.3.2,,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Apr 27 02:55:54 UTC 2023,,,,,,,,,,"0|z1hjsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/23 02:55;cloud_fan;Issue resolved by pull request 40961
[https://github.com/apache/spark/pull/40961];;;",,,,,,,,,,,,,,
ArtifactManagerSuite can't run using maven,SPARK-43292,13534111,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,26/Apr/23 08:00,08/May/23 15:10,30/Oct/23 17:26,08/May/23 15:10,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"run
{code:java}
build/mvn  clean install -DskipTests -Phive 
build/mvn test -pl connector/connect/server {code}
ArtifactManagerSuite failed due to 

 
{code:java}
23/04/26 16:00:07.666 ScalaTest-main-running-DiscoverySuite ERROR Executor: Could not find org.apache.spark.repl.ExecutorClassLoader on classpath! {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-04-26 08:00:46.0,,,,,,,,,,"0|z1hjgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect JVM client REPL not correctly shut down if killed,SPARK-43287,13534052,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,WweiL,WweiL,25/Apr/23 20:45,27/Apr/23 00:55,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Connect,,,,,0,,,,,"How to reproduce:
 # Start a scala client `./connector/connect/bin/spark-connect-scala-client`
 # in another terminal, kill the process `kill <pid>`
 # Back to the client terminal, you can't see anything you type, but the command still works

 

 
{code:java}
Spark session available as 'spark'.
   _____                  __      ______                            __
  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_
  \__ \/ __ \/ __ `/ ___/ //_/  / /   / __ \/ __ \/ __ \/ _ \/ ___/ __/
 ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_
/____/ .___/\__,_/_/  /_/|_|   \____/\____/_/ /_/_/ /_/\___/\___/\__/
    /_/


@ wei.liu:~/oss-spark$ CONTRIBUTING.md  appveyor.yml  conf                        examples            logs         resource-managers                    target
LICENSE          artifacts     connector                   graphx              mllib        sbin                                 tools
LICENSE-binary   assembly      core                        hadoop-cloud        mllib-local  scalastyle-config.xml
NOTICE           bin           data                        hs_err_pid9062.log  pom.xml      scalastyle-on-compile.generated.xml
NOTICE-binary    binder        dependency-reduced-pom.xml  launcher            project      spark-warehouse
R                build         dev                         licenses            python       sql
README.md        common        docs                        licenses-binary     repl         streaming
wei.liu:~/oss-spark$ wei.liu:~/oss-spark$ wei.liu:~/oss-spark$ 

{code}
 

I ran 'ls' above, and clicked return multiple times

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Apr 27 00:55:24 UTC 2023,,,,,,,,,,"0|z1hj3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/23 00:55;Zing;i can try to fix this issue : -) [~WweiL] ;;;",,,,,,,,,,,,,,
ReplE2ESuite consistently fails with JDK 17,SPARK-43285,13534031,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,25/Apr/23 16:16,25/Apr/23 19:11,30/Oct/23 17:26,25/Apr/23 19:11,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"[[Comment|https://github.com/apache/spark/pull/40675#discussion_r1174696470] from [~gurwls223]]

This test consistently fails with JDK 17:
{code:java}
[info] ReplE2ESuite:
[info] - Simple query *** FAILED *** (10 seconds, 4 milliseconds)
[info] java.lang.RuntimeException: REPL Timed out while running command: 
[info] spark.sql(""select 1"").collect()
[info] 
[info] Console output: 
[info] Error output: Compiling (synthetic)/ammonite/predef/ArgsPredef.sc
[info] at org.apache.spark.sql.application.ReplE2ESuite.runCommandsInShell(ReplE2ESuite.scala:87)
[info] at org.apache.spark.sql.application.ReplE2ESuite.$anonfun$new$1(ReplE2ESuite.scala:102)
[info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info] at org.scalatest.Transformer.apply(Transformer.scala:22)
[info] at org.scalatest.Transformer.apply(Transformer.scala:20)
[info] at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
[info] at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
[info] at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224){code}

[https://github.com/apache/spark/actions/runs/4780630672/jobs/8498505928#step:9:4647]
[https://github.com/apache/spark/actions/runs/4774942961/jobs/8488946907]
[https://github.com/apache/spark/actions/runs/4769162286/jobs/8479293802]
[https://github.com/apache/spark/actions/runs/4759278349/jobs/8458399201]
[https://github.com/apache/spark/actions/runs/4748319019/jobs/8434392414]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-04-25 16:16:10.0,,,,,,,,,,"0|z1hiyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix concurrent writer does not update file metrics,SPARK-43281,13533987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,25/Apr/23 12:31,16/May/23 01:42,30/Oct/23 17:26,16/May/23 01:42,3.5.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,,,0,,,,,"It uses temp file path to get file status after commit task. However, the temp file has already moved to new path during commit task.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 16 01:42:54 UTC 2023,,,,,,,,,,"0|z1hiow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/May/23 01:42;cloud_fan;Issue resolved by pull request 40952
[https://github.com/apache/spark/pull/40952];;;",,,,,,,,,,,,,,
"Exception in thread ""main"" java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;",SPARK-43278,13533964,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jiangjiguang0719,jiangjiguang0719,25/Apr/23 09:16,26/Apr/23 17:07,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Java API,,,,,0,,,,,"Java version: 1.8.0_331, Apache Maven 3.8.4

I run next steps:
 # git clone [https://github.com/apache/spark.git]
 # git checkout -b v3.3.0 3.3.0
 #  mvn clean install -DskipTests
 # copy hive-site.xml to examples/src/main/resources/
 # execute TPC-H Q6 

 
{code:java}
public static void main(String[] args) throws InterruptedException {
        SparkConf sparkConf = new SparkConf()
                .setAppName(""demo"")
                .setMaster(""local[1]"")
                ;        
         SparkSession sparkSession = SparkSession.builder()
                .config(sparkConf)
                .enableHiveSupport()
                .getOrCreate();
        sparkSession.sql(""use local_tpch_sf10_uncompressed_etl"");        
        sparkSession.sql(TPCH.SQL6).show();
} {code}
 

 

get the error info:

Exception in thread ""main"" java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;
    at org.apache.spark.util.io.ChunkedByteBufferOutputStream.toChunkedByteBuffer(ChunkedByteBufferOutputStream.scala:115)
    at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:325)
    at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:140)
    at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)
    at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
    at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)
    at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1529)
    at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:235)
    at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:457)
    at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:448)
    at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:547)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:221)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
    at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:217)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Apr 26 17:07:32 UTC 2023,,,,,,,,,,"0|z1hijs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/23 17:07;gurwls223;Seems like your JRE is not 8?;;;",,,,,,,,,,,,,,
Use proper error classes when exceptions are constructed with a message,SPARK-43268,13533876,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,aokolnychyi,aokolnychyi,24/Apr/23 16:57,25/Apr/23 00:25,30/Oct/23 17:26,25/Apr/23 00:25,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"As discussed [here|https://github.com/apache/spark/pull/40679/files#r1159264585].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Apr 25 00:25:39 UTC 2023,,,,,,,,,,"0|z1hi08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/23 00:25;csun;Issue resolved by pull request 40934
[https://github.com/apache/spark/pull/40934];;;",,,,,,,,,,,,,,
df.sql() should send metrics back(),SPARK-43249,13533794,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,grundprinzip-db,grundprinzip-db,grundprinzip-db,24/Apr/23 08:30,24/Apr/23 08:34,30/Oct/23 17:26,24/Apr/23 08:34,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Connect,,,,,0,,,,,df.sql() does not return the metrics to the client when executed as a command.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 24 08:34:12 UTC 2023,,,,,,,,,,"0|z1hhi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/23 08:34;podongfeng;Issue resolved by pull request 40899
[https://github.com/apache/spark/pull/40899];;;",,,,,,,,,,,,,,
diagnoseCorruption should not throw Unexpected type of BlockId for ShuffleBlockBatchId,SPARK-43242,13533710,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,John Zhang,John Zhang,23/Apr/23 13:35,28/Jul/23 04:22,30/Oct/23 17:26,,3.2.4,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Some of our spark app throw ""Unexpected type of BlockId"" exception as shown below

According to BlockId.scala, we can found format such as *shuffle_12_5868_518_523* is type of `ShuffleBlockBatchId`, which is not handled properly in `ShuffleBlockFetcherIterator.diagnoseCorruption`.

 

Moreover, the new exception thrown in `diagnose` swallow the real exception in certain cases. Since diagnoseCorruption is always used in exception handling as a side dish, I think it shouldn't throw exception at all

 
{code:java}
23/03/07 03:01:24,485 [task-result-getter-1] WARN TaskSetManager: Lost task 104.0 in stage 36.0 (TID 6169): java.lang.IllegalArgumentException: Unexpected type of BlockId, shuffle_12_5868_518_523 at org.apache.spark.storage.ShuffleBlockFetcherIterator.diagnoseCorruption(ShuffleBlockFetcherIterator.scala:1079)at org.apache.spark.storage.BufferReleasingInputStream.$anonfun$tryOrFetchFailedException$1(ShuffleBlockFetcherIterator.scala:1314) at scala.Option.map(Option.scala:230)at org.apache.spark.storage.BufferReleasingInputStream.tryOrFetchFailedException(ShuffleBlockFetcherIterator.scala:1313) at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:1299) at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) at java.io.BufferedInputStream.read(BufferedInputStream.java:345) at java.io.DataInputStream.read(DataInputStream.java:149) at org.sparkproject.guava.io.ByteStreams.read(ByteStreams.java:899) at org.sparkproject.guava.io.ByteStreams.readFully(ByteStreams.java:733) at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:127) at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110) at scala.collection.Iterator$$anon$11.next(Iterator.scala:496) at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40) at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.sort_addToSorter_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759) at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:82) at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:1065) at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:1024) at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:1201) at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:1240) at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225) at org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:137) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1510) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)
``` {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jul 28 04:22:44 UTC 2023,,,,,,,,,,"0|z1hgzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/23 07:09;zhixingheyi-tian;@cloud-fan ,

[~John Zhang] 

[~davidonlaptop] 

 

How about  this issue?;;;","28/Jul/23 04:22;snoot;User 'CavemanIV' has created a pull request for this issue:
https://github.com/apache/spark/pull/40921;;;",,,,,,,,,,,,,
df.describe() method may- return wrong result if the last RDD is RDD[UnsafeRow],SPARK-43240,13533667,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Jk_Self,Jk_Self,Jk_Self,23/Apr/23 02:50,26/Apr/23 09:25,30/Oct/23 17:26,26/Apr/23 09:25,3.3.2,,,,,,,,,,,,,,,,,,,3.3.3,,,,Spark Core,,,,,0,,,,,"When calling the df.describe() method, the result  maybe wrong when the last RDD is RDD[UnsafeRow]. It is because the UnsafeRow will be released after the row is used. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Apr 26 09:25:27 UTC 2023,,,,,,,,,,"0|z1hgqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/23 09:25;cloud_fan;Issue resolved by pull request 40914
[https://github.com/apache/spark/pull/40914];;;",,,,,,,,,,,,,,
Remove null_counts from info(),SPARK-43239,13533659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bjornjorgensen,bjornjorgensen,bjornjorgensen,22/Apr/23 20:13,24/Apr/23 00:14,30/Oct/23 17:26,24/Apr/23 00:14,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.5.0,,,,Pandas API on Spark,,,,,0,,,,,"df.info() is broken now. 
It prints 

TypeError                                 Traceback (most recent call last)
Cell In[12], line 1
----> 1 F05.info()

File /opt/spark/python/pyspark/pandas/frame.py:12167, in DataFrame.info(self, verbose, buf, max_cols, null_counts)
  12163     count_func = self.count
  12164     self.count = (  # type: ignore[assignment]
  12165         lambda: count_func()._to_pandas()  # type: ignore[assignment, misc, union-attr]
  12166     )
> 12167     return pd.DataFrame.info(
  12168         self,  # type: ignore[arg-type]
  12169         verbose=verbose,
  12170         buf=buf,
  12171         max_cols=max_cols,
  12172         memory_usage=False,
  12173         null_counts=null_counts,
  12174     )
  12175 finally:
  12176     del self._data

TypeError: DataFrame.info() got an unexpected keyword argument 'null_counts'
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 24 00:14:37 UTC 2023,,,,,,,,,,"0|z1hgow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/23 20:31;bjornjorgensen;https://github.com/apache/spark/pull/40913;;;","24/Apr/23 00:14;gurwls223;Issue resolved by pull request 40913
[https://github.com/apache/spark/pull/40913];;;",,,,,,,,,,,,,
Handle null exception message in event log,SPARK-43237,13533654,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,22/Apr/23 17:48,29/Apr/23 04:15,30/Oct/23 17:26,29/Apr/23 04:14,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Apr 29 04:15:32 UTC 2023,,,,,,,,,,"0|z1hgns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/23 04:14;mridulm80;Issue resolved by pull request 40911
[https://github.com/apache/spark/pull/40911];;;","29/Apr/23 04:15;snoot;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/40911;;;",,,,,,,,,,,,,
ClientDistributedCacheManager doesn't set the LocalResourceVisibility.PRIVATE if isPublic throws exception,SPARK-43235,13533625,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,pralabhkumar,pralabhkumar,22/Apr/23 09:40,09/May/23 16:02,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Hi Spark Team .

Currently *ClientDistributedCacheManager* *getVisibility* methods checks whether resource visibility can be set to private or public. 

In order to set  *LocalResourceVisibility.PUBLIC* ,isPublic checks permission of all the ancestors directories for the executable directory . It goes till the root folder to check permission of all the parents (ancestorsHaveExecutePermissions) 

checkPermissionOfOther calls  FileStatus getFileStatus to check the permission .

If the   FileStatus getFileStatus throws exception Spark Submit fails . It didn't sets the permission to Private.

if (isPublic(conf, uri, statCache))

{ LocalResourceVisibility.PUBLIC }

else

{ LocalResourceVisibility.PRIVATE }

Generally if the user doesn't have permission to check for root folder (specifically in case of cloud file system(GCS)  (for the buckets)  , methods throws error IOException(Error accessing Bucket).

 

*Ideally if there is an error in isPublic , which means Spark isn't able to determine the execution permission of all the parents directory , it should set the LocalResourceVisibility.PRIVATE.  However, it currently throws an exception in isPublic and hence Spark Submit fails*

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 09 16:02:32 UTC 2023,,,,,,,,,,"0|z1hghc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/23 09:20;pralabhkumar;[~gurwls223] Can u please look into this . ;;;","01/May/23 05:59;pralabhkumar;Gentle ping to review . I can create a PR for the same ;;;","09/May/23 16:02;pralabhkumar;can any one please look into this . If ok I can create PR for it . ;;;",,,,,,,,,,,,
Fix deserialisation issue when UDFs contain a lambda expression,SPARK-43227,13533491,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,vicennial,vicennial,21/Apr/23 05:08,23/May/23 09:43,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Connect,,,,,0,,,,,"The following code:
{code:java}
class A(x: Int) { def get = x * 20 + 5 }
val dummyUdf = (x: Int) => new A(x).get
val myUdf = udf(dummyUdf)
spark.range(5).select(myUdf(col(""id""))).as[Int].collect() {code}
hits the following error:
{noformat}
io.grpc.StatusRuntimeException: INTERNAL: cannot assign instance of java.lang.invoke.SerializedLambda to field ammonite.$sess.cmd26$Helper.dummyUdf of type scala.Function1 in instance of ammonite.$sess.cmd26$Helper
  io.grpc.Status.asRuntimeException(Status.java:535)
  io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:62)
  org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:114)
  org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:131)
  org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2687)
  org.apache.spark.sql.Dataset.withResult(Dataset.scala:3088)
  org.apache.spark.sql.Dataset.collect(Dataset.scala:2686)
  ammonite.$sess.cmd28$Helper.<init>(cmd28.sc:1)
  ammonite.$sess.cmd28$.<init>(cmd28.sc:7)
  ammonite.$sess.cmd28$.<clinit>(cmd28.sc){noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue May 23 09:19:25 UTC 2023,,,,,,,,,,"0|z1hfns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/23 09:19;juliuszsompolski;Possibly related to https://issues.apache.org/jira/browse/SPARK-43744?;;;",,,,,,,,,,,,,,
Executor should not be removed when decommissioned in standalone,SPARK-43224,13533465,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,warrenzhu25,warrenzhu25,20/Apr/23 21:59,22/Apr/23 18:36,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Currently, executor info in standalone master will be removed once decommissioned. The executor should be removed after executor shutdown.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 21 04:20:28 UTC 2023,,,,,,,,,,"0|z1hfi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/23 04:20;gurwls223;[~warrenzhu25]would be great if we have some description on this issue.;;;",,,,,,,,,,,,,,
Executor obtained error information ,SPARK-43221,13533426,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,yorksity,yorksity,20/Apr/23 15:56,12/Sep/23 16:47,30/Oct/23 17:26,,3.1.1,3.2.0,3.3.0,,,,,,,,,,,,,,,,,,,,,Block Manager,,,,,0,,,,,"Spark on Yarn Cluster

When multiple executors exist on a node, and the same block exists on both executors, with some in memory and some on disk.

Probabilistically, the executor failed to obtain the block,throw Exception:

java.lang.ArrayIndexOutofBoundsException: 0

    at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:183)

 

Next, I will replay the process of the problem occurring: 

step 1:

The executor requests the driver to obtain block information(locationsAndStatusOption). The input parameters are BlockId and the host of its own node. Please note that it does not carry port information

line:1092

!image-2023-04-21-00-24-22-059.png!

step 2:

On the driver side, the driver obtains all blockManagers holding the block based on the BlockId. For non remote shuffle scenarios, the driver will retrieve the first one with the blockId and blockManager from the locations

Assuming that there are two BlockManagers holding the BlockId on this node, BM-1 holds the Block and stores it in memory, and BM-2 holds the Block and stores it in disk

Assuming the returned status is of type memory and its disksize is 0

line: 852, 856

!image-2023-04-21-00-30-41-851.png!

step 3:

This method will return a BlockLocationsAndStatus object. If there are BMs using disk, the disk's path information will be stored in localDirs

!image-2023-04-21-00-50-10-918.png!

step 4:

When the executor obtains locationsAndStatusOption, localDirs is not empty, but status.diskSize is 0

line: 1102

!image-2023-04-21-00-54-11-968.png!

step 5:

The readDiskBlockFromSameHostExecutor only determines whether the Block file exists, and then directly uses the incoming blocksize to read the byte array. If the blocksize is 0, it returns an empty byte array

Only checked if the file exists

line: 1234, 1240

!image-2023-04-21-00-57-29-140.png!

Taking values from an empty array, causing an out of bounds problem",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/23 16:20;yorksity;image-2023-04-21-00-19-58-021.png;https://issues.apache.org/jira/secure/attachment/13057452/image-2023-04-21-00-19-58-021.png","20/Apr/23 16:24;yorksity;image-2023-04-21-00-24-22-059.png;https://issues.apache.org/jira/secure/attachment/13057453/image-2023-04-21-00-24-22-059.png","20/Apr/23 16:30;yorksity;image-2023-04-21-00-30-41-851.png;https://issues.apache.org/jira/secure/attachment/13057454/image-2023-04-21-00-30-41-851.png","20/Apr/23 16:50;yorksity;image-2023-04-21-00-50-10-918.png;https://issues.apache.org/jira/secure/attachment/13057455/image-2023-04-21-00-50-10-918.png","20/Apr/23 16:53;yorksity;image-2023-04-21-00-53-20-720.png;https://issues.apache.org/jira/secure/attachment/13057456/image-2023-04-21-00-53-20-720.png","20/Apr/23 16:54;yorksity;image-2023-04-21-00-54-11-968.png;https://issues.apache.org/jira/secure/attachment/13057457/image-2023-04-21-00-54-11-968.png","20/Apr/23 16:57;yorksity;image-2023-04-21-00-57-29-140.png;https://issues.apache.org/jira/secure/attachment/13057458/image-2023-04-21-00-57-29-140.png",,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-04-20 15:56:59.0,,,,,,,,,,"0|z1hf9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IsolatedClassLoader should close barrier class InputStream after reading,SPARK-43208,13533312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,20/Apr/23 05:30,20/Apr/23 16:53,30/Oct/23 17:26,20/Apr/23 16:53,3.3.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Apr 20 16:53:55 UTC 2023,,,,,,,,,,"0|z1hek0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/23 16:53;csun;Issue resolved by pull request 40867
[https://github.com/apache/spark/pull/40867];;;",,,,,,,,,,,,,,
Fix DROP table behavior in session catalog,SPARK-43203,13533279,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,aokolnychyi,aokolnychyi,19/Apr/23 19:16,11/Sep/23 13:48,30/Oct/23 17:26,19/Jun/23 12:43,3.4.0,,,,,,,,,,,,,,,,,,,3.4.2,3.5.0,,,SQL,,,,,0,pull-request-available,,,,"DROP table behavior is not working correctly in 3.4.0 because we always invoke V1 drop logic if the identifier looks like a V1 identifier. This is a big blocker for external data sources that provide custom session catalogs.

See [here|https://github.com/apache/spark/pull/37879/files#r1170501180] for details.",,,,,,,,,,,,,,,,,,,,SPARK-40425,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Sep 08 15:27:15 UTC 2023,,,,,,,,,,"0|z1heco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/May/23 04:45;fanjia;https://github.com/apache/spark/pull/41348;;;","06/Jun/23 03:44;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41348;;;","19/Jun/23 12:43;cloud_fan;Issue resolved by pull request 41348
[https://github.com/apache/spark/pull/41348];;;","27/Jun/23 22:32;aokolnychyi;I unfortunately created this initially as improvement. It is actually a bug and regression, which breaks DROP in custom sessions catalogs. Can we include it in 3.4.2?;;;","29/Aug/23 09:08;githubbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41765;;;","08/Sep/23 15:27;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/41765 ;;;",,,,,,,,,
Make InlineCTE idempotent,SPARK-43199,13533269,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,19/Apr/23 16:34,26/Apr/23 07:33,30/Oct/23 17:26,26/Apr/23 07:33,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Apr 26 07:33:38 UTC 2023,,,,,,,,,,"0|z1heag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/23 09:23;githubbot;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40856;;;","26/Apr/23 07:33;cloud_fan;Issue resolved by pull request 40856
[https://github.com/apache/spark/pull/40856];;;",,,,,,,,,,,,,
"Fix ""Could not initialise class ammonite..."" error when using filter",SPARK-43198,13533265,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,19/Apr/23 16:07,26/Apr/23 16:13,30/Oct/23 17:26,26/Apr/23 16:13,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,,,0,,,,,"When
{code:java}
spark.range(10).filter(n => n % 2 == 0).collectAsList()`{code}
 is run in the ammonite REPL (Spark Connect), the following error is thrown:
{noformat}
io.grpc.StatusRuntimeException: UNKNOWN: ammonite/repl/ReplBridge$
  io.grpc.Status.asRuntimeException(Status.java:535)
  io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:62)
  org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:114)
  org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:131)
  org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2687)
  org.apache.spark.sql.Dataset.withResult(Dataset.scala:3088)
  org.apache.spark.sql.Dataset.collect(Dataset.scala:2686)
  org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:2700)
  ammonite.$sess.cmd0$.<init>(cmd0.sc:1)
  ammonite.$sess.cmd0$.<clinit>(cmd0.sc){noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-04-19 16:07:06.0,,,,,,,,,,"0|z1he9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark connect's user agent validations are too restrictive,SPARK-43192,13533218,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nija,nija,nija,19/Apr/23 12:43,21/Apr/23 06:23,30/Oct/23 17:26,21/Apr/23 06:23,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Connect,PySpark,,,,0,,,,,"The current restriction on allowed charset and length are too restrictive

 

https://github.com/apache/spark/blob/cac6f58318bb84d532f02d245a50d3c66daa3e4b/python/pyspark/sql/connect/client.py#L274-L275",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 21 06:23:55 UTC 2023,,,,,,,,,,"0|z1hdz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/23 06:23;gurwls223;Issue resolved by pull request 40853
[https://github.com/apache/spark/pull/40853];;;",,,,,,,,,,,,,,
ListQuery.childOutput should be consistent with child output,SPARK-43190,13533216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,19/Apr/23 12:42,20/Apr/23 07:14,30/Oct/23 17:26,20/Apr/23 07:14,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Apr 20 07:14:15 UTC 2023,,,,,,,,,,"0|z1hdyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/23 07:14;cloud_fan;Issue resolved by pull request 40851
[https://github.com/apache/spark/pull/40851];;;",,,,,,,,,,,,,,
Cannot write to Azure Datalake Gen2 (abfs/abfss) after Spark 3.1.2,SPARK-43188,13533209,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Workaround,,nphung,nphung,19/Apr/23 12:20,26/May/23 15:52,30/Oct/23 17:26,26/May/23 15:52,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,,,,,PySpark,Spark Core,,,,0,,,,,"Hello,

I have an issue with Spark 3.3.2 & Spark 3.4.0 to write into Azure Data Lake Storage Gen2 (abfs/abfss scheme). I've got the following errors:
{code:java}
warn 13:12:47.554: StdErr from Kernel Process 23/04/19 13:12:47 ERROR FileFormatWriter: Aborting job 6a75949c-1473-4445-b8ab-d125be3f0f21.org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (myhost executor driver): org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for datablock-0001-    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)    at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)    at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)    at org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory.createTmpFileForWrite(DataBlocks.java:980)    at org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory.create(DataBlocks.java:960)    at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.createBlockIfNeeded(AbfsOutputStream.java:262)    at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.<init>(AbfsOutputStream.java:173)    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createFile(AzureBlobFileSystemStore.java:580)    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.create(AzureBlobFileSystem.java:301)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)    at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)    at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)    at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:480)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)    at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)    at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)    at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)    at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)    at org.apache.spark.scheduler.Task.run(Task.scala:139)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.Thread.run(Thread.java:748)
Driver stacktrace:    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)    at scala.Option.foreach(Option.scala:407)    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)    at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)    at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)    at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)    at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)    at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)    at py4j.Gateway.invoke(Gateway.java:282)    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)    at py4j.commands.CallCommand.execute(CallCommand.java:79)    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)    at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for datablock-0001-    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)    at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)    at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)    at org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory.createTmpFileForWrite(DataBlocks.java:980)    at org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory.create(DataBlocks.java:960)    at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.createBlockIfNeeded(AbfsOutputStream.java:262)    at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.<init>(AbfsOutputStream.java:173)    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createFile(AzureBlobFileSystemStore.java:580)    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.create(AzureBlobFileSystem.java:301)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)    at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)    at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)    at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:480)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)    at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)    at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)    at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)    at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)    at org.apache.spark.scheduler.Task.run(Task.scala:139)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    ... 1 more {code}
Before that, I was able to write into Azure Data Lake Storage with Spark 3.1.2 with hadoop-azure 3.2.1 without encountering this error.

Here's what I have tried but with no success:
 * Spark 3.3.2 with hadoop-azure 3.3.2
 * Spark 3.3.2 with hadoop-azure 3.3.5
 * Spark 3.4.0 with hadoop-azure 3.3.4
 * Spark 3.4.0 with hadoop-azure 3.3.5

Regards,

 ",,,,,,,,,,,HADOOP-18707,,,,,,,,,HADOOP-17195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri May 26 15:51:57 UTC 2023,,,,,,,,,,"0|z1hdx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/23 14:48;srowen;Looks like a local disk problem of some kind, not really a Spark issue;;;","26/May/23 15:51;nphung;Hello [~srowen]  I don't think so, but I manage to get it work Thanks to HADOOP-18707. It was a new default configuration in hadoop-azure that wasn't working for me anymore on local windows setup.;;;",,,,,,,,,,,,,
Mutilple tables join with limit when AE is enabled and one table is skewed,SPARK-43182,13533169,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,Resol1992,Resol1992,19/Apr/23 08:48,19/Sep/23 08:14,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"When we test AE in Spark3.4.0 with the following case, we find If we disable AE or enable Ae but disable skewJoin, the sql will finish in 20s, but if we enable AE and enable skewJoin，it will take very long time.

The test case:
{code:java}
###uncompress the part-m-***.zip attachment, and put these files under '/tmp/spark-warehouse/data/' dir.

create table source_aqe(c1 int,c18 string) using csv options(path 'file:///tmp/spark-warehouse/data/');
create table hive_snappy_aqe_table1(c1 int)stored as PARQUET partitioned by(c18 string); 
insert into table hive_snappy_aqe_table1 partition(c18=1)select c1 from source_aqe;
insert into table hive_snappy_aqe_table1 partition(c18=2)select c1 from source_aqe limit 120000;
insert into table hive_snappy_aqe_table1 partition(c18=3)select c1 from source_aqe limit 150000;create table hive_snappy_aqe_table2(c1 int)stored as PARQUET partitioned by(c18 string); 
insert into table hive_snappy_aqe_table2 partition(c18=1)select c1 from source_aqe limit 160000;
insert into table hive_snappy_aqe_table2 partition(c18=2)select c1 from source_aqe limit 120000;create table hive_snappy_aqe_table3(c1 int)stored as PARQUET partitioned by(c18 string); 
insert into table hive_snappy_aqe_table3 partition(c18=1)select c1 from source_aqe limit 160000;
insert into table hive_snappy_aqe_table3 partition(c18=2)select c1 from source_aqe limit 120000;
set spark.sql.adaptive.enabled=false;
set spark.sql.adaptive.forceOptimizeSkewedJoin = false;
set spark.sql.adaptive.skewJoin.skewedPartitionFactor=1;
set spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=10KB;
set spark.sql.adaptive.advisoryPartitionSizeInBytes=100KB;
set spark.sql.autoBroadcastJoinThreshold = 51200;
 
###it will finish in 20s 
select * from hive_snappy_aqe_table1 join hive_snappy_aqe_table2 on hive_snappy_aqe_table1.c18=hive_snappy_aqe_table2.c18 join hive_snappy_aqe_table3 on hive_snappy_aqe_table1.c18=hive_snappy_aqe_table3.c18 limit 10;


set spark.sql.adaptive.enabled=true;
set spark.sql.adaptive.forceOptimizeSkewedJoin = true;
set spark.sql.adaptive.skewJoin.skewedPartitionFactor=1;
set spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=10KB;
set spark.sql.adaptive.advisoryPartitionSizeInBytes=100KB;
set spark.sql.autoBroadcastJoinThreshold = 51200;
###it will take very long time 
select * from hive_snappy_aqe_table1 join hive_snappy_aqe_table2 on hive_snappy_aqe_table1.c18=hive_snappy_aqe_table2.c18 join hive_snappy_aqe_table3 on hive_snappy_aqe_table1.c18=hive_snappy_aqe_table3.c18 limit 10;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/23 09:33;Resol1992;part-m-00000.zip;https://issues.apache.org/jira/secure/attachment/13057414/part-m-00000.zip","19/Apr/23 09:33;Resol1992;part-m-00001.zip;https://issues.apache.org/jira/secure/attachment/13057413/part-m-00001.zip","19/Apr/23 09:33;Resol1992;part-m-00002.zip;https://issues.apache.org/jira/secure/attachment/13057412/part-m-00002.zip","19/Apr/23 09:33;Resol1992;part-m-00003.zip;https://issues.apache.org/jira/secure/attachment/13057411/part-m-00003.zip","19/Apr/23 09:33;Resol1992;part-m-00004.zip;https://issues.apache.org/jira/secure/attachment/13057410/part-m-00004.zip","19/Apr/23 09:33;Resol1992;part-m-00005.zip;https://issues.apache.org/jira/secure/attachment/13057409/part-m-00005.zip","19/Apr/23 09:33;Resol1992;part-m-00006.zip;https://issues.apache.org/jira/secure/attachment/13057408/part-m-00006.zip","19/Apr/23 09:33;Resol1992;part-m-00007.zip;https://issues.apache.org/jira/secure/attachment/13057407/part-m-00007.zip","19/Apr/23 09:33;Resol1992;part-m-00008.zip;https://issues.apache.org/jira/secure/attachment/13057406/part-m-00008.zip","19/Apr/23 09:33;Resol1992;part-m-00009.zip;https://issues.apache.org/jira/secure/attachment/13057405/part-m-00009.zip","19/Apr/23 09:33;Resol1992;part-m-00010.zip;https://issues.apache.org/jira/secure/attachment/13057404/part-m-00010.zip","19/Apr/23 09:33;Resol1992;part-m-00011.zip;https://issues.apache.org/jira/secure/attachment/13057403/part-m-00011.zip","19/Apr/23 09:33;Resol1992;part-m-00012.zip;https://issues.apache.org/jira/secure/attachment/13057402/part-m-00012.zip","19/Apr/23 09:33;Resol1992;part-m-00013.zip;https://issues.apache.org/jira/secure/attachment/13057401/part-m-00013.zip","19/Apr/23 09:33;Resol1992;part-m-00014.zip;https://issues.apache.org/jira/secure/attachment/13057400/part-m-00014.zip","19/Apr/23 09:33;Resol1992;part-m-00015.zip;https://issues.apache.org/jira/secure/attachment/13057399/part-m-00015.zip","19/Apr/23 09:33;Resol1992;part-m-00016.zip;https://issues.apache.org/jira/secure/attachment/13057398/part-m-00016.zip","19/Apr/23 09:33;Resol1992;part-m-00017.zip;https://issues.apache.org/jira/secure/attachment/13057417/part-m-00017.zip","19/Apr/23 09:33;Resol1992;part-m-00018.zip;https://issues.apache.org/jira/secure/attachment/13057416/part-m-00018.zip","19/Apr/23 09:33;Resol1992;part-m-00019.zip;https://issues.apache.org/jira/secure/attachment/13057415/part-m-00019.zip",,20.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Sep 19 08:13:45 UTC 2023,,,,,,,,,,"0|z1hdo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/23 08:13;dcoliversun;Hi [~Resol1992]

I ran your sql, tried different configuration combinations and believe regression caused by *spark.sql.adaptive.forceOptimizeSkewedJoin* , which introduces 
extra shuffles. AQE can give up skewJoin Optimization if extra shuffle introduced when *spark.sql.adaptive.forceOptimizeSkewedJoin* is false. cc [~cloud_fan] 

 

ref: 

[https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala#L225-L229];;;",,,,,,,,,,,,,,
decom.sh can cause an UnsupportedOperationException,SPARK-43175,13533049,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,icardnell,icardnell,18/Apr/23 13:12,18/Apr/23 13:16,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Kubernetes,,,,,0,,,,,"decom.sh can cause an UnsupportedOperationException which then causes the Executor to die with a SparkUncaughtException and does not complete the decommission properly.

 
*Problem:*
SignalUtils.scala line 124:
 
{code:java}
if (escalate) {
   prevHandler.handle(sig)
}{code}
 
 
*Logs:*
 
{noformat}
failed - error: command '/opt/decom.sh' exited with 137: + echo 'Asked to decommission' + date + tee -a ++ ps -o pid -C java ++ awk '{ sub(/^[ \t]+/, """"""""); print }' ++ tail -n 1 + WORKER_PID=17 + echo 'Using worker pid 17' + kill -s SIGPWR 17 + echo 'Waiting for worker pid to exit' + timeout 60 tail --pid=17 -f /dev/null , message: """"Asked to decommission\nMon Apr 17 23:44:35 UTC 2023\nUsing worker pid 17\nWaiting for worker pid to exit\n+ echo 'Asked to decommission'\n+ date\n+ tee -a\n++ ps -o pid -C java\n++ awk '{ sub(/^[ \\t]+/, \""""\""""); print }'\n++ tail -n 1\n+ WORKER_PID=17\n+ echo 'Using worker pid 17'\n+ kill -s SIGPWR 17\n+ echo 'Waiting for worker pid to exit'\n+ timeout 60 tail --pid=17 -f /dev/null\n"""""",2023-04-17T23:44:39Z,
""java.lang.UnsupportedOperationException: invoking native signal handle not supported
 at java.base/jdk.internal.misc.Signal$NativeHandler.handle(Unknown Source)
 at jdk.unsupported/sun.misc.Signal$SunMiscHandler.handle(Unknown Source)
 at org.apache.spark.util.SignalUtils$ActionHandler.handle(SignalUtils.scala:124)
 at jdk.unsupported/sun.misc.Signal$InternalMiscHandler.handle(Unknown Source)
 at java.base/jdk.internal.misc.Signal$1.run(Unknown Source) at java.base/java.lang.Thread.run(Unknown Source)"",2023-04-17T23:44:35.407488217Z ""2023-04-17 23:44:35
[SIGPWR handler] ERROR org.apache.spark.util.SparkUncaughtExceptionHandler - Uncaught exception in thread Thread[SIGPWR handler,9,system] - {}"",2023-04-17T23:44:35.407457859Z
 "" ... 1 more"",2023-04-17T23:44:35.405548994Z ""
 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)"",2023-04-17T23:44:35.405542621Z ""
 at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)"",2023-04-17T23:44:35.405536674Z ""
 at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)"",2023-04-17T23:44:35.405516396Z ""
 at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)"",2023-04-17T23:44:35.405416352Z ""
 at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)"",2023-04-17T23:44:35.405410491Z ""
...
 at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)"",2023-04-17T23:44:35.405262304Z ""
 at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)"",2023-04-17T23:44:35.405256591Z ""
 at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:209)"",2023-04-17T23:44:35.405250814Z{noformat}
 

In this case prevHandler is the NativeHandler (See [https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/19fb8f93c59dfd791f62d41f332db9e306bc1422/src/java.base/share/classes/jdk/internal/misc/Signal.java#L280]) and it throws the exception.

*Possible Solutions:*

 * Check if prevHandler is an instance of NativeHandler and do not call it in that case.
 * try catch around the invoke of the handler and log a warning/error on exceptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-04-18 13:12:38.0,,,,,,,,,,"0|z1hcxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SparkSQLCLIDriver completer,SPARK-43174,13533018,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,18/Apr/23 09:52,22/Apr/23 00:16,30/Oct/23 17:26,22/Apr/23 00:16,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Apr 22 00:16:24 UTC 2023,,,,,,,,,,"0|z1hcqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/23 00:16;yumwang;Issue resolved by pull request 40838
[https://github.com/apache/spark/pull/40838];;;",,,,,,,,,,,,,,
"The spark sql like statement is pushed down to parquet for execution, but the data cannot be queried",SPARK-43170,13532968,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Bug,,todd5167,todd5167,18/Apr/23 02:50,28/Apr/23 10:52,30/Oct/23 17:26,19/Apr/23 03:07,3.2.2,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"--DDL

CREATE TABLE `ecom_dwm`.`dwm_user_app_action_sum_all` (
  `gaid` STRING COMMENT '',
  `beyla_id` STRING COMMENT '',
  `dt` STRING,
  `hour` STRING,
  `appid` STRING COMMENT '包名')
USING parquet
PARTITIONED BY (dt, hour, appid)
LOCATION 's3://xxxxx/dwm_user_app_action_sum_all'

– partitions  info
show partitions ecom_dwm.dwm_user_app_action_sum_all PARTITION (dt='20230412');
 
dt=20230412/hour=23/appid=blibli.mobile.commerce
dt=20230412/hour=23/appid=cn.shopee.app
dt=20230412/hour=23/appid=cn.shopee.br
dt=20230412/hour=23/appid=cn.shopee.id
dt=20230412/hour=23/appid=cn.shopee.my
dt=20230412/hour=23/appid=cn.shopee.ph
 
— query
select DISTINCT(appid) from ecom_dwm.dwm_user_app_action_sum_all
where dt='20230412' and appid like '%shopee%'
 
--result
 nodata 
 
— other
I use spark3.0.1 version and trino query engine to query the data。
 
 
The physical execution node formed by spark 3.2
(3) Scan parquet ecom_dwm.dwm_user_app_action_sum_all Output [3]: [dt#63, hour#64, appid#65|#63, hour#64, appid#65] Batched: true Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(dt#63), isnotnull(appid#65), (dt#63 = 20230412), Contains(appid#65, shopee)|#63), isnotnull(appid#65), (dt#63 = 20230412), Contains(appid#65, shopee)] ReadSchema: struct<>
 
 
!image-2023-04-18-10-59-30-199.png!
 
 – sql plan detail
{code:java}
== Physical Plan ==
CollectLimit (9)
+- InMemoryTableScan (1)
      +- InMemoryRelation (2)
            +- * HashAggregate (8)
               +- Exchange (7)
                  +- * HashAggregate (6)
                     +- * Project (5)
                        +- * ColumnarToRow (4)
                           +- Scan parquet ecom_dwm.dwm_user_app_action_sum_all (3)


(1) InMemoryTableScan
Output [1]: [appid#65]
Arguments: [appid#65]

(2) InMemoryRelation
Arguments: [appid#65], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@ab5af13,StorageLevel(disk, memory, deserialized, 1 replicas),*(2) HashAggregate(keys=[appid#65], functions=[], output=[appid#65])
+- Exchange hashpartitioning(appid#65, 200), ENSURE_REQUIREMENTS, [plan_id=24]
   +- *(1) HashAggregate(keys=[appid#65], functions=[], output=[appid#65])
      +- *(1) Project [appid#65]
         +- *(1) ColumnarToRow
            +- FileScan parquet ecom_dwm.dwm_user_app_action_sum_all[dt#63,hour#64,appid#65] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(0 paths)[], PartitionFilters: [isnotnull(dt#63), isnotnull(appid#65), (dt#63 = 20230412), StartsWith(appid#65, com)], PushedFilters: [], ReadSchema: struct<>
,None)

(3) Scan parquet ecom_dwm.dwm_user_app_action_sum_all
Output [3]: [dt#63, hour#64, appid#65]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(dt#63), isnotnull(appid#65), (dt#63 = 20230412), StartsWith(appid#65, com)]
ReadSchema: struct<>

(4) ColumnarToRow [codegen id : 1]
Input [3]: [dt#63, hour#64, appid#65]

(5) Project [codegen id : 1]
Output [1]: [appid#65]
Input [3]: [dt#63, hour#64, appid#65]

(6) HashAggregate [codegen id : 1]
Input [1]: [appid#65]
Keys [1]: [appid#65]
Functions: []
Aggregate Attributes: []
Results [1]: [appid#65]

(7) Exchange
Input [1]: [appid#65]
Arguments: hashpartitioning(appid#65, 200), ENSURE_REQUIREMENTS, [plan_id=24]

(8) HashAggregate [codegen id : 2]
Input [1]: [appid#65]
Keys [1]: [appid#65]
Functions: []
Aggregate Attributes: []
Results [1]: [appid#65]

(9) CollectLimit
Input [1]: [appid#65]
Arguments: 1 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/23 02:59;todd5167;image-2023-04-18-10-59-30-199.png;https://issues.apache.org/jira/secure/attachment/13057348/image-2023-04-18-10-59-30-199.png","19/Apr/23 02:59;yumwang;image-2023-04-19-10-59-44-118.png;https://issues.apache.org/jira/secure/attachment/13057382/image-2023-04-19-10-59-44-118.png","19/Apr/23 03:06;yumwang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13057384/screenshot-1.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 28 10:52:27 UTC 2023,,,,,,,,,,"0|z1hcg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/23 03:57;gurwls223;Spark 3.2.X is EOL. Mind trying if the same persists in higher versions?;;;","18/Apr/23 06:13;todd5167;Spark3.2.x is currently used in production, and there is no plan to upgrade to a higher version for the time being.   If it's a bug, isn't the spark3.2 version going to be fixed?;;;","18/Apr/23 06:32;gurwls223;There'd be no more releases in Spark 3.2.X unless there's an important security issue, etc.;;;","18/Apr/23 09:20;yumwang;Have you cached dwm_user_app_action_sum_all?;;;","18/Apr/23 11:14;todd5167;[~yumwang]  no cache;;;","18/Apr/23 12:37;yumwang;Why it is {{CachedRDDBuilder}}?
{noformat}
(2) InMemoryRelation
Arguments: [appid#65], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@ab5af13,StorageLevel(disk, memory, deserialized, 1 replicas),*(2) HashAggregate(keys=[appid#65], functions=[], output=[appid#65])
+- Exchange hashpartitioning(appid#65, 200), ENSURE_REQUIREMENTS, [plan_id=24]
   +- *(1) HashAggregate(keys=[appid#65], functions=[], output=[appid#65])
      +- *(1) Project [appid#65]
         +- *(1) ColumnarToRow
            +- FileScan parquet ecom_dwm.dwm_user_app_action_sum_all[dt#63,hour#64,appid#65] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(0 paths)[], PartitionFilters: [isnotnull(dt#63), isnotnull(appid#65), (dt#63 = 20230412), StartsWith(appid#65, com)], PushedFilters: [], ReadSchema: struct<>
,None)
{noformat}
;;;","19/Apr/23 02:31;todd5167;[~yumwang]  The code only executes spark.sql(""xxx""), but does not perform cache-related operations. But the same code, why spark3.0 and spark3.2 have different results.If it is convenient for you, you can reproduce it.;;;","19/Apr/23 02:59;yumwang;I can't reproduce this issue:

!image-2023-04-19-10-59-44-118.png!;;;","19/Apr/23 03:06;yumwang;Maybe your partition exists, but there is no data under the partition, such as the following:
 !screenshot-1.png! 
{noformat}
yumwang@LM-SHC-16508156 dwm_user_app_action_sum_all2 % ls -R
dt=20230412

./dt=20230412:
hour=23

./dt=20230412/hour=23:
appid=blibli.mobile.commerce	appid=cn.shopee.br		appid=cn.shopee.my
appid=cn.shopee.app		appid=cn.shopee.id		appid=cn.shopee.ph

./dt=20230412/hour=23/appid=blibli.mobile.commerce:

./dt=20230412/hour=23/appid=cn.shopee.app:

./dt=20230412/hour=23/appid=cn.shopee.br:

./dt=20230412/hour=23/appid=cn.shopee.id:

./dt=20230412/hour=23/appid=cn.shopee.my:

./dt=20230412/hour=23/appid=cn.shopee.ph:
{noformat};;;","28/Apr/23 10:52;stevel@apache.org;FWIW, using  S3 URLs  's3://xxxxx/dwm_user_app_action_sum_all' means it's an AWS EMR deployment, with their private fork of spark, etc. you might want to raise a support case there;;;",,,,,
Docker images are missing passwd entry for UID 185,SPARK-43166,13532929,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,derektbrown,derektbrown,17/Apr/23 17:46,04/Jul/23 02:40,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,R,,,,0,,,,,"Currently, the official Spark docker images run as the UID {{{}185{}}}, but no corresponding entry exists in {{{}/etc/passwd{}}}. This causes [issues|https://stackoverflow.com/questions/41864985/hadoop-ioexception-failure-to-login] when libraries try to fetch the current unix username.",,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 17 17:47:30 UTC 2023,,,,,,,,,,"0|z1hc7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 17:47;derektbrown;I created a PR for this here:

https://github.com/apache/spark/pull/40798;;;",,,,,,,,,,,,,,
Spark overwrites existing FILES/ARCHIVES/... settings instead of merging them,SPARK-43164,13532888,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,advancedxy,advancedxy,17/Apr/23 13:12,17/Apr/23 13:14,30/Oct/23 17:26,,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"https://issues.apache.org/jira/browse/SPARK-27575 fixes --files/–archives merging issues for spark on yarn mode.

 

However the overwrite logic remains in other resource managers, such as standalone, mesos and k8s. 

I'd like to propose that the merging behavior should be consistent across resource managers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 17 13:14:57 UTC 2023,,,,,,,,,,"0|z1hby8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 13:14;advancedxy;hi [~vanzin], [~kabhwan] do you guys have any input for this issue?;;;",,,,,,,,,,,,,,
An exception occurred while hive table join tidb table,SPARK-43163,13532868,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,shanhai3000,shanhai3000,17/Apr/23 11:36,18/Apr/23 03:59,30/Oct/23 17:26,18/Apr/23 03:59,3.2.3,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"When executing a query of a hive partition table (big one) inner join a tidb table(small one), the hive partition table is auto broadcasted, which leads an error.
The query is somelike
 {{select hive_table.col1,tidb_table.col2 from hive_table inner join tidb_table on hive_table.col2=tidb_table.col3 where ...}}
== Physical Plan ==
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [... 109 more fields]
+- Generate HiveGenericUDTF#udf.json.JsonExtractValueUDTF(xxx), [, ... 101 more fields], false, [...]
+- Project [, ... 102 more fields]
+- BroadcastHashJoin [xxx#94], [xxxx#475], Inner, BuildRight, false
:- TiKV CoprocessorRDD\{[table: xxx] TableReader, Columns: xxxx(): { TableRangeScan: { RangeFilter: [], Range: [([t\200\000\000\000\000\000\004\253_r\200\000\000\000\000\000\000\000], [t\200\000\000\000\000\000\004\253_s\000\000\000\000\000\000\000\000])([t\200\000\000\000\000\000\004\253_r\000\000\000\000\000\000\000\000], [t\200\000\000\000\000\000\004\253_r\200\000\000\000\000\000\000\000])] } }, startTs: 440854942292115639} EstimatedCount:20837
+- BroadcastExchange HashedRelationBroadcastMode(List(input[107, string, false]),false), [plan_id=32]
+- Filter isnotnull(xxx#475)
+- Scan hive xx.xxxxxxxx [, ... 100 more fields], HiveTableRelation [{{{}xx{}}}.{{{}xxx{}}}, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, Data Cols: [..., Partition Cols: [[#520|https://github.com/pingcap/tispark/issues/520], [#521|https://github.com/pingcap/tispark/issues/521], [#522|https://github.com/pingcap/tispark/issues/522], [#523|https://github.com/pingcap/tispark/pull/523]], Pruned Partitions: [(, , , )]], [isnotnull(), (), (xx = xx)]

Here I got some log info maybe helpful.
The {{plan.stats.sizeInBytes}} of the LogicalPlan of the hive table is too small and the {{plan.stats.sizeInBytes}} of LogicalPlan of the tidb table is too big.
The stats of the LogicalPlans of the two seems reversed.

*Spark and TiSpark version info*
Spark 3.2.3
TiSpark 3.1.2(with a profile of spark-3.2)
*Additional context*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 17 12:36:29 UTC 2023,,,,,,,,,,"0|z1hbts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 12:36;yumwang;It seems like TiSpark issue.;;;",,,,,,,,,,,,,,
Throw error in planning phase when the query redefines watermark for a stream,SPARK-43161,13532839,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kabhwan,kabhwan,17/Apr/23 07:36,17/Apr/23 07:36,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Structured Streaming,,,,,0,,,,,"With SPARK-42376, we started to disallow redefinition of watermark in a single stream. But the check is actually performed in watermark propagator, which does not throw an exception in the batch 0.

Since we tend to give a fast feedback for improper usage, throwing exception should also happen in planning phase, ideally in UnsupportedOperationChecker.

In addition, we don't seem to document this restriction. Also update the structured streaming doc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-04-17 07:36:13.0,,,,,,,,,,"0|z1hbnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set upperbound of pandas version in binder integrations,SPARK-43158,13532809,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,17/Apr/23 00:48,17/Apr/23 04:00,30/Oct/23 17:26,17/Apr/23 01:13,3.4.0,,,,,,,,,,,,,,,,,,,3.3.3,3.4.0,3.5.0,,Documentation,,,,,0,,,,,"{code}
df.toPandas
{code}

fails with

{code}

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[14], line 1
----> 1 df.toPandas()

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:251, in PandasConversionMixin.toPandas(self)
    248 should_check_timedelta = is_timedelta64_dtype(t) and len(pdf) == 0
    250 if (t is not None and not is_timedelta64_dtype(t)) or should_check_timedelta:
--> 251     series = series.astype(t, copy=False)
    253 with catch_warnings():
    254     from pandas.errors import PerformanceWarning

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pandas/core/generic.py:6324, in NDFrame.astype(self, dtype, copy, errors)
   6317     results = [
   6318         self.iloc[:, i].astype(dtype, copy=copy)
   6319         for i in range(len(self.columns))
   6320     ]
   6322 else:
   6323     # else, only a single dtype is given
-> 6324     new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)
   6325     return self._constructor(new_data).__finalize__(self, method=""astype"")
   6327 # GH 33113: handle empty frame or series

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pandas/core/internals/managers.py:451, in BaseBlockManager.astype(self, dtype, copy, errors)
    448 elif using_copy_on_write():
    449     copy = False
--> 451 return self.apply(
    452     ""astype"",
    453     dtype=dtype,
    454     copy=copy,
    455     errors=errors,
    456     using_cow=using_copy_on_write(),
    457 )

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pandas/core/internals/managers.py:352, in BaseBlockManager.apply(self, f, align_keys, **kwargs)
    350         applied = b.apply(f, **kwargs)
    351     else:
--> 352         applied = getattr(b, f)(**kwargs)
    353     result_blocks = extend_blocks(applied, result_blocks)
    355 out = type(self).from_blocks(result_blocks, self.axes)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 17 04:00:58 UTC 2023,,,,,,,,,,"0|z1hbgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 01:13;gurwls223;Issue resolved by pull request 40814
[https://github.com/apache/spark/pull/40814];;;","17/Apr/23 04:00;snoot;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/40814;;;",,,,,,,,,,,,,
TreeNode tags can become corrupted and hang driver when the dataset is cached,SPARK-43157,13532807,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,robreeves,robreeves,robreeves,16/Apr/23 22:18,18/May/23 06:25,30/Oct/23 17:26,18/May/23 06:25,2.3.0,3.5.0,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,,,0,,,,,"If a cached dataset is used by multiple other datasets materialized in separate threads it can corrupt the TreeNode.tags map in any of the cached plan nodes. This will hang the driver forever. This happens because TreeNode.tags is not thread-safe. How this happens:
 # Multiple datasets are materialized at the same time in different threads that reference the same cached dataset
 # AdaptiveSparkPlanExec.onUpdatePlan will call ExplainMode.fromString
 # ExplainUtils uses the TreeNode.tags map to store the operator Id for every node in the plan. This is usually okay because the plan is cloned. When there is an InMemoryScanExec the InMemoryRelation.cachedPlan is not cloned so multiple threads can set the operator Id.

Making the TreeNode.tags field thread-safe does not solve this problem because there is still a correctness issue. The threads may be overwriting each other's operator Ids, which could be different.

Example stack trace of the infinite loop:
{code:scala}
scala.collection.mutable.HashTable.resize(HashTable.scala:265)
scala.collection.mutable.HashTable.addEntry0(HashTable.scala:158)
scala.collection.mutable.HashTable.findOrAddEntry(HashTable.scala:170)
scala.collection.mutable.HashTable.findOrAddEntry$(HashTable.scala:167)
scala.collection.mutable.HashMap.findOrAddEntry(HashMap.scala:44)
scala.collection.mutable.HashMap.put(HashMap.scala:126)
scala.collection.mutable.HashMap.update(HashMap.scala:131)
org.apache.spark.sql.catalyst.trees.TreeNode.setTagValue(TreeNode.scala:108)
org.apache.spark.sql.execution.ExplainUtils$.setOpId$1(ExplainUtils.scala:134)
…
org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:175)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:662){code}
Example to show the cachedPlan object is not cloned:
{code:java}
import org.apache.spark.sql.execution.SparkPlan
import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec
import spark.implicits._

def findCacheOperator(plan: SparkPlan): Option[InMemoryTableScanExec] = {
  if (plan.isInstanceOf[InMemoryTableScanExec]) {
    Some(plan.asInstanceOf[InMemoryTableScanExec])
  } else if (plan.children.isEmpty && plan.subqueries.isEmpty) {
    None
  } else {
    (plan.subqueries.flatMap(p => findCacheOperator(p)) ++
      plan.children.flatMap(findCacheOperator)).headOption
  }
}

val df = spark.range(10).filter($""id"" < 100).cache()
val df1 = df.limit(1)
val df2 = df.limit(1)

// Get the cache operator (InMemoryTableScanExec) in each plan
val plan1 = findCacheOperator(df1.queryExecution.executedPlan).get
val plan2 = findCacheOperator(df2.queryExecution.executedPlan).get

// Check if InMemoryTableScanExec references point to the same object
println(plan1.eq(plan2))
// returns false// Check if InMemoryRelation references point to the same object

println(plan1.relation.eq(plan2.relation))
// returns false

// Check if the cached SparkPlan references point to the same object
println(plan1.relation.cachedPlan.eq(plan2.relation.cachedPlan))
// returns true
// This shows that the cloned plan2 still has references to the original plan1 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 18 06:25:50 UTC 2023,,,,,,,,,,"0|z1hbgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/23 09:22;githubbot;User 'robreeves' has created a pull request for this issue:
https://github.com/apache/spark/pull/40812;;;","18/May/23 06:25;cloud_fan;Issue resolved by pull request 40812
[https://github.com/apache/spark/pull/40812];;;",,,,,,,,,,,,,
"Pyspark 3.4 fails when running ""pivot"" function on a dataframe using the values arguement",SPARK-43154,13532781,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ofrik,ofrik,16/Apr/23 10:25,18/Apr/23 04:04,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"After upgrading to Pyspark version 3.4.0 from version 3.3.2, workflows using the ""pivot"" function fails with the following error:

 
`File ""/usr/local/lib/python3.8/site-packages/pyspark/sql/group.py"", line 512, in pivot
[847|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L847] jgd = self._jgd.pivot(pivot_col, values)
[848|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L848] File ""/usr/local/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1322, in __call__
[849|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L849] return_value = get_return_value(
[850|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L850] File ""/usr/local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py"", line 169, in deco
[851|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L851] return f(*a, **kw)
[852|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L852] File ""/usr/local/lib/python3.8/site-packages/py4j/protocol.py"", line 330, in get_return_value
[853|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L853] raise Py4JError(
[854|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L854]py4j.protocol.Py4JError: An error occurred while calling o999.pivot. Trace:
[855|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L855]py4j.Py4JException: Method pivot([class java.lang.String, class [Ljava.lang.String;]) does not exist
[856|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L856] at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)
[857|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L857] at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)
[858|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L858] at py4j.Gateway.invoke(Gateway.java:274)
[859|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L859] at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[860|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L860] at py4j.commands.CallCommand.execute(CallCommand.java:79)
[861|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L861] at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[862|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L862] at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[863|https://gitlab.booking.com/core/content-intelligence/content-ml/-/jobs/140674703#L863] at java.base/java.lang.Thread.run(Thread.java:829)`
 
My workflow calls the ""pivot"" method using the ""values"" argument.
I didn't have the chance to try calling pivot without it to see if the error occurs also when omitting the ""values"" argument.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Apr 18 04:04:35 UTC 2023,,,,,,,,,,"0|z1hbb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/23 12:33;yumwang;cc [~gurwls223];;;","18/Apr/23 04:04;gurwls223;Would be great if we have a self-cotained reproducer. I can't reproduce it from my end.;;;",,,,,,,,,,,,,
"When CTAS with USING fails to store metadata in metastore, data gets left around",SPARK-43149,13532727,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,bersprockets,bersprockets,15/Apr/23 01:31,06/Sep/23 13:29,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"For example:
{noformat}
drop table if exists parquet_ds1;

-- try creating table with invalid column name
-- use 'using parquet' to designate the data source
create table parquet_ds1 using parquet as
select id, date'2018-01-01' + make_dt_interval(0, id)
from range(0, 10);

Cannot create a table having a column whose name contains commas in Hive metastore. Table: `spark_catalog`.`default`.`parquet_ds1`; Column: DATE '2018-01-01' + make_dt_interval(0, id, 0, 0.000000)

-- show that table did not get created
show tables;


-- try again with valid column name
-- spark will complain that directory already exists
create table parquet_ds1 using parquet as
select id, date'2018-01-01' + make_dt_interval(0, id) as ts
from range(0, 10);

[LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`parquet_ds1`, as its associated location 'file:/Users/bruce/github/spark_upstream/spark-warehouse/parquet_ds1' already exists. Please pick a different table name, or remove the existing location first.
org.apache.spark.SparkRuntimeException: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`parquet_ds1`, as its associated location 'file:/Users/bruce/github/spark_upstream/spark-warehouse/parquet_ds1' already exists. Please pick a different table name, or remove the existing location first.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.locationAlreadyExists(QueryExecutionErrors.scala:2804)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:414)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:176)
...
{noformat}
One must manually remove the directory {{spark-warehouse/parquet_ds1}} before the {{create table}} command will succeed.

It seems that datasource table creation runs the data-creation job first, then stores the metadata into the metastore.

When using Spark to create Hive tables, the issue does not happen:
{noformat}
drop table if exists parquet_hive1;

-- try creating table with invalid column name,
-- but use 'stored as parquet' instead of 'using'
create table parquet_hive1 stored as parquet as
select id, date'2018-01-01' + make_dt_interval(0, id)
from range(0, 10);

Cannot create a table having a column whose name contains commas in Hive metastore. Table: `spark_catalog`.`default`.`parquet_hive1`; Column: DATE '2018-01-01' + make_dt_interval(0, id, 0, 0.000000)

-- try again with valid column name. This will succeed;
create table parquet_hive1 stored as parquet as
select id, date'2018-01-01' + make_dt_interval(0, id) as ts
from range(0, 10);
{noformat}

It seems that Hive table creation stores metadata into the metastore first, then runs the data-creation job.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Sep 06 13:29:57 UTC 2023,,,,,,,,,,"0|z1haz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/23 13:29;Zing;pr:https://github.com/apache/spark/pull/42574;;;",,,,,,,,,,,,,,
DSL expressions fail on attribute with special characters,SPARK-43142,13532662,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rshkv,rshkv,rshkv,14/Apr/23 13:11,25/Apr/23 10:23,30/Oct/23 17:26,25/Apr/23 10:23,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"Expressions on implicitly converted attributes fail if the attributes have names containing special characters. They fail even if the attributes are backtick-quoted:
{code:java}
scala> import org.apache.spark.sql.catalyst.dsl.expressions._
import org.apache.spark.sql.catalyst.dsl.expressions._

scala> ""`slashed/col`"".attr
res0: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'slashed/col

scala> ""`slashed/col`"".attr.asc
org.apache.spark.sql.catalyst.parser.ParseException:
mismatched input '/' expecting {<EOF>, '.', '-'}(line 1, pos 7)

== SQL ==
slashed/col
-------^^^
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Apr 25 10:23:52 UTC 2023,,,,,,,,,,"0|z1hako:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 13:17;rshkv;Here's what's happening: {{ImplicitOperators}} methods like {{asc}} rely on a call to {{expr}} [(Github)|https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L149]. 

The {{UnresolvedAttribute}} returned by {{.attr}} is implicitly converted to {{DslAttr}}. But {{DslAttr}} does not implement {{expr}} by returning the attribute it's already wrapping. Instead, it only implements how to convert the attribute it's wrapping to a string name [(Github)|https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L273-L275].

Returning an attribute for an implicitly wrapped attribute is implemented on the super class {{ImplicitAttribute}} by creating a new {{UnresolvedAttribute}} on the string name return by {{DslAttr}} (the method call {{s}}, [Github|https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L278-L280]).

The problem is that this string name returned by {{DslAttr}} no longer has the quotes and thus the new {{UnresolvedAttribute}} parses an unquoted identifier.

{code}
scala> ""`col/slash`"".attr.name
res1: String = col/slash
{code};;;","14/Apr/23 13:18;rshkv;The solution I'd propose is to have {{DslAttr.attr}} return the attribute it's wrapping instead of creating a new attribute.

I'll put up a PR.;;;","14/Apr/23 13:45;rshkv;https://github.com/apache/spark/pull/40794;;;","19/Apr/23 17:08;hudson;User 'rshkv' has created a pull request for this issue:
https://github.com/apache/spark/pull/40794;;;","21/Apr/23 12:55;cloud_fan;Issue resolved by pull request 40794
[https://github.com/apache/spark/pull/40794];;;","22/Apr/23 09:16;githubbot;User 'rshkv' has created a pull request for this issue:
https://github.com/apache/spark/pull/40902;;;","25/Apr/23 10:23;cloud_fan;Issue resolved by pull request 40902
[https://github.com/apache/spark/pull/40902];;;",,,,,,,,
Ignore generated Java files in checkstyle,SPARK-43141,13532652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,14/Apr/23 11:36,17/Apr/23 02:57,30/Oct/23 17:26,16/Apr/23 10:47,3.4.1,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Build,,,,,0,,,,,Files such as {{.../spark/core/target/scala-2.12/src_managed/main/org/apache/spark/status/protobuf/StoreTypes.java}} are checked in checkstyle. We shouldn't check them in the linter.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Apr 16 10:47:41 UTC 2023,,,,,,,,,,"0|z1haig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/23 10:47;gurwls223;Issue resolved by pull request 40792
[https://github.com/apache/spark/pull/40792];;;",,,,,,,,,,,,,,
Override computeStats in DummyLeafNode,SPARK-43140,13532638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,14/Apr/23 09:19,17/Apr/23 03:17,30/Oct/23 17:26,17/Apr/23 03:13,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 17 03:17:08 UTC 2023,,,,,,,,,,"0|z1hafc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 10:18;yumwang;https://github.com/apache/spark/pull/40791;;;","17/Apr/23 03:13;gurwls223;Issue resolved by pull request 40791
[https://github.com/apache/spark/pull/40791];;;","17/Apr/23 03:17;snoot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/40791;;;",,,,,,,,,,,,
ClassNotFoundException during RDD block replication/migration,SPARK-43138,13532606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,14/Apr/23 06:06,11/May/23 13:25,30/Oct/23 17:26,11/May/23 13:25,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,,,0,,,,,"During RDD block migration during decommissioning we are seeing `ClassNotFoundException` on the receiving Executor. This seems to happen when the blocks contain classes that are from the user jars.
```
2023-04-08 04:15:11,791 ERROR server.TransportRequestHandler: Error while invoking RpcHandler#receive() on RPC id 6425687122551756860
java.lang.ClassNotFoundException: com.class.from.user.jar.ClassName
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
    at java.base/java.lang.Class.forName0(Native Method)
    at java.base/java.lang.Class.forName(Class.java:398)
    at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:71)
    at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2003)
    at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)
    at java.base/java.io.ObjectInputStream.readClass(ObjectInputStream.java:1833)
    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1658)
    at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)
    at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)
    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)
    at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)
    at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)
    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)
    at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)
    at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)
    at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
    at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)
    at org.apache.spark.network.netty.NettyBlockRpcServer.deserializeMetadata(NettyBlockRpcServer.scala:180)
    at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:119)
    at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)
    at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:829)
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 11 13:25:58 UTC 2023,,,,,,,,,,"0|z1ha88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 06:47;yumwang;Did you set some config to `com.class.from.user.jar.ClassName`?;;;","16/Apr/23 13:14;eejbyfeldt;No. The class `com.class.from.user.jar.ClassName` is just a `case class` that is used inside an RDD. 

But I think I have a good idea of what is causing it. So I created this PR which I hope solves it: https://github.com/apache/spark/pull/40808;;;","11/May/23 13:25;srowen;Issue resolved by pull request 40808
[https://github.com/apache/spark/pull/40808];;;",,,,,,,,,,,,
mark two Hive UDF expressions as stateful,SPARK-43126,13532498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,13/Apr/23 13:29,14/Apr/23 00:50,30/Oct/23 17:26,14/Apr/23 00:50,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-04-13 13:29:11.0,,,,,,,,,,"0|z1h9k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect Server Can't Handle Exception With Null Message Normally,SPARK-43125,13532493,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,fanjia,fanjia,fanjia,13/Apr/23 13:04,14/Apr/23 03:46,30/Oct/23 17:26,14/Apr/23 03:46,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Connect,,,,,0,,,,,"!image-2023-04-13-21-05-43-617.png|width=626,height=310!!image-2023-04-13-21-05-16-994.png|width=621,height=294!

When Server Throw Exception which without message Like NPE. The setMessage method will report NPE again. But can't throw to client.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/23 13:05;fanjia;image-2023-04-13-21-05-16-994.png;https://issues.apache.org/jira/secure/attachment/13057253/image-2023-04-13-21-05-16-994.png","13/Apr/23 13:05;fanjia;image-2023-04-13-21-05-43-617.png;https://issues.apache.org/jira/secure/attachment/13057254/image-2023-04-13-21-05-43-617.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 14 03:46:23 UTC 2023,,,,,,,,,,"0|z1h9j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 03:46;gurwls223;Issue resolved by pull request 40780
[https://github.com/apache/spark/pull/40780];;;",,,,,,,,,,,,,,
Dataset.show should not trigger job execution on CommandResults,SPARK-43124,13532488,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,13/Apr/23 12:32,21/Apr/23 00:33,30/Oct/23 17:26,21/Apr/23 00:33,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 21 00:33:56 UTC 2023,,,,,,,,,,"0|z1h9i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/23 17:08;hudson;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40779;;;","21/Apr/23 00:33;gurwls223;Issue resolved by pull request 40779
[https://github.com/apache/spark/pull/40779];;;",,,,,,,,,,,,,
special internal field metadata should not be leaked to catalogs,SPARK-43123,13532483,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,13/Apr/23 11:46,12/Sep/23 18:40,30/Oct/23 17:26,14/Apr/23 09:09,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 14 09:09:00 UTC 2023,,,,,,,,,,"0|z1h9gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 09:09;cloud_fan;Issue resolved by pull request 40776
[https://github.com/apache/spark/pull/40776];;;",,,,,,,,,,,,,,
Support Get SQL Keywords Dynamically,SPARK-43119,13532435,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,13/Apr/23 03:57,21/Apr/23 02:24,30/Oct/23 17:26,21/Apr/23 02:24,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,Implements the JDBC standard API and an auxiliary function,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 21 02:24:22 UTC 2023,,,,,,,,,,"0|z1h968:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/23 04:17;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40768;;;","13/Apr/23 04:17;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40768;;;","21/Apr/23 02:24;Qin Yao;Issue resolved by pull request 40768
[https://github.com/apache/spark/pull/40768];;;",,,,,,,,,,,,
Fix Cast.forceNullable,SPARK-43116,13532427,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,yumwang,yumwang,13/Apr/23 02:48,13/Apr/23 02:49,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"https://github.com/apache/spark/blob/ac105ccebf5f144f2d506cbe102c362a195afa9a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala#L381-L399

1. It should include TimestampNTZType,AnsiIntervalType.
2. ArrayType/MapType/StructType to other data type seems should be true.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-04-13 02:48:13.0,,,,,,,,,,"0|z1h94g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Codegen error when full outer join's bound condition has multiple references to the same stream-side column,SPARK-43113,13532412,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,12/Apr/23 22:33,24/Apr/23 00:59,30/Oct/23 17:26,18/Apr/23 04:11,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,,,0,,,,,"Example # 1 (sort merge join):
{noformat}
create or replace temp view v1 as
select * from values
(1, 1),
(2, 2),
(3, 1)
as v1(key, value);

create or replace temp view v2 as
select * from values
(1, 22, 22),
(3, -1, -1),
(7, null, null)
as v2(a, b, c);

select *
from v1
full outer join v2
on key = a
and value > b
and value > c;
{noformat}
The join's generated code causes the following compilation error:
{noformat}
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 277, Column 9: Redefinition of local variable ""smj_isNull_7""
{noformat}
Example #2 (shuffle hash join):
{noformat}
select /*+ SHUFFLE_HASH(v2) */ *
from v1
full outer join v2
on key = a
and value > b
and value > c;
{noformat}
The shuffle hash join's generated code causes the following compilation error:
{noformat}
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 174, Column 5: Redefinition of local variable ""shj_value_1"" 
{noformat}
With default configuration, both queries end up succeeding, since Spark falls back to running each query with whole-stage codegen disabled.

The issue happens only when the join's bound condition refers to the same stream-side column more than once.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Apr 20 15:49:34 UTC 2023,,,,,,,,,,"0|z1h914:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/23 00:32;bersprockets;PR here: https://github.com/apache/spark/pull/40766;;;","18/Apr/23 04:11;gurwls223;Fixed in https://github.com/apache/spark/pull/40766;;;","20/Apr/23 15:49;hudson;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/40881;;;",,,,,,,,,,,,
"Spark may  use a column other than the actual specified partitioning column for partitioning, for Hive format tables",SPARK-43112,13532409,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Bug,,ashahid7,ashahid7,12/Apr/23 22:11,20/Apr/23 01:16,30/Oct/23 17:26,20/Apr/23 01:16,3.3.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"The class org.apache.spark.sql.catalyst.catalog.HiveTableRelation has its output method implemented as 
  // The partition column should always appear after data columns.
  override def output: Seq[AttributeReference] = dataCols ++ partitionCols

But the DataWriting commands of spark like InsertIntoHiveDirCommand, expect that the output from HiveTableRelation is in the order in which the columns are actually defined in the DDL.

As a result, multiple mismatch scenarios can happen like:
1) data type casting exception being thrown , even though the data frame being inserted has schema which is identical to what is used for creating ddl.
              OR
2) Wrong column being used for partitioning , if the datatypes are same or cast-able, like date type and long

will be creating a PR with the bug test",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Apr 12 23:48:27 UTC 2023,,,,,,,,,,"0|z1h90g:",9223372036854775807,,,,,petertoth,,,,,,,,,,,,,,,,,,"12/Apr/23 23:48;ashahid7;Open a WIP PR [SPARK-43112|https://github.com/apache/spark/pull/40765/] which has bug tests as of now;;;",,,,,,,,,,,,,,
" JavaRDD.saveAsTextFile Directory Creation issue using Spark 3.3.2, with hadoop3",SPARK-43109,13532377,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,shamim_er123,shamim_er123,12/Apr/23 16:30,12/Apr/23 16:30,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"We are using Spark 3.3.2, with hadoop3 in Java Appliaction .

While Saving file using JavaRDD.saveAsTextFile(DIR_NAME/Sample.txt).

 

Spark is creating DIR_NAME with the same user by which Application is Running ex vs.

However we have provide another user to create the DIR_NAME like below in our Spark code .

UserGroupInformation theSparkUser= UserGroupInformation.createRemoteUser(""spark"");
UserGroupInformation.setLoginUser(theSparkUser);

 

Below is the dir structure created by the Application.

drwxrwxrwx+ 2  vs vs 6 Apr 12 17:09  main

 

We want Dir should be created by the user that we have provided, Like below

drwxrwxrwx+ 2  spark spark  6 Apr 12 17:09  main

 

When we are verifying the user by printing its showing same user as we pass in UserGroupInformation  both below printing same user ""spark""

 

UserGroupInformation.getLoginUser().getUserName());   --> printing user as ""spark""
UserGroupInformation.getCurrentUser().getUserName());  --> printing user as ""spark""

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-04-12 16:30:49.0,,,,,,,,,,"0|z1h8tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.spark.storage.StorageStatus NotSerializableException when try to access StorageStatus in a MapPartitionsFunction,SPARK-43108,13532367,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,surendergodara888,surendergodara888,12/Apr/23 14:58,25/Oct/23 06:44,30/Oct/23 17:26,,3.2.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,1,,,,,"When you try to access the *storage status (org.apache.spark.storage.StorageStatus)* inside a MapPartitionsFunction,then getStorageStatus method throw the NotSerializableException. This exception is thrown because the StorageStatus object is not serializable.

Here is an example code snippet that demonstrates how to access the storage status inside a MapPartitionsFunction in Spark:
{code:java}
StorageStatus[] storageStatus = SparkEnv.get().blockManager().master().getStorageStatus();{code}
*Error stacktrace --*
{code:java}
Caused by: java.io.NotSerializableException: org.apache.spark.storage.StorageStatus
Serialization stack:
    - object not serializable (class: org.apache.spark.storage.StorageStatus, value: org.apache.spark.storage.StorageStatus@715b4e82)
    - element of array (index: 0)
    - array (class [Lorg.apache.spark.storage.StorageStatus;, size 2)
    at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
    at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
    at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
    at org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:286)
    at org.apache.spark.rpc.netty.RemoteNettyRpcCallContext.send(NettyRpcCallContext.scala:64)
    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:156)
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264){code}
*Steps to reproduce*

step 1  Initialize spark session with spark standalone mode.

step 2  Create a Dataset using the SparkSession and load data

step 3  Define the MapPartitionsFunction on Dataset and get storage status inside it.

Here is the code snippet of MapPartitionsFunction

 
{code:java}
df = df.mapPartitions(new MapPartitionsFunction<Row, Row>() {
            @Override
            public Iterator<Row> call(Iterator<Row> input) throws Exception {
                StorageStatus[] storageStatus = SparkEnv.get().blockManager().master().getStorageStatus();
                return input;
            }
        }, RowEncoder.apply(df.schema()));
{code}
 

Step4 - submit the spark job. 

 

*Solution -*

Implement the Serializable interface for org.apache.spark.storage.StorageStatus.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Oct 25 06:44:11 UTC 2023,,,,,,,,,,"0|z1h8r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/23 06:44;berglh;We are hitting this when reading from Snowflake using Spark 3.4 then printing some data to stdout or writing to Parquet.

It only occurs on the invocation, then the error no longer appears.;;;",,,,,,,,,,,,,,
Data lost from the table if the INSERT OVERWRITE query fails,SPARK-43106,13532320,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,vaibhavb,vaibhavb,12/Apr/23 10:03,05/Jun/23 06:03,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"When we run an INSERT OVERWRITE query for an unpartitioned table on Spark-3, Spark has the following behavior:

1) It will first clean up all the data from the actual table path.

2) It will then launch a job that performs the actual insert.

 

There are 2 major issues with this approach:

1) If the insert job launched in step 2 above fails for any reason, the data from the original table is lost. 

2) If the insert job in step 2 above takes a huge time to complete, then table data is unavailable to other readers for the entire duration the job takes.

This behavior is the same even for the partitioned tables when using static partitioning. For dynamic partitioning, we do not delete the table data before the job launch.

 

Is there a reason as to why we perform this delete before the job launch and not as part of the Job commit operation? This issue is not there with Hive - where the data is cleaned up as part of the Job commit operation probably. As part of SPARK-19183, we did add a new hook in the commit protocol for this exact same purpose, but seems like its default behavior is still to delete the table data before the job launch.",,,,,,,,,,,,,,,,,,,,,,SPARK-19183,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jun 05 06:03:10 UTC 2023,,,,,,,,,,"0|z1h8go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/23 05:02;itskals;[~cloud_fan] , [~dongjoon]  , [~gurwls223] 

Appreciate input on this!

Myself(kalyan) and [~vaibhavb] are from Uber and we see our workloads getting into this problem more regularly.

While we are willing to work on it, we would like to know if there were any gotchas in taking forward the idea proposed in SPARK-19183.

 ;;;","29/Apr/23 00:32;dongjoon;Thank you for reporting. To narrow down your issue more, let me ask more information, [~itskals].
# Is this specific to Apache Spark 3.3.2, could you try to use other Apache Spark versions like Apache Spark 3.4.0 or Apache Spark 3.3.1?
# What storage backend are you using now, HDFS or S3?
# Do you think you can provide us a reproducible example?;;;","29/Apr/23 05:43;itskals;Thank you for the response [~dongjoon] .

Most of the workloads have been running on 2.4, while we have made good progress moving workloads to 3.X this year.

We notice this in a few long-running workloads on static partitions/unpartitioned datasets.

While HDFS has been our primary storage backend, moving to object stores on GCP has been making this problem more pronounced, due to inherent slowness in writing to these.

[~vaibhavb] can you share some test code to help here?;;;","29/Apr/23 13:48;vaibhavb;[~dongjoon] Thank you for taking a look at this issue.

1) This is not specific to just Spark-3.3.2. The issue exists in the master branch as well.

2) As [~itskals] mentioned above, at Uber we mostly use HDFS as the storage backend but this same issue would exist for cloud object storage as well.

3) Running any *_INSERT OVERWRITE TABLE_* query over any unpartitioned table would help you quickly reproduce this issue. You will notice that Spark would first clean up the table output path and then launch a job that does the computation for the new data making the table data unavailable if this Spark job fails.

 

Some code pointers on this:

1) Refer to the class _*InsertIntoHadoopFsRelation*_ -> method *_run._*

2) Inside the _*run*_ method, you would see that we first call {_}*deleteMatchingPartitions*{_}(this will clean up the table data) and then later call  {_}*FileFormatWriter.write*{_}(this will trigger the actual job).;;;","05/Jun/23 06:03;vaibhavb;[~dongjoon] Did you get a chance to take a look at this?

Any feedback from you would be really helpful and as [~itskals] mentioned, we at Uber are willing to work on this - just looking out for any gotchas as to why the idea proposed in  SPARK-19183 was not taken forward.;;;",,,,,,,,,,
Dynamic Catalogs,SPARK-43101,13532268,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,melin,melin,12/Apr/23 04:12,13/Apr/23 01:12,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Convenient registration of the catalog, in sts

ref: [https://github.com/trinodb/trino/issues/12709]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-04-12 04:12:02.0,,,,,,,,,,"0|z1h85c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mismatch of field name in log event writer and parser for push shuffle metrics,SPARK-43100,13532252,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,zhouyejoe,zhouyejoe,11/Apr/23 23:08,12/Apr/23 00:04,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"For push based shuffle metrics, when writting out the event to log file, the field name is ""Push Based Shuffle"", in [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L548]

But when parsing it out in SHS, the expected field name is ""Shuffle Push Read Metrics"", as shown [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L1264]

This mismatch makes all the push shuffle metrics 0 from SHS rest calls.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Apr 12 00:04:32 UTC 2023,,,,,,,,,,"0|z1h81s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/23 00:04;zhouyejoe;Created PR to fix the issue https://github.com/apache/spark/pull/40749;;;",,,,,,,,,,,,,,
"`Class.getCanonicalName` return null for anonymous class on JDK15+, impacting function registry",SPARK-43099,13532248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,alexjinghn,alexjinghn,alexjinghn,11/Apr/23 22:19,17/Apr/23 03:22,30/Oct/23 17:26,17/Apr/23 03:15,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,,,0,,,,,"On JDK15+, lambda and method references are implemented using hidden classes ([https://openjdk.org/jeps/371)] According to the JEP, 
{quote}{{Class::getCanonicalName}} returns {{{}null{}}}, indicating the hidden class has no canonical name. (Note that the {{Class}} object for an anonymous class in the Java language has the same behavior.)
{quote}

This means [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala#L53] will always be null.

 

This can be fixed by replacing `getCanonicalName` with `getName`

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 17 03:15:16 UTC 2023,,,,,,,,,,"0|z1h80w:",9223372036854775807,,,,,gengliang,,,,,,,,,,,,,,,,,,"17/Apr/23 01:50;ggintegration;User 'alexjinghn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40747;;;","17/Apr/23 03:15;cloud_fan;Issue resolved by pull request 40747
[https://github.com/apache/spark/pull/40747];;;",,,,,,,,,,,,,
Avoid Once strategy's idempotence is broken for batch: Infer Filters,SPARK-43095,13532191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,11/Apr/23 13:27,15/Apr/23 01:25,30/Oct/23 17:26,15/Apr/23 01:25,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Apr 15 01:25:22 UTC 2023,,,,,,,,,,"0|z1h7o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/23 09:03;githubbot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/40742;;;","13/Apr/23 09:04;githubbot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/40742;;;","15/Apr/23 01:25;yumwang;Issue resolved by pull request 40742
[https://github.com/apache/spark/pull/40742];;;",,,,,,,,,,,,
Array Insert Should Throw On 'Pos' Value 0,SPARK-43094,13532168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,ddavies1,ddavies1,11/Apr/23 11:11,11/Apr/23 11:32,30/Oct/23 17:26,11/Apr/23 11:32,3.3.2,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Wanted to start a discussion about the following comment:

[https://github.com/apache/spark/pull/38867/files#r1157449697]

If a 'pos' of 0 is provided, the code here operates as if the user provided a 1. This could be confusing to users (two incides now overlap and lead to the same result, which is logically a little strange), but perhaps more compellingly, the other functions relying on a provided index, such as elementAt [here|https://github.com/Daniel-Davies/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L2344], will raise an exception if provided with an index of 0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Apr 11 11:32:30 UTC 2023,,,,,,,,,,"0|z1h7jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/23 11:32;ddavies1;Fixed already, my apologies;;;",,,,,,,,,,,,,,
"Test case ""Add a directory when spark.sql.legacy.addSingleFileInAddFile set to false"" should use random directories for testing",SPARK-43093,13532126,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,11/Apr/23 05:43,12/Apr/23 02:23,30/Oct/23 17:26,12/Apr/23 02:23,3.2.3,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Apr 12 02:23:18 UTC 2023,,,,,,,,,,"0|z1h7a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/23 02:23;gurwls223;Issue resolved by pull request 40737
[https://github.com/apache/spark/pull/40737];;;",,,,,,,,,,,,,,
Improve the error message of UNRECOGNIZED_SQL_TYPE,SPARK-43077,13532012,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,10/Apr/23 02:49,11/Apr/23 02:47,30/Oct/23 17:26,11/Apr/23 02:47,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"UNRECOGNIZED_SQL_TYPE prints the jdbc type id in the error message currently. This is difficult for spark users to understand the meaning of this kind of error, especially when the type id is from a vendor extension.

For example, 
{code:java}
 org.apache.spark.SparkSQLException: Unrecognized SQL type -102{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Apr 11 02:47:48 UTC 2023,,,,,,,,,,"0|z1h6ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/23 09:22;githubbot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40718;;;","11/Apr/23 02:47;Qin Yao;Issue resolved by pull request 40718
[https://github.com/apache/spark/pull/40718];;;",,,,,,,,,,,,,
Add the function without constant parameters of `SessionState#executePlan`,SPARK-43074,13531936,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,Zing,Zing,08/Apr/23 13:41,10/Apr/23 03:10,30/Oct/23 17:26,10/Apr/23 03:10,3.2.3,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,," 
{code:java}
```
df = spark.sql(""select 1"") 
catalyst_plan = df._jdf.queryExecution().logical()
print('catalyst_plan: ', catalyst_plan)
df_size = spark._jsparkSession.sessionState().executePlan(catalyst_plan)
```
{code}
 

 

will get exception beacause py4j not support default value of scala.

 

[https://github.com/apache/spark/blob/d8b720a579a456aec9c7f843d7feaa1454ebf9d4/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala#L124]

 
{code:java}
```
def executePlan(
plan: LogicalPlan,
mode: CommandExecutionMode.Value = CommandExecutionMode.ALL): QueryExecution =
createQueryExecution(plan, mode)
```
{code}
 

 

Will get Exception:

 

 
{code:java}
```
py4j.protocol.Py4JError: An error occurred while calling o87.executePlan. Trace:
py4j.Py4JException: Method executePlan([class org.apache.spark.sql.catalyst.plans.logical.Project]) does not exist
    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)
    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)
    at py4j.Gateway.invoke(Gateway.java:274)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
    at java.lang.Thread.run(Thread.java:748)
```
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 10 03:10:44 UTC 2023,,,,,,,,,,"0|z1h63s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/23 03:35;snoot;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/40714;;;","10/Apr/23 03:10;gurwls223;This isn't an API.;;;",,,,,,,,,,,,,
Use `sbt-eclipse` instead of `sbteclipse-plugin`,SPARK-43069,13531901,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,07/Apr/23 18:57,07/Apr/23 19:55,30/Oct/23 17:26,07/Apr/23 19:55,3.2.3,3.3.2,3.4.0,,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,,Build,,,,,0,,,,,"Since SPARK-34959, Apache Spark 3.2+ uses SBT 1.5.0.

And, SBT 1.4+ can use `set-eclipse` instead of old `sbteclipse-plugin`.
- https://github.com/sbt/sbt-eclipse/releases",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 07 19:55:04 UTC 2023,,,,,,,,,,"0|z1h5w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/23 19:55;dongjoon;Issue resolved by pull request 40708
[https://github.com/apache/spark/pull/40708];;;",,,,,,,,,,,,,,
Error class resource file in Kafka connector is misplaced,SPARK-43067,13531847,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,07/Apr/23 10:17,08/Apr/23 22:19,30/Oct/23 17:26,08/Apr/23 21:39,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Structured Streaming,,,,,0,,,,,"SPARK-41387 adopted error class framework in Kafka connector. Since it's a connector, the error class file is intentionally put to the Kafka connector module instead of being added to the Spark's central error class file.

Unfortunately I've figured out somehow that the place is wrong. It should be placed to the resources directory in source, not test. The problem cannot be captured in test suite as it's available in test artifact, but I can trigger the problem via adding Kafka connector jar into classpath and initialize KafkaExceptions object.

Hopefully the blast radius of the problem is trivial as Kafka connector uses error class only for assertions which should not be triggered unless some accident happens e.g. topic is deleted and recreated while the streaming query is running with Trigger.Available.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Apr 08 21:39:19 UTC 2023,,,,,,,,,,"0|z1h5k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/23 10:24;kabhwan;Will submit a PR sooner. The fix is very straightforward but I need to validate the fix manually since test cases cannot verify this as described above.;;;","08/Apr/23 21:39;kabhwan;Issue resolved by pull request 40705
[https://github.com/apache/spark/pull/40705];;;",,,,,,,,,,,,,
Possible logic issue,SPARK-43053,13531758,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Invalid,,robharrison,robharrison,06/Apr/23 15:53,11/Apr/23 08:49,30/Oct/23 17:26,10/Apr/23 03:14,3.0.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"I found this bug when the following query returned no results:
{code:java}
WHERE cast(leistungsnummer as long) NOT IN (123)){code}
However so did the following query:
{code:java}
WHERE cast(leistungsnummer as long) IN (123)) {code}
This can be simplified to the following:
{code:java}
WHERE NULL IN (123)
{code}
Returns zero results but so does:
{code:java}
WHERE NULL NOT IN (123) {code}
Logically one would assume `NULL IN (123, 456)` would be false.  However if that is the case then you would think `NULL NOT IN (123, 456)` would be true.  It seems they both equate to false.

I don't know if this is a quirk of the SQL spec or if this is a bug in Spark SQL implementation but it is causing unexpected results.",Kubernetes sql operator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Apr 11 08:49:51 UTC 2023,,,,,,,,,,"0|z1h508:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/23 01:19;yumwang;It's not a issue.;;;","11/Apr/23 08:49;robharrison;[~yumwang] would you mind providing a little more information as to your reasoning?  Just so I can better understand what to report and what not to report in future?;;;",,,,,,,,,,,,,
Handle stacktrace with null file name in event log,SPARK-43052,13531757,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,06/Apr/23 15:15,27/Apr/23 04:33,30/Oct/23 17:26,27/Apr/23 04:33,3.3.2,,,,,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Apr 27 04:33:21 UTC 2023,,,,,,,,,,"0|z1h500:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/23 03:14;gurwls223;https://github.com/apache/spark/pull/40687;;;","27/Apr/23 04:33;mridulm80;Issue resolved by pull request 40687
[https://github.com/apache/spark/pull/40687];;;",,,,,,,,,,,,,
Fix construct aggregate expressions by replacing grouping functions,SPARK-43050,13531751,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,06/Apr/23 14:20,15/Apr/23 02:22,30/Oct/23 17:26,15/Apr/23 02:22,3.5.0,,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,,,0,,,,,"
{code:sql}
CREATE TEMPORARY VIEW grouping AS SELECT * FROM VALUES
  (""1"", ""2"", ""3"", 1),
  (""4"", ""5"", ""6"", 1),
  (""7"", ""8"", ""9"", 1)
  as grouping(a, b, c, d);
{code}

{noformat}
spark-sql (default)> SELECT CASE WHEN a IS NULL THEN count(b) WHEN b IS NULL THEN count(c) END
                   > FROM grouping
                   > GROUP BY GROUPING SETS (a, b, c);
[MISSING_AGGREGATION] The non-aggregating expression ""b"" is based on columns which are not participating in the GROUP BY clause.
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Apr 15 02:22:06 UTC 2023,,,,,,,,,,"0|z1h4yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/23 03:15;gurwls223;https://github.com/apache/spark/pull/40685;;;","11/Apr/23 12:01;awsthni;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/40685;;;","15/Apr/23 02:22;yumwang;Issue resolved by pull request 40685
https://github.com/apache/spark/pull/40685;;;",,,,,,,,,,,,
Use CLOB instead of VARCHAR(255) for StringType for Oracle jdbc,SPARK-43049,13531696,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,06/Apr/23 05:39,28/Sep/23 20:14,30/Oct/23 17:26,07/Apr/23 03:02,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 07 03:30:15 UTC 2023,,,,,,,,,,"0|z1h4mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/23 03:02;yao;resolved by https://github.com/apache/spark/pull/40683;;;","07/Apr/23 03:30;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40683;;;",,,,,,,,,,,,,
Restore constructors of exceptions for compatibility in connector API,SPARK-43041,13531648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,aokolnychyi,XinrongM,XinrongM,05/Apr/23 19:03,06/Apr/23 05:26,30/Oct/23 17:26,06/Apr/23 05:11,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,,,0,,,,,"Thanks [~aokolnychyi] for raising the issue as shown below:
{quote}
I have a question about changes to exceptions used in the public connector API, such as NoSuchTableException and TableAlreadyExistsException.

I consider those as part of the public Catalog API (TableCatalog uses them in method definitions). However, it looks like PR #37887 has changed them in an incompatible way. Old constructors accepting Identifier objects got removed. The only way to construct such exceptions is either by passing database and table strings or Scala Seq. Shall we add back old constructors to avoid breaking connectors?
{quote}
We should restore constructors of those exceptions to preserve the compatibility in connector API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Apr 06 05:26:56 UTC 2023,,,,,,,,,,"0|z1h4c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/23 20:17;aokolnychyi;https://github.com/apache/spark/pull/40679;;;","06/Apr/23 05:11;Gengliang.Wang;Issue resolved by pull request 40679
[https://github.com/apache/spark/pull/40679];;;","06/Apr/23 05:26;ci-cassandra.apache.org;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40679;;;",,,,,,,,,,,,
deduplicate relations with metadata columns,SPARK-43030,13531456,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,04/Apr/23 14:24,07/Apr/23 03:47,30/Oct/23 17:26,07/Apr/23 03:47,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 07 03:47:09 UTC 2023,,,,,,,,,,"0|z1h35c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/23 03:22;snoot;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40662;;;","07/Apr/23 03:47;cloud_fan;Issue resolved by pull request 40662
[https://github.com/apache/spark/pull/40662];;;",,,,,,,,,,,,,
PySpark Breaks with Pandas 2.0,SPARK-43029,13531440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Cannot Reproduce,,cpcloud,cpcloud,04/Apr/23 12:37,10/Apr/23 03:17,30/Oct/23 17:26,10/Apr/23 03:17,3.3.2,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"PySpark is apparently using {{iteritems}}, which was removed in and deprecated long before pandas 2.0 and this makes PySpark unusable with pandas 2.0.","Linux
Python 3.10.10
PySpark 3.3.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 10 03:17:50 UTC 2023,,,,,,,,,,"0|z1h31s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/23 20:32;bjornjorgensen;[It is deprecated in 3.4.0|https://github.com/apache/spark/blob/0e9e34c1bd9bd16ad5efca77ce2763eb950f3103/python/pyspark/pandas/frame.py#L2069]

3.4.0 is in RC 7 now. So it will soon be out. ;;;","10/Apr/23 03:17;gurwls223;Yup, it's fixed in 3.4.;;;",,,,,,,,,,,,,
Shuffle happens when Coalesce Buckets should occur,SPARK-43021,13531368,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Zing,neshkeev,neshkeev,04/Apr/23 02:35,14/Apr/23 03:20,30/Oct/23 17:26,14/Apr/23 03:20,3.3.1,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"h1. What I did

I define the following code:

{{from pyspark.sql import SparkSession}}

{{spark = (}}
{{  SparkSession}}
{{    .builder}}
{{    .appName(""Bucketing"")}}
{{    .master(""local[4]"")}}
{{    .config(""spark.sql.bucketing.coalesceBucketsInJoin.enabled"", True)}}
{{    .config(""spark.sql.autoBroadcastJoinThreshold"", ""-1"")}}
{{    .getOrCreate()}}
{{)}}

{{df1 = spark.range(0, 100)}}
{{df2 = spark.range(0, 100, 2)}}

{{df1.write.bucketBy(4, ""id"").mode(""overwrite"").saveAsTable(""t1"")}}
{{df2.write.bucketBy(2, ""id"").mode(""overwrite"").saveAsTable(""t2"")}}

{{t1 = spark.table(""t1"")}}
{{t2 = spark.table(""t2"")}}

{{t2.join(t1, ""id"").explain()}}

h1. What happened

There is an Exchange node in the join plan

h1. What is expected

The plan should not contain any Exchange/Shuffle nodes, because {{t1}}'s number of buckets is 4 and {{t2}}'s number of buckets is 2, and their ratio is 2 which is less than 4 ({{spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio}}) and [CoalesceBucketsInJoin|https://github.com/apache/spark/blob/c9878a212958bc54be529ef99f5e5d1ddf513ec8/sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/CoalesceBucketsInJoin.scala] should be applied",,,,,,,,,,,,,,,,,,SPARK-43087,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Apr 14 03:20:56 UTC 2023,,,,,,,,,,"0|z1h2m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/23 16:12;Zing;PR：https://github.com/apache/spark/pull/40688;;;","13/Apr/23 03:48;snoot;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/40688;;;","14/Apr/23 03:20;cloud_fan;Issue resolved by pull request 40688
[https://github.com/apache/spark/pull/40688];;;",,,,,,,,,,,,
self.deserialized == self.deserialized typo in StorageLevel __eq__(),SPARK-43006,13531147,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tienhoayu,tienhoayu,tienhoayu,03/Apr/23 03:16,03/Apr/23 13:27,30/Oct/23 17:26,03/Apr/23 13:27,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,PySpark,,,,,0,,,,,"We should fix {{self.deserialized == self.deserialized}} with {{self.deserialized == other.deserialized}}

The original expression is always True, which is likely to be a typo.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 03 13:27:02 UTC 2023,,,,,,,,,,"0|z1h18w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/23 03:18;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40619;;;","03/Apr/23 03:19;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40619;;;","03/Apr/23 13:27;srowen;Resolved by 40619;;;",,,,,,,,,,,,
`v is v >= 0` typo in pyspark/pandas/config.py,SPARK-43005,13531146,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tienhoayu,tienhoayu,tienhoayu,03/Apr/23 03:11,03/Apr/23 13:25,30/Oct/23 17:26,03/Apr/23 13:25,3.2.0,,,,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,3.5.0,PySpark,,,,,0,,,,,"By comparing compute.isin_limit and plotting.max_rows, {{v is v}} is likely to be a typo.

We should fix {{v is v >= 0}} with {{{}v >= 0{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 03 13:25:36 UTC 2023,,,,,,,,,,"0|z1h18o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/23 03:19;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40620;;;","03/Apr/23 03:19;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40620;;;","03/Apr/23 13:25;srowen;Resolved by https://github.com/apache/spark/pull/40620;;;",,,,,,,,,,,,
vendor==vendor typo in ResourceRequest.equals(),SPARK-43004,13531144,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tienhoayu,tienhoayu,tienhoayu,03/Apr/23 03:03,03/Apr/23 03:38,30/Oct/23 17:26,03/Apr/23 03:38,3.0.0,,,,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,3.5.0,Spark Core,,,,,0,,,,,"vendor == vendor is always true, this is likely to be a typo.

We should fix `vendor == vendor` with `that.vendor == vendor`, and `discoveryScript == discoveryScript` with `that.discoveryScript == discoveryScript`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 03 03:38:11 UTC 2023,,,,,,,,,,"0|z1h188:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/23 03:37;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40622;;;","03/Apr/23 03:37;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40622;;;","03/Apr/23 03:38;srowen;Resolved by https://github.com/apache/spark/pull/40622;;;",,,,,,,,,,,,
Spark last window dont flush in append mode,SPARK-43001,13531064,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,padavan,padavan,01/Apr/23 07:52,20/Jul/23 08:49,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,PySpark,Structured Streaming,,,,0,,,,,"The problem is very simple, when you use *TUMBLING* *window* with {*}append mode{*}, then the window is closed +only when the next message arrives+ ({_}+watermark logic{_}). 

In the current implementation, if you *stop* *incoming* streaming data, the *last* window will *NEVER close* and we LOSE the last window data.

 

Business situation:

Worked correctly and new messages stop incoming and next message come in 5 hours  later and the client will get the message after 5 hours instead of the 10 seconds delay of window.

!https://user-images.githubusercontent.com/61819835/226478055-dc4a123c-4397-4eb0-b6ed-1e185b6fab76.png|width=707,height=294!

The current implementation needs to be improved. Include in spark internal mechanisms to close windows automatically.

 

*What we propose:*

Add third parameter 

{{{}DataFrame.{}}}{{{}withWatermark{}}}({_}eventTime{_}, {_}delayThreshold, *maxDelayClose*{_}). And then trigger will execute 
{code:java}
if(now - window.upper_bound > maxDelayClose){
     window.close().flush();
}
{code}
I assume it can be done in a day. It wasn't expected for us that our customers couldn't get the notifications. (the company is in the medical field).

 

simple code for problem:
{code:java}
kafka_stream_df = spark \
    .readStream \
    .format(""kafka"") \
    .option(""kafka.bootstrap.servers"", KAFKA_BROKER) \
    .option(""subscribe"", KAFKA_TOPIC) \
    .option(""includeHeaders"", ""true"") \
    .load()

sel = (kafka_stream_df.selectExpr(""CAST(key AS STRING)"", ""CAST(value AS STRING)"")
       .select(from_json(col(""value"").cast(""string""), json_schema).alias(""data""))
       .select(""data.*"")
       .withWatermark(""dt"", ""1 seconds"")
       .groupBy(window(""dt"", ""10 seconds""))
       .agg(sum(""price""))
      )
 
console = sel \
    .writeStream \
    .trigger(processingTime='10 seconds') \
    .format(""console"") \
    .outputMode(""append"")\
    .start()
{code}
 

 ",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,,,Thu Jul 20 08:49:10 UTC 2023,,,,,,,,,,"0|z1h0qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/23 05:48;vindhyag;These two seem to be similar issue
https://issues.apache.org/jira/browse/SPARK-38078.;;;","08/Apr/23 21:57;kabhwan;This is a known issue. The current available option for this is to use update mode with handling upsert on user logic (either using MERGE INTO in foreach batch sink or custom logic to do it), but I understand update mode does not work for every sink.

I think the thing SS lacks now is advancing watermark despite of idleness. Current implementation requires to run a microbatch as we store the value of watermark into the WAL log. That said, it should be no-data batch with advancing watermark. How much SS needs to advance the watermark if it comes with specified idleness is another thing to think hard. We use event time which is out of sync with wall clock, and when idleness happens we have to either 1) make specified duration of advancement based on previous watermark value or 2) sync with wall clock (e.g. wall clock - delay threshold).

I'm not saying it's impossible - I'd just like to say it may not be very trivial to fix it nicely.;;;","09/Apr/23 04:16;vindhyag;Agree.I think there should be some way for users to also provide wall clock threshold. If it crosses this mark dont use eventtime but wall clock and flush pending data. What do you think [~kabhwan] ;;;","20/Jul/23 08:49;padavan;[~kabhwan]  ? ;;;",,,,,,,,,,,
Correct code highlights in SQL protobuf documentation,SPARK-42987,13530882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lucaspompeun,lucaspompeun,lucaspompeun,31/Mar/23 01:13,10/Apr/23 03:24,30/Oct/23 17:26,10/Apr/23 03:24,3.3.2,,,,,,,,,,,,,,,,,,,3.5.0,,,,Documentation,,,,,0,docs,,,,"Correct code highlights in SQL protobuf documentation.

Some of code highlights was in different format and not in markdown pattern",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 10 03:24:01 UTC 2023,,,,,,,,,,"0|z1gzm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/23 01:34;lucaspompeun;The PR was created in github

https://github.com/apache/spark/pull/40614;;;","10/Apr/23 03:24;gurwls223;Fixed in https://github.com/apache/spark/pull/40614;;;",,,,,,,,,,,,,
 Derby&PG: RENAME cannot qualify a new-table-Name with a schema-Name.,SPARK-42978,13530739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,30/Mar/23 06:26,31/Mar/23 08:15,30/Oct/23 17:26,31/Mar/23 08:15,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,https://db.apache.org/derby/docs/10.2/ref/rrefnewtablename.html#rrefnewtablename,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 31 08:15:49 UTC 2023,,,,,,,,,,"0|z1gyqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/23 09:21;githubbot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40602;;;","31/Mar/23 08:15;yao;issue resolved by [https://github.com/apache/spark/pull/40602];;;",,,,,,,,,,,,,
Restore `Utils#createTempDir` use  `ShutdownHookManager.registerShutdownDeleteDir` to cleanup tempDir,SPARK-42974,13530721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,30/Mar/23 04:45,04/Apr/23 07:27,30/Oct/23 17:26,03/Apr/23 13:28,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Apr 04 07:22:31 UTC 2023,,,,,,,,,,"0|z1gymo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/23 13:28;srowen;Resolved by https://github.com/apache/spark/pull/40613;;;","04/Apr/23 07:22;dongjoon;This landed at branch-3.4 via https://github.com/apache/spark/pull/40647 .;;;",,,,,,,,,,,,,
ExecutorAllocationManager cannot allocate new instances when all executors down.,SPARK-42972,13530710,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,yangjiandan,yangjiandan,30/Mar/23 01:41,18/Jul/23 09:23,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Structured Streaming,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jul 18 09:23:02 UTC 2023,,,,,,,,,,"0|z1gyk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/23 06:24;ylllllllll;Using structured streaming, when we set the config to use dynamic allocation, there is a bug which will make the program hang. Here is how it happened:

{code:scala}
// Some comments here
  private def manageAllocation(): Unit = synchronized {
    logInfo(s""Managing executor allocation with ratios = [$scalingUpRatio, $scalingDownRatio]"")
    if (batchProcTimeCount > 0) {
      val averageBatchProcTime = batchProcTimeSum / batchProcTimeCount
      val ratio = averageBatchProcTime.toDouble / batchDurationMs
//When the ratio is lower than the scalingDownRatio, the client will try to kill executors, but if all executors are dead accidentally, the program will hung, because there is no executors to kill. 
      logInfo(s""Average: $averageBatchProcTime, ratio = $ratio"")
      if (ratio >= scalingUpRatio) {
        logDebug(""Requesting executors"")
        val numNewExecutors = math.max(math.round(ratio).toInt, 1)
        requestExecutors(numNewExecutors)
      } else if (ratio <= scalingDownRatio) {
        logDebug(""Killing executors"")
        killExecutor()
      }
    }
    batchProcTimeSum = 0
    batchProcTimeCount = 0
//Then there will be no more batch jobs to complete, and batchProcTimeCount will always be 0, the program will stuck in suspended animation.
  }
{code}

 When the ratio is lowe than the scalingDownRatio, the client will try to kill executors, but if all executors are dead accidentally at the same time, the program will hung, because there is no executors to kill. Then there will be no more batch jobs to complete, and batchProcTimeCount will always be 0, the program will stuck in suspended animation, because last time it tried to kill executors and requestExecutors function will never be triggered

;;;","31/Mar/23 07:49;ylllllllll;I created a PR [PR-40621|https://github.com/apache/spark/pull/40621]
[~vkolpakov]
Please help me review this PR;;;","18/Jul/23 08:05;ylllllllll;[~tdas]  I created a PR [PR-42058|https://github.com/apache/spark/pull/42058] on github, would you please help me review it?;;;","18/Jul/23 09:23;lvkaihua;I also encountered this issue and tested that the modification was correct;;;",,,,,,,,,,,
"When processing the WorkDirCleanup event, if appDirs is empty, should print workdir ",SPARK-42971,13530706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,30/Mar/23 00:48,30/Mar/23 04:53,30/Oct/23 17:26,30/Mar/23 04:53,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,,,0,,,,,"          val appDirs = workDir.listFiles()
          if (appDirs == null) {
            throw new IOException(
              s""ERROR: Failed to list files in ${appDirs.mkString(""dirs("", "", "", "")"")}"")
          }

 

Otherwise, npe will be thrown here when appDirs is null

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 30 04:53:37 UTC 2023,,,,,,,,,,"0|z1gyjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/23 03:26;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40597;;;","30/Mar/23 04:53;gurwls223;Issue resolved by pull request 40597
[https://github.com/apache/spark/pull/40597];;;",,,,,,,,,,,,,
Fix SparkListenerTaskStart.stageAttemptId when a task is started after the stage is cancelled,SPARK-42967,13530669,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,29/Mar/23 18:26,30/Mar/23 22:49,30/Oct/23 17:26,30/Mar/23 22:49,3.0.3,3.1.3,3.2.3,3.3.2,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,Spark Core,,,,,0,,,,,"When a task is started after the stage is cancelled, the stageAttemptId field in  SparkListenerTaskStart event is set to -1. This could lead to unexpected problem for subscribers of SparkListener because -1 is not a legal stageAttemptId.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 30 22:49:05 UTC 2023,,,,,,,,,,"0|z1gybc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/23 22:49;Gengliang.Wang;Issue resolved by pull request 40592
[https://github.com/apache/spark/pull/40592];;;",,,,,,,,,,,,,,
`release-build.sh` should not remove SBOM artifacts,SPARK-42957,13530543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,29/Mar/23 05:30,29/Mar/23 06:30,30/Oct/23 17:26,29/Mar/23 06:30,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Project Infra,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 29 06:30:35 UTC 2023,,,,,,,,,,"0|z1gxjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/23 06:30;dongjoon;Issue resolved by pull request 40585
[https://github.com/apache/spark/pull/40585];;;",,,,,,,,,,,,,,
"Execution plan error, unable to obtain desired results",SPARK-42948,13530398,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,muser,muser,28/Mar/23 10:19,10/Apr/23 03:42,30/Oct/23 17:26,,3.2.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"A jar is packaged using SparkSession to submit Spark SQL：
{code:java}
//SparkSession.builder().appName(args(0)).config(""spark.sql.crossJoin.enabled"", true).enableHiveSupport().getOrCreate() spark.sql(arg(1)) {code}
Execute the following SQL fragment：
{code:java}
//INSERT INTO gjdw.aa partition(dt='20230327')
SELECT t1.mandt,
       t1.pur_no,
       t1.pur_item,
       t1.pur_comp_code,
       t1.pur_pur_org,
       t1.zzcoca,
       t1.zzycgdd
FROM
  (SELECT *
   FROM gjdw.aa
   WHERE dt=from_unixtime(unix_timestamp(date_add(from_unixtime(unix_timestamp('20230327','yyyymmdd'),'yyyy-mm-dd'),-1),'yyyy-mm-dd'),'yyyymmdd')) t1
LEFT JOIN
  (SELECT *
   FROM gjdw.aa
   WHERE dt='20230327') t ON t.pur_no = t1.pur_no
AND t.pur_item = t1.pur_item
WHERE (t.pur_no = ''
       AND t.pur_item = ''
       OR (t.pur_no IS NULL
           AND t.pur_item IS NULL)) {code}
 

Strangely, I didn't get the desired result. There was data in the table, and the correct value should have data inserted. However, there was no data output, and there was no task error message for the job. This occurred in the execution plan

!image-2023-04-10-11-39-06-501.png!

!image-2023-04-10-11-39-33-658.png!","!image-2023-03-28-18-15-55-189.png!

!image-2023-03-28-18-17-08-017.png!

!image-2023-03-28-18-18-41-754.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/23 03:39;muser;image-2023-04-10-11-39-06-501.png;https://issues.apache.org/jira/secure/attachment/13057161/image-2023-04-10-11-39-06-501.png","10/Apr/23 03:39;muser;image-2023-04-10-11-39-33-658.png;https://issues.apache.org/jira/secure/attachment/13057162/image-2023-04-10-11-39-33-658.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 10 03:42:17 UTC 2023,,,,,,,,,,"0|z1gwn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/23 03:26;gurwls223;[~muser]the last image is broken. Mind reuploading it please?;;;","10/Apr/23 03:42;muser;[~gurwls223] A new image has been added. Please check it again when you have time. Thank you;;;",,,,,,,,,,,,,
Spark Thriftserver LDAP should not use DN pattern if user contains domain,SPARK-42947,13530394,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,liujiayi771,liujiayi771,28/Mar/23 10:01,26/Sep/23 00:17,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,pull-request-available,,,,"When the LDAP provider has domain configuration, such as Active Directory, the principal should not be constructed according to the DN pattern, but the username containing the domain should be directly passed to the LDAP provider as the principal. We can refer to the implementation of Hive LdapUtils.

When the username contains a domain or domain passes from hive.server2.authentication.ldap.Domain configuration, if we construct the principal according to the DN pattern (For example, uid=user@domain,dc=test,dc=com), we will get the following error:


{code:java}
23/03/28 11:01:48 ERROR TSaslTransport: SASL negotiation failure
javax.security.sasl.SaslException: Error validating the login
	at org.apache.hive.service.auth.PlainSaslServer.evaluateResponse(PlainSaslServer.java:108) ~[spark-hive-thriftserver_2.12-3.3.1.jar:3.3.1]
	at org.apache.thrift.transport.TSaslTransport$SaslParticipant.evaluateChallengeOrResponse(TSaslTransport.java:537) ~[libthrift-0.12.0.jar:0.12.0]
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:283) ~[libthrift-0.12.0.jar:0.12.0]
	at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:43) ~[libthrift-0.12.0.jar:0.12.0]
	at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:223) ~[libthrift-0.12.0.jar:0.12.0]
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:293) ~[libthrift-0.12.0.jar:0.12.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_352]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_352]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_352]
Caused by: javax.security.sasl.AuthenticationException: Error validating LDAP user
	at org.apache.hive.service.auth.LdapAuthenticationProviderImpl.Authenticate(LdapAuthenticationProviderImpl.java:76) ~[spark-hive-thriftserver_2.12-3.3.1.jar:3.3.1]
	at org.apache.hive.service.auth.PlainSaslHelper$PlainServerCallbackHandler.handle(PlainSaslHelper.java:105) ~[spark-hive-thriftserver_2.12-3.3.1.jar:3.3.1]
	at org.apache.hive.service.auth.PlainSaslServer.evaluateResponse(PlainSaslServer.java:101) ~[spark-hive-thriftserver_2.12-3.3.1.jar:3.3.1]
	... 8 more
Caused by: javax.naming.AuthenticationException: [LDAP: error code 49 - 80090308: LdapErr: DSID-0C0903D9, comment: AcceptSecurityContext error, data 52e, v2580]
	at com.sun.jndi.ldap.LdapCtx.mapErrorCode(LdapCtx.java:3261) ~[?:1.8.0_352]
	at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:3207) ~[?:1.8.0_352]
	at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:2993) ~[?:1.8.0_352]
	at com.sun.jndi.ldap.LdapCtx.connect(LdapCtx.java:2907) ~[?:1.8.0_352]
	at com.sun.jndi.ldap.LdapCtx.<init>(LdapCtx.java:347) ~[?:1.8.0_352]
	at com.sun.jndi.ldap.LdapCtxFactory.getLdapCtxFromUrl(LdapCtxFactory.java:229) ~[?:1.8.0_352]
	at com.sun.jndi.ldap.LdapCtxFactory.getUsingURL(LdapCtxFactory.java:189) ~[?:1.8.0_352]
	at com.sun.jndi.ldap.LdapCtxFactory.getUsingURLs(LdapCtxFactory.java:247) ~[?:1.8.0_352]
	at com.sun.jndi.ldap.LdapCtxFactory.getLdapCtxInstance(LdapCtxFactory.java:154) ~[?:1.8.0_352]
	at com.sun.jndi.ldap.LdapCtxFactory.getInitialContext(LdapCtxFactory.java:84) ~[?:1.8.0_352]
	at javax.naming.spi.NamingManager.getInitialContext(NamingManager.java:695) ~[?:1.8.0_352]
	at javax.naming.InitialContext.getDefaultInitCtx(InitialContext.java:313) ~[?:1.8.0_352]
	at javax.naming.InitialContext.init(InitialContext.java:244) ~[?:1.8.0_352]
	at javax.naming.InitialContext.<init>(InitialContext.java:216) ~[?:1.8.0_352]
	at javax.naming.directory.InitialDirContext.<init>(InitialDirContext.java:101) ~[?:1.8.0_352]
	at org.apache.hive.service.auth.LdapAuthenticationProviderImpl.Authenticate(LdapAuthenticationProviderImpl.java:73) ~[spark-hive-thriftserver_2.12-3.3.1.jar:3.3.1]
	at org.apache.hive.service.auth.PlainSaslHelper$PlainServerCallbackHandler.handle(PlainSaslHelper.java:105) ~[spark-hive-thriftserver_2.12-3.3.1.jar:3.3.1]
	at org.apache.hive.service.auth.PlainSaslServer.evaluateResponse(PlainSaslServer.java:101) ~[spark-hive-thriftserver_2.12-3.3.1.jar:3.3.1]
	... 8 more
{code}


we should pass user@domain directly to the LDAP provider, just like HiveServer did.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 16 16:47:31 UTC 2023,,,,,,,,,,"0|z1gwm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/23 10:49;liujiayi771;I will try to fix this.;;;","28/Mar/23 13:10;liujiayi771;issue fixed by https://github.com/apache/spark/pull/40577;;;","16/Aug/23 16:47;ignitetcbot;User 'liujiayi771' has created a pull request for this issue:
https://github.com/apache/spark/pull/40577;;;",,,,,,,,,,,,
Join with subquery in condition can fail with wholestage codegen and adaptive execution disabled,SPARK-42937,13530284,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,27/Mar/23 19:45,28/Mar/23 12:41,30/Oct/23 17:26,28/Mar/23 12:41,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,,,SQL,,,,,0,,,,,"The below left outer join gets an error:
{noformat}
create or replace temp view v1 as
select * from values
(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1),
(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),
(3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
as v1(key, value1, value2, value3, value4, value5, value6, value7, value8, value9, value10);

create or replace temp view v2 as
select * from values
(1, 2),
(3, 8),
(7, 9)
as v2(a, b);

create or replace temp view v3 as
select * from values
(3),
(8)
as v3(col1);

set spark.sql.codegen.maxFields=10; -- let's make maxFields 10 instead of 100
set spark.sql.adaptive.enabled=false;

select *
from v1
left outer join v2
on key = a
and key in (select col1 from v3);
{noformat}
The join fails during predicate codegen:
{noformat}
23/03/27 12:24:12 WARN Predicate: Expr codegen error and falling back to interpreter mode
java.lang.IllegalArgumentException: requirement failed: input[0, int, false] IN subquery#34 has not finished
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.sql.execution.InSubqueryExec.prepareResult(subquery.scala:144)
	at org.apache.spark.sql.execution.InSubqueryExec.doGenCode(subquery.scala:156)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:201)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:196)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressions$2(CodeGenerator.scala:1278)
	at scala.collection.immutable.List.map(List.scala:293)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:1278)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:41)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.generate(GeneratePredicate.scala:33)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:73)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:70)
	at org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:51)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.create(predicates.scala:86)
	at org.apache.spark.sql.execution.joins.HashJoin.boundCondition(HashJoin.scala:146)
	at org.apache.spark.sql.execution.joins.HashJoin.boundCondition$(HashJoin.scala:140)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.boundCondition$lzycompute(BroadcastHashJoinExec.scala:40)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.boundCondition(BroadcastHashJoinExec.scala:40)
{noformat}
It fails again after fallback to interpreter mode:
{noformat}
23/03/27 12:24:12 ERROR Executor: Exception in task 2.0 in stage 2.0 (TID 7)
java.lang.IllegalArgumentException: requirement failed: input[0, int, false] IN subquery#34 has not finished
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.sql.execution.InSubqueryExec.prepareResult(subquery.scala:144)
	at org.apache.spark.sql.execution.InSubqueryExec.eval(subquery.scala:151)
	at org.apache.spark.sql.catalyst.expressions.InterpretedPredicate.eval(predicates.scala:52)
	at org.apache.spark.sql.execution.joins.HashJoin.$anonfun$boundCondition$2(HashJoin.scala:146)
	at org.apache.spark.sql.execution.joins.HashJoin.$anonfun$boundCondition$2$adapted(HashJoin.scala:146)
	at org.apache.spark.sql.execution.joins.HashJoin.$anonfun$outerJoin$1(HashJoin.scala:205)
{noformat}
Both the predicate codegen and the evaluation fail for the same reason: {{PlanSubqueries}} creates {{InSubqueryExec}} with {{shouldBroadcast=false}}. The driver waits for the subquery to finish, but it's the executor that uses the results of the subquery (for predicate codegen or evaluation). Because {{shouldBroadcast}} is set to false, the result is stored in a transient field ({{InSubqueryExec#result}}), so the result of the subquery is not serialized when the {{InSubqueryExec}} instance is sent to the executor.

When wholestage codegen is enabled, the predicate codegen happens on the driver, so the subquery's result is available. When adaptive execution is enabled, {{PlanAdaptiveSubqueries}} always sets {{shouldBroadcast=true}}, so the subquery's result is available on the executor, if needed.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 28 12:41:20 UTC 2023,,,,,,,,,,"0|z1gvy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/23 22:46;bersprockets;PR at https://github.com/apache/spark/pull/40569;;;","28/Mar/23 03:56;snoot;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/40569;;;","28/Mar/23 12:41;dongjoon;Issue resolved by pull request 40569
[https://github.com/apache/spark/pull/40569];;;",,,,,,,,,,,,
Unresolved having at the end of analysis when using with LCA with the having clause that can be resolved directly by its child Aggregate,SPARK-42936,13530280,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xyyu,xyyu,xyyu,27/Mar/23 17:54,28/Mar/23 08:42,30/Oct/23 17:26,28/Mar/23 08:42,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"{code:java}
select sum(value1) as total_1, total_1
from values(1, 'name', 100, 50) AS data(id, name, value1, value2)
having total_1 > 0

SparkException: [INTERNAL_ERROR] Found the unresolved operator: 'UnresolvedHaving (total_1#353L > cast(0 as bigint)) {code}
To trigger the issue, the having condition need to be (can be resolved by) an attribute in the select.
Without the LCA {{{}total_1{}}}, the query works fine.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 28 08:42:20 UTC 2023,,,,,,,,,,"0|z1gvx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/23 17:54;xyyu;Created PR fixing the issue: https://github.com/apache/spark/pull/40558;;;","28/Mar/23 08:42;cloud_fan;Issue resolved by pull request 40558
[https://github.com/apache/spark/pull/40558];;;",,,,,,,,,,,,,
"Spark 3.3.2, with hadoop3,  Error with java.io.IOException: Mkdirs failed to create file",SPARK-42932,13530193,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,shamim_er123,shamim_er123,27/Mar/23 07:31,10/Apr/23 03:27,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Spark Core,Spark Submit,,,,0,,,,,"We are using spark 3.3.2 with hadoop 3  coming with spark.

[https://www.apache.org/dyn/closer.lua/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz] 

[https://www.apache.org/dyn/closer.lua/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.tgz] 

Spark in our application is used as standalone , and we are not using HDFS file system.

Spark is writing on local file system.

Same spark version 3.3.2 is working fine with hadoop 2. but with hadoop 3 , we are getting below issue. 

 

23/03/18 20:23:24 WARN TaskSetManager: Lost task 4.0 in stage 0.0 (TID 4) (10.64.109.72 executor 0): java.io.IOException: Mkdirs failed to create [file:/var/backup/_temporary/0/_temporary/attempt_202301182023173234741341853025716_0005_m_000004_0|file://var/backup/_temporary/0/_temporary/attempt_202301182023173234741341853025716_0005_m_000004_0] (exists=false, cwd=[file:/opt/spark-3.3.2/work/app-20230118202317-0001/0|file://opt/spark-3.3.0/work/app-20230118202317-0001/0])
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)
        at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)
        at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)
        at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)
        at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750) ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Apr 10 03:27:56 UTC 2023,,,,,,,,,,"0|z1gvds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/23 05:44;yumwang;I can't reproduce this issue. Maybe it's your disk issue?;;;","31/Mar/23 04:22;shamim_er123;its working fine with hadoop2, getting issue when using hadoop3.

if there is any issue in disc then it should come with hadoop2 also.;;;","10/Apr/23 03:27;gurwls223;I can't reproduce with Hadoop 3. Would be great to try this out and see if the same thing happens in other computers.;;;",,,,,,,,,,,,
Make resolvePersistentFunction synchronized,SPARK-42928,13530180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,27/Mar/23 05:46,28/Mar/23 08:43,30/Oct/23 17:26,28/Mar/23 08:43,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,Make resolvePersistentFunction synchronized,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 28 08:43:49 UTC 2023,,,,,,,,,,"0|z1gvaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/23 08:43;cloud_fan;Issue resolved by pull request 40557
[https://github.com/apache/spark/pull/40557];;;",,,,,,,,,,,,,,
Delayed scheduling doesn’t work in some situations in local mode if different localities present in loaded files leading to tasks getting stuck,SPARK-42923,13530120,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,dolmio,dolmio,26/Mar/23 12:23,16/Apr/23 18:32,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Scheduler,,,,,0,,,,,"I stumbled on the following issue when running spark in local mode where part of the loaded files were present in the same host as the spark and others not.

Symptom: Some task in larger job would consistently get stuck without no immediately clear errors in logs. My hope/expectation would have been that even if some tasks would have failed to complete on during some expected time the job would have retried the task or failed completely with some exception and not just get stuck forever.

Workaround:
Setting spark.locality.wait.node to 0s seemed to fix the getting stuck in my environment.

Potential root cause:
I managed to reproduce the issue with the spark codebase by adding a test case to FileSourceStrategySuite, which is trying to read two files to a table where another is located in the same host as the local spark executor and another in some other host. https://github.com/apache/spark/commit/c23db78863c7342ae7b7bc3922a200a523e45538

While digging into the issue with the debugger I finally noticed that the LocalSchedulerBackend is missing the reviveThread present in CoarseGrainedSchedulerBackend, which forces the periodic calling of resourceOffsers in TaskSchedulerImpl and not just in taskUpdates.

Potential fix:
Add the revive thread also to LocalSchedulerBackend.
I don’t really have understanding of the codebase whether simply adding the revive thread to LocalSchedulerBackend could have some unwanted side effects.

Questions/Observations:
Should delayed scheduling work at all in local mode?
This issue probably effect also the case where instead of local file there is file which is rack local to the executor and then some non rack local file, which are being loaded.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Apr 16 18:32:30 UTC 2023,,,,,,,,,,"0|z1guxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/23 03:28;gurwls223;[~dolmio] does ""local mode"" mean master=""local[*]"" just to be extra clear?;;;","16/Apr/23 18:32;dolmio;[~gurwls223]: yeah;;;",,,,,,,,,,,,,
"Use SecureRandom, instead of Random in security sensitive contexts",SPARK-42922,13530074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mridulm80,mridulm80,mridulm80,25/Mar/23 16:15,28/Mar/23 03:49,30/Oct/23 17:26,28/Mar/23 03:49,3.2.3,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,,,0,,,,,"Most uses of Random in spark are either in test cases or where we need a pseudo random number which is repeatable.
The following are usages where moving from Random to SecureRandom would be useful

a) HttpAuthUtils.createCookieToken
b) ThriftHttpServlet.RAN",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 28 03:49:10 UTC 2023,,,,,,,,,,"0|z1gunc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/23 03:49;srowen;Resolved by https://github.com/apache/spark/pull/40568;;;",,,,,,,,,,,,,,
SQLQueryTestSuite test failed with `SPARK_ANSI_SQL_MODE=true`,SPARK-42921,13530047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,25/Mar/23 02:45,27/Mar/23 02:20,30/Oct/23 17:26,27/Mar/23 02:20,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,,,0,,,,,"Run 
{code:java}
SPARK_ANSI_SQL_MODE=true build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite"" {code}
{code:java}
[info] - timestampNTZ/datetime-special.sql_analyzer_test *** FAILED *** (11 milliseconds)
[info]   timestampNTZ/datetime-special.sql_analyzer_test
[info]   Expected ""...date(999999, 3, 18, [false) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, fals]e) AS make_date(-1, ..."", but got ""...date(999999, 3, 18, [true) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, tru]e) AS make_date(-1, ..."" Result did not match for query #1
[info]   select make_date(999999, 3, 18), make_date(-1, 1, 28) (SQLQueryTestSuite.scala:777)
[info]   org.scalatest.exceptions.TestFailedException: {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Mar 27 02:20:13 UTC 2023,,,,,,,,,,"0|z1guhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/23 02:20;gurwls223;Issue resolved by pull request 40552
[https://github.com/apache/spark/pull/40552];;;",,,,,,,,,,,,,,
Generic annotation of class attribute in abstract class is NOT initalized in inherited classes,SPARK-42910,13529870,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,jonfarz,jonfarz,23/Mar/23 22:04,10/Apr/23 03:30,30/Oct/23 17:26,10/Apr/23 03:30,3.3.0,3.3.2,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"We are trying to leverage generics to better type our code base.  The example below shows the problem we are having, however without generics this works completely fine in pyspark however with generics it doesn't but does locally without leveraging pyspark.  

Output for local: 

 
{code:java}
<class '__main__.Foo'>{code}
 

TraceBack for pyspark: 
{code:java}
AttributeError: type object 'C' has no attribute 'base_record'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more {code}
 

Code:

 
{code:java}
from abc import ABC
from typing import Generic, TypeVar, Callable
from operator import add

from pyspark.sql import SparkSession

T = TypeVar(""T"")

class Foo:
    ...

class A(ABC, Generic[T]):
    base_record: Callable[..., T]

class B(A):
    base_record = Foo

class C(B):
    ...

def f(_: int) -> int:
    print(C.base_record)
    return 1

spark = SparkSession\
    .builder\
    .appName(""schema_test"")\
    .getOrCreate()

spark.sparkContext.parallelize(range(1, 100)).map(f).reduce(add) {code}
 

 ","Tested in two environments:
 # Databricks
Pyspark Version: 3.3.0
Python Version: 3.9.15
 # Local
Pyspark Verison: 3.3.2
Python Version: 3.3.10",,,,,,,,,,,,,,,,,,SPARK-40991,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,pyspark,Python3,,,Mon Mar 27 01:55:04 UTC 2023,,,,,,,,,,"0|z1gte0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/23 01:54;gurwls223;cc [~zero323] FYI;;;","24/Mar/23 11:06;zero323;Thanks [~gurwls223]  Only glanced over this, but an obvious observation is that type hierarchy is messed up on the worker {{C.mro()}} is (to the module)

{code:python}
[__main__.C, __main__.B, __main__.A, abc.ABC, typing.Generic, object
{code}

at the point of definition / import,  and

{code:python}
[<class 'abc.C'>, <class 'abc.ABC'>, <class 'typing.Generic'>, <class 'object'>]
{code}

on the worker.  This seems to be consistent across {{serializers}}, as far as I can tell.

It seems to me, that {{B}} should be properly initialized as {{A[Foo]}}, i.e.

{code:python}
class B(A[Foo]):
    base_record = Foo
{code}

This also adjusts worker-side {{mro}} to 

{code:python}
[<class 'abc.C'>, <class '__main__.A'>, <class 'abc.ABC'>, <class 'typing.Generic'>, <class 'object'>]
{code}

but I don't see the problem with {{C}} definition. 
;;;","24/Mar/23 11:15;zero323;It is no longer generic, so that cannot be a problem. 

Additionally, the issue seems to disappear when classes are defined externally:

{code:python}
# foo,py

from abc import ABC
from typing import Generic, TypeVar, Callable

T = TypeVar(""T"")

class Foo:
    ...

class A(ABC, Generic[T]):
    base_record: Callable[..., T]

class B(A):
    base_record = Foo

class C(B):
    ...

def f(_: int) -> int:
    print(C.base_record)
    return 1
{code}

and then

{code:python}
from operator import add
from foo import C, f
from pyspark.sql import SparkSession

spark = SparkSession\
    .builder\
    .appName(""schema_test"")\
    .getOrCreate()

spark.sparkContext.parallelize(range(1, 100)).map(f).reduce(add) 
{code}

so it makes sense to focus further investigation on the way how we prepare locally defined classes for shipping over the wire.;;;","26/Mar/23 11:54;zero323;[~gurwls223] After further investigation it looks like it is {{cloudpickle}} issue and has been resolved somewhere between 2.0.0 and 2.2.0.

I guess we could just backport SPARK-40991.;;;","27/Mar/23 01:55;gurwls223;Thank you for the investigation, [~zero323];;;",,,,,,,,,,
INSERT INTO with column list does not work,SPARK-42909,13529764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,tjomme,tjomme,23/Mar/23 12:32,24/Mar/23 01:54,30/Oct/23 17:26,24/Mar/23 01:54,3.3.2,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,databricks,documentation,spark-sql,sql,"Hi,

When performing a INSERT INTO with a defined incomplete column list, the missing columns should get a NULL value. However, an error is thrown indicating that the column is missing.

*Case simulation:*

drop table if exists default.TVTest;
create table default.TVTest
( col1 int NOT NULL
, col2 int
);
insert into default.TVTest select 1,2;
insert into default.TVTest select 2,NULL; --> col2 can contain NULL values
insert into default.TVTest (col1) select 3; -- Error in SQL statement: DeltaAnalysisException: Column col2 is not specified in INSERT
insert into default.TVTest (col1) VALUES (3); -- Error in SQL statement: DeltaAnalysisException: Column col2 is not specified in INSERT
select * from default.TVTest;","Databricks DBR12.2 on AZure, running Spark 3.3.2

Documentation: [INSERT - Azure Databricks - Databricks SQL | Microsoft Learn|https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/sql-ref-syntax-dml-insert-into]",,,,,,,,,,,,,,,,,,SPARK-42521,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 23 21:36:13 UTC 2023,,,,,,,,,,"0|z1gsqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/23 21:36;bersprockets;It looks like this capability landed in 3.4/3.5 with SPARK-42521.;;;",,,,,,,,,,,,,,
Replace a starting digit with `x` in resource name prefix,SPARK-42906,13529709,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,23/Mar/23 07:16,27/Mar/23 22:35,30/Oct/23 17:26,27/Mar/23 22:32,3.2.3,3.3.2,3.4.0,,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,,Kubernetes,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Mar 27 22:32:14 UTC 2023,,,,,,,,,,"0|z1gse8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:04;githubbot;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/40533;;;","27/Mar/23 22:32;dongjoon;Issue resolved by pull request 40533
[https://github.com/apache/spark/pull/40533];;;",,,,,,,,,,,,,
pyspark.ml.stat.Correlation - Spearman Correlation method giving incorrect and inconsistent results for the same DataFrame if it has huge amount of Ties.,SPARK-42905,13529702,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,dronzer,dronzer,23/Mar/23 05:20,28/Aug/23 11:04,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,ML,,,,,0,correctness,,,,"pyspark.ml.stat.Correlation

Following is the Scenario where the Correlation function fails for giving correct Spearman Coefficient Results.

Tested E.g -> Spark DataFrame has 2 columns A and B.

!image-2023-03-23-10-55-26-879.png|width=562,height=162!

Column A has 3 Distinct Values and total of 108Million rows

Column B has 4 Distinct Values and total of 108Million rows

If I Calculate the correlation for this DataFrame in Python Pandas DF.corr, it gives the correct answer even if i run the same code multiple times the same answer is produced. (Each column has only 3-4 distinct values)

!image-2023-03-23-10-53-37-461.png|width=468,height=287!

 

Coming to Spark and using Spearman Correlation produces a *different results* for the *same dataframe* on multiple runs. (see below) (each column in this df has only 3-4 distinct values)

!image-2023-03-23-10-52-49-392.png|width=516,height=322!

 

Basically in python Pandas Df.corr it gives same results on same dataframe on multiple runs which is expected behaviour. However, in Spark using the same data it gives different result, moreover running the same cell with same data multiple times produces different results meaning the output is inconsistent.

Coming to data the only observation I could conclude is Ties in data. (Only 3-4 Distinct values over 108M Rows.) This scenario is not handled in Spark Correlation method as the same data when used in python using df.corr produces consistent results.

The only Workaround we could find to get consistent and the same output as from python in Spark is by using Pandas UDF as shown below:

!image-2023-03-23-10-52-11-481.png|width=518,height=111!

!image-2023-03-23-10-51-28-420.png|width=509,height=270!

 

We also tried pyspark.pandas.DataFrame .corr method and it produces incorrect and inconsistent results for this case too.

Only PandasUDF seems to provide consistent results.

 

Another point to note is : If i add some random noise to the data, which will inturn increase the distinct values in the data. It again gives consistent results for any runs. Which makes me believe that the Python version handles ties correctly and gives consistent results no matter how many ties exist. However, pyspark method is somehow not able to handle many ties in data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/23 05:21;dronzer;image-2023-03-23-10-51-28-420.png;https://issues.apache.org/jira/secure/attachment/13056595/image-2023-03-23-10-51-28-420.png","23/Mar/23 05:22;dronzer;image-2023-03-23-10-52-11-481.png;https://issues.apache.org/jira/secure/attachment/13056596/image-2023-03-23-10-52-11-481.png","23/Mar/23 05:22;dronzer;image-2023-03-23-10-52-49-392.png;https://issues.apache.org/jira/secure/attachment/13056597/image-2023-03-23-10-52-49-392.png","23/Mar/23 05:23;dronzer;image-2023-03-23-10-53-37-461.png;https://issues.apache.org/jira/secure/attachment/13056598/image-2023-03-23-10-53-37-461.png","23/Mar/23 05:25;dronzer;image-2023-03-23-10-55-26-879.png;https://issues.apache.org/jira/secure/attachment/13056599/image-2023-03-23-10-55-26-879.png",,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Aug 23 07:31:23 UTC 2023,,,,,,,,,,"0|z1gsco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 07:31;zhenhaozhang;minimal reproducible example. the result is incorrect and inconsistent when tied value size > 10_000_000

 
{code:java}
import org.apache.spark.ml.linalg.{Matrix, Vectors, Vector}
import org.apache.spark.ml.stat.Correlation
import org.apache.spark.sql.Row

val N = 10000002
val x = sc.range(0, N).map(i => if (i < N - 1) 1.0 else 2.0)
val y = sc.range(0, N).map(i => if (i < N - 1) 2.0 else 1.0)
//val s1 = Statistics.corr(x, y, ""spearman"")
val df = x.zip(y)
  .map{case (x, y) => Vectors.dense(x, y)}
  .map(Tuple1.apply)
  .repartition(1) 
  .toDF(""features"")
  
val Row(coeff1: Matrix) = Correlation.corr(df, ""features"", ""spearman"").head
val r = coeff1(0, 1)
println(s""spearman correlation in spark: $r"")
// spearman correlation in spark: -9.999990476024495E-8 {code}
 

 

the correct result is -1.0;;;",,,,,,,,,,,,,,
DataFrame.to(schema) fails when it contains non-nullable nested field in nullable field,SPARK-42899,13529674,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,22/Mar/23 21:53,23/Mar/23 02:16,30/Oct/23 17:26,23/Mar/23 02:16,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,SQL,,,,,0,,,,,"{{DataFrame.to(schema)}} fails when it contains non-nullable nested field in nullable field:
{code:scala}
scala> val df = spark.sql(""VALUES (1, STRUCT(1 as i)), (NULL, NULL) as t(a, b)"")
df: org.apache.spark.sql.DataFrame = [a: int, b: struct<i: int>]
scala> df.printSchema()
root
 |-- a: integer (nullable = true)
 |-- b: struct (nullable = true)
 |    |-- i: integer (nullable = false)

scala> df.to(df.schema)
org.apache.spark.sql.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `b`.`i` is nullable while it's required to be non-nullable.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 23 02:16:18 UTC 2023,,,,,,,,,,"0|z1gs6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/23 02:16;gurwls223;Fixed in https://github.com/apache/spark/pull/40526;;;",,,,,,,,,,,,,,
"Cast from string to date and date to string say timezone is needed, but it is not used",SPARK-42898,13529642,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,revans2,revans2,revans2,22/Mar/23 15:21,21/Jul/23 03:58,30/Oct/23 17:26,21/Jul/23 03:58,3.2.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"This is really minor but SPARK-35581 removed the need for a timezone when casting from a `StringType` to a `DateType`, but the patch didn't update the `needsTimeZone` function to indicate that it was not longer required.

Currently Casting from a DateType to a StringType also says that it needs the timezone, but it only uses the `DateFormatter` with it's default parameters that do not use the time zone at all.

I think this can be fixed with just a two line change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jul 21 03:58:38 UTC 2023,,,,,,,,,,"0|z1grzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/23 15:52;apachespark;User 'revans2' has created a pull request for this issue:
https://github.com/apache/spark/pull/40524;;;","21/Jul/23 03:58;yao;Issue resolved https://github.com/apache/spark/pull/42089;;;",,,,,,,,,,,,,
ClassNotFoundException: scala.math.Ordering$Reverse,SPARK-42886,13529493,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,StevenC,StevenC,21/Mar/23 17:40,25/Sep/23 17:27,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Build,,,,,0,,,,,"Hi,

We are using the spark-mllib_2.12 dependency in a Java project.

We are attempting to upgrade from version 3.3.1 to 3.3.2. This results in unit tests breaking with exception: ClassNotFoundException: scala.math.Ordering$Reverse

A change was made to add the class to the KyroSerializer  https://issues.apache.org/jira/browse/SPARK-42071

scala.math.Ordering$Reverse was introduced int Scala 2.12.12. The maven dependency tree (mvn dependency:tree) shows that spark-mllib_2.12 brings in scala-library version 2.12.8. Therefore, it doesn't contain scala.math.Ordering$Reverse. 

If the scala-library transitive dependency is excluded from the POM and an explicit dependency declared on with version >=2.12.12, the tests will pass.

Should the scala-library version contained in 3.3.2 be upgraded to >=2.12.12?

 

 ","Development environment

MacBook Pro

Java JDK ibm-1.8-362

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Sep 25 17:27:40 UTC 2023,,,,,,,,,,"0|z1gr28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/23 17:27;legojoey17;Adding another anecdote here. I've got a codebase on Scala 2.12.10 and I recently just hit this bug while getting Spark 3.3.2 working for our systems.

There wasn't anything documented about a minimum version within 2.12.x as well as no conditional on the dependency. I see the Spark main branch does have at least one case of conditionals for Scala 2.13 so I wager this would be a reasonable solution, e.g. [https://github.com/dongjoon-hyun/spark/blame/master/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala#L232-L233];;;",,,,,,,,,,,,,,
Upgrade `kubernetes-client` to 6.5.1,SPARK-42885,13529477,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,21/Mar/23 16:14,21/Mar/23 20:53,30/Oct/23 17:26,21/Mar/23 20:53,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Build,Kubernetes,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 21 20:53:26 UTC 2023,,,,,,,,,,"0|z1gqyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/23 16:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40509;;;","21/Mar/23 20:53;dongjoon;Issue resolved by pull request 40509
[https://github.com/apache/spark/pull/40509];;;",,,,,,,,,,,,,
can not analyze window exp on sub query,SPARK-42869,13529231,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,hongnv110,hongnv110,20/Mar/23 09:59,19/Apr/23 08:56,30/Oct/23 17:26,,3.2.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,," 

CREATE TABLE test_noindex_table(`name` STRING,`age` INT,`city` STRING) PARTITIONED BY (`date` STRING);

 

SELECT
    *
FROM
(
    SELECT *, COUNT(1) OVER itr AS grp_size
    FROM test_noindex_table 
    WINDOW itr AS (PARTITION BY city)
) tbl
WINDOW itr2 AS (PARTITION BY
    city
)
 
Window specification itr is not defined in the WINDOW clause.
  !image-2023-03-20-18-00-40-578.png|width=560,height=361!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/23 10:00;hongnv110;image-2023-03-20-18-00-40-578.png;https://issues.apache.org/jira/secure/attachment/13056505/image-2023-03-20-18-00-40-578.png","17/Apr/23 11:06;fanjia;image-2023-04-17-19-06-28-069.png;https://issues.apache.org/jira/secure/attachment/13057320/image-2023-04-17-19-06-28-069.png","17/Apr/23 11:09;fanjia;image-2023-04-17-19-09-41-485.png;https://issues.apache.org/jira/secure/attachment/13057322/image-2023-04-17-19-09-41-485.png",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Apr 19 08:56:27 UTC 2023,,,,,,,,,,"0|z1gpg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 11:09;fanjia;Can you try with master branch? I tested without any problem. Maybe already fixed.
!image-2023-04-17-19-06-28-069.png|width=678,height=472!

!image-2023-04-17-19-09-41-485.png|width=673,height=435!;;;","19/Apr/23 08:56;hongnv110;OK, thanks;;;",,,,,,,,,,,,,
Inconsistent output from label propagation algorithm in Graph X due to tie-breaking logic in vertexProgram,SPARK-42856,13529149,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,m.haghpanah,m.haghpanah,19/Mar/23 23:34,19/Mar/23 23:54,30/Oct/23 17:26,,1.1.0,3.3.2,,,,,,,,,,,,,,,,,,,,,,GraphX,,,,,0,,,,,"We are experiencing inconsistent output from the label propagation algorithm in Graph X. When we run the algorithm on the same input, we observe different outputs each time. This behavior is unexpected since the algorithm is not designed to be random, and we should be getting the same output for the same input.

We suspect that the issue lies in the tie-breaking logic used by the [vertexProgram|https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/LabelPropagation.scala#L65] when picking labels. Currently, the vertexProgram chooses the label with the maximum frequency, and in case of a tie, it selects the label that appears first. This logic does not handle tie cases correctly, resulting in different outputs for the same input.

{code:scala}
def vertexProgram(vid: VertexId, attr: Long, message: Map[VertexId, Long]): VertexId = {  
    if (message.isEmpty) attr else message.maxBy(_._2)._1
}
{code}

To solve this issue, we propose changing the tie-breaking logic to something like vertex ID. This change will ensure that the same label is always selected in case of a tie, resulting in consistent output from the algorithm for the same input.

{code:scala}
def vertexProgram(vid: VertexId, attr: Long, message: Map[VertexId, Long]): VertexId = {
    if (message.isEmpty) attr else message.maxBy{ case (key, value) => (value, key) }._1
}
{code}

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-03-19 23:34:50.0,,,,,,,,,,"0|z1goxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert NamedLambdaVariable related changes from EquivalentExpressions,SPARK-42852,13529090,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,18/Mar/23 15:51,20/Mar/23 19:08,30/Oct/23 17:26,20/Mar/23 00:57,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,SQL,,,,,0,,,,,See discussion https://github.com/apache/spark/pull/40473#issuecomment-1474848224,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Mar 20 00:57:22 UTC 2023,,,,,,,,,,"0|z1gokw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/23 16:26;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40475;;;","20/Mar/23 00:57;gurwls223;Fixed in https://github.com/apache/spark/pull/40475;;;",,,,,,,,,,,,,
EquivalentExpressions methods need to be consistently guarded by supportedExpression,SPARK-42851,13529035,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,18/Mar/23 01:04,21/Mar/23 13:28,30/Oct/23 17:26,21/Mar/23 13:28,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"SPARK-41468 tried to fix a bug but introduced a new regression. Its change to {{EquivalentExpressions}} added a {{supportedExpression()}} guard to the {{addExprTree()}} and {{getExprState()}} methods, but didn't add the same guard to the other ""add"" entry point -- {{addExpr()}}.

As such, uses that add single expressions to CSE via {{addExpr()}} may succeed, but upon retrieval via {{getExprState()}} it'd inconsistently get a {{None}} due to failing the guard.

We need to make sure the ""add"" and ""get"" methods are consistent. It could be done by one of:
1. Adding the same {{supportedExpression()}} guard to {{addExpr()}}, or
2. Removing the guard from {{getExprState()}}, relying solely on the guard on the ""add"" path to make sure only intended state is added.
(or other alternative refactorings to fuse the guard into various methods to make it more efficient)

There are pros and cons to the two directions above, because {{addExpr()}} used to allow (potentially incorrect) more expressions to get CSE'd, making it more restrictive may cause performance regressions (for the cases that happened to work).

Example:
{code:sql}
select max(transform(array(id), x -> x)), max(transform(array(id), x -> x)) from range(2)
{code}

Running this query on Spark 3.2 branch returns the correct value:
{code}
scala> spark.sql(""select max(transform(array(id), x -> x)), max(transform(array(id), x -> x)) from range(2)"").collect
res0: Array[org.apache.spark.sql.Row] = Array([WrappedArray(1),WrappedArray(1)])
{code}
Here, {{transform(array(id), x -> x)}} is an {{AggregateExpression}} that was (potentially unsafely) recognized by {{addExpr()}} as a common subexpression, and {{getExprState()}} doesn't do extra guarding, so during physical planning, in {{PhysicalAggregation}} this expression gets CSE'd in both the aggregation expression list and the result expressions list.
{code}
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[], functions=[max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=11]
      +- SortAggregate(key=[], functions=[partial_max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))])
         +- Range (0, 2, step=1, splits=16)
{code}

Running the same query on current master triggers an error when binding the result expression to the aggregate expression in the Aggregate operators (for a WSCG-enabled operator like {{HashAggregateExec}}, the same error would show up during codegen):
{code}
ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 16) (ip-10-110-16-93.us-west-2.compute.internal executor driver): java.lang.IllegalStateException: Couldn't find max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))#4 in [max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))#3]
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:532)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:456)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)
	at scala.collection.immutable.List.map(List.scala:297)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:161)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.generateResultProjection(AggregationIterator.scala:246)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.<init>(AggregationIterator.scala:296)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:49)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:79)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)
...
{code}
Note that the aggregate expressions are deduplicated in {{PhysicalAggregation}}, but the result expressions were unable to deduplicate consistently due to the bug mentioned in this ticket.
{code}
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[], functions=[max(transform(array(id#15L), lambdafunction(lambda x#16L, lambda x#16L, false)))])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=38]
      +- SortAggregate(key=[], functions=[partial_max(transform(array(id#15L), lambdafunction(lambda x#16L, lambda x#16L, false)))])
         +- Range (0, 2, step=1, splits=16)
{code}

Fixing it via method 1 is more correct than method 2 in terms of avoiding incorrect CSE:
{code:diff}
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala
index 330d66a21b..12def60042 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala
@@ -40,7 +40,11 @@ class EquivalentExpressions {
    * Returns true if there was already a matching expression.
    */
   def addExpr(expr: Expression): Boolean = {
-    updateExprInMap(expr, equivalenceMap)
+    if (supportedExpression(expr)) {
+      updateExprInMap(expr, equivalenceMap)
+    } else {
+      false
+    }
   }
 
   /**
{code}
the query runs correctly again, but this time the aggregate expression is NOT CSE'd anymore, done consistently for both aggregate expressions and result expressions:
{code}
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[], functions=[max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false))), max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=11]
      +- SortAggregate(key=[], functions=[partial_max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false))), partial_max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))])
         +- Range (0, 2, step=1, splits=16)
{code}
and for this particular case, the CSE that used to take place was actually okay, so losing CSE here means performance regression.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 21 13:28:44 UTC 2023,,,,,,,,,,"0|z1go8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/23 01:25;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/40473;;;","20/Mar/23 12:34;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40488;;;","20/Mar/23 12:35;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40488;;;","21/Mar/23 13:28;cloud_fan;Issue resolved by pull request 40473
[https://github.com/apache/spark/pull/40473];;;",,,,,,,,,,,
spark-submit - issue when resolving dependencies hosted on a private repository in kubernetes cluster mode,SPARK-42837,13528947,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,lionelh,lionelh,17/Mar/23 11:19,20/Mar/23 01:14,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Kubernetes,Spark Submit,,,,0,,,,,"When using [spark operator|https://github.com/GoogleCloudPlatform/spark-on-k8s-operator], if dependencies are hosted on a private repository with authentication needed (like S3 or OCI) the spark operator submitting the job need to have all the secrets to access all dependencies. If not the spark-submit fails.

On a multi tenant kubernetes cluster where the spark operator and spark jobs execution are on seperate namespaces, it involves duplicating all secrets or it won't work.

It seems that spark-submit need to acces dependencies (with credentials) only to resolveGlobPath ([https://github.com/apache/spark/blob/v3.3.2/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L364-L367)] . It seems to me (but need to be confirmed by someone more skilled than me on spark internals behavior) that this resolveGlobPath task is also done when the driver is downloading the jars.

Would it be possible to have this resolveGlobPath task skipped when running on a  Kubernetes Cluster in cluster mode ?

For example add a condition like this arround the 364-367 lines :
{code:java}
if (isKubernetesCluster) {
...
} {code}
We could even, for compatibility reason with old behavior if needed, add also a condition on a spark parameter like this :
{code:java}
if (isKubernetesCluster && sparkConf.getBoolean(""spark.kubernetes.resolevGlobPathsInSubmit"", true)) { 
...
}{code}
i tested both solution locally and it seems to resolve the case.

Do yout think I need to consider other elements ?

I may submit a patch depending on your feedback",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-03-17 11:19:17.0,,,,,,,,,,"0|z1gnp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark type hint returns Any for methods on GroupedData,SPARK-42828,13528832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,apachespark,j03wang,j03wang,16/Mar/23 16:44,03/Jul/23 06:38,30/Oct/23 17:26,03/Jul/23 06:38,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,,,,,3.5.0,,,,PySpark,,,,,0,,,,,"Since upgrading to PySpark 3.3.x, type hints for
{code:java}
df.groupBy(...).count(){code}
are now returning Any instead of DataFrame, causing type inference issues downstream. This used to be correctly typed prior to 3.3.x.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jul 03 06:38:11 UTC 2023,,,,,,,,,,"0|z1gmzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/23 19:20;apachespark;User 'j03wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40460;;;","03/Jul/23 06:38;gurwls223;Issue resolved by pull request 40460
[https://github.com/apache/spark/pull/40460];;;",,,,,,,,,,,,,
Update ORC to 1.8.3,SPARK-42820,13528715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,16/Mar/23 02:38,16/Mar/23 05:01,30/Oct/23 17:26,16/Mar/23 05:00,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 16 05:00:44 UTC 2023,,,,,,,,,,"0|z1gm9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/23 02:43;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40453;;;","16/Mar/23 02:44;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40453;;;","16/Mar/23 05:00;dongjoon;Issue resolved by pull request 40453
[https://github.com/apache/spark/pull/40453];;;",,,,,,,,,,,,
Spark driver logs are filled with Initializing service data for shuffle service using name,SPARK-42817,13528669,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csingh,csingh,csingh,15/Mar/23 16:53,20/Mar/23 19:10,30/Oct/23 17:26,16/Mar/23 21:27,3.2.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,,,0,,,,,"With SPARK-34828, we added the ability to make the shuffle service name configurable and we added a log [here|https://github.com/apache/spark/blob/8860f69455e5a722626194c4797b4b42cccd4510/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala#L118] that will log the shuffle service name. However, this log is printed in the driver logs whenever there is new executor launched and pollutes the log. 
{code}
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
{code}
We can just log this once in the driver.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 16 21:27:49 UTC 2023,,,,,,,,,,"0|z1glzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 18:09;csingh;Created PR https://github.com/apache/spark/pull/40448;;;","15/Mar/23 18:20;apachespark;User 'otterc' has created a pull request for this issue:
https://github.com/apache/spark/pull/40448;;;","16/Mar/23 21:27;dongjoon;Issue resolved by pull request 40448
[https://github.com/apache/spark/pull/40448];;;",,,,,,,,,,,,
Print application info when waitAppCompletion is false,SPARK-42813,13528641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,15/Mar/23 14:02,21/Mar/23 16:08,30/Oct/23 17:26,21/Mar/23 16:08,3.3.2,,,,,,,,,,,,,,,,,,,3.5.0,,,,Kubernetes,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 21 16:08:51 UTC 2023,,,,,,,,,,"0|z1glt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 14:19;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/40444;;;","15/Mar/23 14:20;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/40444;;;","21/Mar/23 16:08;dongjoon;Issue resolved by pull request 40444
[https://github.com/apache/spark/pull/40444];;;",,,,,,,,,,,,
client_type is missing from AddArtifactsRequest proto message,SPARK-42812,13528635,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,15/Mar/23 13:26,20/Mar/23 19:10,30/Oct/23 17:26,20/Mar/23 19:10,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Connect,,,,,0,,,,,The client_type is missing from AddArtifactsRequest proto message,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 15 13:33:33 UTC 2023,,,,,,,,,,"0|z1glrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 13:32;apachespark;User 'vicennial' has created a pull request for this issue:
https://github.com/apache/spark/pull/40443;;;","15/Mar/23 13:33;apachespark;User 'vicennial' has created a pull request for this issue:
https://github.com/apache/spark/pull/40443;;;",,,,,,,,,,,,,
'Conflicting attributes' exception is thrown when joining checkpointed dataframe,SPARK-42805,13528602,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,maciejsmolenski,maciejsmolenski,15/Mar/23 10:18,19/Mar/23 14:29,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Optimizer,,,,,0,,,,,"Performing join using checkpointed dataframe leads to error in prepared 'execution plan' because columns ids/names in 'execution plan' are not unique.



This issue can be reproduced with this simple code (fails on 3.3.2, succeeds on 3.1.2):
{code:java}
import spark.implicits._
spark.sparkContext.setCheckpointDir(""file:///tmp/cdir"")
val df = spark.range(10).toDF(""id"")
val cdf = df.checkpoint()
cdf.join(df) // org.apache.spark.sql.AnalysisException thrown on 3.3.2  {code}
 

The failure message is:
{noformat}
org.apache.spark.sql.AnalysisException:
Failure when resolving conflicting references in Join:
'Join Inner
:- LogicalRDD [id#2L], false
+- Project [id#0L AS id#2L]
   +- Range (0, 10, step=1, splits=Some(16))Conflicting attributes: id#2L
;
'Join Inner
:- LogicalRDD [id#2L], false
+- Project [id#0L AS id#2L]
   +- Range (0, 10, step=1, splits=Some(16))  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:57)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:56)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:188)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:540)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
  at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3887)
  at org.apache.spark.sql.Dataset.join(Dataset.scala:920)
  ... 49 elided
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Mar 19 14:29:58 UTC 2023,,,,,,,,,,"0|z1glkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/23 14:29;apachespark;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/40477;;;","19/Mar/23 14:29;apachespark;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/40477;;;",,,,,,,,,,,,,
when target table format is textfile using `insert into select` will got error,SPARK-42804,13528596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,kevinshin,kevinshin,15/Mar/23 09:46,16/Mar/23 07:17,30/Oct/23 17:26,16/Mar/23 00:00,3.2.3,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"*create* *table* test.tex_t1(name string, address string) *ROW* FORMAT DELIMITED FIELDS TERMINATED *BY* ',' STORED *AS* TEXTFILE;


*insert* *into* test.tex_t1 *select* 'a', 'b';

will got alot of message about :
WARN RetryingMetaStoreClient: MetaStoreClient lost connection. Attempting to reconnect (24 of 24) after 5s. fireListenerEvent
org.apache.thrift.transport.TTransportException
 
But the data was actual write to table.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 16 07:17:03 UTC 2023,,,,,,,,,,"0|z1glj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/23 00:00;yumwang;It looks like this is a problem of your metastore.;;;","16/Mar/23 01:59;kevinshin;orc and parquet table won't have this problem.

directly use hive beeline connect to hive also have no problem.;;;","16/Mar/23 05:15;yumwang;How to reproduce?;;;","16/Mar/23 07:04;yumwang;I can't reproduce it. Did you set any configs?;;;","16/Mar/23 07:17;kevinshin;@[~yumwang]  below is my step by step reproduce  this issue : 
 
hive version is HDP 3.1.0.3.1.4.0-315
 
[bigtop@hdpdev243 spark3]$ {color:#4c9aff}cat conf/spark-defaults.conf{color}
# Generated by Apache Ambari. Tue Apr 27 11:19:24 2021
 
spark.sql.hive.convertMetastoreOrc true
spark.sql.orc.filterPushdown true
spark.sql.orc.impl native
spark.sql.legacy.createHiveTableByDefault false
 
[bigtop@hdpdev243 spark3]$ {color:#4c9aff}bin/spark-sql{color}
23/03/16 15:03:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.incremental does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.headers does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.hook.proto.base-directory does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.load.data.owner does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.service.metrics.codahale.reporter.classes does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.strict.managed.tables does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.create.as.insert.only does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.metastore.db.type does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.tez.cartesian-product.enabled does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.metastore.warehouse.external.dir does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.heapsize does not exist
23/03/16 15:03:29 WARN HiveConf: HiveConf of name hive.server2.webui.enable.cors does not exist
23/03/16 15:03:29 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic
23/03/16 15:03:30 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
Spark master: local[*], Application Id: local-1678950211606
spark-sql> select version();
3.2.3 b53c341e0fefbb33d115ab630369a18765b7763d
Time taken: 3.956 seconds, Fetched 1 row(s)
spark-sql> {color:#4c9aff}create table test.tex_t1(name string, address string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;{color}
23/03/16 15:03:51 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
Time taken: 0.753 seconds
spark-sql> {color:#4c9aff}create table test.tex_t2(name string, address string);{color}
Time taken: 0.326 seconds
spark-sql> {color:#4c9aff}insert into test.tex_t2 select 'a', 'b';{color}
Time taken: 2.011 seconds
spark-sql> {color:#4c9aff}insert into test.tex_t1 select 'a', 'b';{color}
23/03/16 15:04:13 WARN HdfsUtils: Unable to inherit permissions for file hdfs://nsdev/warehouse/tablespace/managed/hive/test.db/tex_t1/part-00000-57c15f7a-7462-4101-af5d-9f4a22cf69df-c000 from file hdfs://nsdev/warehouse/tablespace/man
aged/hive/test.db/tex_t1
23/03/16 15:04:13 WARN RetryingMetaStoreClient: MetaStoreClient lost connection. Attempting to reconnect (1 of 24) after 5s. fireListenerEvent
org.apache.thrift.transport.TTransportException
at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_fire_listener_event(ThriftHiveMetastore.java:4977)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.fire_listener_event(ThriftHiveMetastore.java:4964)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.fireListenerEvent(HiveMetaStoreClient.java:2296)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
at com.sun.proxy.$Proxy21.fireListenerEvent(Unknown Source)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)
at com.sun.proxy.$Proxy21.fireListenerEvent(Unknown Source)
at org.apache.hadoop.hive.ql.metadata.Hive.fireInsertEvent(Hive.java:2381)
at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2066)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.spark.sql.hive.client.Shim_v2_1.loadTable(HiveShim.scala:1286)
at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$loadTable$1(HiveClientImpl.scala:908)
at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)
at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)
at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)
at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)
at org.apache.spark.sql.hive.client.HiveClientImpl.loadTable(HiveClientImpl.scala:903)
at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$loadTable$1(HiveExternalCatalog.scala:893)
at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
at org.apache.spark.sql.hive.HiveExternalCatalog.loadTable(HiveExternalCatalog.scala:887)
at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.loadTable(ExternalCatalogWithListener.scala:167)
at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.processInsert(InsertIntoHiveTable.scala:348)
at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:106)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
at org.apache.spark.sql.Dataset.(Dataset.scala:219)
at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:67)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:384)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:504)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1$adapted(SparkSQLCLIDriver.scala:498)
at scala.collection.Iterator.foreach(Iterator.scala:943)
at scala.collection.Iterator.foreach$(Iterator.scala:943)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
at scala.collection.IterableLike.foreach(IterableLike.scala:74)
at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:498)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:287)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
 ;;;",,,,,,,,,,
Fix Flaky ClientE2ETestSuite,SPARK-42801,13528561,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,15/Mar/23 05:50,15/Mar/23 07:43,30/Oct/23 17:26,15/Mar/23 06:28,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Connect,Tests,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 15 06:28:14 UTC 2023,,,,,,,,,,"0|z1glbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 06:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40434;;;","15/Mar/23 06:28;dongjoon;Issue resolved by pull request 40434
[https://github.com/apache/spark/pull/40434];;;",,,,,,,,,,,,,
Update SBT build `xercesImpl` version to match with pom.xml,SPARK-42799,13528551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,15/Mar/23 03:32,15/Mar/23 07:43,30/Oct/23 17:26,15/Mar/23 07:42,3.2.2,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-39183,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 15 07:42:32 UTC 2023,,,,,,,,,,"0|z1gl94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 03:34;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40431;;;","15/Mar/23 07:42;dongjoon;Issue resolved by pull request 40431
[https://github.com/apache/spark/pull/40431];;;",,,,,,,,,,,,,
`connect` module requires `build_profile_flags`,SPARK-42793,13528513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/Mar/23 22:04,15/Mar/23 07:43,30/Oct/23 17:26,14/Mar/23 23:56,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-42656,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 14 23:56:24 UTC 2023,,,,,,,,,,"0|z1gl0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/23 22:06;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40424;;;","14/Mar/23 22:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40424;;;","14/Mar/23 23:56;gurwls223;Issue resolved by pull request 40424
[https://github.com/apache/spark/pull/40424];;;",,,,,,,,,,,,
"[K8S][Core] When spark submit without --deploy-mode, will face NPE in Kubernetes Case",SPARK-42785,13528391,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zWangSheng,zWangSheng,zWangSheng,14/Mar/23 09:49,14/Mar/23 15:51,30/Oct/23 17:26,14/Mar/23 15:50,3.2.4,3.3.3,3.4.0,,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,,Kubernetes,,,,,0,,,,,"According to this PR [https://github.com/apache/spark/pull/37880#issuecomment-1347777890,] when user spark submit without `--deploy-mode XXX` or `–conf spark.submit.deployMode=XXXX`, may face NPE with this code

 
args.deployMode.equals(""client"")
 
 ",,,,,,,,,,,,,,,,,,,,SPARK-39399,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 14 15:50:11 UTC 2023,,,,,,,,,,"0|z1gk9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/23 11:07;apachespark;User 'zwangsheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/40414;;;","14/Mar/23 15:50;dongjoon;Issue resolved by pull request 40414
[https://github.com/apache/spark/pull/40414];;;",,,,,,,,,,,,,
Fix the problem of incomplete creation of subdirectories in push merged localDir,SPARK-42784,13528377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,StoveM,StoveM,StoveM,14/Mar/23 08:50,01/Jul/23 03:52,30/Oct/23 17:26,01/Jul/23 03:52,3.3.2,,,,,,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Shuffle,Spark Core,,,,0,,,,,"After we massively enabled push-based shuffle in our production environment, we found some warn messages appearing in the server-side log messages.

the warning log like:

ShuffleBlockPusher: Pushing block shufflePush_3_0_5352_935 to BlockManagerId(shuffle-push-merger, zw06-data-hdp-dn08251.mt, 7337, None) failed.
java.lang.RuntimeException: java.lang.RuntimeException: Cannot initialize merged shuffle partition for appId application_1671244879475_44020960 shuffleId 3 shuffleMergeId 0 reduceId 935.

After investigation, we identified the triggering mechanism of the bug。

The driver requested two different containers on the same physical machine. During the creation of the 'push-merged' directory in the first container (container_1), the mergeDir was created first, then the subDir were created based on the value of the ""spark.diskStore.subDirectories"" parameter. However, the resources of container_1 were preempted during the creation of the sub-directories, resulting in subDir not being created (only part of it was created ). As the mergeDir still existed, the second container (container_2) was unable to create further subDir (as it assumed that all directories had already been created).

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 14 09:26:34 UTC 2023,,,,,,,,,,"0|z1gk6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/23 09:26;apachespark;User 'Stove-hust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40412;;;",,,,,,,,,,,,,,
approx_percentile produces wrong results for large decimals.,SPARK-42775,13528301,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,mashplant,mashplant,13/Mar/23 20:53,08/Oct/23 00:19,30/Oct/23 17:26,,2.1.0,2.2.0,2.3.0,2.4.0,3.0.0,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,,,,,SQL,,,,,0,pull-request-available,,,,"In the {{approx_percentile}} expression, Spark casts decimal to double to update the aggregation state ([ApproximatePercentile.scala#L181|https://github.com/apache/spark/blob/933dc0c42f0caf74aaa077fd4f2c2e7208452b9b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproximatePercentile.scala#L181]) and casts the result double back to decimal ([ApproximatePercentile.scala#L206|https://github.com/apache/spark/blob/933dc0c42f0caf74aaa077fd4f2c2e7208452b9b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproximatePercentile.scala#L206]). The precision loss in the casts can make the result decimal out of its precision range. This can lead to the following counter-intuitive results:
{code:sql}
spark-sql> select approx_percentile(col, 0.5) from values (9999999999999999999) as tab(col);
NULL
spark-sql> select approx_percentile(col, 0.5) is null from values (9999999999999999999) as tab(col);
false
spark-sql> select cast(approx_percentile(col, 0.5) as string) from values (9999999999999999999) as tab(col);
10000000000000000000
spark-sql> desc select approx_percentile(col, 0.5) from values (9999999999999999999) as tab(col);
approx_percentile(col, 0.5, 10000)	decimal(19,0) 
{code}
The result is actually not null, so the second query returns false. The first query returns null because the result cannot fit into {{{}decimal(19, 0){}}}.

A suggested fix is to use {{Decimal.changePrecision}} here to ensure the result fits, and really returns a null or throws an exception when the result doesn't fit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 15 01:21:44 UTC 2023,,,,,,,,,,"0|z1gjpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 01:21;apachespark;User 'chenhao-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/40429;;;",,,,,,,,,,,,,,
SQLImplicitsTestSuite test failed with Java 17,SPARK-42770,13528185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,13/Mar/23 07:18,14/Mar/23 15:53,30/Oct/23 17:26,14/Mar/23 15:53,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.4.1,,,,Connect,Tests,,,,0,,,,,"[https://github.com/apache/spark/actions/runs/4318647315/jobs/7537203682]
{code:java}
[info] - test implicit encoder resolution *** FAILED *** (1 second, 329 milliseconds)
4429[info]   2023-03-02T23:00:20.404434 did not equal 2023-03-02T23:00:20.404434875 (SQLImplicitsTestSuite.scala:63)
4430[info]   org.scalatest.exceptions.TestFailedException:
4431[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
4432[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
4433[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
4434[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
4435[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.testImplicit$1(SQLImplicitsTestSuite.scala:63)
4436[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.$anonfun$new$2(SQLImplicitsTestSuite.scala:133)
4437[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
4438[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
4439[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
4440[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
4441[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
4442[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
4443[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
4444[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
4445[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
4446[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
4447[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
4448[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
4449[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
4450[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
4451[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
4452[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)
4453[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
4454[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
4455[info]   at scala.collection.immutable.List.foreach(List.scala:431)
4456[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
4457[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
4458[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
4459[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
4460[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
4461[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
4462[info]   at org.scalatest.Suite.run(Suite.scala:1114)
4463[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
4464[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
4465[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
4466[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
4467[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
4468[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
4469[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.org$scalatest$BeforeAndAfterAll$$super$run(SQLImplicitsTestSuite.scala:34)
4470[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
4471[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
4472[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
4473[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.run(SQLImplicitsTestSuite.scala:34)
4474[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
4475[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
4476[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
4477[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
4478[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
4479[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
4480[info]   at java.base/java.lang.Thread.run(Thread.java:833) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 14 15:53:33 UTC 2023,,,,,,,,,,"0|z1gizs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/23 07:19;LuciferYang;Maybe it can only be reproduced on Linux

 ;;;","13/Mar/23 10:11;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40395;;;","14/Mar/23 15:53;dongjoon;Issue resolved by pull request 40395
[https://github.com/apache/spark/pull/40395];;;",,,,,,,,,,,,
The partition of result data frame of join is always 1,SPARK-42760,13528082,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,bdaiab,bdaiab,11/Mar/23 14:48,19/May/23 14:56,30/Oct/23 17:26,19/May/23 14:56,3.3.2,,,,,,,,,,,,,,,,,,,,,,,PySpark,SQL,,,,0,,,,,"I am using pyspark. The partition of result data frame of join is always 1.

Here is my code from https://stackoverflow.com/questions/51876281/is-partitioning-retained-after-a-spark-sql-join

 

print(spark.version)

def example_shuffle_partitions(data_partitions=10, shuffle_partitions=4):
    spark.conf.set(""spark.sql.shuffle.partitions"", shuffle_partitions)
    spark.sql(""SET spark.sql.autoBroadcastJoinThreshold=-1"")
    df1 = spark.range(1, 1000).repartition(data_partitions)
    df2 = spark.range(1, 2000).repartition(data_partitions)
    df3 = spark.range(1, 3000).repartition(data_partitions)

    print(""Data partitions is: {}. Shuffle partitions is {}"".format(data_partitions, shuffle_partitions))
    print(""Data partitions before join: {}"".format(df1.rdd.getNumPartitions()))

    df = (df1.join(df2, df1.id == df2.id)
          .join(df3, df1.id == df3.id))

    print(""Data partitions after join : {}"".format(df.rdd.getNumPartitions()))

example_shuffle_partitions()

 


In Spark 3.0.3, it prints out:
3.0.3
Data partitions is: 10. Shuffle partitions is 4
Data partitions before join: 10
Data partitions after join : 4


However, it prints out the following in the latest 3.3.2
3.3.2
Data partitions is: 10. Shuffle partitions is 4
Data partitions before join: 10
Data partitions after join : 1","standard spark 3.0.3/3.3.2, using in jupyter notebook, local mode",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 31 05:36:08 UTC 2023,,,,,,,,,,"0|z1gicw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/23 23:22;yumwang;Could you try to disable AQE(set spark.sql.adaptive.enabled = false)?;;;","17/Mar/23 13:31;bdaiab;Disabling AQE solved my problem. Thank you!;;;","31/Mar/23 05:36;rangareddy.avula@gmail.com;From Spark version 3.2.0 onwards AQE is enabled by default. AQE is disabled in Spark version < 3.2.0.

 ;;;",,,,,,,,,,,,
ReusedExchange refers to non-existent node,SPARK-42753,13528042,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,steven.chen,steven.chen,10/Mar/23 22:13,12/Mar/23 19:28,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,Web UI,,,,0,,,,,"There is an AQE “issue“ where during AQE planning, the Exchange ""that's being"" reused could be replaced in the plan tree. So, when we print the query plan, the ReusedExchange will refer to an “unknown“ Exchange. An example below:

 
{code:java}
(2775) ReusedExchange [Reuses operator id: unknown]
 Output [3]: [sr_customer_sk#271, sr_store_sk#275, sum#377L]{code}
 

 

Below is an example to demonstrate the root cause:

 
{code:java}
AdaptiveSparkPlan
  |-- SomeNode X (subquery xxx)
      |-- Exchange A
          |-- SomeNode Y
              |-- Exchange B
Subquery:Hosting operator = SomeNode Hosting Expression = xxx dynamicpruning#388
AdaptiveSparkPlan
  |-- SomeNode M
      |-- Exchange C
          |-- SomeNode N
              |-- Exchange D
{code}
 

 

Step 1: Exchange B is materialized and the QueryStage is added to stage cache

Step 2: Exchange D reuses Exchange B

Step 3: Exchange C is materialized and the QueryStage is added to stage cache

Step 4: Exchange A reuses Exchange C

 

Then the final plan looks like:

 
{code:java}
AdaptiveSparkPlan
  |-- SomeNode X (subquery xxx)
      |-- Exchange A -> ReusedExchange (reuses Exchange C)

Subquery:Hosting operator = SomeNode Hosting Expression = xxx dynamicpruning#388
AdaptiveSparkPlan
  |-- SomeNode M
      |-- Exchange C -> PhotonShuffleMapStage ....
          |-- SomeNode N
              |-- Exchange D -> ReusedExchange (reuses Exchange B)
{code}
 

 

As a result, the ReusedExchange (reuses Exchange B) will refer to a non-exist node. This *DOES NOT* affect query execution but will cause the query visualization malfunction in the following ways:
 # The ReusedExchange child subtree will still appear in the Spark UI graph but will contain no node IDs.
 # The ReusedExchange node details in the Explain plan will refer to a UNKNOWN node. Example below.

{code:java}
(2775) ReusedExchange [Reuses operator id: unknown]{code}
 # The child exchange and its subtree may be missing from the Explain text completely. No node details or tree string shown.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Mar 12 19:28:40 UTC 2023,,,,,,,,,,"0|z1gi40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/23 02:33;Zing;[~steven.chen] 

Can you provide the reproduction code?;;;","12/Mar/23 19:28;apachespark;User 'StevenChenDatabricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/40385;;;","12/Mar/23 19:28;apachespark;User 'StevenChenDatabricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/40385;;;",,,,,,,,,,,,
Pyspark.pandas.series.str.findall can't handle tuples that are returned by regex,SPARK-42751,13528021,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ionk,ionk,10/Mar/23 18:05,20/Mar/23 01:18,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Pandas API on Spark,,,,,0,,,,,"When you use the str.findall accessor method on a ps.series and you're passing a regex pattern that will return match groups, it will return a pyarrow data error.

In pandas the result is this:
{code:java}
df.to_pandas()[col].str.findall(regex_pattern, flags=re.IGNORECASE)

returns 

 [(""value"", , , , )],
 [(""value"", , , , )],
 [(, , ,""value"", )]{code}
 

In pyspark.pandas the result is:
{code:java}
org.apache.spark.api.python.PythonException: 'pyarrow.lib.ArrowTypeError: Expected bytes, got a 'tuple' object'.{code}
 

My temporary workaround is using 
{code:java}
df.apply(lambda x: re.findall(regex_pattern, x, flags=re.IGNORECASE)[0]{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Mar 20 01:18:23 UTC 2023,,,,,,,,,,"0|z1ghzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/23 01:18;gurwls223;cc [~itholic] FYI;;;",,,,,,,,,,,,,,
CAST(x as int) does not generate error with overflow,SPARK-42749,13528009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tjomme,tjomme,10/Mar/23 16:27,13/Mar/23 08:00,30/Oct/23 17:26,13/Mar/23 08:00,3.2.1,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Hi,

When performing the following code:

{{select cast(7.415246799222789E19 as int)}}

according to the documentation, an error is expected as {{7.415246799222789E19 }}is an overflow value for datatype INT.

However, the value 2147483647 is returned. 

The behaviour of the following is correct as it returns NULL:

{{select try_cast(7.415246799222789E19 as int) }}

This results in unexpected behaviour and data corruption.","It was tested on a DataBricks environment with DBR 10.4 and above, running Spark v3.2.1 and above.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 16:30;tjomme;Spark-42749.PNG;https://issues.apache.org/jira/secure/attachment/13056240/Spark-42749.PNG",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Mar 13 08:00:47 UTC 2023,,,,,,,,,,"0|z1ghwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 23:36;yumwang;Please enable ansi:
{code:sql}
spark-sql (default)> set spark.sql.ansi.enabled=true;
spark.sql.ansi.enabled	true
Time taken: 0.088 seconds, Fetched 1 row(s)
spark-sql (default)> select cast(7.415246799222789E19 as int);
[CAST_OVERFLOW] The value 7.415246799222789E19D of the type ""DOUBLE"" cannot be cast to ""INT"" due to an overflow. Use `try_cast` to tolerate overflow and return NULL instead. If necessary set ""spark.sql.ansi.enabled"" to ""false"" to bypass this error.
org.apache.spark.SparkArithmeticException: [CAST_OVERFLOW] The value 7.415246799222789E19D of the type ""DOUBLE"" cannot be cast to ""INT"" due to an overflow. Use `try_cast` to tolerate overflow and return NULL instead. If necessary set ""spark.sql.ansi.enabled"" to ""false"" to bypass this error.
{code};;;","13/Mar/23 07:44;tjomme;Hi,

This does indeed solve the problem. Setting the parameter makes it behave as intended.

Can this be noted in the documentation that this is a requirement?

Thanks,

Tjomme;;;","13/Mar/23 07:59;tjomme;Just checked the documentation again: the warning aparently was recently added;;;","13/Mar/23 08:00;tjomme;Additional settings required to get the intended behaviour.

Documentation is up-to-date;;;",,,,,,,,,,,
Fix incorrect internal status of LoR and AFT,SPARK-42747,13527973,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,10/Mar/23 12:58,11/Mar/23 14:47,30/Oct/23 17:26,11/Mar/23 14:47,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.1,3.5.0,ML,PySpark,,,,0,,,,,"LoR and AFT applied internal status to optimize prediction/transform, but the status is not correctly updated in some case:


{code:java}
from pyspark.sql import Row
from pyspark.ml.classification import *
from pyspark.ml.linalg import Vectors

df = spark.createDataFrame(
    [
        (1.0, 1.0, Vectors.dense(0.0, 5.0)),
        (0.0, 2.0, Vectors.dense(1.0, 2.0)),
        (1.0, 3.0, Vectors.dense(2.0, 1.0)),
        (0.0, 4.0, Vectors.dense(3.0, 3.0)),
    ],
    [""label"", ""weight"", ""features""],
)

lor = LogisticRegression(weightCol=""weight"")
model = lor.fit(df)

# status changes 1
for t in [0.0, 0.1, 0.2, 0.5, 1.0]:
    model.setThreshold(t).transform(df)

# status changes 2
[model.setThreshold(t).predict(Vectors.dense(0.0, 5.0)) for t in [0.0, 0.1, 0.2, 0.5, 1.0]]

for t in [0.0, 0.1, 0.2, 0.5, 1.0]:
    print(t)
    model.setThreshold(t).transform(df).show()                                        #  <- error results
{code}


results:

{code:java}
0.0
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

0.1
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

0.2
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

0.5
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

1.0
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Mar 11 14:47:06 UTC 2023,,,,,,,,,,"0|z1ghoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 13:30;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/40367;;;","11/Mar/23 14:47;srowen;Resolved by https://github.com/apache/spark/pull/40367;;;",,,,,,,,,,,,,
Improved AliasAwareOutputExpression works with DSv2,SPARK-42745,13527953,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,10/Mar/23 09:16,14/Mar/23 15:41,30/Oct/23 17:26,10/Mar/23 12:59,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,SQL,,,,,0,,,,,"After SPARK-40086 / SPARK-42049 the following, simple subselect expression containing query:
{noformat}
select (select sum(id) from t1)
{noformat}
fails with:

{noformat}
09:48:57.645 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.NullPointerException
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.batch$lzycompute(BatchScanExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.batch(BatchScanExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.hashCode(BatchScanExec.scala:60)
	at scala.runtime.Statics.anyHash(Statics.java:122)
        ...
	at org.apache.spark.sql.catalyst.trees.TreeNode.hashCode(TreeNode.scala:249)
	at scala.runtime.Statics.anyHash(Statics.java:122)
	at scala.collection.mutable.HashTable$HashUtils.elemHashCode(HashTable.scala:416)
	at scala.collection.mutable.HashTable$HashUtils.elemHashCode$(HashTable.scala:416)
	at scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:44)
	at scala.collection.mutable.HashTable.addEntry(HashTable.scala:149)
	at scala.collection.mutable.HashTable.addEntry$(HashTable.scala:148)
	at scala.collection.mutable.HashMap.addEntry(HashMap.scala:44)
	at scala.collection.mutable.HashTable.init(HashTable.scala:110)
	at scala.collection.mutable.HashTable.init$(HashTable.scala:89)
	at scala.collection.mutable.HashMap.init(HashMap.scala:44)
	at scala.collection.mutable.HashMap.readObject(HashMap.scala:195)
        ...
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:85)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
{noformat}
when DSv2 is enabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 12:59:13 UTC 2023,,,,,,,,,,"0|z1ghk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 09:58;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40364;;;","10/Mar/23 12:59;cloud_fan;Issue resolved by pull request 40364
[https://github.com/apache/spark/pull/40364];;;",,,,,,,,,,,,,
access apiserver by pod env,SPARK-42742,13527924,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ziqian hu,ziqian hu,10/Mar/23 06:28,10/Mar/23 06:57,30/Oct/23 17:26,,3.1.2,,,,,,,,,,,,,,,,,,,,,,,Kubernetes,,,,,0,,,,,"When start spark on k8s，driver pod  use spark.kubernetes.driver.master to get apiserver address. This config  us  [https://kubernetes.default.svc|https://kubernetes.default.svc/] as default and do not care about the apiserver port.

In our case, apiserver port is not 443 will driver will throw connectException. As k8s doc mentioned （https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/#directly-accessing-the-rest-api）, we can get master url by getting {{KUBERNETES_SERVICE_HOST}} and {{KUBERNETES_SERVICE_PORT_HTTPS}} environment variables from pod. So we add a new conf spark.kubernetes.driver.master.from.pod.env to allow driver get master url from env in cluster mode on k8s",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 06:57:19 UTC 2023,,,,,,,,,,"0|z1ghds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 06:57;ziqian hu;Created PR [https://github.com/apache/spark/pull/40361] for this issue.;;;",,,,,,,,,,,,,,
BlockManagerMaster requests removal of non-existent executor after executor has already gracefully decommissioned,SPARK-42738,13527844,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,yeachan153,yeachan153,09/Mar/23 15:34,09/Mar/23 15:34,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"During testing of graceful decommissioning, we noticed that we are getting a request to remove executors that have already been gracefully decommissioned:

{code:bash}
KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asked to remove non-existent executor 3
{code}

We enabled:

- spark.decommission.enabled 
- spark.storage.decommission.rddBlocks.enabled
- spark.storage.decommission.shuffleBlocks.enabled
- spark.storage.decommission.enabled
and set spark.storage.decommission.fallbackStorage.path to a path in our bucket. We are running Spark on Kubernetes.

The full driver logs can be found below:

{code:bash}
23/03/09 15:22:42 INFO KubernetesClusterSchedulerBackend: Decommission executors: 3
23/03/09 15:22:42 INFO BlockManagerMasterEndpoint: Mark BlockManagers (BlockManagerId(3, 100.96.5.11, 44707, None)) as being decommissioning.
23/03/09 15:22:42 WARN KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Received executor 1 decommissioned message
23/03/09 15:22:42 INFO KubernetesClusterSchedulerBackend: Decommission executors: 1
23/03/09 15:22:42 INFO BlockManagerMasterEndpoint: Mark BlockManagers (BlockManagerId(1, 100.96.5.9, 44491, None)) as being decommissioning.
23/03/09 15:22:42 WARN KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Received executor 2 decommissioned message
23/03/09 15:22:42 INFO KubernetesClusterSchedulerBackend: Decommission executors: 2
23/03/09 15:22:42 INFO BlockManagerMasterEndpoint: Mark BlockManagers (BlockManagerId(2, 100.96.5.10, 39011, None)) as being decommissioning.
23/03/09 15:22:44 ERROR TaskSchedulerImpl: Lost executor 3 on 100.96.5.11: Executor decommission.
23/03/09 15:22:44 INFO ExecutorMonitor: Executor 3 is removed. Remove reason statistics: (gracefully decommissioned: 1, decommision unfinished: 0, driver killed: 0, unexpectedly exited: 0).
23/03/09 15:22:44 INFO DAGScheduler: Executor lost: 3 (epoch 0)
23/03/09 15:22:44 ERROR TaskSchedulerImpl: Lost executor 1 on 100.96.5.9: Executor decommission.
23/03/09 15:22:44 INFO ExecutorMonitor: Executor 1 is removed. Remove reason statistics: (gracefully decommissioned: 2, decommision unfinished: 0, driver killed: 0, unexpectedly exited: 0).
23/03/09 15:22:44 ERROR TaskSchedulerImpl: Lost executor 2 on 100.96.5.10: Executor decommission.
23/03/09 15:22:44 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
23/03/09 15:22:44 INFO ExecutorMonitor: Executor 2 is removed. Remove reason statistics: (gracefully decommissioned: 3, decommision unfinished: 0, driver killed: 0, unexpectedly exited: 0).
23/03/09 15:22:44 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(3, 100.96.5.11, 44707, None)
23/03/09 15:22:44 INFO BlockManagerMaster: Removed 3 successfully in removeExecutor
23/03/09 15:22:44 INFO DAGScheduler: Shuffle files lost for executor: 3 (epoch 0)
23/03/09 15:22:44 INFO DAGScheduler: Executor lost: 1 (epoch 1)
23/03/09 15:22:45 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
23/03/09 15:22:45 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 100.96.5.9, 44491, None)
23/03/09 15:22:45 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
23/03/09 15:22:45 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 1)
23/03/09 15:22:45 INFO DAGScheduler: Executor lost: 2 (epoch 2)
23/03/09 15:22:45 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
23/03/09 15:22:45 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, 100.96.5.10, 39011, None)
23/03/09 15:22:45 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor
23/03/09 15:22:45 INFO DAGScheduler: Shuffle files lost for executor: 2 (epoch 2)
23/03/09 15:22:52 INFO BlockManagerMaster: Removal of executor 1 requested
23/03/09 15:22:52 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asked to remove non-existent executor 1
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-03-09 15:34:20.0,,,,,,,,,,"0|z1ggw0:",9223372036854775807,,,,,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle files lost with graceful decommission fallback storage enabled,SPARK-42737,13527842,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,yeachan153,yeachan153,09/Mar/23 15:26,10/Mar/23 10:59,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"During testing of graceful decommissioning, the driver logs indicate that shuffle files were lost - `DAGScheduler: Shuffle files lost for executor`:

{code:bash}
23/03/09 15:22:42 INFO KubernetesClusterSchedulerBackend: Decommission executors: 3
23/03/09 15:22:42 INFO BlockManagerMasterEndpoint: Mark BlockManagers (BlockManagerId(3, 100.96.5.11, 44707, None)) as being decommissioning.
23/03/09 15:22:42 WARN KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Received executor 1 decommissioned message
23/03/09 15:22:42 INFO KubernetesClusterSchedulerBackend: Decommission executors: 1
23/03/09 15:22:42 INFO BlockManagerMasterEndpoint: Mark BlockManagers (BlockManagerId(1, 100.96.5.9, 44491, None)) as being decommissioning.
23/03/09 15:22:42 WARN KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Received executor 2 decommissioned message
23/03/09 15:22:42 INFO KubernetesClusterSchedulerBackend: Decommission executors: 2
23/03/09 15:22:42 INFO BlockManagerMasterEndpoint: Mark BlockManagers (BlockManagerId(2, 100.96.5.10, 39011, None)) as being decommissioning.
23/03/09 15:22:44 ERROR TaskSchedulerImpl: Lost executor 3 on 100.96.5.11: Executor decommission.
23/03/09 15:22:44 INFO ExecutorMonitor: Executor 3 is removed. Remove reason statistics: (gracefully decommissioned: 1, decommision unfinished: 0, driver killed: 0, unexpectedly exited: 0).
23/03/09 15:22:44 INFO DAGScheduler: Executor lost: 3 (epoch 0)
23/03/09 15:22:44 ERROR TaskSchedulerImpl: Lost executor 1 on 100.96.5.9: Executor decommission.
23/03/09 15:22:44 INFO ExecutorMonitor: Executor 1 is removed. Remove reason statistics: (gracefully decommissioned: 2, decommision unfinished: 0, driver killed: 0, unexpectedly exited: 0).
23/03/09 15:22:44 ERROR TaskSchedulerImpl: Lost executor 2 on 100.96.5.10: Executor decommission.
23/03/09 15:22:44 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
23/03/09 15:22:44 INFO ExecutorMonitor: Executor 2 is removed. Remove reason statistics: (gracefully decommissioned: 3, decommision unfinished: 0, driver killed: 0, unexpectedly exited: 0).
23/03/09 15:22:44 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(3, 100.96.5.11, 44707, None)
23/03/09 15:22:44 INFO BlockManagerMaster: Removed 3 successfully in removeExecutor
23/03/09 15:22:44 INFO DAGScheduler: Shuffle files lost for executor: 3 (epoch 0)
23/03/09 15:22:44 INFO DAGScheduler: Executor lost: 1 (epoch 1)
23/03/09 15:22:45 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
23/03/09 15:22:45 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 100.96.5.9, 44491, None)
23/03/09 15:22:45 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
23/03/09 15:22:45 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 1)
23/03/09 15:22:45 INFO DAGScheduler: Executor lost: 2 (epoch 2)
23/03/09 15:22:45 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
23/03/09 15:22:45 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, 100.96.5.10, 39011, None)
23/03/09 15:22:45 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor
23/03/09 15:22:45 INFO DAGScheduler: Shuffle files lost for executor: 2 (epoch 2)
23/03/09 15:22:52 INFO BlockManagerMaster: Removal of executor 1 requested
23/03/09 15:22:52 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asked to remove non-existent executor 1
{code}

The decommission logs from the executor also seems to indicate that no shuffle data was necessary to migrate:

{code:java}
23/03/09 15:22:42 INFO CoarseGrainedExecutorBackend: Decommission executor 1.
23/03/09 15:22:42 INFO CoarseGrainedExecutorBackend: Will exit when finished decommissioning
23/03/09 15:22:42 INFO BlockManager: Starting block manager decommissioning process...
23/03/09 15:22:43 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
23/03/09 15:22:43 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
23/03/09 15:22:43 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
23/03/09 15:22:43 INFO BlockManagerDecommissioner: Starting block migration
23/03/09 15:22:43 INFO BlockManagerDecommissioner: Attempting to migrate all RDD blocks
23/03/09 15:22:43 INFO BlockManagerDecommissioner: Attempting to migrate all shuffle blocks
23/03/09 15:22:43 INFO BlockManagerDecommissioner: Start refreshing migratable shuffle blocks
23/03/09 15:22:44 INFO BlockManagerDecommissioner: Attempting to migrate all cached RDD blocks
23/03/09 15:22:44 INFO BlockManagerDecommissioner: 0 of 0 local shuffles are added. In total, 0 shuffles are remained.
23/03/09 15:22:44 INFO BlockManagerDecommissioner: Starting shuffle block migration thread for BlockManagerId(fallback, remote, 7337, None)
23/03/09 15:22:44 INFO BlockManagerDecommissioner: Finished current round refreshing migratable shuffle blocks, waiting for 30000ms before the next round refreshing.
23/03/09 15:22:44 WARN BlockManagerDecommissioner: Asked to decommission RDD cache blocks, but no blocks to migrate
23/03/09 15:22:44 INFO BlockManagerDecommissioner: Finished current round RDD blocks migration, waiting for 30000ms before the next round migration.
23/03/09 15:22:44 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
23/03/09 15:22:44 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
23/03/09 15:22:44 INFO CoarseGrainedExecutorBackend: No running tasks, all blocks migrated, stopping.
23/03/09 15:22:44 INFO CoarseGrainedExecutorBackend: Executor self-exiting due to : Finished decommissioning
23/03/09 15:22:44 INFO BlockManagerDecommissioner: Stop RDD blocks migration().
23/03/09 15:22:44 INFO BlockManagerDecommissioner: Stop refreshing migratable shuffle blocks.
23/03/09 15:22:44 INFO BlockManagerDecommissioner: Stopping migrating shuffle blocks.
23/03/09 15:22:44 INFO BlockManagerDecommissioner: Stopped block migration
23/03/09 15:22:44 INFO BlockManagerDecommissioner: Stop shuffle block migration().
{code}


This seems incorrect as there were no shuffle files to migrate to begin with. We enabled:

- spark.decommission.enabled 
- spark.storage.decommission.rddBlocks.enabled
- spark.storage.decommission.shuffleBlocks.enabled
- spark.storage.decommission.enabled
and set spark.storage.decommission.fallbackStorage.path to a path in our bucket.

The same message was also shown when there were actually shuffle files that were stored in the bucket.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-03-09 15:26:25.0,,,,,,,,,,"0|z1ggvk:",9223372036854775807,,,,,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deal with timestamp partition column when spark.sql.storeAssignmentPolicy=LEGACY,SPARK-42734,13527816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Abandoned,,Resol1992,Resol1992,09/Mar/23 13:47,10/Mar/23 03:01,30/Oct/23 17:26,10/Mar/23 03:01,3.1.0,3.3.1,3.4.0,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"We create a table which has a partition column with timestamp type, when we execute `alter table partition rename to partition`, it probably fail.

We can reproduce it with the following sql:
{code:java}
set spark.sql.storeAssignmentPolicy = LEGACY;
create table t1(c_timestamp timestamp) using parquet partitioned by (p_id int, p_timestamp timestamp); 
insert into t1 partition(p_id=1, p_timestamp='2016-08-01 11:45:15.0') values('2016-8-1 11:45:15.0');
alter table t1 partition(p_id=1, p_timestamp='2016-08-01 11:45:15.0')  rename to partition(p_id=2, p_timestamp='2016-08-01 11:45:15.0');{code}
The above case will fail with the following error.

Partition not found in table 't1' database 'default':
p_id -> 1
p_timestamp -> 2016-08-01 11:45:15.0
org.apache.spark.sql.catalyst.analysis.NoSuchPartitionException: Partition not found in table 't1' database 'default':
p_id -> 1
p_timestamp -> 2016-08-01 11:45:15.0
    at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.$anonfun$requirePartitionsExist$1(InMemoryCatalog.scala:82)
    at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.$anonfun$requirePartitionsExist$1$adapted(InMemoryCatalog.scala:80)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requirePartitionsExist(InMemoryCatalog.scala:80)
    at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.renamePartitions(InMemoryCatalog.scala:494)
    at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.renamePartitions(ExternalCatalogWithListener.scala:219)
    at org.apache.spark.sql.catalyst.catalog.SessionCatalog.renamePartitions(SessionCatalog.scala:1213)
    at org.apache.spark.sql.execution.command.AlterTableRenamePartitionCommand.run(ddl.scala:568)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:111)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:171)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
    at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
    at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
    at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
    at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)
    at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    at org.apache.spark.sql.test.SQLTestUtilsBase.$anonfun$sql$1(SQLTestUtils.scala:232)
    at org.apache.spark.sql.SQLQuerySuite.$anonfun$new$991(SQLQuerySuite.scala:4747)
    at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54)
    at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38)
    at org.apache.spark.sql.SQLQuerySuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(SQLQuerySuite.scala:60)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:247)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:245)
    at org.apache.spark.sql.SQLQuerySuite.withSQLConf(SQLQuerySuite.scala:60)
    at org.apache.spark.sql.SQLQuerySuite.$anonfun$new$990(SQLQuerySuite.scala:4741)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:306)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:304)
    at org.apache.spark.sql.SQLQuerySuite.withTable(SQLQuerySuite.scala:60)
    at org.apache.spark.sql.SQLQuerySuite.$anonfun$new$989(SQLQuerySuite.scala:4741)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
    at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
    at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
    at org.scalatest.Transformer.apply(Transformer.scala:22)
    at org.scalatest.Transformer.apply(Transformer.scala:20)
    at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
    at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)
    at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
    at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
    at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
    at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)
    at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
    at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
    at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)
    at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
    at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
    at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
    at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
    at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
    at org.scalatest.Suite.run(Suite.scala:1114)
    at org.scalatest.Suite.run$(Suite.scala:1096)
    at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
    at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
    at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
    at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
    at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
    at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)
    at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
    at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
    at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
    at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)
    at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:47)
    at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1321)
    at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1315)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1315)
    at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:992)
    at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:970)
    at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1481)
    at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:970)
    at org.scalatest.tools.Runner$.run(Runner.scala:798)
    at org.scalatest.tools.Runner.run(Runner.scala)
    at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2or3(ScalaTestRunner.java:38)
    at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:25)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 03:01:07 UTC 2023,,,,,,,,,,"0|z1ggps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 03:01;Resol1992;The following PR has fixed this issue

[SPARK-40798][SQL] Alter partition should verify value follow storeAssignmentPolicy;;;",,,,,,,,,,,,,,
Support executing spark commands in the root directory when local mode is specified,SPARK-42727,13527769,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,xiaoping.huang,xiaoping.huang,09/Mar/23 09:06,06/Oct/23 00:17,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 09 09:23:20 UTC 2023,,,,,,,,,,"0|z1ggfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/23 09:22;apachespark;User 'huangxiaopingRD' has created a pull request for this issue:
https://github.com/apache/spark/pull/40351;;;","09/Mar/23 09:23;apachespark;User 'huangxiaopingRD' has created a pull request for this issue:
https://github.com/apache/spark/pull/40351;;;",,,,,,,,,,,,,
DataSourceV2 cannot report KeyGroupedPartitioning with multiple keys per partition,SPARK-42716,13527620,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,EnricoMi,EnricoMi,08/Mar/23 11:14,10/Oct/23 00:17,30/Oct/23 17:26,,3.3.0,3.3.1,3.3.2,3.4.0,3.4.1,,,,,,,,,,,,,,,,,,,SQL,,,,,0,pull-request-available,,,,"From Spark 3.0.0 until 3.2.3, a DataSourceV2 could report its partitioning as {{KeyGroupedPartitioning}} via {{SupportsReportPartitioning}}, even if multiple keys belong to a partition.

With SPARK-37377, only if all partitions implement {{HasPartitionKey}}, the partition information reported through {{SupportsReportPartitioning}} is considered by catalyst. But this limits the number of keys per partition to 1.

Spark should continue to support the more general situation of {{KeyGroupedPartitioning}} with multiple keys per partition, like {{HashPartitioning}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 08 11:23:06 UTC 2023,,,,,,,,,,"0|z1gfi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/23 11:22;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40334;;;","08/Mar/23 11:23;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40334;;;",,,,,,,,,,,,,
Sparksql temporary file conflict,SPARK-42714,13527589,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,hao.duan,hao.duan,08/Mar/23 07:17,10/Mar/23 17:10,30/Oct/23 17:26,,3.3.2,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"When sparksql inserts overwrite, the name of the temporary file in the middle is not unique. This will cause that when multiple applications write different partition data to the same partition table, it will be possible to delete each other's temporary files between applications, resulting in task failure

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 17:10:47 UTC 2023,,,,,,,,,,"0|z1gfbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/23 07:19;hao.duan;This problem will cause the task to throw the problem of deleting the current temporary file. The detailed error is as follows：
 -----------------------------------------

User class threw exception: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at com.ly.process.SparkSQL.main(SparkSQL.java:55)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:732)
Caused by: java.io.FileNotFoundException: File /ns-tcly/com/xxxx/_temporary/0/task_202303070204281920649928402071557_0031_m_001866/type=2 does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:1058)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$1000(DistributedFileSystem.java:131)
	at org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1118)
	at org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1115)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:1125)
	at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:270)
	at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.listStatus(ChRootedFileSystem.java:255)
	at org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:411)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:484)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:403)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:375)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:220)
	... 25 more;;;","10/Mar/23 17:10;srowen;Not really enough info here. How does it happen?;;;",,,,,,,,,,,,,
Do not rely on __file__,SPARK-42709,13527576,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,08/Mar/23 05:09,08/Mar/23 18:00,30/Oct/23 17:26,08/Mar/23 18:00,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,PySpark,,,,,0,,,,,We have a lot of places using __file__ which is actually optional. We shouldn't reply on them,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 08 18:00:01 UTC 2023,,,,,,,,,,"0|z1gf8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/23 05:52;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/40328;;;","08/Mar/23 18:00;dongjoon;Issue resolved by pull request 40328
[https://github.com/apache/spark/pull/40328];;;",,,,,,,,,,,,,
SubqueryAlias should propagate metadata columns its child already selects ,SPARK-42704,13527495,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ryan.johnson@databricks.com,ryan.johnson@databricks.com,07/Mar/23 17:43,07/Mar/23 18:08,30/Oct/23 17:26,,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"The `AddMetadataColumns` analyzer rule intends to make resolve available metadata columns, even if the plan already contains projections that did not explicitly mention the metadata column.

The `SubqueryAlias` plan node intentionally does not propagate metadata columns automatically from a non-leaf/non-subquery child node, because the following should _not_ work:

 
{code:java}
spark.read.table(""t"").select(""a"", ""b"").as(""s"").select(""_metadata""){code}
However, today it is too strict in breaks the metadata chain, in case the child node's output already includes the metadata column:

 
{code:java}
// expected to work (and does)
spark.read.table(""t"")
  .select(""a"", ""b"").select(""_metadata"")

// by extension, should also work (but does not)
spark.read.table(""t"").select(""a"", ""b"", ""_metadata"").as(""s"")
  .select(""a"", ""b"").select(""_metadata""){code}
The solution is for `SubqueryAlias` to always propagate metadata columns that are already in the child's output, thus preserving the `metadataOutput` chain for that column.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 07 18:08:42 UTC 2023,,,,,,,,,,"0|z1geqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/23 18:08;apachespark;User 'ryan-johnson-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/40321;;;",,,,,,,,,,,,,,
Add h2 as test dependency of connect-server module,SPARK-42700,13527451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,07/Mar/23 12:40,08/Mar/23 04:46,30/Oct/23 17:26,08/Mar/23 04:46,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Build,Tests,,,,0,,,,,"run 
 # mvn clean install -DskipTests -pl connector/connect/server -am
 # mvn test -pl connector/connect/server

{code:java}
*** RUN ABORTED ***
  java.lang.ClassNotFoundException: org.h2.Driver
  at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)
  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
  at java.base/java.lang.Class.forName0(Native Method)
  at java.base/java.lang.Class.forName(Class.java:398)
  at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
  at org.apache.spark.sql.connect.ProtoToParsedPlanTestSuite.beforeAll(ProtoToParsedPlanTestSuite.scala:68)
  at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)
  at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
  at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
  ...
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 08 04:46:50 UTC 2023,,,,,,,,,,"0|z1gego:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/23 12:56;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40317;;;","08/Mar/23 04:46;gurwls223;Issue resolved by pull request 40317
[https://github.com/apache/spark/pull/40317];;;",,,,,,,,,,,,,
/api/v1/applications return 0 for duration,SPARK-42697,13527432,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,07/Mar/23 10:12,09/Mar/23 05:35,30/Oct/23 17:26,09/Mar/23 05:35,3.1.3,3.2.3,3.3.2,3.4.0,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,Web UI,,,,,0,,,,,which should be total uptime,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 09 05:35:14 UTC 2023,,,,,,,,,,"0|z1gecg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/23 10:50;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40313;;;","09/Mar/23 05:35;Qin Yao;Issue resolved by pull request 40313
[https://github.com/apache/spark/pull/40313];;;",,,,,,,,,,,,,
Data duplication and loss occur after executing 'insert overwrite...' in Spark 3.1.1,SPARK-42694,13527390,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,bigdata_feng,bigdata_feng,07/Mar/23 07:51,17/Aug/23 02:32,30/Oct/23 17:26,,3.1.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,shuffle,spark,,,"We are currently using Spark version 3.1.1 in our production environment. We have noticed that occasionally, after executing 'insert overwrite ... select', the resulting data is inconsistent, with some data being duplicated or lost. This issue does not occur all the time and seems to be more prevalent on large tables with tens of millions of records.

We compared the execution plans for two runs of the same SQL and found that they were identical. In the case where the SQL was executed successfully, the amount of data being written and read during the shuffle stage was the same. However, in the case where the problem occurred, the amount of data being written and read during the shuffle stage was different. Please see the attached screenshots for the write/read data during shuffle stage.
 
Normal SQL:
!image-2023-03-07-15-59-08-818.png!


SQL with issues:
!image-2023-03-07-15-59-27-665.png!
 
Is this problem caused by a bug in version 3.1.1, specifically (SPARK-34534): 'New protocol FetchShuffleBlocks in OneForOneBlockFetcher lead to data loss or correctness'? Or is it caused by something else? What could be the root cause of this problem?","Spark 3.1.1

Hadoop 3.2.1

Hive 3.1.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/23 07:59;bigdata_feng;image-2023-03-07-15-59-08-818.png;https://issues.apache.org/jira/secure/attachment/13056090/image-2023-03-07-15-59-08-818.png","07/Mar/23 07:59;bigdata_feng;image-2023-03-07-15-59-27-665.png;https://issues.apache.org/jira/secure/attachment/13056091/image-2023-03-07-15-59-27-665.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Aug 17 02:32:43 UTC 2023,,,,,,,,,,"0|z1ge34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/23 09:10;yumwang;Could you upgrade to Spark 3.1.3 or Spark 3.3.2?;;;","07/Mar/23 19:52;bjornjorgensen;Spark 3.1 [is EOL|https://github.com/apache/spark-website/commit/40f58f884bd258d6a332d583dc91c717b6b461f0 ] 
Try Spark 3.3.2 or 3.2.3 ;;;","08/Mar/23 02:09;bigdata_feng;[~yumwang] 
This version has been running in production environment for over a year, and upgrading now would have a significant impact. Upgrading to 3.3.2 requires retesting and validation of the associated Ranger and Spark permission plugins. Therefore, the only option for a short-term upgrade is to choose 3.1.3. However, it's unclear whether upgrading to 3.1.3 will solve the problem since the cause of the issue is unknown, as it only occurs occasionally, which is confusing us.;;;","08/Mar/23 02:09;bigdata_feng;[~bjornjorgensen] 
As the current upgrade would have a significant impact, is there any other faster way to locate and solve the problem besides upgrading?;;;","17/Aug/23 02:32;bigboy001;Are there any exeptions when the data-loss occur?;;;",,,,,,,,,,
"When I execute the spark-shell command, ""WARN ui.JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready."" will appear.",SPARK-42682,13527233,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,lijie1912,lijie1912,06/Mar/23 10:11,07/Mar/23 01:22,30/Oct/23 17:26,06/Mar/23 12:20,3.2.3,,,,,,,,,,,,,,,,,,,,,,,Spark Shell,,,,,0,,,,,"!image-2023-03-06-18-12-51-441.png!

 

2023-03-06 17:11:52,464 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
2023-03-06 17:11:57,217 WARN ui.JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.
java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.
    at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)
    at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)
    at org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)
    at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:503)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
    at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
    at org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1631)
    at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
    at org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
    at org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
    at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
    at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)
    at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    at org.sparkproject.jetty.server.Server.handle(Server.java:516)
    at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:400)
    at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:645)
    at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:392)
    at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
    at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)
    at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
    at java.lang.Thread.run(Thread.java:748)
2023-03-06 17:11:57,221 WARN server.HttpChannel: /jobs/
java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.
    at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)
    at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)
    at org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)
    at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:503)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
    at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
    at org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1631)
    at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
    at org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
    at org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
    at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
    at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)
    at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    at org.sparkproject.jetty.server.Server.handle(Server.java:516)
    at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:400)
    at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:645)
    at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:392)
    at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
    at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)
    at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
    at java.lang.Thread.run(Thread.java:748)
2023-03-06 17:11:57,251 WARN ui.JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.
java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.
    at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)
    at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)
    at org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)
    at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:503)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
    at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
    at org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1631)
    at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
    at org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
    at org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
    at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
    at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)
    at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    at org.sparkproject.jetty.server.Server.handle(Server.java:516)
    at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:400)
    at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:645)
    at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:392)
    at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
    at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)
    at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
    at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
    at java.lang.Thread.run(Thread.java:748)
2023-03-06 17:11:57,252 WARN server.HttpChannel: /jobs/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/23 10:12;lijie1912;image-2023-03-06-18-12-51-441.png;https://issues.apache.org/jira/secure/attachment/13056069/image-2023-03-06-18-12-51-441.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 07 01:22:10 UTC 2023,,,,,,,,,,"0|z1gd4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/23 10:14;lijie1912;Have anyone ever encountered this kind of problem？

 

请问有人遇到过这种问题么;;;","06/Mar/23 12:20;yumwang;This is because the spark context is not fully started.;;;","07/Mar/23 01:22;lijie1912;[~yumwang]  Why does this happen and how to avoid it;;;",,,,,,,,,,,,
"Relax ordering constraint for ALTER TABLE ADD|REPLACE column options",SPARK-42681,13527202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vli-databricks,vli-databricks,vli-databricks,06/Mar/23 08:36,08/Mar/23 04:04,30/Oct/23 17:26,08/Mar/23 04:04,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"Currently the grammar for ALTER TABLE ADD|REPLACE column is:

qualifiedColTypeWithPosition
    : name=multipartIdentifier dataType (NOT NULL)? defaultExpression? commentSpec? colPosition?
    ;

This enforces a constraint on the order of: (NOT NULL, DEFAULT value, COMMENT value FIRST|AFTER value). We can update the grammar to allow these options in any order instead, to improve usability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 08 04:04:59 UTC 2023,,,,,,,,,,"0|z1gcxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/23 08:43;apachespark;User 'vitaliili-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/40295;;;","08/Mar/23 04:04;Gengliang.Wang;Issue resolved by pull request 40295
[https://github.com/apache/spark/pull/40295];;;",,,,,,,,,,,,,
Fix the invalid tests for broadcast hint,SPARK-42677,13527168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,06/Mar/23 04:05,06/Mar/23 08:17,30/Oct/23 17:26,06/Mar/23 08:17,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"Currently, there are a lot of test cases for broadcast hint is invalid. Because the data size is smaller than broadcast threshold.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Mar 06 08:17:42 UTC 2023,,,,,,,,,,"0|z1gcq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/23 04:12;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/40293;;;","06/Mar/23 04:13;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/40293;;;","06/Mar/23 08:17;podongfeng;Issue resolved by pull request 40293
[https://github.com/apache/spark/pull/40293];;;",,,,,,,,,,,,
Make build/mvn build Spark only with the verified maven version,SPARK-42673,13527115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,05/Mar/23 08:21,06/Mar/23 19:07,30/Oct/23 17:26,06/Mar/23 19:06,3.2.4,3.3.3,3.4.0,,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,Build,,,,,0,,,,,GA ,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-42380,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Mar 06 19:06:37 UTC 2023,,,,,,,,,,"0|z1gce8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/23 08:24;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40283;;;","06/Mar/23 19:06;dongjoon;Issue resolved by pull request 40283
[https://github.com/apache/spark/pull/40283];;;",,,,,,,,,,,,,
Fix bug for createDataFrame from complex type schema,SPARK-42671,13527102,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,05/Mar/23 02:21,15/Mar/23 21:31,30/Oct/23 17:26,06/Mar/23 02:08,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Mar 05 02:43:19 UTC 2023,,,,,,,,,,"0|z1gcbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/23 02:42;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40280;;;","05/Mar/23 02:43;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40280;;;",,,,,,,,,,,,,
`simple udf` test failed using Maven ,SPARK-42665,13527027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,03/Mar/23 17:41,07/Mar/23 07:36,30/Oct/23 17:26,07/Mar/23 07:36,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Connect,,,,,0,,,,,"{code:java}
simple udf *** FAILED ***
  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.ClientE2ETestSuite
  at io.grpc.Status.asRuntimeException(Status.java:535)
  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  at org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:61)
  at org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:106)
  at org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:123)
  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2426)
  at org.apache.spark.sql.Dataset.withResult(Dataset.scala:2747)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2425)
  at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$8(ClientE2ETestSuite.scala:85)
  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 07 07:36:37 UTC 2023,,,,,,,,,,"0|z1gbuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/23 16:53;apachespark;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/40304;;;","06/Mar/23 16:53;apachespark;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/40304;;;","06/Mar/23 16:54;apachespark;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/40304;;;","07/Mar/23 07:36;gurwls223;Issue resolved by pull request 40304
[https://github.com/apache/spark/pull/40304];;;",,,,,,,,,,,
CSV Reader - multiline without quoted fields,SPARK-42661,13526976,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,flof076,flof076,03/Mar/23 11:16,10/Mar/23 17:36,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Hello,

We are facing an issue with the CSV format.
When we try to read a ""multiline file without quoted fields"" the expected result is not good.

With quoted fields, all is ok. ( cf the screenshot ) 

You can reproduce it easily with this code (just replace file path ) :
{code:java}
spark.read.options(Map(
        ""multiline"" -> ""true"",
        ""quote"" -> """",
        ""header"" -> ""true"",
      )).csv(""/Users/fferreira/correct_multiline.csv"").show(false)

spark.read.options(Map(
        ""multiline"" -> ""true"",
        ""header"" -> ""true"",      )).csv(""/Users/fferreira/correct_multiline_with_quote.csv"").show(false)
{code}
We continue to investigate on our side.

Thanks you.

!image-2023-03-03-12-11-21-258.png!","unquoted data
{code}
NAME,Address,CITY
Atlassian,Level 6 341 George Street
Sydney NSW 2000 Australia,Sydney
Github,88 Colin P Kelly Junior Street
San Francisco CA 94107 USA,San Francisco
{code}

quoted data : 
{code}
""NAME"",""Address"",""CITY""
""Atlassian"",""Level 6 341 George Street
Sydney NSW 2000 Australia"",""Sydney""
""Github"",""88 Colin P Kelly Junior Street
San Francisco CA 94107 USA"",""San Francisco""
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/23 11:18;flof076;Capture d’écran 2023-03-03 à 12.18.07.png;https://issues.apache.org/jira/secure/attachment/13056008/Capture+d%E2%80%99e%CC%81cran+2023-03-03+a%CC%80+12.18.07.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 17:36:27 UTC 2023,,,,,,,,,,"0|z1gbjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 17:36;srowen;I don't know that multi-line makes sense without quoting in your case, as you have values broken across lines. You should quote;;;",,,,,,,,,,,,,,
Incorrect ambiguous column reference error,SPARK-42655,13526898,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,unamesk15,unamesk15,unamesk15,02/Mar/23 18:43,04/Apr/23 13:17,30/Oct/23 17:26,04/Apr/23 13:16,3.2.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,,,0,,,,,"val df1 = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(""id"",""col2"",""col3"",""col4"", ""col5"")
val op_cols_same_case = List(""id"",""col2"",""col3"",""col4"", ""col5"", ""id"")
val df2 = df1.select(op_cols_same_case.head, op_cols_same_case.tail: _*)
df2.select(""id"").show()
 
This query runs fine.
 
But when we change the casing of the op_cols to have mix of upper & lower case (""id"" & ""ID"") it throws an ambiguous col ref error:
 
val df1 = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(""id"",""col2"",""col3"",""col4"", ""col5"")
val op_cols_mixed_case = List(""id"",""col2"",""col3"",""col4"", ""col5"", ""ID"")
val df3 = df1.select(op_cols_mixed_case.head, op_cols_mixed_case.tail: _*)
df3.select(""id"").show()



org.apache.spark.sql.AnalysisException: Reference 'id' is ambiguous, could be: id, id.

  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:363)

  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:112)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$resolveExpressionByPlanChildren$1(Analyzer.scala:1857)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$resolveExpression$2(Analyzer.scala:1787)

  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:60)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.innerResolve$1(Analyzer.scala:1794)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpression(Analyzer.scala:1812)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpressionByPlanChildren(Analyzer.scala:1863)

  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$17.$anonfun$applyOrElse$94(Analyzer.scala:1577)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)

  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:209)

  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)

  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

  at scala.collection.TraversableLike.map(TraversableLike.scala:286)

  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)

  at scala.collection.AbstractTraversable.map(Traversable.scala:108)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:209)

 


Since, Spark is case insensitive, it should work for second case also when we have upper and lower case column names in the column list.

It also works fine in Spark 2.3.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Apr 04 13:16:42 UTC 2023,,,,,,,,,,"0|z1gb28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/23 20:19;apachespark;User 'shrprasa' has created a pull request for this issue:
https://github.com/apache/spark/pull/40258;;;","04/Apr/23 13:16;cloud_fan;Issue resolved by pull request 40258
[https://github.com/apache/spark/pull/40258];;;",,,,,,,,,,,,,
insert overwrite table will casue table location lost if java.lang.ArithmeticException is thrown,SPARK-42650,13526823,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kevinshin,kevinshin,02/Mar/23 09:29,08/Mar/23 03:05,30/Oct/23 17:26,,3.2.3,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"When use [KyuubiSparkSQLExtension|https://kyuubi.readthedocs.io/en/v1.6.1-incubating/extensions/engines/spark/] and when a `insert overwrite` statment meet exception ，a no partion table's home directory will lost ,partion table will lost partion directory.
 
my spark-defaults.conf config : 
spark.sql.extensions org.apache.kyuubi.sql.KyuubiSparkSQLExtension
 

because I can't reopen SPARK-42550 , for detail and reproduce please reference: 

https://issues.apache.org/jira/browse/SPARK-42550

 ",,,,,,,,,,SPARK-42550,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 07 10:43:45 UTC 2023,,,,,,,,,,"0|z1galk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/23 03:23;yumwang;It seems like a Kyuubi bug?;;;","03/Mar/23 06:42;kevinshin;Spark and Kyuubi are both belong to apache.

May Apache community help to figure out the detail of this issue? Will this issue keep exist the next releases.;;;","03/Mar/23 07:50;ulysses;To be clear,  it is the issue of Spark 3.2.3. Spark3.2.1, 3.3.x and master are fine.

It can be reproduced by:

{code:java}

CREATE TABLE IF NOT EXISTS spark32_overwrite(amt1 int) STORED AS ORC;
CREATE TABLE IF NOT EXISTS spark32_overwrite2(amt1 long) STORED AS ORC;

INSERT OVERWRITE TABLE spark32_overwrite2 select 6000044164;

set spark.sql.ansi.enabled=true;
INSERT OVERWRITE TABLE spark32_overwrite select amt1 from (select cast(amt1 as int) as amt1 from spark32_overwrite2 distribute by amt1);

{code}
;;;","06/Mar/23 01:39;kevinshin;Thanks [~ulysses] , I can reproduced it without any spark's extension.;;;","06/Mar/23 03:18;yumwang;It seems to be caused by https://github.com/apache/spark/pull/38358.;;;","06/Mar/23 09:27;kevinshin;Thank you @[~yumwang] ;;;","07/Mar/23 10:43;gurwls223;[~kevinshin] mind self-contained reproducer without Kyuubi? So is this a regression from SPARK-40588? Please fix the JIRA title to summarize the issue.;;;",,,,,,,,
Remove the standard Apache License header from the top of third-party source files,SPARK-42649,13526815,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,02/Mar/23 08:12,02/Mar/23 09:05,30/Oct/23 17:26,02/Mar/23 09:05,1.1.1,1.2.2,1.3.1,1.4.1,1.5.2,1.6.3,2.1.3,2.2.3,2.3.4,2.4.8,3.0.3,3.1.3,3.2.3,3.3.2,3.4.0,,,,,3.2.4,3.3.3,3.4.1,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 02 09:05:59 UTC 2023,,,,,,,,,,"0|z1gajs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/23 08:33;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40249;;;","02/Mar/23 09:05;dongjoon;Issue resolved by pull request 40249
[https://github.com/apache/spark/pull/40249];;;",,,,,,,,,,,,,
Add `hive` dependency to `connect` module,SPARK-42644,13526797,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,02/Mar/23 04:49,02/Mar/23 06:46,30/Oct/23 17:26,02/Mar/23 06:46,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Project Infra,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 02 06:46:45 UTC 2023,,,,,,,,,,"0|z1gafs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/23 05:31;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40246;;;","02/Mar/23 06:46;dongjoon;Issue resolved by pull request 40246
[https://github.com/apache/spark/pull/40246];;;",,,,,,,,,,,,,
"current_user() is blocked from VALUES, but current_timestamp() is not",SPARK-42638,13526747,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,srielau,srielau,01/Mar/23 20:30,04/Mar/23 06:26,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"VALUES(current_user());
returns:

cannot evaluate expression current_user() in inline table definition.; line 1 pos 8

 

The same with current_timestamp() works.

It appears current_user() is recognized as non-deterministic. But it is constant within the statement, just like current_timestanmp().

PS: It's not clear why we block non-deterministic functions to begin with....",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Mar 04 06:26:01 UTC 2023,,,,,,,,,,"0|z1ga4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/23 06:26;Zing;Maybe we can use `insert as select` to achieve the same effect?;;;",,,,,,,,,,,,,,
Several counter-intuitive behaviours in the TimestampAdd expression,SPARK-42635,13526728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mashplant,mashplant,mashplant,01/Mar/23 18:16,04/Mar/23 19:16,30/Oct/23 17:26,03/Mar/23 06:39,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,,,,,3.3.3,3.4.1,,,SQL,,,,,0,,,,,"# When the time is close to daylight saving time transition, the result may be discontinuous and not monotonic.

We currently have:
{code:scala}
scala> spark.conf.set(""spark.sql.session.timeZone"", ""America/Los_Angeles"")
scala> spark.sql(""select timestampadd(second, 24 * 3600 - 1, timestamp'2011-03-12 03:00:00')"").show
+------------------------------------------------------------------------+
|timestampadd(second, ((24 * 3600) - 1), TIMESTAMP '2011-03-12 03:00:00')|
+------------------------------------------------------------------------+
|                                                     2011-03-13 03:59:59|
+------------------------------------------------------------------------+
scala> spark.sql(""select timestampadd(second, 24 * 3600, timestamp'2011-03-12 03:00:00')"").show
+------------------------------------------------------------------+
|timestampadd(second, (24 * 3600), TIMESTAMP '2011-03-12 03:00:00')|
+------------------------------------------------------------------+
|                                               2011-03-13 03:00:00|
+------------------------------------------------------------------+ {code}
 

In the second query, adding one more second will set the time back one hour instead. Plus, there are only {{23 * 3600}} seconds from {{2011-03-12 03:00:00}} to {{2011-03-13 03:00:00}}, instead of {{24 * 3600}} seconds, due to the daylight saving time transition.

The root cause of the problem is the Spark code at [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L790] wrongly assumes every day has {{MICROS_PER_DAY}} seconds, and does the day and time-in-day split before looking at the timezone.

2. Adding month, quarter, and year silently ignores Int overflow during unit conversion.

The root cause is [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L1246]. {{quantity}} is multiplied by {{3}} or {{MONTHS_PER_YEAR}} without checking overflow. Note that we do have overflow checking in adding the amount to the timestamp, so the behavior is inconsistent.

This can cause counter-intuitive results like this:

{code:scala}
scala> spark.sql(""select timestampadd(quarter, 1431655764, timestamp'1970-01-01')"").show
+------------------------------------------------------------------+
|timestampadd(quarter, 1431655764, TIMESTAMP '1970-01-01 00:00:00')|
+------------------------------------------------------------------+
|                                               1969-09-01 00:00:00|
+------------------------------------------------------------------+{code}

3. Adding sub-month units (week, day, hour, minute, second, millisecond, microsecond)silently ignores Long overflow during unit conversion.

This is similar to the previous problem:

{code:scala}
 scala> spark.sql(""select timestampadd(day, 106751992, timestamp'1970-01-01')"").show(false)
+-------------------------------------------------------------+
|timestampadd(day, 106751992, TIMESTAMP '1970-01-01 00:00:00')|
+-------------------------------------------------------------+
|-290308-12-22 15:58:10.448384                                |
+-------------------------------------------------------------+{code}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 03 06:53:04 UTC 2023,,,,,,,,,,"0|z1ga0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/23 19:30;apachespark;User 'chenhao-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/40237;;;","03/Mar/23 06:39;maxgekk;Issue resolved by pull request 40237
[https://github.com/apache/spark/pull/40237];;;","03/Mar/23 06:53;apachespark;User 'chenhao-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/40264;;;",,,,,,,,,,,,
Upgrade zstd-jni to 1.5.4-2,SPARK-42625,13526582,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,28/Feb/23 21:54,01/Mar/23 01:17,30/Oct/23 17:26,01/Mar/23 01:17,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 01 01:17:27 UTC 2023,,,,,,,,,,"0|z1g940:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 21:55;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40225;;;","01/Mar/23 01:17;dongjoon;Issue resolved by pull request 40225
[https://github.com/apache/spark/pull/40225];;;",,,,,,,,,,,,,
parameter markers not blocked in DDL,SPARK-42623,13526563,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,srielau,srielau,28/Feb/23 17:34,14/Mar/23 15:42,30/Oct/23 17:26,10/Mar/23 02:35,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,,,0,,,,,"The parameterized query code does not block DDL statements from referencing parameter markers.
E.g. a 

 
{code:java}
scala> spark.sql(sqlText = ""CREATE VIEW v1 AS SELECT current_timestamp() + :later as stamp, :x * :x AS square"", args = Map(""later"" -> ""INTERVAL'3' HOUR"", ""x"" -> ""15.0"")).show()
++
||
++
++
{code}
It appears we have some protection that fails us when the view is invoked:

 
{code:java}
scala> spark.sql(sqlText = ""SELECT * FROM v1"", args = Map(""later"" -> ""INTERVAL'3' HOUR"", ""x"" -> ""15.0"")).show()
org.apache.spark.sql.AnalysisException: [UNBOUND_SQL_PARAMETER] Found the unbound parameter: `later`. Please, fix `args` and provide a mapping of the parameter to a SQL literal.; line 1 pos 29
{code}

Right now I think affected are:
* DEFAULT definition
* VIEW definition

but any other future standard expression popping up is at risk, such as SQL Functions, or GENERATED COLUMN.

CREATE TABLE AS is debatable, since it it executes the query at definition only.
For simplicity I propose to block the feature from ANY DDL statement (CREATE, ALTER).

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 02:35:51 UTC 2023,,,,,,,,,,"0|z1g8zs:",9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,,,"08/Mar/23 04:32;cloud_fan;I think the problem occurs when we store the original SQL text, like CREATE VIEW, or GENERATED COLUMN. A proper fix should be replacing the placeholder in the original SQL text with the actual parameters, before storing them. But this would be a bit hard to implement. Given that the main motivation of SQL parameters is protecting from SQL injection, I think applying it in SELECT query should be sufficient.;;;","08/Mar/23 09:15;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40333;;;","10/Mar/23 01:50;gurwls223;Reverted in https://github.com/apache/spark/commit/b36966f6588f92548f9edad73ffd1f5795503780 and https://github.com/apache/spark/commit/12c7e75f119f6d49ce3608da6b2a34f71c0fa0aa;;;","10/Mar/23 02:35;cloud_fan;Issue resolved by pull request 40333
[https://github.com/apache/spark/pull/40333];;;",,,,,,,,,,,
StackOverflowError reading json that does not conform to schema,SPARK-42622,13526546,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jelmer1,jelmer1,jelmer1,28/Feb/23 15:11,02/Mar/23 14:44,30/Oct/23 17:26,02/Mar/23 14:44,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,,,,Input/Output,,,,,0,,,,,"Databricks runtime 12.1 uses a pre-release version of spark 3.4.x we encountered the following problem

 

!https://user-images.githubusercontent.com/133639/221866500-99f187a0-8db3-42a7-85ca-b027fdec160d.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,,,,,Thu Mar 02 14:44:10 UTC 2023,,,,,,,,,,"0|z1g8w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 15:12;jelmer1;Patch available here https://github.com/apache/spark/pull/40219;;;","28/Feb/23 15:53;apachespark;User 'jelmerk' has created a pull request for this issue:
https://github.com/apache/spark/pull/40219;;;","28/Feb/23 15:54;apachespark;User 'jelmerk' has created a pull request for this issue:
https://github.com/apache/spark/pull/40219;;;","02/Mar/23 14:44;srowen;Issue resolved by pull request 40219
[https://github.com/apache/spark/pull/40219];;;",,,,,,,,,,,
SparkSQLCLIDriver shall only close started hive sessionState,SPARK-42616,13526451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,28/Feb/23 05:31,01/Mar/23 01:48,30/Oct/23 17:26,01/Mar/23 01:48,3.3.2,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 01 01:48:27 UTC 2023,,,,,,,,,,"0|z1g8b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 05:44;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40211;;;","01/Mar/23 01:48;Qin Yao;Issue resolved by pull request 40211
[https://github.com/apache/spark/pull/40211];;;",,,,,,,,,,,,,
PythonRunner should set OMP_NUM_THREADS to task cpus instead of executor cores by default,SPARK-42613,13526423,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jzhuge,jzhuge,jzhuge,28/Feb/23 02:03,02/Mar/23 00:18,30/Oct/23 17:26,02/Mar/23 00:18,3.3.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,PySpark,YARN,,,,0,,,,,"Follow up from [https://github.com/apache/spark/pull/40199#discussion_r1119453996]

If OMP_NUM_THREADS is not set explicitly, we should set it to `spark.task.cpus` instead of `spark.executor.cores` as described in [PR #38699|https://github.com/apache/spark/pull/38699].",,,,,,,,,,,,,,,,,,,,,,SPARK-41188,SPARK-42596,SPARK-28843,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 02 00:18:41 UTC 2023,,,,,,,,,,"0|z1g84w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 05:44;apachespark;User 'jzhuge' has created a pull request for this issue:
https://github.com/apache/spark/pull/40212;;;","02/Mar/23 00:18;gurwls223;Issue resolved by pull request 40212
[https://github.com/apache/spark/pull/40212];;;",,,,,,,,,,,,,
Insert char/varchar length checks for inner fields during resolution,SPARK-42611,13526417,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,aokolnychyi,aokolnychyi,28/Feb/23 00:34,01/Mar/23 07:51,30/Oct/23 17:26,01/Mar/23 07:51,3.3.0,3.3.1,3.3.2,3.3.3,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"In SPARK-36498, we added support for reordering inner fields in structs during resolution. Unfortunately, we don't add any length validation for nested char/varchar columns in that path.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Mar 01 07:51:07 UTC 2023,,,,,,,,,,"0|z1g83k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 02:26;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40206;;;","28/Feb/23 02:26;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40206;;;","01/Mar/23 07:51;cloud_fan;Issue resolved by pull request 40206
[https://github.com/apache/spark/pull/40206];;;",,,,,,,,,,,,
Use full column names for inner fields in resolution errors,SPARK-42608,13526379,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,aokolnychyi,aokolnychyi,27/Feb/23 19:37,28/Feb/23 09:00,30/Oct/23 17:26,28/Feb/23 09:00,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"If there are multiple inner columns with the same name, resolution errors may be confusing as we only use field names, not full column names.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Feb 28 09:00:40 UTC 2023,,,,,,,,,,"0|z1g7v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 00:41;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40202;;;","28/Feb/23 09:00;dongjoon;Issue resolved by pull request 40202
[https://github.com/apache/spark/pull/40202];;;",,,,,,,,,,,,,
[MESOS] OMP_NUM_THREADS not set to number of executor cores by default,SPARK-42607,13526378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,jzhuge,jzhuge,27/Feb/23 19:26,10/Mar/23 17:12,30/Oct/23 17:26,10/Mar/23 17:12,3.3.2,,,,,,,,,,,,,,,,,,,,,,,Mesos,,,,,0,,,,,"We could have similar issue to SPARK-42596 (YARN) in Mesos.

Could someone verify? Unfortunately I am not able to due to lack of infra. ",,,,,,,,,,,,,,,,,,,,,,SPARK-42596,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 17:12:12 UTC 2023,,,,,,,,,,"0|z1g7uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 17:12;srowen;I don't think we're touching Mesos at this point - all but deprecated;;;",,,,,,,,,,,,,,
currentDatabase Shall use  NamespaceHelper instead of MultipartIdentifierHelper,SPARK-42600,13526309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,27/Feb/23 09:49,28/Feb/23 01:59,30/Oct/23 17:26,28/Feb/23 01:58,3.3.2,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Feb 28 01:58:49 UTC 2023,,,,,,,,,,"0|z1g7fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 09:57;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40192;;;","28/Feb/23 01:58;cloud_fan;Issue resolved by pull request 40192
[https://github.com/apache/spark/pull/40192];;;",,,,,,,,,,,,,
[YARN] OMP_NUM_THREADS not set to number of executor cores by default,SPARK-42596,13526279,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jzhuge,jzhuge,jzhuge,27/Feb/23 07:28,28/Feb/23 02:20,30/Oct/23 17:26,28/Feb/23 02:20,3.3.2,,,,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,PySpark,YARN,,,,0,,,,,"Run this PySpark script with `spark.executor.cores=1`
{code:python}
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf

spark = SparkSession.builder.getOrCreate()

var_name = 'OMP_NUM_THREADS'

def get_env_var():
  return os.getenv(var_name)

udf_get_env_var = udf(get_env_var)
spark.range(1).toDF(""id"").withColumn(f""env_{var_name}"", udf_get_env_var()).show(truncate=False)
{code}
Output with release `3.3.2`:
{noformat}
+---+-----------------------+
|id |env_OMP_NUM_THREADS    |
+---+-----------------------+
|0  |null                   |
+---+-----------------------+
{noformat}
Output with release `3.3.0`:
{noformat}
+---+-----------------------+
|id |env_OMP_NUM_THREADS    |
+---+-----------------------+
|0  |1                      |
+---+-----------------------+
{noformat}",,,,,,,,,,,,,,,,,,,,SPARK-41188,,,,,SPARK-42607,SPARK-42613,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Feb 28 02:20:22 UTC 2023,,,,,,,,,,"0|z1g78w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 07:52;jzhuge;Looks like a regression from SPARK-41188 where it removed the code that sets the default OMP_NUM_THREADS from PythonRunner.

Its PR assumes the code can be moved to SparkContext, unfortunately `SparkContext#executorEnvs` is only used by StandaloneSchedulerBackend for Spark's standalone cluster manager, thus the PR broke YARN as shown in the test case above, probably Mesos as well but I don't have a way to test.;;;","27/Feb/23 19:23;apachespark;User 'jzhuge' has created a pull request for this issue:
https://github.com/apache/spark/pull/40199;;;","28/Feb/23 02:20;gurwls223;Issue resolved by pull request 40199
[https://github.com/apache/spark/pull/40199];;;",,,,,,,,,,,,
spark can not read lastest view sql when run `create or replace view` by hive,SPARK-42594,13526266,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Bug,,Zing,Zing,27/Feb/23 05:30,27/Feb/23 07:08,30/Oct/23 17:26,27/Feb/23 07:08,3.3.2,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"1. Spark would save view schema as tabel param. 
2. Spark will make tabel param as output schema when select the view .
3. Hive will not update tabel param when runing `create or replace view` to update the view.

!image-2023-02-27-13-31-20-420.png!

So when hive and spark are mixed and update the view, spark may ignore some col.

To reproduce this issue:

1. running in spark
```
create table test_spark (id string);
create view test_spark_view as select id from test_spark;
```

2. running in hive

```
create or replace view test_spark_view as select id , ""test"" as new_id from test_spark;

```

3. We can see spark will ignore `test_spark_view#new_id` when select test_spark_view using spark. But hive can read it.

I'm not sure if this is a feature of spark.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 05:31;Zing;image-2023-02-27-13-31-20-420.png;https://issues.apache.org/jira/secure/attachment/13055837/image-2023-02-27-13-31-20-420.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Feb 27 06:29:53 UTC 2023,,,,,,,,,,"0|z1g760:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 05:33;Zing;[~yumwang] [~gurwls223]  gentel ping ~;;;","27/Feb/23 06:06;yumwang;Spark saves information to table properties, Hive does not update this information. Please avoid updating view definition through Hive.;;;","27/Feb/23 06:29;Zing;OK ,Thanks~ [~yumwang] ;;;",,,,,,,,,,,,
Logic error for StateStore.validateStateRowFormat,SPARK-42572,13526138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WweiL,WweiL,WweiL,25/Feb/23 00:57,28/Feb/23 03:13,30/Oct/23 17:26,28/Feb/23 03:13,3.5.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Structured Streaming,,,,,0,,,,,SPARK-42484 Changed the logic of whether to check state store format in StateStore.validateStateRowFormat. Revert it and add unit test to make sure this won't happen again,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Feb 28 03:13:34 UTC 2023,,,,,,,,,,"0|z1g6e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 01:29;gurwls223;[~WweiL] are you saying that we should revert https://github.com/apache/spark/pull/40073? It won't need a new jira for that;;;","27/Feb/23 07:46;WweiL;I'm not very sure what's the true process here..

We should still use some changes in #40073 (especially the logging part)

I've create a PR for the fix: [https://github.com/apache/spark/pull/40187]

But I could also revert it and combine the two PRs if that's the correct flow;;;","27/Feb/23 07:47;apachespark;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/40187;;;","28/Feb/23 03:13;kabhwan;Issue resolved by pull request 40187
[https://github.com/apache/spark/pull/40187];;;",,,,,,,,,,,
"NonReserved keyword ""interval"" can't be column name",SPARK-42553,13526036,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiang13021,jiang13021,jiang13021,24/Feb/23 09:42,02/Mar/23 15:25,30/Oct/23 17:26,02/Mar/23 06:38,3.2.3,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,,,,3.3.3,3.4.1,,,SQL,,,,,0,,,,,"INTERVAL is a Non-Reserved keyword in spark. ""Non-Reserved keywords"" have a special meaning in particular contexts and can be used as identifiers in other contexts. So by design, interval can be used as a column name.
{code:java}
scala> spark.sql(""select interval from mytable"")
org.apache.spark.sql.catalyst.parser.ParseException:
at least one time unit should be given for interval literal(line 1, pos 7)== SQL ==
select interval from mytable
-------^^^  at org.apache.spark.sql.errors.QueryParsingErrors$.invalidIntervalLiteralError(QueryParsingErrors.scala:196)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$parseIntervalLiteral$1(AstBuilder.scala:2481)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.parseIntervalLiteral(AstBuilder.scala:2466)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitInterval$1(AstBuilder.scala:2432)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInterval(AstBuilder.scala:2431)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInterval(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntervalContext.accept(SqlBaseParser.java:17308)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitIntervalLiteral(SqlBaseBaseVisitor.java:1581)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntervalLiteralContext.accept(SqlBaseParser.java:16929)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitConstantDefault(SqlBaseBaseVisitor.java:1511)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ConstantDefaultContext.accept(SqlBaseParser.java:15905)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitValueExpressionDefault(SqlBaseBaseVisitor.java:1392)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ValueExpressionDefaultContext.accept(SqlBaseParser.java:15298)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1412)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitPredicated$1(AstBuilder.scala:1548)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:1547)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext.accept(SqlBaseParser.java:14745)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitExpression(SqlBaseBaseVisitor.java:1343)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ExpressionContext.accept(SqlBaseParser.java:14606)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1412)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedExpression$1(AstBuilder.scala:1434)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:1433)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext.accept(SqlBaseParser.java:14124)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedExpressionSeq$2(AstBuilder.scala:628)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpressionSeq(AstBuilder.scala:628)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$withSelectQuerySpecification$1(AstBuilder.scala:734)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.withSelectQuerySpecification(AstBuilder.scala:728)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitRegularQuerySpecification$1(AstBuilder.scala:620)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:608)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext.accept(SqlBaseParser.java:9679)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryPrimaryDefault(SqlBaseBaseVisitor.java:846)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryPrimaryDefaultContext.accept(SqlBaseParser.java:9184)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryTermDefault(SqlBaseBaseVisitor.java:832)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryTermDefaultContext.accept(SqlBaseParser.java:8953)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:112)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$1(AstBuilder.scala:118)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:117)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept(SqlBaseParser.java:6398)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitStatementDefault(SqlBaseBaseVisitor.java:69)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$StatementDefaultContext.accept(SqlBaseParser.java:1835)
  at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitSingleStatement$1(AstBuilder.scala:78)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:78)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.$anonfun$parsePlan$1(ParseDriver.scala:78)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:110)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
  ... 47 elided
{code}
Since there must be at least one time unit after the interval, why in SqlBaseParser.g4, the definition of interval is
{code:java}
interval
    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)?
    ;  {code}
instead of
{code:java}
interval
    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)
    ; {code}
If we remove the ""?"", we ensure that there must be at least one time unit after the interval from the parsing level","Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_345)

Spark version 3.2.3-SNAPSHOT",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Mar 02 12:45:06 UTC 2023,,,,,,,,,,"0|z1g5rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 12:43;apachespark;User 'jiang13021' has created a pull request for this issue:
https://github.com/apache/spark/pull/40195;;;","02/Mar/23 06:38;maxgekk;Issue resolved by pull request 40195
[https://github.com/apache/spark/pull/40195];;;","02/Mar/23 06:47;dongjoon;Since RC2 tag is created, I changed the Fixed Version from 3.4.0 to 3.4.1 for now. We can adjust it later according to the RC2 result.;;;","02/Mar/23 12:45;apachespark;User 'jiang13021' has created a pull request for this issue:
https://github.com/apache/spark/pull/40253;;;",,,,,,,,,,,
"Get ParseException when run sql: ""SELECT 1 UNION SELECT 1;""",SPARK-42552,13526023,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,jiang13021,jiang13021,24/Feb/23 08:46,19/Apr/23 08:38,30/Oct/23 17:26,19/Apr/23 08:37,3.2.3,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"When I run sql
{code:java}
scala> spark.sql(""SELECT 1 UNION SELECT 1;"") {code}
I get ParseException:
{code:java}
org.apache.spark.sql.catalyst.parser.ParseException:
mismatched input 'SELECT' expecting {<EOF>, ';'}(line 1, pos 15)== SQL ==
SELECT 1 UNION SELECT 1;
---------------^^^  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
  ... 47 elided
 {code}
If I run with parentheses , it works well 
{code:java}
scala> spark.sql(""(SELECT 1) UNION (SELECT 1);"") 
res4: org.apache.spark.sql.DataFrame = [1: int]{code}
This should be a bug

 

 ","Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_345)
Spark version 3.2.3-SNAPSHOT",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Apr 19 08:37:30 UTC 2023,,,,,,,,,,"0|z1g5og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/23 03:18;jiang13021;The problem may be in this location: [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala#L126]

When the `PredictionMode` is `SLL`, `AstBuilder` will throw `ParseException` instead of `ParseCancellationException`，so the parser doesn't try `LL` mode. In fact, if we use `LL` mode, we can parse the sql correctly.;;;","18/Apr/23 09:04;ignitetcbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/40823;;;","19/Apr/23 08:37;cloud_fan;Issue resolved by pull request 40835
[https://github.com/apache/spark/pull/40835];;;",,,,,,,,,,,,
Make PySpark working with Python 3.7,SPARK-42547,13525982,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,gurwls223,gurwls223,24/Feb/23 03:04,24/Feb/23 05:15,30/Oct/23 17:26,24/Feb/23 05:15,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,PySpark,,,,,0,,,,,"{code}
+ ./python/run-tests --python-executables=python3
Running PySpark tests. Output is in /home/ec2-user/spark/python/unit-tests.log
Will test against the following Python executables: ['python3']
Will test the following Python modules: ['pyspark-connect', 'pyspark-core', 'pyspark-errors', 'pyspark-ml', 'pyspark-mllib', 'pyspark-pandas', 'pyspark-pandas-slow', 'pyspark-resource', 'pyspark-sql', 'pyspark-streaming']
python3 python_implementation is CPython
python3 version is: Python 3.7.16
Starting test(python3): pyspark.ml.tests.test_feature (temp output: /home/ec2-user/spark/python/target/8ca9ab1a-05cc-4845-bf89-30d9001510bc/python3__pyspark.ml.tests.test_feature__kg6sseie.log)
Starting test(python3): pyspark.ml.tests.test_base (temp output: /home/ec2-user/spark/python/target/f2264f3b-6b26-4e61-9452-8d6ddd7eb002/python3__pyspark.ml.tests.test_base__0902zf9_.log)
Starting test(python3): pyspark.ml.tests.test_algorithms (temp output: /home/ec2-user/spark/python/target/d1dc4e07-e58c-4c03-abe5-09d8fab22e6a/python3__pyspark.ml.tests.test_algorithms__lh3wb2u8.log)
Starting test(python3): pyspark.ml.tests.test_evaluation (temp output: /home/ec2-user/spark/python/target/3f42dc79-c945-4cf2-a1eb-83e72b40a9ee/python3__pyspark.ml.tests.test_evaluation__89idc7fa.log)
Finished test(python3): pyspark.ml.tests.test_base (16s)
Starting test(python3): pyspark.ml.tests.test_functions (temp output: /home/ec2-user/spark/python/target/5a3b90f0-216b-4edd-9d15-6619d3e03300/python3__pyspark.ml.tests.test_functions__g5u1290s.log)
Traceback (most recent call last):
  File ""/usr/lib64/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib64/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ec2-user/spark/python/pyspark/ml/tests/test_functions.py"", line 21, in <module>
    from pyspark.ml.functions import predict_batch_udf
  File ""/home/ec2-user/spark/python/pyspark/ml/functions.py"", line 38, in <module>
    from typing import Any, Callable, Iterator, List, Mapping, Protocol, TYPE_CHECKING, Tuple, Union
ImportError: cannot import name 'Protocol' from 'typing' (/usr/lib64/python3.7/typing.py)
Had test failures in pyspark.ml.tests.test_functions with python3; see logs.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Feb 24 05:15:50 UTC 2023,,,,,,,,,,"0|z1g5fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/23 03:15;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/40153;;;","24/Feb/23 05:15;dongjoon;Issue resolved by pull request 40153
[https://github.com/apache/spark/pull/40153];;;",,,,,,,,,,,,,
SPARK-42045 is incomplete in supporting ANSI_MODE fro round() and bround(),SPARK-42546,13525981,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,srielau,srielau,24/Feb/23 03:01,19/Mar/23 18:30,30/Oct/23 17:26,,3.3.2,3.4.0,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"under ANSI mode SPARK-42045 added error conditions insetad of silent overflows for edge cases in round() and bround().
However it appears this fix works only for the INT data type. Trying it on a e.g. SMALLINT the function still returns wrong results:
{code:java}
spark-sql> select round(2147483647, -1);
[ARITHMETIC_OVERFLOW] Overflow. If necessary set ""spark.sql.ansi.enabled"" to ""false"" to bypass this error.{code}
{code:java}
spark-sql> select round(127y, -1);
-126 {code}
   ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Mar 19 18:30:14 UTC 2023,,,,,,,,,,"0|z1g5f4:",9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,,,"24/Feb/23 03:02;cloud_fan;[~beliefer] do you have time to take a look?;;;","17/Mar/23 20:26;ddavies1;Can I take this?;;;","19/Mar/23 18:30;srielau;Can't speak for [~cloud_fan] , but +1 [~ddavies1] ;;;",,,,,,,,,,,,
"User-provided JARs can override Spark's Hive metadata client JARs when using ""builtin""",SPARK-42539,13525927,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xkrogen,xkrogen,xkrogen,23/Feb/23 17:52,19/Apr/23 00:06,30/Oct/23 17:26,18/Apr/23 23:59,3.1.3,3.2.3,3.3.2,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"Recently we observed that on version 3.2.0 and Java 8, it is possible for user-provided Hive JARs to break the ability for Spark, via the Hive metadata client / {{IsolatedClientLoader}}, to communicate with Hive Metastore, when using the default behavior of the ""builtin"" Hive version. After SPARK-35321, when Spark is compiled against Hive >= 2.3.9 and the ""builtin"" Hive client version is used, we will call the method {{Hive.getWithoutRegisterFns()}} (from HIVE-21563) instead of {{Hive.get()}}. If the user has included, for example, {{hive-exec-2.3.8.jar}} on their classpath, the client will break with a {{NoSuchMethodError}}. This particular failure mode was resolved in 3.2.1 by SPARK-37446, but while investigating, we found a general issue that it's possible for user JARs to override Spark's own JARs -- but only inside of the IsolatedClientLoader when using ""builtin"". This happens because even when Spark is configured to use the ""builtin"" Hive classes, it still creates a separate URLClassLoader for the HiveClientImpl used for HMS communication. To get the set of JAR URLs to use for this classloader, Spark [collects all of the JARs used by the user classloader (and its parent, and that classloader's parent, and so on)|https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L412-L438]. Thus the newly created classloader will have all of the same JARs as the user classloader, but the ordering has been reversed! User JARs get prioritized ahead of system JARs, because the classloader hierarchy is traversed from bottom-to-top. For example let's say we have user JARs ""foo.jar"" and ""hive-exec-2.3.8.jar"". The user classloader will look like this:
{code}
MutableURLClassLoader
-- foo.jar
-- hive-exec-2.3.8.jar
-- parent: URLClassLoader
----- spark-core_2.12-3.2.0.jar
----- ...
----- hive-exec-2.3.9.jar
----- ...
{code}

This setup provides the expected behavior within the user classloader; it will first check the parent, so hive-exec-2.3.9.jar takes precedence, and the MutableURLClassLoader is only checked if the class doesn't exist in the parent. But when a JAR list is constructed for the IsolatedClientLoader, it traverses the URLs from MutableURLClassLoader first, then it's parent, so the final list looks like (in order):
{code}
URLClassLoader [IsolatedClientLoader]
-- foo.jar
-- hive-exec-2.3.8.jar
-- spark-core_2.12-3.2.0.jar
-- ...
-- hive-exec-2.3.9.jar
-- ...
-- parent: boot classloader (JVM classes)
{code}
Now when a lookup happens, all of the JARs are within the same URLClassLoader, and the user JARs are in front of the Spark ones, so the user JARs get prioritized. This is the opposite of the expected behavior when using the default user/application classloader in Spark, which has parent-first behavior, prioritizing the Spark/system classes over the user classes. (Note that this behavior is correct when using the {{ChildFirstURLClassLoader}}.)

After SPARK-37446, the NoSuchMethodError is no longer an issue, but this still breaks assumptions about how user JARs should be treated vs. system JARs, and presents the ability for the client to break in other ways. For example in SPARK-37446 it describes a scenario whereby Hive 2.3.8 JARs have been included; the changes in Hive 2.3.9 were needed to improve compatibility with older HMS, so if a user were to accidentally include these older JARs, it could break the ability of Spark to communicate with HMS 1.x

I see two solutions to this:

*(A) Remove the separate classloader entirely when using ""builtin""*
Starting from 3.0.0, due to SPARK-26839, when using Java 9+, we don't even create a new classloader when using ""builtin"". This makes sense, as [called out in this comment|https://github.com/apache/spark/pull/24057#discussion_r265142878], since the point of ""builtin"" is to use the existing JARs on the classpath anyway. This proposes simply extending the changes from SPARK-26839 to all Java versions, instead of restricting to Java 9+ only.

*(B) Reverse the ordering of parent/child JARs when constructing the URL list*
The most targeted fix that can be made is to simply reverse the ordering on [this line in HiveUtils|https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L419], which prioritizes child-classloader JARs over parent-classloader JARs, reversing the expected ordering. There is already special handling for {{ChildFirstURLClassLoader}}, so all that needs to be done is to reverse this order.

I prefer (A) because I think it is a clean solution in that it both simplifies the classloader setup, and reduces divergence / special handling for different Java versions. At the time SPARK-26839 went in (2019), Java 9+ support was newer and less well-tested. Now after a few years we can see that the approach in SPARK-26839 clearly works well, so I see no reason _not_ to extend this approach to other Java versions as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Apr 19 00:06:30 UTC 2023,,,,,,,,,,"0|z1g534:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/23 18:13;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40144;;;","27/Feb/23 22:58;csun;Issue resolved by pull request 40144
[https://github.com/apache/spark/pull/40144];;;","28/Feb/23 04:06;gurwls223;Reverted in https://github.com/apache/spark/commit/5627ceeddb45f2796fb8ad08b9f1c8a163823b2b and https://github.com/apache/spark/commit/26009d47c1f80897d65445fe48d8d5f2edcf848c;;;","28/Feb/23 21:48;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40224;;;","28/Feb/23 21:49;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40224;;;","18/Apr/23 23:59;xkrogen;[~csun] it looks like this didn't get marked as closed / fix-version updated when the PR was merged. I believe this went only into 3.5.0; the original PR went into branch-3.4 but was reverted and the second PR didn't make it to branch-3.4. I've marked the fix version as 3.5.0 but please correct me if I'm wrong here:
{code:java}
> glog apache/branch-3.4 | grep SPARK-42539
* 26009d47c1f 2023-02-28 Revert ""[SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client"" [Hyukjin Kwon <gurwls223@apache.org>]
* 40a4019dfc5 2023-02-27 [SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client [Erik Krogen <xkrogen@apache.org>]


> glog apache/master | grep SPARK-42539
* 2e34427d4f3 2023-03-01 [SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client [Erik Krogen <xkrogen@apache.org>]
* 5627ceeddb4 2023-02-28 Revert ""[SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client"" [Hyukjin Kwon <gurwls223@apache.org>]
* 27ad5830f9a 2023-02-27 [SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client [Erik Krogen <xkrogen@apache.org>] {code};;;","19/Apr/23 00:06;sunchao;Oops my bad [~xkrogen] - you're right, this is not in Spark 3.5 release, sorry! I must have forgotten to mark it resolved when the second PR got merged. ;;;",,,,,,,,
Fix DB2 Limit clause,SPARK-42534,13525804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,23/Feb/23 01:40,24/Feb/23 12:44,30/Oct/23 17:26,24/Feb/23 12:44,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Feb 24 12:44:12 UTC 2023,,,,,,,,,,"0|z1g4bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/23 01:46;ivan.sadikov;I am going to open a PR to fix this.;;;","23/Feb/23 01:52;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40134;;;","23/Feb/23 01:53;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40134;;;","24/Feb/23 05:14;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40155;;;","24/Feb/23 12:44;cloud_fan;Issue resolved by pull request 40155
[https://github.com/apache/spark/pull/40155];;;",,,,,,,,,,
Non-captured session time zone in view creation,SPARK-42516,13525621,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,21/Feb/23 17:05,22/Feb/23 11:04,30/Oct/23 17:26,22/Feb/23 11:04,3.3.3,3.4.0,,,,,,,,,,,,,,,,,,3.3.3,3.4.0,,,SQL,,,,,0,,,,,"The session time zone config is captured only when it is set explicitly but if it is not the view is instantiated with the current settings. That's might confuse users since query results depends on explicit SQL config settings while creating a view.

The example below portraits the issue:
{code:java}
val viewName = ""v1_capture_test""
withView(viewName) {
  assert(get.sessionLocalTimeZone === ""America/Los_Angeles"")
  createView(viewName,
    """"""select hour(ts) as H from (
      |  select cast('2022-01-01T00:00:00.000 America/Los_Angeles' as timestamp) as ts
      |)"""""".stripMargin, Seq(""H""))
  withDefaultTimeZone(java.time.ZoneId.of(""UTC-09:00"")) {
    withSQLConf(SESSION_LOCAL_TIMEZONE.key -> ""UTC-10:00"") {
      sql(s""select H from $viewName"").show(false)
    }
  }
} {code}
It is expected to output:
{code:java}
+---+
|H  |
+---+
|0  |
+---+ {code}
but actual output is:
{code:java}
+---+
|H  |
+---+
|8  |
+---+ {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 22 11:04:06 UTC 2023,,,,,,,,,,"0|z1g374:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/23 17:08;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/40103;;;","22/Feb/23 11:04;maxgekk;Issue resolved by pull request 40103
[https://github.com/apache/spark/pull/40103];;;",,,,,,,,,,,,,
ClientE2ETestSuite local test failed,SPARK-42515,13525616,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,21/Feb/23 16:39,28/Feb/23 03:58,30/Oct/23 17:26,28/Feb/23 03:58,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.4.0,,,,Connect,,,,,0,,,,," 

local run `build/sbt clean ""connect-client-jvm/test""`, `ClientE2ETestSuite#write table` failed, GA not failed.

 
{code:java}
[info] - rite table *** FAILED *** (41 milliseconds)
[info]   io.grpc.StatusRuntimeException: UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport
[info]   at io.grpc.Status.asRuntimeException(Status.java:535)
[info]   at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
[info]   at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)
[info]   at scala.collection.Iterator.foreach(Iterator.scala:943)
[info]   at scala.collection.Iterator.foreach$(Iterator.scala:943)
[info]   at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[info]   at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:169)
[info]   at org.apache.spark.sql.DataFrameWriter.executeWriteOperation(DataFrameWriter.scala:255)
[info]   at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:338)
[info]   at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$12(ClientE2ETestSuite.scala:145)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.sql.ClientE2ETestSuite.org$scalatest$BeforeAndAfterAll$$super$run(ClientE2ETestSuite.scala:33)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:33)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:750) {code}
 

 

local run ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Feb 28 03:58:39 UTC 2023,,,,,,,,,,"0|z1g360:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/23 16:40;LuciferYang;will fix this later;;;","23/Feb/23 03:21;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40136;;;","28/Feb/23 03:58;gurwls223;Issue resolved by pull request 40136
[https://github.com/apache/spark/pull/40136];;;",,,,,,,,,,,,
"Spark MasterWebUI and WorkerWebUI fail to start when NSSDB used as keystore, getting java.security.KeyStoreException: PKCS11 not found.",SPARK-42511,13525562,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,sshukla05,sshukla05,21/Feb/23 13:13,21/Feb/23 13:24,30/Oct/23 17:26,,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,,,,,,,,,Spark Core,Spark Submit,,,,0,,,,,"When we are running Spark by setting up below SSL configurations, Spark masterwebui and workerwebui is fail to start.
{quote}
        ""spark.ssl.enabled"":""true"",
        ""spark.ssl.keyStore"":""/opt/ibm/jdk/conf/security/nss.fips.cfg"",
        ""spark.ssl.keyStorePassword"":""<keystore passwd>"",
        ""spark.ssl.keyStoreType"":""PKCS11""
{quote}

*Errors :*

{quote}23/02/21 12:29:43 INFO Master: Running Spark version 3.3.1
23/02/21 12:29:43 ERROR MasterWebUI: Failed to bind MasterWebUI
java.security.KeyStoreException: PKCS11 not found
	at java.base/java.security.KeyStore.getInstance(KeyStore.java:878)
	at org.sparkproject.jetty.util.security.CertificateUtils.getKeyStore(CertificateUtils.java:46)
	at org.sparkproject.jetty.util.ssl.SslContextFactory.loadKeyStore(SslContextFactory.java:1203)
	at org.sparkproject.jetty.util.ssl.SslContextFactory.load(SslContextFactory.java:322)
	at org.sparkproject.jetty.util.ssl.SslContextFactory.doStart(SslContextFactory.java:244)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
	at org.sparkproject.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)
	at org.sparkproject.jetty.server.SslConnectionFactory.doStart(SslConnectionFactory.java:97)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
	at org.sparkproject.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)
	at org.sparkproject.jetty.server.AbstractConnector.doStart(AbstractConnector.java:323)
	at org.sparkproject.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)
	at org.sparkproject.jetty.server.ServerConnector.doStart(ServerConnector.java:234)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.JettyUtils$.newConnector$1(JettyUtils.scala:303)
	at org.apache.spark.ui.JettyUtils$.sslConnect$1(JettyUtils.scala:322)
	at org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$4(JettyUtils.scala:326)
	at org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$4$adapted(JettyUtils.scala:326)
	at org.apache.spark.util.Utils$.$anonfun$startServiceOnPort$2(Utils.scala:2401)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2393)
	at org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$2(JettyUtils.scala:326)
	at org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$2$adapted(JettyUtils.scala:315)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:315)
	at org.apache.spark.ui.WebUI.initServer(WebUI.scala:144)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:153)
	at org.apache.spark.deploy.master.Master.onStart(Master.scala:138)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:120)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:839)
Caused by: java.security.NoSuchAlgorithmException: PKCS11 KeyStore not available
	at java.base/sun.security.jca.GetInstance.getInstance(GetInstance.java:159)
	at java.base/java.security.Security.getImpl(Security.java:719)
	at java.base/java.security.KeyStore.getInstance(KeyStore.java:875)
	... 37 more
{quote}

content of nss fips config file.

{quote}name = NSS-FIPS
nssLibraryDirectory = /usr/lib64
nssSecmodDirectory = /etc/pki/nssdb
nssDbMode = readOnly
nssModule = fips

attributes(*,CKO_SECRET_KEY,CKK_GENERIC_SECRET)={ CKA_SIGN=true }{quote}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Feb 21 13:19:26 UTC 2023,,,,,,,,,,"0|z1g2u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/23 13:19;sshukla05;setting up below options as well in spark submit command.
{quote}--conf spark.driver.extraJavaOptions= -Dderby.system.home=/.local/share/jupyter/runtime/kernel-21f4c67c-bcf6-4600-96dc-e59510692c6a-20230221_122939 -Dlog4j.logFile=/home/spark/shared/logs/kernel-python3.10-python3.10-20230221_122939.log -Dlog4j.configuration=file:/opt/ibm/jkg/log4j/log4j.properties -Dsemeru.fips=true -Djavax.net.ssl.keyStore=/opt/ibm/jdk/conf/security/nss.fips.cfg -Djavax.net.ssl.keyStorePassword=changeit -Djavax.net.ssl.keyStoreType=PKCS11 -Dfile.encoding=UTF-8

--conf spark.executor.extraJavaOptions= -Dderby.system.home=/.local/share/jupyter/runtime/kernel-21f4c67c-bcf6-4600-96dc-e59510692c6a-20230221_122939 -Dlog4j.logFile=/home/spark/shared/logs/kernel-python3.10-python3.10-20230221_122939.log -Dlog4j.configuration=file:/opt/ibm/jkg/log4j/log4j.properties -Dsemeru.fips=true -Djavax.net.ssl.keyStore=/opt/ibm/jdk/conf/security/nss.fips.cfg -Djavax.net.ssl.keyStorePassword=changeit -Djavax.net.ssl.keyStoreType=PKCS11 -Dfile.encoding=UTF-8
{quote};;;",,,,,,,,,,,,,,
Log4j2 doesn't works with Spark 3.3.0,SPARK-42479,13525201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,pratik.malani,pratik.malani,17/Feb/23 12:31,10/Mar/23 17:14,30/Oct/23 17:26,10/Mar/23 17:14,3.2.1,3.3.0,,,,,,,,,,,,,,,,,,,,,,Spark Core,Spark Submit,,,,0,,,,,"Hi All,
Was trying to run spark application on the cluster mode using log4j2 and Spark 3.3.0.

When I run the below spark-submit command, only one worker (out of 3) starts executing the job.
{code:java}
// code placeholder
spark-submit --master spark://spark-master-svc:7077 \
    --conf spark.cores.max=4 \
    --conf spark.sql.broadcastTimeout=3600 \
    --conf spark.executor.cores=1 \
    --jars /opt/spark/work-dir/<main_jar_name>.jar \
    --deploy-mode cluster \
    --class <class_name> \
    --properties-file /opt/spark/conf/spark-defaults.conf \
    --conf spark.driver.extraJavaOptions=""-Dcom.amazonaws.sdk.disableCertChecking=true -Dlog4j.configurationFile=file:/opt/spark/work-dir/<log4j2>.properties"" \
    --conf spark.executor.extraJavaOptions=""-Dcom.amazonaws.sdk.disableCertChecking=true -Dlog4j.configurationFile=file:/opt/spark/work-dir/<log4j2>.properties"" \
    --files ""/opt/spark/work-dir/<log4j2>.properties"" \
    /opt/spark/work-dir/<jar_name>.jar /opt/spark/work-dir/application.properties >> /var/log/containers/hourly.log 2>&1 {code}
It means, on only one worker, I can see the driver logs and the other workers are idle and there are no app or executors logs created on other workers.

Below is the log4j2.properties file being used.
{code:java}
// code placeholder
rootLogger.level = INFO
rootLogger.appenderRef.rolling.ref = loggerId

appender.rolling.type = RollingFile
appender.rolling.name = loggerId
appender.rolling.fileName=/var/log/containers/hourly.log
appender.rolling.filePattern=hourly-.%d{yyyyMMdd}.log.gz
appender.rolling.layout.type = PatternLayout
appender.rolling.layout.pattern=%d [%t] %-5p (%F:%L) - %m%n
appender.rolling.policies.type = Policies
appender.rolling.policies.size.type = TimeBasedTriggeringPolicy
appender.rolling.strategy.type = DefaultRolloverStrategy
appender.rolling.strategy.max = 5


logger.spark.name = org.apache.spark
logger.spark.level = WARN
logger.spark.additivity = false

logger.spark.repl.SparkIMain$exprTyper.level = INFO
logger.spark.repl.SparkILoop$SparkILoopInterpreter.level = INFO

# Settings to quiet third party logs that are too verbose
logger.jetty.name = org.eclipse.jetty
logger.jetty.level = WARN
logger.jetty.util.component.AbstractLifeCycle.level = ERROR
logger.parquet.name = org.apache.parquet
logger.parquet.level = ERROR

logger.kafka.name = org.apache.kafka
logger.kafka.level = WARN
logger.kafka.clients.consumer.internals.Fetcher.level=WARN {code}
All log4j2 jars are included in the Spark home classpath under the jars directory.
 * log4j-1.2-api-2.17.2.jar
 * log4j-api-2.17.2.jar
 * log4j-api-scala_2.12-12.0.jar
 * log4j-core-2.17.2.jar
 * log4j-slf4j-impl-2.17.2.jar

Can you please check and let me know whether I need to add or update anything to start the job in cluster mode supporting log4j2

Note : Things work fine with log4j1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 17:14:26 UTC 2023,,,,,,,,,,"0|z1g0m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 17:14;srowen;This is because you're pointing at some local path not visible on the workers. It's quite expected;;;",,,,,,,,,,,,,,
Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory,SPARK-42478,13525197,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kaifeiYi,kaifeiYi,kaifeiYi,17/Feb/23 11:53,06/Mar/23 22:08,30/Oct/23 17:26,27/Feb/23 08:56,3.3.2,,,,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,SQL,,,,,0,,,,,"https://issues.apache.org/jira/browse/SPARK-41448 make consistent MR job IDs in FileBatchWriter and FileFormatWriter, but it breaks a serializable issue, JobId is non-serializable",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Mar 06 03:19:39 UTC 2023,,,,,,,,,,"0|z1g0l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/23 12:06;apachespark;User 'Yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/40064;;;","27/Feb/23 08:56;cloud_fan;Issue resolved by pull request 40064
[https://github.com/apache/spark/pull/40064];;;","06/Mar/23 03:18;apachespark;User 'Yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/40289;;;","06/Mar/23 03:19;apachespark;User 'Yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/40290;;;","06/Mar/23 03:19;apachespark;User 'Yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/40289;;;",,,,,,,,,,
An explicit cast will be needed when INSERT OVERWRITE SELECT UNION ALL,SPARK-42473,13525148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,runyao,kevinshin,kevinshin,17/Feb/23 06:27,03/Mar/23 06:58,30/Oct/23 17:26,03/Mar/23 06:58,3.3.1,,,,,,,,,,,,,,,,,,,3.3.3,,,,Optimizer,,,,,0,,,,,"*when 'union all' and one select statement use* *Literal as column value , the other* *select statement  has computed expression at the same column , then the whole statement will compile failed. A explicit cast will be needed.*

for example:

{color:#4c9aff}explain{color}

{color:#4c9aff}*INSERT* OVERWRITE *TABLE* test.spark33_decimal_orc{color}

{color:#4c9aff}*select* *null* *as* amt1, {*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2{color}

{color:#4c9aff}*union* *all*{color}

{color:#4c9aff}*select* {*}cast{*}('200.99' *as* {*}decimal{*}(20,8)){*}/{*}100 *as* amt1,{*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2;{color}

*will got error :* 

org.apache.spark.{*}sql{*}.catalyst.expressions.Literal cannot be *cast* *to* org.apache.spark.{*}sql{*}.catalyst.expressions.AnsiCast

The SQL will need to change to : 

{color:#4c9aff}explain{color}

{color:#4c9aff}*INSERT* OVERWRITE *TABLE* test.spark33_decimal_orc{color}

{color:#4c9aff}*select* *null* *as* amt1,{*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2{color}

{color:#4c9aff}*union* *all*{color}

{color:#4c9aff}*select* {color:#de350b}{*}cast{*}({color}{*}cast{*}('200.99' *as* {*}decimal{*}(20,8)){*}/{*}100 *as* {*}decimal{*}(20,8){color:#de350b}){color} *as* amt1,{*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2;{color}

 

*but this is not need in spark3.2.1 , is this a bug for spark3.3.1 ?* ",spark 3.3.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Feb 23 19:14:38 UTC 2023,,,,,,,,,,"0|z1g0a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/23 03:26;yumwang;What is your test.spark33_decimal_orc column type?;;;","20/Feb/23 01:47;kevinshin;[~yumwang]  'What is your test.spark33_decimal_orc column type?'

{color:#4c9aff}*CREATE* *TABLE* *IF* *NOT* *EXISTS* test.spark33_decimal_orc({color}

{color:#4c9aff}   amt1        {*}decimal{*}(20,8),{color}

{color:#4c9aff}   amt2        {*}decimal{*}(20,8){color}

{color:#4c9aff})STORED *AS* ORC;{color};;;","20/Feb/23 05:29;yumwang;It seems we should backport https://github.com/apache/spark/pull/39855.;;;","23/Feb/23 19:14;apachespark;User 'RunyaoChen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40140;;;",,,,,,,,,,,
spark.kubernetes.file.upload.path not deleting files under HDFS after job completes,SPARK-42466,13525079,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jagadeesh.n,jagadeesh.n,16/Feb/23 19:02,16/Sep/23 00:16,30/Oct/23 17:26,,3.2.0,3.3.2,,,,,,,,,,,,,,,,,,,,,,Kubernetes,,,,,0,pull-request-available,,,,"In cluster mode after uploading files to HDFS location using spark.kubernetes.file.upload.path property files are not getting cleared . 

File is successfully uploaded to hdfs location in this format spark-upload-[randomUUID] using {{KubernetesUtils}} is requested to  uploadFileUri . [https://github.com/apache/spark/blob/76a134ade60a9f354aca01eaca0b2e2477c6bd43/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala#L310]

following is driver log  , driver is completed successfully and shutdownhook is not cleared the hdfs files.
{code:java}
23/02/16 18:06:56 INFO KubernetesClusterSchedulerBackend: Shutting down all executors
23/02/16 18:06:56 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down
23/02/16 18:06:56 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.
23/02/16 18:06:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/02/16 18:06:57 INFO MemoryStore: MemoryStore cleared
23/02/16 18:06:57 INFO BlockManager: BlockManager stopped
23/02/16 18:06:57 INFO BlockManagerMaster: BlockManagerMaster stopped
23/02/16 18:06:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/02/16 18:06:57 INFO SparkContext: Successfully stopped SparkContext
23/02/16 18:06:57 INFO ShutdownHookManager: Shutdown hook called
23/02/16 18:06:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-efb8f725-4ead-4729-a8e0-f478280121b7
23/02/16 18:06:57 INFO ShutdownHookManager: Deleting directory /spark-local2/spark-66dbf7e6-fe7e-4655-8724-69d76d93fc1f
23/02/16 18:06:57 INFO ShutdownHookManager: Deleting directory /spark-local1/spark-53aefaee-58a5-4fce-b5b0-5e29f42e337f{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 22 19:43:01 UTC 2023,,,,,,,,,,"0|z1fzuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/23 13:03;unamesk15;working on the fix.;;;","22/Feb/23 19:42;apachespark;User 'shrprasa' has created a pull request for this issue:
https://github.com/apache/spark/pull/40128;;;","22/Feb/23 19:43;apachespark;User 'shrprasa' has created a pull request for this issue:
https://github.com/apache/spark/pull/40128;;;",,,,,,,,,,,,
Prevent `docker-image-tool.sh` from publishing OCI manifests,SPARK-42462,13524945,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,16/Feb/23 04:41,16/Feb/23 05:53,30/Oct/23 17:26,16/Feb/23 05:53,3.2.4,3.3.3,3.4.0,,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,Kubernetes,,,,,0,,,,,https://github.com/docker/buildx/issues/1509,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Feb 16 05:53:08 UTC 2023,,,,,,,,,,"0|z1fz14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/23 04:48;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40051;;;","16/Feb/23 04:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40051;;;","16/Feb/23 05:53;dongjoon;Issue resolved by pull request 40051
[https://github.com/apache/spark/pull/40051];;;",,,,,,,,,,,,
dataset.where() omit quotes if where IN clause has more than 10 operands,SPARK-42450,13524873,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ebious,ebious,15/Feb/23 15:43,16/Feb/23 11:45,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"dataset.where()/filter() omit string quotes if where IN clause has more than 10 operands. With datasourceV1 works as expected. 
Attached files: java-code.txt, stacktrace.txt, sql.txt
 - Spark verison 3.3.0
 - Scala version 2.12
 - DatasourceV2
 - Postgres
 - Postrgres JDBC Driver: 42+
 - Java8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/23 15:43;ebious;java-code.txt;https://issues.apache.org/jira/secure/attachment/13055475/java-code.txt","15/Feb/23 15:49;ebious;sql.txt;https://issues.apache.org/jira/secure/attachment/13055477/sql.txt","15/Feb/23 15:43;ebious;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/13055476/stacktrace.txt",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-02-15 15:43:10.0,,,,,,,,,,"0|z1fyl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark sql shell prompts wrong database info,SPARK-42448,13524813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yao,yao,yao,15/Feb/23 10:23,23/Feb/23 08:47,30/Oct/23 17:26,23/Feb/23 08:47,2.4.8,3.0.3,3.1.3,3.2.3,3.3.1,3.4.0,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,The current db info is from hive sessionState instead of spark sessionState,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Feb 23 08:47:24 UTC 2023,,,,,,,,,,"0|z1fy7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/23 10:45;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40036;;;","23/Feb/23 08:47;Qin Yao;Issue resolved by pull request 40036
[https://github.com/apache/spark/pull/40036];;;",,,,,,,,,,,,,
Fix SparkR install.spark function,SPARK-42445,13524767,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,15/Feb/23 05:50,15/Feb/23 17:32,30/Oct/23 17:26,15/Feb/23 17:32,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,,,,,,,,,,3.3.3,3.4.0,,,R,,,,,0,,,,,"{code}
$ R

R version 4.2.1 (2022-06-23) -- ""Funny-Looking Kid""
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: aarch64-apple-darwin20 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(SparkR)

Attaching package: ‘SparkR’

The following objects are masked from ‘package:stats’:

    cov, filter, lag, na.omit, predict, sd, var, window

The following objects are masked from ‘package:base’:

    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,
    rank, rbind, sample, startsWith, subset, summary, transform, union

> install.spark()
Spark not found in the cache directory. Installation will start.
MirrorUrl not provided.
Looking for preferred site from apache website...
Preferred mirror site found: https://dlcdn.apache.org/spark
Downloading spark-3.3.2 for Hadoop 2.7 from:
- https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.7.tgz
trying URL 'https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.7.tgz'
simpleWarning in download.file(remotePath, localPath): downloaded length 0 != reported length 196
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 15 17:32:10 UTC 2023,,,,,,,,,,"0|z1fxxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/23 05:53;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40031;;;","15/Feb/23 17:32;dongjoon;Issue resolved by pull request 40031
[https://github.com/apache/spark/pull/40031];;;",,,,,,,,,,,,,
DataFrame.drop should handle multi columns properly,SPARK-42444,13524752,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,podongfeng,podongfeng,podongfeng,15/Feb/23 02:46,24/Feb/23 00:03,30/Oct/23 17:26,24/Feb/23 00:03,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,PySpark,,,,,0,,,,,"{code:java}
from pyspark.sql import Row
df1 = spark.createDataFrame([(14, ""Tom""), (23, ""Alice""), (16, ""Bob"")], [""age"", ""name""])
df2 = spark.createDataFrame([Row(height=80, name=""Tom""), Row(height=85, name=""Bob"")])
df1.join(df2, df1.name == df2.name, 'inner').drop('name', 'age').show()
{code}

This works in 3.3

{code:java}
+------+
|height|
+------+
|    85|
|    80|
+------+
{code}

but fails in 3.4


{code:java}
---------------------------------------------------------------------------
AnalysisException                         Traceback (most recent call last)
Cell In[1], line 4
      2 df1 = spark.createDataFrame([(14, ""Tom""), (23, ""Alice""), (16, ""Bob"")], [""age"", ""name""])
      3 df2 = spark.createDataFrame([Row(height=80, name=""Tom""), Row(height=85, name=""Bob"")])
----> 4 df1.join(df2, df1.name == df2.name, 'inner').drop('name', 'age').show()

File ~/Dev/spark/python/pyspark/sql/dataframe.py:4913, in DataFrame.drop(self, *cols)
   4911     jcols = [_to_java_column(c) for c in cols]
   4912     first_column, *remaining_columns = jcols
-> 4913     jdf = self._jdf.drop(first_column, self._jseq(remaining_columns))
   4915 return DataFrame(jdf, self.sparkSession)

File ~/Dev/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)
   1316 command = proto.CALL_COMMAND_NAME +\
   1317     self.command_header +\
   1318     args_command +\
   1319     proto.END_COMMAND_PART
   1321 answer = self.gateway_client.send_command(command)
-> 1322 return_value = get_return_value(
   1323     answer, self.gateway_client, self.target_id, self.name)
   1325 for temp_arg in temp_args:
   1326     if hasattr(temp_arg, ""_detach""):

File ~/Dev/spark/python/pyspark/errors/exceptions/captured.py:159, in capture_sql_exception.<locals>.deco(*a, **kw)
    155 converted = convert_exception(e.java_exception)
    156 if not isinstance(converted, UnknownException):
    157     # Hide where the exception came from that shows a non-Pythonic
    158     # JVM exception message.
--> 159     raise converted from None
    160 else:
    161     raise

AnalysisException: [AMBIGUOUS_REFERENCE] Reference `name` is ambiguous, could be: [`name`, `name`].

{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Feb 24 00:03:57 UTC 2023,,,,,,,,,,"0|z1fxug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/23 02:01;podongfeng;I am going to fix this one;;;","23/Feb/23 03:07;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/40135;;;","23/Feb/23 03:07;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/40135;;;","24/Feb/23 00:03;podongfeng;Issue resolved by pull request 40135
[https://github.com/apache/spark/pull/40135];;;",,,,,,,,,,,
Job description in v2 FileWrites can have the wrong committer,SPARK-42439,13524697,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,LorenzoMartini94,LorenzoMartini94,14/Feb/23 17:00,14/Sep/23 00:17,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,bug,pull-request-available,,,"There is a difference in behavior between v1 writes and v2 writes in the order of events happening when configuring the file writer and the committer.

v1:
 # writer.prepareWrite()
 # committer.setupJob()

v2:
 # committer.setupJob()
 # writer.prepareWrite()

 

This is because the `prepareWrite()` call (that is the one performing the call `
job.setOutputFormatClass(classOf[ParquetOutputFormat[Row]])`)
happens as part of the `createWriteJobDescription` which is `lazy val` in the `toBatch` call and therefore is evaluated after the `committer.setupJob` at the end of the `toBatch`

This causes issues when evaluating the committer as some elements might be missing, for example the aforementioned output format class not being set, causing the committer being set up as generic write instead of parquet write.

 

The fix is very simple and it is to make the `createJobDescription` call non-lazy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Feb 14 17:14:03 UTC 2023,,,,,,,,,,"0|z1fxi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/23 17:05;apachespark;User 'LorenzoMartini' has created a pull request for this issue:
https://github.com/apache/spark/pull/40017;;;","14/Feb/23 17:06;apachespark;User 'LorenzoMartini' has created a pull request for this issue:
https://github.com/apache/spark/pull/40017;;;","14/Feb/23 17:14;apachespark;User 'LorenzoMartini' has created a pull request for this issue:
https://github.com/apache/spark/pull/40018;;;",,,,,,,,,,,,
spark-hadoop-cloud is not provided in the default Spark distribution,SPARK-42425,13524547,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,tashoyan,tashoyan,13/Feb/23 21:46,11/Mar/23 18:03,30/Oct/23 17:26,10/Mar/23 17:15,3.3.1,,,,,,,,,,,,,,,,,,,,,,,Input/Output,,,,,0,,,,,"The library spark-hadoop-cloud is absent in the default Spark distribution (as well as its dependencies like hadoop-aws). Therefore the dependency management section described in [Integration with Cloud Infrastructures|https://spark.apache.org/docs/3.3.1/cloud-integration.html#installation] is invalid. Actually the libraries for cloud integration are not provided.

A naive workaround would be to add the spark-hadoop-cloud library as a compile-scope dependency. However, this does not work due to Spark classpath hierarchy. Spark system classloader does not see classes loaded by the application classloader.

Therefore a proper fix would be to enable the hadoop-cloud build profile by default: -Phadoop-cloud",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Mar 11 18:03:53 UTC 2023,,,,,,,,,,"0|z1fwkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 17:15;srowen;The docs don't say it's part of the Spark distro. in fact it tells you to bundle it in your app. It is not bundled on purpose.;;;","11/Mar/23 18:03;tashoyan;The doc says to declare this dependency as provided, hence assumes this jar is bundled in the Spark distro. Either the doc is wrong or the distro is missing the lib.;;;",,,,,,,,,,,,,
Dateset operations should not resolve the analyzed logical plan again,SPARK-42416,13524401,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,13/Feb/23 05:36,13/Feb/23 18:58,30/Oct/23 17:26,13/Feb/23 18:58,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"For the following query

 
{code:java}
      sql(
        """"""
          |CREATE TABLE app_open (
          |  uid STRING,
          |  st TIMESTAMP,
          |  ds INT
          |) USING parquet PARTITIONED BY (ds);
          |"""""".stripMargin)

      sql(
        """"""
          |create or replace temporary view group_by_error as WITH new_app_open AS (
          |  SELECT
          |    ao.*
          |  FROM
          |    app_open ao
          |)
          |SELECT
          |    uid,
          |    20230208 AS ds
          |  FROM
          |    new_app_open
          |  GROUP BY
          |    1,
          |    2
          |"""""".stripMargin)

      sql(
        """"""
          |select
          |  `uid`
          |from
          |  group_by_error
          |"""""".stripMargin).show(){code}
Spark will throw the following error

 

 
{code:java}
[GROUP_BY_POS_OUT_OF_RANGE] GROUP BY position 20230208 is not in select list (valid range is [1, 2]).; line 9 pos 4 {code}
 

 

This is because the logical plan is not set as analyzed and it is analyzed again. The analyzer rules about aggregation/sort ordinals are not idempotent.",,,,,,,,,,,,,,,,,,,,SPARK-40595,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Feb 13 18:58:18 UTC 2023,,,,,,,,,,"0|z1fvo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/23 05:56;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39988;;;","13/Feb/23 18:58;Gengliang.Wang;Issue resolved by pull request 39988
[https://github.com/apache/spark/pull/39988];;;",,,,,,,,,,,,,
Support Scala 2.12/2.13 tests in connect module,SPARK-42410,13524371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,12/Feb/23 22:58,01/Mar/23 16:39,30/Oct/23 17:26,13/Feb/23 01:31,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Connect,Tests,,,,0,,,,,"{code}
$ build/sbt -Dscala.version=2.13.8 -Pscala-2.13 -Phadoop-3 assembly/package ""connect/test""
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Feb 13 02:37:52 UTC 2023,,,,,,,,,,"0|z1fvhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/23 23:38;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39982;;;","12/Feb/23 23:39;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39982;;;","13/Feb/23 01:31;dongjoon;Issue resolved by pull request 39982
[https://github.com/apache/spark/pull/39982];;;","13/Feb/23 02:37;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39986;;;",,,,,,,,,,,
[PROTOBUF] Recursive field handling is incompatible with delta,SPARK-42406,13524334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rangadi,rangadi,rangadi,11/Feb/23 20:53,28/Feb/23 04:45,30/Oct/23 17:26,28/Feb/23 04:45,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Protobuf,,,,,0,,,,,"Protobuf deserializer (`from_protobuf()` function()) optionally supports recursive fields by limiting the depth to certain level. See example below. It assigns a 'NullType' for such a field when allowed depth is reached. 

It causes a few issues. E.g. a repeated field as in the following example results in a Array field with 'NullType'. Delta does not support null type in a complex type.

Actually `Array[NullType]` is not really useful anyway.

How about this fix: Drop the recursive field when the limit reached rather than using a NullType. 

The example below makes it clear:

Consider a recursive Protobuf:

 
{code:python}
message TreeNode {
  string value = 1;
  repeated TreeNode children = 2;
}
{code}
Allow depth of 2: 

 
{code:python}
   df.select(
    'proto',
     messageName = 'TreeNode',
     options = { ... ""recursive.fields.max.depth"" : ""2"" }
  ).printSchema()
{code}
Schema looks like this:
{noformat}
root
|– from_protobuf(proto): struct (nullable = true)|
| |– value: string (nullable = true)|
| |– children: array (nullable = false)|
| | |– element: struct (containsNull = false)|
| | | |– value: string (nullable = true)|
| | | |– children: array (nullable = false)|
| | | | |– element: struct (containsNull = false)|
| | | | | |– value: string (nullable = true)|
| | | | | |– children: array (nullable = false). [ === Proposed fix: Drop this field === ]|
| | | | | | |– element: void (containsNull = false) [ === NOTICE 'void' HERE === ] 
{noformat}
When we try to write this to a delta table, we get an error:
{noformat}
AnalysisException: Found nested NullType in column from_protobuf(proto).children which is of ArrayType. Delta doesn't support writing NullType in complex types.
{noformat}
 
We could just drop the field 'element' when recursion depth is reached. It is simpler and does not need to deal with NullType. We are ignoring the value anyway. There is no use in keeping the field.

Another issue is setting for 'recursive.fields.max.depth': It is not enforced correctly. '0' does not make sense. 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Feb 28 04:45:01 UTC 2023,,,,,,,,,,"0|z1fv9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/23 21:00;rangadi;cc: [~sanysandish@gmail.com] PTAL.;;;","13/Feb/23 17:11;rangadi;I am working on a fix.;;;","14/Feb/23 08:42;apachespark;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40011;;;","14/Feb/23 23:12;Gengliang.Wang;Issue resolved by pull request 40011
[https://github.com/apache/spark/pull/40011];;;","14/Feb/23 23:48;rangadi;Thanks for merging [https://github.com/apache/spark/pull/40011] 

Keeping this ticket open to fix the issue with 'nullType' and delta.;;;","19/Feb/23 05:16;apachespark;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40080;;;","19/Feb/23 05:16;apachespark;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40080;;;","22/Feb/23 01:35;Gengliang.Wang;Resolved in https://github.com/apache/spark/pull/40080;;;","23/Feb/23 09:04;rangadi;The main for still to be merged.;;;","23/Feb/23 09:41;apachespark;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40141;;;","28/Feb/23 04:45;Gengliang.Wang;Issue resolved by pull request 40141
[https://github.com/apache/spark/pull/40141];;;",,,,
JsonProtocol should handle null JSON strings,SPARK-42403,13524305,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,dongjoon,dongjoon,11/Feb/23 01:12,11/Feb/23 05:55,30/Oct/23 17:26,11/Feb/23 05:54,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,,,0,,,,,"*Event Log*
{code}
{""Declaring Class"":""org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1"",""Method Name"":""columnartorow_nextBatch_0$"",""File Name"":null,""Line Number"":-1}
{code}

*Apache Spark 3.4*
{code}
3/02/10 16:54:46 ERROR ReplayListenerBus: Exception parsing Spark event log: file:/Users/dongjoon/data/history/eventlog_v2_spark-1676069204164-1qq70hioosynfzib9rmi77wbavnao-driver-job/events_1_spark-1676069204164-1qq70hioosynfzib9rmi77wbavnao-driver-job.zstd
java.lang.IllegalArgumentException: requirement failed: Expected string, got NULL
        at scala.Predef$.require(Predef.scala:281)
        at org.apache.spark.util.JsonProtocol$JsonNodeImplicits.extractString(JsonProtocol.scala:1614)
        at org.apache.spark.util.JsonProtocol$.$anonfun$stackTraceFromJson$1(JsonProtocol.scala:1561)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
        at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
        at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
        at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
        at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
        at scala.collection.AbstractIterator.to(Iterator.scala:1431)
        at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
        at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)
        at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
        at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
        at org.apache.spark.util.JsonProtocol$.stackTraceFromJson(JsonProtocol.scala:1564)
        at org.apache.spark.util.JsonProtocol$.taskEndReasonFromJson(JsonProtocol.scala:1361)
        at org.apache.spark.util.JsonProtocol$.taskEndFromJson(JsonProtocol.scala:938)
        at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:876)
        at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:865)
        at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:88)
        at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:59)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3(FsHistoryProvider.scala:1140)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3$adapted(FsHistoryProvider.scala:1138)
        at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2777)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1(FsHistoryProvider.scala:1138)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1$adapted(FsHistoryProvider.scala:1136)
        at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
        at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
        at org.apache.spark.deploy.history.FsHistoryProvider.parseAppEventLogs(FsHistoryProvider.scala:1136)
        at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:1117)
{code}",,,,,,,,,,,,,,,,,,,,SPARK-39489,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Feb 11 05:54:46 UTC 2023,,,,,,,,,,"0|z1fv34:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,,"11/Feb/23 01:33;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39972;;;","11/Feb/23 01:34;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39972;;;","11/Feb/23 02:10;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/39973;;;","11/Feb/23 05:54;dongjoon;Issue resolved by pull request 39973
[https://github.com/apache/spark/pull/39973];;;",,,,,,,,,,,
Incorrect results or NPE when inserting null value into array using array_insert/array_append,SPARK-42401,13524302,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,10/Feb/23 23:18,15/Feb/23 02:45,30/Oct/23 17:26,13/Feb/23 06:50,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,correctness,,,,"Example:
{noformat}
create or replace temp view v1 as
select * from values
(array(1, 2, 3, 4), 5, 5),
(array(1, 2, 3, 4), 5, null)
as v1(col1,col2,col3);

select array_insert(col1, col2, col3) from v1;
{noformat}
This produces an incorrect result:
{noformat}
[1,2,3,4,5]
[1,2,3,4,0] <== should be [1,2,3,4,null]
{noformat}
A more succint example:
{noformat}
select array_insert(array(1, 2, 3, 4), 5, cast(null as int));
{noformat}
This also produces an incorrect result:
{noformat}
[1,2,3,4,0] <== should be [1,2,3,4,null]
{noformat}
Another example:
{noformat}
create or replace temp view v1 as
select * from values
(array('1', '2', '3', '4'), 5, '5'),
(array('1', '2', '3', '4'), 5, null)
as v1(col1,col2,col3);

select array_insert(col1, col2, col3) from v1;
{noformat}
The above query throws a {{NullPointerException}}:
{noformat}
23/02/10 11:08:05 ERROR SparkSQLDriver: Failed in [select array_insert(col1, col2, col3) from v1]
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.LocalTableScanExec.$anonfun$unsafeRows$1(LocalTableScanExec.scala:44)
{noformat}
{{array_append}} has the same issue:
{noformat}
spark-sql> select array_append(array(1, 2, 3, 4), cast(null as int));
[1,2,3,4,0] <== should be [1,2,3,4,null]
Time taken: 3.679 seconds, Fetched 1 row(s)
spark-sql> select array_append(array('1', '2', '3', '4'), cast(null as string));
23/02/10 11:13:36 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 15 02:45:56 UTC 2023,,,,,,,,,,"0|z1fv2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/23 23:37;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39970;;;","10/Feb/23 23:38;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39970;;;","13/Feb/23 06:50;gurwls223;Issue resolved by pull request 39970
[https://github.com/apache/spark/pull/39970];;;","15/Feb/23 00:27;bersprockets;There is another case:
{noformat}
spark-sql> select array_insert(array('1', '2', '3', '4'), -6, '5');
23/02/14 16:10:19 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
{noformat}
{{array_insert}} might implicitly add nulls, and my fix does not cover that case. I will follow up.;;;","15/Feb/23 02:45;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/40026;;;","15/Feb/23 02:45;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/40026;;;",,,,,,,,,
CONV() silently overflows returning wrong results,SPARK-42399,13524280,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,srielau,srielau,10/Feb/23 19:03,15/Feb/23 16:29,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"spark-sql> SELECT CONV(SUBSTRING('0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff', 3), 16, 10);

18446744073709551615

Time taken: 2.114 seconds, Fetched 1 row(s)

spark-sql> set spark.sql.ansi.enabled = true;

spark.sql.ansi.enabled true

Time taken: 0.068 seconds, Fetched 1 row(s)

spark-sql> SELECT CONV(SUBSTRING('0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff', 3), 16, 10);

18446744073709551615

Time taken: 0.05 seconds, Fetched 1 row(s)


In ANSI mode we should raise an error for sure.
In non ANSI either an error or a NULL maybe be acceptable.

Alternatively, of course, we could consider if we can support arbitrary domains since the result is a STRING again. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 15 16:29:01 UTC 2023,,,,,,,,,,"0|z1fuxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/23 16:37;narekdw;Why do we need to throw an exception in ANSI mode, is it described somewhere in SQL standards? 

What do you think if such a case will be considered as a valid scenario and it will give a correct result?

For example, such a query:
{code:java}
spark-sql> SELECT CONV(SUBSTRING('0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff', 3), 16, 10); {code}
will be evaluated to:
{code:java}
115792089237316195423570985008687907853269984665640564039457584007913129639935 {code}
 

It could be implemented if we use BigInt, instead of `NumberConverter.convert(...)` which uses Long as a data type.

 

P.S. But it might affect the performance.;;;","15/Feb/23 00:09;srielau;Adding support is of course best. If it can be done quickly, if not we should stop the wrong results first.;;;","15/Feb/23 16:29;apachespark;User 'NarekDW' has created a pull request for this issue:
https://github.com/apache/spark/pull/40040;;;",,,,,,,,,,,,
Inconsistent data produced by `FlatMapCoGroupsInPandas`,SPARK-42397,13524199,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,tedjenks,tedjenks,10/Feb/23 10:26,13/Feb/23 08:23,30/Oct/23 17:26,,3.3.0,3.3.1,,,,,,,,,,,,,,,,,,,,,,Pandas API on Spark,SQL,,,,0,,,,,"We are seeing inconsistent data returned when using `FlatMapCoGroupsInPandas`. In the PySpark example from the comments, when we call `grouped_df.collect()` we get:

 

{{[Row(left_colms=""Index(['cluster', 'event', 'abc'], dtype='object')"", right_colms=""Index(['cluster', 'event', 'def'], dtype='object')"")] }}

 

When we call `grouped_df.show(5, truncate=False)` we get:

 

{{[Row(left_colms=""Index(['cluster', 'abc'], dtype='object')"", right_colms=""Index(['cluster', 'event', 'def'], dtype='object')"", xyz='1234')] }}

 

When we call `grouped_df_1.collect()` we get:

 

{{[Row(left_colms=""Index(['cluster', 'abc'], dtype='object')"", right_colms=""Index(['cluster', 'event', 'def'], dtype='object')"", xyz='1234')] }}

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Feb 13 08:23:58 UTC 2023,,,,,,,,,,"0|z1fufk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/23 10:28;tedjenks;{{    test_df = spark.createDataFrame(}}
{{        [}}
{{            [""1"", ""23"", ""abc"", ""blah"", ""def"", ""1""],}}
{{            [""1"", ""23"", ""abc"", ""blah"", ""def"", ""1""],}}
{{            [""1"", ""23"", ""abc"", ""blah"", ""def"", ""2""],}}
{{            [""1"", ""23"", ""abc"", ""blah"", ""def"", ""2""],}}
{{        ],}}
{{        [""cluster"", ""partition"", ""event"", ""abc"", ""def"", ""one_or_two""]}}
{{    )}}
{{    df1 = test_df.filter(}}
{{        F.col(""one_or_two"") == ""1""}}
{{    ).select(}}
{{        ""cluster"", ""event"", ""abc""}}
{{    )}}{{    df2 = test_df.filter(}}
{{        F.col(""one_or_two"") == ""2""}}
{{    ).select(}}
{{        ""cluster"", ""event"", ""def""}}
{{    )}}
{{    def get_schema(l, r):}}
{{            return pd.DataFrame(}}
{{                [(str(l.columns), str(r.columns))],}}
{{                columns=[""left_colms"", ""right_colms""]}}
{{            )}}{{   grouped_df = df1.groupBy(""cluster"").cogroup(df2.groupBy(""cluster"")).applyInPandas(}}
{{        get_schema, ""left_colms string, right_colms string""}}
{{    )}}
{{    grouped_df_1 = grouped_df.withColumn(}}
{{       ""xyz"", F.lit(""1234"")}}
{{     )}};;;","13/Feb/23 06:53;gurwls223;It's probably related the order which Spark doesn't guarantee. Is the actual value different?;;;","13/Feb/23 08:23;tedjenks;Is it ever expected for df.show() and df.collect() to give different results? That is what struck me as odd in this case and yes those two give different values.;;;",,,,,,,,,,,,
The code logic of the configmap max size validation lacks extra content,SPARK-42395,13524189,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ninebigbig,ninebigbig,10/Feb/23 09:55,10/Feb/23 09:58,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,3.3.1,,,,Kubernetes,,,,,0,,,,,"In each configmap, Spark adds an extra content in a fixed format,this extra content of the configmap is as belows:
  spark.kubernetes.namespace: default
  spark.properties: |
    #Java properties built from Kubernetes config map with name: spark-exec-b47b438630eec12d-conf-map
    #Wed Feb 08 20:10:19 CST 2023
    spark.kubernetes.namespace=default

But the max size validation code logic does not consider this part ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Feb 10 09:58:53 UTC 2023,,,,,,,,,,"0|z1fudc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/23 09:58;apachespark;User 'ninebigbig' has created a pull request for this issue:
https://github.com/apache/spark/pull/39967;;;",,,,,,,,,,,,,,
Mask function's generated code does not handle null input,SPARK-42384,13523803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,08/Feb/23 16:46,16/Feb/23 00:26,30/Oct/23 17:26,16/Feb/23 00:26,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"Example:
{noformat}
create or replace temp view v1 as
select * from values
(null),
('AbCD123-@$#')
as data(col1);

cache table v1;

select mask(col1) from v1;
{noformat}
This query results in a {{NullPointerException}}:
{noformat}
23/02/07 16:36:06 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
{noformat}
The generated code calls {{UnsafeWriter.write(0, value_0)}} regardless of whether {{Mask.transformInput}} returns null or not. The {{UnsafeWriter.write}} method for {{UTF8String}} does not expect a null pointer.
{noformat}
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     UTF8String value_1 = isNull_1 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */
/* 035 */
/* 036 */
/* 037 */
/* 038 */     UTF8String value_0 = null;
/* 039 */     value_0 = org.apache.spark.sql.catalyst.expressions.Mask.transformInput(value_1, ((UTF8String) references[0] /* literal */), ((UTF8String) references[1] /* literal */), ((UTF8String) references[2] /* literal */), ((UTF8String) references[3] /* literal */));;
/* 040 */     if (false) {
/* 041 */       mutableStateArray_0[0].setNullAt(0);
/* 042 */     } else {
/* 043 */       mutableStateArray_0[0].write(0, value_0);
/* 044 */     }
/* 045 */     return (mutableStateArray_0[0].getRow());
/* 046 */   }
{noformat}

The bug is not exercised by a literal null input value, since there appears to be some optimization that simply replaces the entire function call with a null literal:
{noformat}
spark-sql> explain SELECT mask(NULL);
== Physical Plan ==
*(1) Project [null AS mask(NULL, X, x, n, NULL)#47]
+- *(1) Scan OneRowRelation[]

Time taken: 0.026 seconds, Fetched 1 row(s)
spark-sql> SELECT mask(NULL);
NULL
Time taken: 0.042 seconds, Fetched 1 row(s)
spark-sql> 
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Feb 16 00:26:50 UTC 2023,,,,,,,,,,"0|z1frzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/23 17:23;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39945;;;","16/Feb/23 00:26;Gengliang.Wang;Issue resolved by pull request 39945
[https://github.com/apache/spark/pull/39945];;;",,,,,,,,,,,,,
Spark History Server fails to start on CentOS7 aarch64,SPARK-42370,13523459,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,wuzhiguo,wuzhiguo,07/Feb/23 08:54,07/Feb/23 11:28,30/Oct/23 17:26,,3.2.3,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"When I run `./sbin/start-history-server.sh`

I'll get the error below

!image-2023-02-07-16-54-43-593.png!

 

Although we already use org.openlabtesting.leveldbjni on aarch64, which can works on aarch64,

we still load org.fusesource.hawtjni.runtime.Library on wrong jar file

When we run `export SPARK_DAEMON_JAVA_OPTS=-verbose:class`, we can see the class is loaded from jline-2.14.6.jar where the correct class file is under leveldbjni-all-1.8.jar

 

Incorrect(now):

[Loaded org.fusesource.hawtjni.runtime.Library from file:/yourdir/spark/jars/jline-2.14.6.jar] 

Correct(expected):

[Loaded org.fusesource.hawtjni.runtime.Library from file:/yourdir/spark/jars/leveldbjni-all-1.8.jar] ",,,,,,,,,,,,,,,,,,,,,,BIGTOP-3907,,,,,,,,"07/Feb/23 08:54;wuzhiguo;image-2023-02-07-16-54-43-593.png;https://issues.apache.org/jira/secure/attachment/13055213/image-2023-02-07-16-54-43-593.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-02-07 08:54:10.0,,,,,,,,,,"0|z1fpvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot resolve orderby attributes in DISTINCT ,SPARK-42356,13523190,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,blackpig,blackpig,06/Feb/23 08:45,07/Mar/23 12:08,30/Oct/23 17:26,06/Feb/23 09:24,3.3.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"query:
{code:java}
CREATE TABLE students (name VARCHAR(64), address VARCHAR(64)) USING PARQUET PARTITIONED BY (student_id INT);
SELECT DISTINCT trim(name) AS new_name FROM students order by name desc;
{code}
error:
{code:java}
spark-sql> SELECT DISTINCT trim(name) AS new_name FROM students order by name desc;
Error in query: Column 'name' does not exist. Did you mean one of the following? [new_name]; line 1 pos 62;
'Sort ['name DESC NULLS LAST], true
+- Distinct
   +- Project trim(name#1, None) AS new_name#0
      +- SubqueryAlias spark_catalog.default.students
         +- Relation default.studentsname#1,address#2,student_id#3 parquet {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Mar 07 12:08:26 UTC 2023,,,,,,,,,,"0|z1fo7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/23 12:08;BetaCat;Hi [~blackpig]  could I know more detail reasons about this issue that why resolution is won't fix, it still not work on the latest version. Thanks;;;",,,,,,,,,,,,,,
Cleanup orphan sst and log files in RocksDB checkpoint directory,SPARK-42353,13523168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Chaoqin,Chaoqin,Chaoqin,06/Feb/23 07:54,09/Feb/23 23:22,30/Oct/23 17:26,09/Feb/23 23:22,3.2.3,,,,,,,,,,,,,,,,,,,3.5.0,,,,Structured Streaming,,,,,0,,,,,"When RocksDB version.zip file get overwritten (e.g. concurrent task execution, task/stage/batch reattempts) or the zip file don't get uploaded successfully, the associated sst and log files don't get garbage collected.([https://github.com/databricks/runtime/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala|https://github.com/databricks/runtime/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala#L305-L309]) These files consume storage. We can clean up these SST files during periodic state store maintenance. The major concern is that sst files for ongoing version also appear to be ""orphan"" because they are uploaded before zip file, we have to be careful not to delete them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Feb 09 23:22:11 UTC 2023,,,,,,,,,,"0|z1fo2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/23 08:15;apachespark;User 'chaoqin-li1123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39897;;;","09/Feb/23 23:22;kabhwan;Issue resolved by pull request 39897
[https://github.com/apache/spark/pull/39897];;;",,,,,,,,,,,,,
Arrow string and binary vectors only support 1 GiB,SPARK-42347,13523085,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kimahriman,kimahriman,04/Feb/23 20:30,04/Feb/23 20:39,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Since Arrow 10.0.0, BaseVariableWidthVector (the parent for string and binary vectors), only supports expanding up to 1 GiB through the safe interfaces, which Spark uses, instead of 2 GiB previously. This is due to [https://github.com/apache/arrow/pull/13815.] I added a comment in there but haven't got any responses yet, will make an issue in Arrow as well.

Basically whenever you try to add data beyond 1 GiB, the vector will try to double itself to the next power of two, which would be {{{}2147483648{}}}, which is greater than {{Integer.MAX_VALUE}} which is {{{}2147483647{}}}, thus throwing a {{{}OversizedAllocationException{}}}.

See [https://github.com/apache/spark/pull/39572#issuecomment-1383195213] and the comment above for how I recreated to show this was now the case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Feb 04 20:31:42 UTC 2023,,,,,,,,,,"0|z1fnk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/23 20:31;kimahriman;[https://github.com/apache/spark/pull/39572] is a potential workaround to allow enabling the large variable width vectors when users hit this limit.;;;",,,,,,,,,,,,,,
distinct(count colname) with UNION ALL causes query analyzer bug,SPARK-42346,13523064,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,RobinLinacre,RobinLinacre,04/Feb/23 08:52,08/Feb/23 19:40,30/Oct/23 17:26,06/Feb/23 13:29,3.3.0,3.4.0,3.5.0,,,,,,,,,,,,,,,,,3.3.2,3.4.0,3.5.0,,SQL,,,,,0,,,,,"If you combine a UNION ALL with a count(distinct colname) you get a query analyzer bug.

 

This behaviour is introduced in 3.3.0.  The bug was not present in 3.2.1.

 

Here is a reprex in PySpark:

{{df_pd = pd.DataFrame([}}
{{    \{'surname': 'a', 'first_name': 'b'}}}
{{])}}
{{df_spark = spark.createDataFrame(df_pd)}}
{{df_spark.createOrReplaceTempView(""input_table"")}}

{{sql = """"""}}

{{SELECT }}
{{    (SELECT Count(DISTINCT first_name) FROM   input_table) }}
{{        AS distinct_value_count}}
{{FROM   input_table}}
{{UNION ALL}}
{{SELECT }}
{{    (SELECT Count(DISTINCT surname) FROM   input_table) }}
{{        AS distinct_value_count}}
{{FROM   input_table """"""}}

{{spark.sql(sql).toPandas()}}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 08 19:40:59 UTC 2023,,,,,,,,,,"0|z1fnfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/23 15:03;yumwang;cc [~petertoth];;;","04/Feb/23 15:58;petertoth;Thanks for pinging me [~yumwang], this might be subquery merge related. I will look into it.;;;","05/Feb/23 10:19;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/39887;;;","05/Feb/23 10:23;petertoth;[~yumwang], [~RobinLinacre], https://github.com/apache/spark/pull/39887 will fix the issue.

[~viirya], as this is a regression from 3.2 to 3.3, if possible please include this in 3.3.2.;;;","05/Feb/23 17:58;viirya;I will wait for this patch before cutting RC1.;;;","06/Feb/23 13:29;yumwang;Issue resolved by pull request 39887
[https://github.com/apache/spark/pull/39887];;;","07/Feb/23 01:35;ritikam;I have Spark 3.3.0 and I do not have 39887 fix . I am not able to reproduce this issue. Am I missing something?

 

scala> val df = Seq((""a"",""b"")).toDF(""surname"",""first_name"")

*df*: *org.apache.spark.sql.DataFrame* = [surname: string, first_name: string]

 

scala> df.createOrReplaceTempView(""input_table"")

 

scala> spark.sql(""select(Select Count(Distinct first_name) from input_table) As distinct_value_count from input_table Union all select (select count(Distinct surname) from input_table) as distinct_value_count from input_table"").show()

+--------------------+                                                          

|distinct_value_count|

+--------------------+

|                   1|

|                   1|

+--------------------+

= Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Union
   :- Project [cast(Subquery subquery#46, [id=#114] as string) AS distinct_value_count#62]
   :  :  +- Subquery subquery#46, [id=#114]
   :  :     +- AdaptiveSparkPlan isFinalPlan=false
   :  :        +- HashAggregate(keys=[], functions=[count(first_name#12)], output=[count(DISTINCT first_name)#53L])
   :  :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#112]
   :  :              +- HashAggregate(keys=[], functions=[partial_count(first_name#12)], output=[count#67L])
   :  :                 +- LocalTableScan [first_name#12]
   :  +- LocalTableScan [_1#6, _2#7]
   +- Project [cast(Subquery subquery#48, [id=#125] as string) AS distinct_value_count#64]
      :  +- Subquery subquery#48, [id=#125]
      :     +- AdaptiveSparkPlan isFinalPlan=false
      :        +- HashAggregate(keys=[], functions=[count(surname#11)], output=[count(DISTINCT surname)#55L])
      :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#123]
      :              +- HashAggregate(keys=[], functions=[partial_count(surname#11)], output=[count#68L])
      :                 +- LocalTableScan [surname#11]
      +- LocalTableScan [_1#50, _2#51|#50, _2#51]


 

This is what I have in my SparkOptimizer.scala


override def defaultBatches: Seq[Batch] = (preOptimizationBatches ++ super.defaultBatches :+
Batch(""Optimize Metadata Only Query"", Once, OptimizeMetadataOnlyQuery(catalog)) :+
Batch(""PartitionPruning"", Once,
PartitionPruning) :+
Batch(""InjectRuntimeFilter"", FixedPoint(1),
InjectRuntimeFilter,
RewritePredicateSubquery) :+
Batch(""MergeScalarSubqueries"", Once,
MergeScalarSubqueries) :+
Batch(""Pushdown Filters from PartitionPruning"", fixedPoint,
PushDownPredicates) :+
Batch;;;","07/Feb/23 07:56;petertoth;[~ritikam], please use the Pyspark repro in description or add a 2nd row to your input_table if you use Scala. That's because Spark can optimize out count distinct from one row local relations.;;;","07/Feb/23 20:58;ritikam;Hello added three rows to input_table. Still no error. I do have DPP enabled.

*********************************************************************

Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 12.0.2)

Type in expressions to have them evaluated.

Type :help for more information.

 

scala> val df = Seq((""a"",""b""),(""c"",""d""),(""e"",""f"")).toDF(""surname"",""first_name"")

*df*: *org.apache.spark.sql.DataFrame* = [surname: string, first_name: string]

 

scala> df.createOrReplaceTempView(""input_table"")

 

scala> spark.sql(""select(Select Count(Distinct first_name) from input_table) As distinct_value_count from input_table Union all select (select count(Distinct surname) from input_table) as distinct_value_count from input_table"").show()

+--------------------+                                                          

|distinct_value_count|

+--------------------+

|                   3|

|                   3|

|                   3|

|                   3|

|                   3|

|                   3|

+--------------------+

 

**************************************************************

AdaptiveSparkPlan isFinalPlan=false
+- Union
   :- Project [cast(Subquery subquery#145, [id=#571] as string) AS distinct_value_count#161]
   :  :  +- Subquery subquery#145, [id=#571]
   :  :     +- AdaptiveSparkPlan isFinalPlan=false
   :  :        +- HashAggregate(keys=[], functions=[count(distinct first_name#8)], output=[count(DISTINCT first_name)#152L])
   :  :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#569]
   :  :              +- HashAggregate(keys=[], functions=[partial_count(distinct first_name#8)], output=[count#167L])
   :  :                 +- HashAggregate(keys=[first_name#8], functions=[], output=[first_name#8])
   :  :                    +- Exchange hashpartitioning(first_name#8, 200), ENSURE_REQUIREMENTS, [id=#565]
   :  :                       +- HashAggregate(keys=[first_name#8], functions=[], output=[first_name#8])
   :  :                          +- LocalTableScan [first_name#8]
   :  +- LocalTableScan [_1#2, _2#3]
   +- Project [cast(Subquery subquery#147, [id=#590] as string) AS distinct_value_count#163]
      :  +- Subquery subquery#147, [id=#590]
      :     +- AdaptiveSparkPlan isFinalPlan=false
      :        +- HashAggregate(keys=[], functions=[count(distinct surname#7)], output=[count(DISTINCT surname)#154L])
      :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#588]
      :              +- HashAggregate(keys=[], functions=[partial_count(distinct surname#7)], output=[count#170L])
      :                 +- HashAggregate(keys=[surname#7], functions=[], output=[surname#7])
      :                    +- Exchange hashpartitioning(surname#7, 200), ENSURE_REQUIREMENTS, [id=#584]
      :                       +- HashAggregate(keys=[surname#7], functions=[], output=[surname#7])
      :                          +- LocalTableScan [surname#7]
      +- LocalTableScan [_1#149, _2#150];;;","08/Feb/23 11:16;petertoth;[~ritikam], you also need to disable the ""ConvertToLocalRelation"" rule optimization `--conf ""spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation""` to get the error from spark-shell.;;;","08/Feb/23 19:40;ritikam;Yes that caused the error to appear. Thanks;;;",,,,
The default size of the CONFIG_MAP_MAXSIZE should not be greater than 1048576,SPARK-42344,13523054,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ninebigbig,ninebigbig,ninebigbig,04/Feb/23 05:08,05/Feb/23 11:13,30/Oct/23 17:26,05/Feb/23 11:09,3.3.1,,,,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,Kubernetes,Spark Submit,,,,0,,,,,"Exception in thread ""main"" io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.18.123.24:6443/api/v1/namespaces/default/configmaps. Message: ConfigMap ""spark-exec-ed9f2c861aa40b48-conf-map"" is invalid: []: Too long: must have at most 1048576 bytes. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=[], message=Too long: must have at most 1048576 bytes, reason=FieldValueTooLong, additionalProperties={})], group=null, kind=ConfigMap, name=spark-exec-ed9f2c861aa40b48-conf-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=ConfigMap ""spark-exec-ed9f2c861aa40b48-conf-map"" is invalid: []: Too long: must have at most 1048576 bytes, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties={}).
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:682)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:661)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:612)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:555)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:518)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:305)
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644)
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83)
        at io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61)
        at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.setUpExecutorConfigMap(KubernetesClusterSchedulerBackend.scala:88)
        at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:112)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:222)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:595)
        at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2714)
        at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
        at org.apache.spark.examples.JavaSparkPi.main(JavaSparkPi.java:37)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Kubernetes: 1.22.0

ETCD: 3.5.0

Spark: 3.3.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Feb 05 11:09:10 UTC 2023,,,,,,,,,,"0|z1fndc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/23 05:43;apachespark;User 'ninebigbig' has created a pull request for this issue:
https://github.com/apache/spark/pull/39884;;;","05/Feb/23 11:09;dongjoon;Issue resolved by pull request 39884
[https://github.com/apache/spark/pull/39884];;;",,,,,,,,,,,,,
Fix metadata col can not been resolved,SPARK-42331,13522905,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,03/Feb/23 06:19,13/Feb/23 04:44,30/Oct/23 17:26,13/Feb/23 04:44,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Feb 13 04:44:08 UTC 2023,,,,,,,,,,"0|z1fmgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/23 07:07;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/39870;;;","13/Feb/23 04:44;cloud_fan;Issue resolved by pull request 39870
[https://github.com/apache/spark/pull/39870];;;",,,,,,,,,,,,,
why executor memory used is shown greater than total available memory on spark ui,SPARK-42293,13522792,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,handong,handong,02/Feb/23 13:48,17/Feb/23 23:24,30/Oct/23 17:26,,2.4.5,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"*I have a spark  streaming job that is running for around last 3 weeks. When I open the Executors tab on spark web UI, it shows*
 # {{memory used - 36.1GB}}
 # total available memory for storage - 3.2GB

*Please refer to the below screenshot of Spark UI*

!https://i.stack.imgur.com/nmk39.jpg!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Feb 17 23:24:22 UTC 2023,,,,,,,,,,"0|z1flrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/23 06:55;gurwls223;[~handong] mind sharing a reproducer if you have?;;;","17/Feb/23 13:11;handong;[~gurwls223]  sorry,I don't understand what you mean;;;","17/Feb/23 23:24;gurwls223;Can you show your codes you executed? In order to fix this issue, we should reproduce this bug to debug.;;;",,,,,,,,,,,,
Spark SQL not use hive partition info,SPARK-42292,13522776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,xuanzhiang,xuanzhiang,02/Feb/23 12:08,03/Feb/23 02:11,30/Oct/23 17:26,03/Feb/23 02:11,3.2.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"I use spark3 to count partition num , like : 

table a is external parquet table, it have 3 partition columns (year ,month, day).

query sql : ""select distinct month , day from a where year = '2022' ""

i think spark can find hive metadata and use partition info, but it load all  ""year = '2022'"" partition data.

in spark2.4, it use TableLocalScanExec ,but spark3 use HiveTableRelation and scan hive parquet.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Feb 03 02:11:36 UTC 2023,,,,,,,,,,"0|z1flns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/23 02:11;xuanzhiang;when i set spark.sql.hive.convertMetastoreParquet=true , spark3 use inner parquet reader.;;;",,,,,,,,,,,,,,
Spark Driver hangs on OOM during Broadcast when AQE is enabled ,SPARK-42290,13522751,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,fanjia,shardulm,shardulm,02/Feb/23 09:45,11/Jun/23 11:25,30/Oct/23 17:26,08/Jun/23 20:24,3.4.0,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,,,0,,,,,"Repro steps:
{code}
$ spark-shell --conf spark.driver.memory=1g

val df = spark.range(5000000).withColumn(""str"", lit(""abcdabcdabcdabcdabasgasdfsadfasdfasdfasfasfsadfasdfsadfasdf""))
val df2 = spark.range(10).join(broadcast(df), Seq(""id""), ""left_outer"")

df2.collect
{code}

This will cause the driver to hang indefinitely. Heres a thread dump of the {{main}} thread when its stuck
{code}
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:285)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2819/629294880.apply(Unknown Source)
org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:809)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236) => holding Monitor(java.lang.Object@1932537396})
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:381)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)
org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4179)
org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3420)
org.apache.spark.sql.Dataset$$Lambda$2390/1803372144.apply(Unknown Source)
org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4169)
org.apache.spark.sql.Dataset$$Lambda$2791/1357377136.apply(Unknown Source)
org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4167)
org.apache.spark.sql.Dataset$$Lambda$2391/1172042998.apply(Unknown Source)
org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
org.apache.spark.sql.execution.SQLExecution$$$Lambda$2402/721269425.apply(Unknown Source)
org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
org.apache.spark.sql.execution.SQLExecution$$$Lambda$2392/11632488.apply(Unknown Source)
org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:809)
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
org.apache.spark.sql.Dataset.withAction(Dataset.scala:4167)
org.apache.spark.sql.Dataset.collect(Dataset.scala:3420)
{code}


When we disable AQE though we get the following exception instead of driver hang.
{code}
Caused by: org.apache.spark.SparkException: Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting spark.sql.autoBroadcastJoinThreshold to -1 or increase the spark driver memory by setting spark.driver.memory to a higher value.
  ... 7 more
Caused by: java.lang.OutOfMemoryError: Java heap space
  at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.grow(HashedRelation.scala:834)
  at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.append(HashedRelation.scala:777)
  at org.apache.spark.sql.execution.joins.LongHashedRelation$.apply(HashedRelation.scala:1086)
  at org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:157)
  at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:1163)
  at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:1151)
  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:148)
  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$Lambda$2999/145945436.apply(Unknown Source)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:217)
  at org.apache.spark.sql.execution.SQLExecution$$$Lambda$3001/1900142693.call(Unknown Source)
  ... 4 more
{code}
I expect to see the same exception even when AQE is enabled. ",,,,,,,,,,,,,,,,,,,,,,,,,SPARK-40663,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jun 11 11:25:22 UTC 2023,,,,,,,,,,"0|z1fli8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/23 20:24;dongjoon;Issue resolved by pull request 41517
[https://github.com/apache/spark/pull/41517];;;","09/Jun/23 00:06;fanjia;[~dongjoon] Seem the assigning people not right.;;;","10/Jun/23 08:31;dongjoon;Oh, sorry, [~fanjia]. I fixed it now.;;;","11/Jun/23 11:25;fanjia;Thanks [~dongjoon] ;;;",,,,,,,,,,,
Fix internal error for valid CASE WHEN expression with CAST when inserting into a table,SPARK-42286,13522606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,runyao,runyao,runyao,01/Feb/23 23:00,24/Feb/23 18:58,30/Oct/23 17:26,03/Feb/23 05:12,3.3.1,3.3.2,3.4.0,,,,,,,,,,,,,,,,,3.3.3,3.4.0,,,Spark Core,,,,,0,,,,,"```

spark-sql> create or replace table es570639t1 as select x FROM values (1), (2), (3) as tab(x);
spark-sql> create or replace table es570639t2 (x Decimal(9, 0));
spark-sql> insert into es570639t2 select 0 - (case when x = 1 then 1 else x end) from es570639t1 where x = 1;

```

hits the following internal error
org.apache.spark.SparkException: [INTERNAL_ERROR] Child is not Cast or ExpressionProxy of Cast
 

Stack trace:
org.apache.spark.SparkException: [INTERNAL_ERROR] Child is not Cast or ExpressionProxy of Cast at org.apache.spark.SparkException$.internalError(SparkException.scala:78) at org.apache.spark.SparkException$.internalError(SparkException.scala:82) at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.checkChild(Cast.scala:2693) at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2697) at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2683) at org.apache.spark.sql.catalyst.trees.UnaryLike.$anonfun$mapChildren$5(TreeNode.scala:1315) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106) at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1314) at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1309) at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:636) at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:570) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:570)
 

This internal error comes from `CheckOverflowInTableInsert``checkChild`, where we covered only `Cast` expr and `ExpressionProxy` expr, but not the `CaseWhen` expr.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Feb 23 06:23:45 UTC 2023,,,,,,,,,,"0|z1fkmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/23 23:03;apachespark;User 'RunyaoChen' has created a pull request for this issue:
https://github.com/apache/spark/pull/39855;;;","01/Feb/23 23:03;apachespark;User 'RunyaoChen' has created a pull request for this issue:
https://github.com/apache/spark/pull/39855;;;","03/Feb/23 05:12;Gengliang.Wang;Resolved in https://github.com/apache/spark/pull/39855;;;","23/Feb/23 06:23;apachespark;User 'RunyaoChen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40140;;;",,,,,,,,,,,
Add ServicesResourceTransformer to connect server module  shade configuration,SPARK-42276,13522433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,01/Feb/23 08:38,10/Feb/23 02:58,30/Oct/23 17:26,10/Feb/23 02:58,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.4.0,,,,Build,Connect,,,,0,,,,,The contents of META-INF/services directory in the shaded connect-server jar have not been relocated.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Feb 10 02:58:41 UTC 2023,,,,,,,,,,"0|z1fjkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/23 13:05;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39848;;;","01/Feb/23 13:06;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39848;;;","10/Feb/23 02:58;gurwls223;Issue resolved by pull request 39848
[https://github.com/apache/spark/pull/39848];;;",,,,,,,,,,,,
Upgrade `compress-lzf` to 1.1.2,SPARK-42274,13522421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,01/Feb/23 07:56,01/Feb/23 10:12,30/Oct/23 17:26,01/Feb/23 10:12,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 01 10:12:33 UTC 2023,,,,,,,,,,"0|z1fjhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/23 08:01;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39841;;;","01/Feb/23 10:12;dongjoon;Issue resolved by pull request 39841
[https://github.com/apache/spark/pull/39841];;;",,,,,,,,,,,,,
Table schema changes via V2SessionCatalog with HiveExternalCatalog,SPARK-42262,13522295,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,planga82,planga82,31/Jan/23 19:50,31/Jan/23 20:17,30/Oct/23 17:26,,3.5.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,When configuring HiveExternalCatalog there are certain schema change operations in V2SessionCatalog that are not performed due to the use of the alterTable method instead of alterTableDataSchema.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 31 20:17:01 UTC 2023,,,,,,,,,,"0|z1fiqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/23 19:51;planga82;I'm working on this;;;","31/Jan/23 20:17;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/39826;;;",,,,,,,,,,,,,
K8s will not allocate more execs if there are any pending execs until next snapshot,SPARK-42261,13522285,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,holden,holden,31/Jan/23 18:51,09/Aug/23 19:16,30/Oct/23 17:26,,3.3.0,3.3.1,3.3.2,3.4.0,3.4.1,3.5.0,,,,,,,,,,,,,,,,,,Kubernetes,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 01 03:42:30 UTC 2023,,,,,,,,,,"0|z1fiog:",9223372036854775807,,,,,,,,,,,,,4.0.0,,,,,,,,,,"31/Jan/23 19:00;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/39825;;;","01/Feb/23 03:42;dongjoon;I'm -1 because this is a feature to prevent pending resource (pod and dependent resources like PVCs) explosions which results EKS control plane congestion and a waste of money.;;;",,,,,,,,,,,,,
ResolveGroupingAnalytics should take care of Python UDAF,SPARK-42259,13522278,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,31/Jan/23 17:49,01/Feb/23 09:42,30/Oct/23 17:26,01/Feb/23 09:42,3.2.0,3.3.0,,,,,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 01 09:42:14 UTC 2023,,,,,,,,,,"0|z1fimw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/23 17:53;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39824;;;","01/Feb/23 09:42;cloud_fan;Issue resolved by pull request 39824
[https://github.com/apache/spark/pull/39824];;;",,,,,,,,,,,,,
predict_batch_udf with float fails when the batch size consists of single value,SPARK-42250,13522150,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,31/Jan/23 03:31,31/Jan/23 10:42,30/Oct/23 17:26,31/Jan/23 10:42,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,ML,PySpark,,,,0,,,,,"{code}
import numpy as np
import pandas as pd
from pyspark.ml.functions import predict_batch_udf
from pyspark.sql.types import ArrayType, FloatType, StructType, StructField
from typing import Mapping

df = spark.createDataFrame([[[0.0, 1.0, 2.0, 3.0], [0.0, 1.0, 2.0]], [[4.0, 5.0, 6.0, 7.0], [4.0, 5.0, 6.0]]], schema=[""t1"", ""t2""])

def make_multi_sum_fn():
    def predict(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:
        return np.sum(x1, axis=1) + np.sum(x2, axis=1)
    return predict

multi_sum_udf = predict_batch_udf(
    make_multi_sum_fn,
    return_type=FloatType(),
    batch_size=1,
    input_tensor_shapes=[[4], [3]],
)

df.select(multi_sum_udf(""t1"", ""t2"")).collect()
{code}

fails as below:

{code}
 File ""/.../spark/python/lib/pyspark.zip/pyspark/worker.py"", line 829, in main
    process()
  File ""/.../spark/python/lib/pyspark.zip/pyspark/worker.py"", line 821, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 345, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 86, in dump_stream
    for batch in iterator:
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 339, in init_stream_yield_batches
    batch = self._create_batch(series)
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 275, in _create_batch
    arrs.append(create_array(s, t))
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 245, in create_array
    raise e
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 233, in create_array
    array = pa.Array.from_pandas(s, mask=mask, type=t, safe=self._safecheck)
  File ""pyarrow/array.pxi"", line 1044, in pyarrow.lib.Array.from_pandas
  File ""pyarrow/array.pxi"", line 316, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 83, in pyarrow.lib._ndarray_to_array
  File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Could not convert array(569.) with type numpy.ndarray: tried to convert to float32

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:554)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:507)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 31 10:42:21 UTC 2023,,,,,,,,,,"0|z1fhug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/23 03:57;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39817;;;","31/Jan/23 10:42;gurwls223;Issue resolved by pull request 39817
[https://github.com/apache/spark/pull/39817];;;",,,,,,,,,,,,,
Upgrade snappy-java to 1.1.9.1,SPARK-42242,13522132,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,30/Jan/23 23:28,01/Feb/23 06:14,30/Oct/23 17:26,01/Feb/23 06:14,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 01 06:14:55 UTC 2023,,,,,,,,,,"0|z1fhqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/23 23:31;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39811;;;","01/Feb/23 06:14;dongjoon;Issue resolved by pull request 39811
[https://github.com/apache/spark/pull/39811];;;",,,,,,,,,,,,,
 Correct the condition for `SparkConnectServerUtils#findSparkConnectJar` to find the correct connect server jar for maven,SPARK-42241,13522105,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,30/Jan/23 17:52,31/Jan/23 00:12,30/Oct/23 17:26,31/Jan/23 00:12,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Connect,Tests,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 31 00:12:09 UTC 2023,,,,,,,,,,"0|z1fhkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/23 17:57;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39810;;;","30/Jan/23 17:58;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39810;;;","31/Jan/23 00:12;gurwls223;Issue resolved by pull request 39810
[https://github.com/apache/spark/pull/39810];;;",,,,,,,,,,,,
Missing typing for pandas_udf,SPARK-42235,13521983,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,donggu,donggu,30/Jan/23 05:45,11/Feb/23 13:47,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"The typing stub {{site-packages/pyspark/sql/pandas/functions.pyi}} has a list of possible signatures of {{pandas_udf}}. It is missing a case

{code:python}
import pyspark.sql.functions as F

# PySpark3's typing stub error
@F.pandas_udf(
    returnType=LongType()
)
def my_udf(dummy_col: pd.Series) -> pd.Series:
    ...
{code}

The stub defined {{pandas_udf(f, returnType, functionType)}} but not {{pandas_udf(f, returnType)}}. The official documentation recommends using a return type hint instead of {{returnType}}.
 
{code:python}
# defined
@overload
def pandas_udf( f: PandasScalarToScalarFunction, returnType: Union[AtomicDataTypeOrString, ArrayType], functionType: PandasScalarUDFType, ) -> UserDefinedFunctionLike: ...
 
# not defined
@overload
def pandas_udf( f: PandasScalarToScalarFunction, returnType: Union[AtomicDataTypeOrString, ArrayType]) -> UserDefinedFunctionLike: ...
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Feb 11 13:47:18 UTC 2023,,,,,,,,,,"0|z1fgu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/23 13:47;apachespark;User 'wayneguow' has created a pull request for this issue:
https://github.com/apache/spark/pull/39974;;;","11/Feb/23 13:47;apachespark;User 'wayneguow' has created a pull request for this issue:
https://github.com/apache/spark/pull/39974;;;",,,,,,,,,,,,,
connect-client-jvm module should shaded+relocation grpc,SPARK-42228,13521912,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,LuciferYang,LuciferYang,LuciferYang,29/Jan/23 08:37,01/Feb/23 18:11,30/Oct/23 17:26,01/Feb/23 18:11,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,3.4.0,,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jan 29 08:51:32 UTC 2023,,,,,,,,,,"0|z1fge8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/23 08:51;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39789;;;",,,,,,,,,,,,,,
Use approx_percentile function running slower than percentile in spark3 ,SPARK-42227,13521888,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,xuanzhiang,xuanzhiang,29/Jan/23 02:13,15/Feb/23 11:18,30/Oct/23 17:26,,3.2.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"approx_percentile(end_ts-start_ts,0.9) cost_p90

in spark3 , it use objectHashAggregate method , but it shuffle very slow. when i use percentile , it become fast. i dont know the reson, i think approx_percentile should fast.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/23 11:14;xuanzhiang;percentile+objectHashAggregateExec.png;https://issues.apache.org/jira/secure/attachment/13055402/percentile%2BobjectHashAggregateExec.png","13/Feb/23 11:14;xuanzhiang;percentile+objectHashAggregateExec_shuffle_task.png;https://issues.apache.org/jira/secure/attachment/13055404/percentile%2BobjectHashAggregateExec_shuffle_task.png","13/Feb/23 11:14;xuanzhiang;percentile_approx+objectHashAggregateExec.png;https://issues.apache.org/jira/secure/attachment/13055401/percentile_approx%2BobjectHashAggregateExec.png","13/Feb/23 11:14;xuanzhiang;percentile_approx+objectHashAggregateExec_shuffle_task.png;https://issues.apache.org/jira/secure/attachment/13055403/percentile_approx%2BobjectHashAggregateExec_shuffle_task.png",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Feb 13 11:14:48 UTC 2023,,,,,,,,,,"0|z1fg8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/23 06:57;gurwls223;How much fast is it?;;;","13/Feb/23 09:57;xuanzhiang;[~gurwls223] The percentile is thirty percent faster than the approx_percentile.  It doesn't make sense. I see approx_percentile have a long time shuffle read task left. But percentile is normal . I'll repeat the problem later.;;;","13/Feb/23 11:14;xuanzhiang;spark version : 3.2.1

hadoop version : 3.0.0

here are job info and shuffle read task info ;;;",,,,,,,,,,,,
Spark 3.3 Backport: SPARK-41344 Reading V2 datasource masks underlying error,SPARK-42222,13521838,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,kecheung,kecheung,28/Jan/23 07:12,11/Feb/23 04:44,30/Oct/23 17:26,11/Feb/23 04:44,3.3.0,3.3.1,,,,,,,,,,,,,,,,,,3.3.2,,,,SQL,,,,,0,,,,,"The underlying error message is thrown away, leading to a misleading/non-user friendly error message.

Full description in https://issues.apache.org/jira/browse/SPARK-41344. I will backport this to Spark 3.3",,,,,,,,,,,,,SPARK-41344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Jan 28 07:27:08 UTC 2023,,,,,,,,,,"0|z1ffy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/23 07:21;kecheung;Added link to backport

https://github.com/apache/spark/pull/39779;;;","28/Jan/23 07:27;kecheung;[~gurwls223] do you think you can review this?;;;",,,,,,,,,,,,,
"Make ""SPARK-34674: Close SparkContext after the Main method has finished"" configurable",SPARK-42219,13521805,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,attilapiros,attilapiros,attilapiros,27/Jan/23 19:59,27/Jan/23 21:48,30/Oct/23 17:26,,3.1.2,3.1.3,3.2.0,3.2.1,3.2.2,3.2.3,3.3.0,3.3.1,,,,,,,,,,,,,,,,Kubernetes,,,,,0,,,,,"We run into an error after an upgrade from Spark 3.1 to Spark 3.2 cased by SPARK-34674 which closed the SparkContext right after the application start. This application was a spark job server built on top of springboot so all the job submits were outside of the main method.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jan 27 21:48:24 UTC 2023,,,,,,,,,,"0|z1ffrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/23 20:18;attilapiros;I will open a PR regarding this;;;","27/Jan/23 21:47;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/39775;;;","27/Jan/23 21:48;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/39775;;;",,,,,,,,,,,,
Branch-3.4 daily test failed,SPARK-42214,13521765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,LuciferYang,LuciferYang,27/Jan/23 14:27,30/Jan/23 12:26,30/Oct/23 17:26,30/Jan/23 12:26,3.5.0,,,,,,,,,,,,,,,,,,,,,,,Project Infra,,,,,0,,,,,https://github.com/apache/spark/actions/runs/4023012095/jobs/6913400923,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jan 30 12:26:32 UTC 2023,,,,,,,,,,"0|z1ffig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/23 14:36;LuciferYang;It seems that the environment of the daily test is different from that of the submitted test. For example, the Pandas version of the daily test is only 1.3.5(should be 1.5.3).

 ;;;","27/Jan/23 15:07;LuciferYang;{code:java}
WARNING: You are using pip version 22.0.3; however, version 22.3.1 is available.
84You should consider upgrading via the '/usr/bin/python3.9 -m pip install --upgrade pip' command.
85WARNING: The directory '/github/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H  {code}
There is such message in the failed tasks

 

and pip list result as follows:

 
{code:java}
Package                   Version
------------------------- --------------------
alembic                   1.7.6
certifi                   2019.11.28
chardet                   3.0.4
click                     8.0.3
cloudpickle               2.0.0
coverage                  6.3.1
cycler                    0.11.0
databricks-cli            0.16.4
dbus-python               1.2.16
docker                    5.0.3
entrypoints               0.4
Flask                     2.0.2
fonttools                 4.29.1
gitdb                     4.0.9
GitPython                 3.1.26
greenlet                  1.1.2
gunicorn                  20.1.0
idna                      2.8
importlib-metadata        4.10.1
itsdangerous              2.0.1
Jinja2                    3.0.3
joblib                    1.1.0
kiwisolver                1.3.2
Mako                      1.1.6
MarkupSafe                2.0.1
matplotlib                3.5.1
mlflow                    1.23.1
numpy                     1.22.2
packaging                 21.3
pandas                    1.3.5
Pillow                    9.0.1
pip                       22.0.3
plotly                    5.5.0
prometheus-client         0.13.1
prometheus-flask-exporter 0.18.7
protobuf                  3.19.4
pyarrow                   7.0.0
PyGObject                 3.36.0
pyparsing                 3.0.7
python-apt                2.0.0+ubuntu0.20.4.6
python-dateutil           2.8.2
pytz                      2021.3
PyYAML                    6.0
querystring-parser        1.2.4
requests                  2.22.0
requests-unixsocket       0.2.0
scikit-learn              1.0.2
scipy                     1.8.0
setuptools                45.2.0
six                       1.14.0
sklearn                   0.0
smmap                     5.0.0
SQLAlchemy                1.4.31
sqlparse                  0.4.2
tabulate                  0.8.9
tenacity                  8.0.1
threadpoolctl             3.1.0
urllib3                   1.25.8
websocket-client          1.2.3
Werkzeug                  2.0.3
wheel                     0.34.2
xmlrunner                 1.7.7
zipp                      3.7.0 {code}
{code:java}
Package             Version
------------------- --------------------
certifi             2019.11.28
cffi                1.14.6
chardet             3.0.4
coverage            6.3.1
cycler              0.11.0
dbus-python         1.2.16
fonttools           4.29.1
greenlet            0.4.13
hpy                 0.0.3
idna                2.8
kiwisolver          1.3.2
matplotlib          3.5.1
numpy               1.21.5
packaging           21.3
pandas              1.3.5
Pillow              9.0.1
pip                 22.0.3
PyGObject           3.36.0
pyparsing           3.0.7
python-apt          2.0.0+ubuntu0.20.4.6
python-dateutil     2.8.2
pytz                2021.3
readline            6.2.4.1
requests            2.22.0
requests-unixsocket 0.2.0
scipy               1.7.3
setuptools          45.2.0
six                 1.14.0
urllib3             1.25.8
wheel               0.34.2 {code}
Some packages are missing here, and the existing package versions are also lower than those used in the master;;;","28/Jan/23 05:58;LuciferYang;[~gurwls223] [~yikun]  branch-3.4 daily build has some failed tasks...
Because I am not familiar with python, I lack a solution to this error. I want to know whether the failed GA task in branch-3.4 daily build needs `infra-image` task.

 

[https://github.com/apache/spark/actions/runs/4023012095/jobs/6913400923];;;","28/Jan/23 06:05;gurwls223;cc [~itholic]and [~XinrongM] FYI;;;","28/Jan/23 06:30;yikunkero;https://github.com/apache/spark/blob/c8b1e526b4a980550c4eeab541f7cd3d5aa6e0f2/.github/workflows/build_and_test.yml#L61
https://github.com/apache/spark/blob/c8b1e526b4a980550c4eeab541f7cd3d5aa6e0f2/.github/workflows/build_and_test.yml#L276

We should also add the `inputs.branch == 'branch-3.4'` in these two lines.

The branch-* sheduled job is based on master branch workflow.;;;","28/Jan/23 06:56;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39778;;;","28/Jan/23 06:57;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39778;;;","30/Jan/23 12:26;yikunkero;Branch-3.4 scheduled job recovered:

https://github.com/apache/spark/pull/39778#issuecomment-1408528171;;;",,,,,,,
`build/sbt` should allow SBT_OPTS to override JVM memory setting,SPARK-42201,13521661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,26/Jan/23 20:16,27/Jan/23 00:39,30/Oct/23 17:26,27/Jan/23 00:39,2.4.8,3.0.3,3.1.3,3.2.3,3.3.1,3.4.0,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jan 27 00:39:57 UTC 2023,,,,,,,,,,"0|z1fevk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/23 20:21;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39758;;;","27/Jan/23 00:39;dongjoon;Issue resolved by pull request 39758
[https://github.com/apache/spark/pull/39758];;;",,,,,,,,,,,,,
groupByKey creates columns that may conflict with exising columns,SPARK-42199,13521621,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,EnricoMi,EnricoMi,26/Jan/23 14:43,26/Jan/23 15:45,30/Oct/23 17:26,,3.0.3,3.1.3,3.2.3,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Calling {{ds.groupByKey(func: V => K)}} creates columns to store the key value. These columns may conflict with columns that already exist in {{ds}}. Function {{Dataset.groupByKey.agg}} accounts for this with a very specific rule, which has some surprising weaknesses:
{code:scala}
spark.range(1)
  // groupByKey adds column 'value'
  .groupByKey(id => id)
  // which cannot be referenced, though it is suggested
  .agg(count(""value""))
{code}
{code:java}
org.apache.spark.sql.AnalysisException: Column 'value' does not exist. Did you mean one of the following? [value, id];
{code}
An existing 'value' column can be referenced:
{code:scala}
// dataset with column 'value'
spark.range(1).select($""id"".as(""value"")).as[Long]
  // groupByKey adds another column 'value'
  .groupByKey(id => id)
  // agg accounts for the extra column and excludes it when resolving 'value'
  .agg(count(""value""))
  .show()
{code}
{code:java}
+---+------------+
|key|count(value)|
+---+------------+
|  0|           1|
+---+------------+
{code}
While column suggestion shows both 'value' columns:
{code:scala}
spark.range(1).select($""id"".as(""value"")).as[Long]
  .groupByKey(id => id)
  .agg(count(""unknown""))
{code}
{code:java}
org.apache.spark.sql.AnalysisException: Column 'unknown' does not exist. Did you mean one of the following? [value, value]
{code}

However, {{mapValues}} introduces another 'value' column, which should be referencable, but it breaks the exclusion introduced by {{agg}}:
{code:scala}
spark.range(1)
  // groupByKey adds column 'value'
  .groupByKey(id => id)
  // adds another 'value' column
  .mapValues(value => value)
  // which cannot be referenced in agg
  .agg(count(""value""))
{code}
{code:java}
org.apache.spark.sql.AnalysisException: Reference 'value' is ambiguous, could be: value, value.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jan 26 15:45:39 UTC 2023,,,,,,,,,,"0|z1femo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/23 15:45;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39754;;;",,,,,,,,,,,,,,
spark.read fails to read filenames with accented characters,SPARK-42198,13521590,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tariqueanwer,tariqueanwer,26/Jan/23 08:16,10/Mar/23 17:19,30/Oct/23 17:26,,3.2.1,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"Unable to read filenames with accented characters in the filename.

*Sample error:*
{code:java}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 43 in stage 1.0 failed 4 times, most recent failure: Lost task 43.3 in stage 1.0 (TID 105) (10.139.64.5 executor 0): java.io.FileNotFoundException: /4842022074360943/user/hive/warehouse/hls_cms_source.db/raw_files/synthea_mass/ccda/Amalia471_Magaña874_3912696a-0aef-492e-83ef-468262b82966.xml{code}
 

*{{Steps to reproduce error:}}*
{code:java}
%sh
mkdir -p /dbfs/user/hive/warehouse/hls_cms_source.db/raw_files/synthea_mass
wget  https://synthetichealth.github.io/synthea-sample-data/downloads/synthea_sample_data_ccda_sep2019.zip -O ./synthea_sample_data_ccda_sep2019.zip 
unzip ./synthea_sample_data_ccda_sep2019.zip -d /dbfs/user/hive/warehouse/hls_cms_source.db/raw_files/synthea_mass/
{code}
 
{code:java}
spark.conf.set(""spark.sql.caseSensitive"", ""true"")
df = (
  spark.read.format('xml')
   .option(""rowTag"", ""ClinicalDocument"")
  .load('/user/hive/warehouse/hls_cms_source.db/raw_files/synthea_mass/ccda/')
){code}
Is there a way to deal with this situation where I don't have control over the file names for some reason?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 17:19:07 UTC 2023,,,,,,,,,,"0|z1fefs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/23 18:06;narekdw;I think, you have missed `dbfs` part in the path.
{code:java}
spark.conf.set(""spark.sql.caseSensitive"", ""true"")
df = (
  spark.read.format('xml')
   .option(""rowTag"", ""ClinicalDocument"")
  .load('/dbfs/user/hive/warehouse/hls_cms_source.db/raw_files/synthea_mass/ccda/José_Emilio366_Macías944_1e740307-8780-4542-abeb-7037a2557a0e.xml')
) {code}
it works for me.;;;","30/Jan/23 12:15;tariqueanwer;I have updated the original comment to remove the specific file name. I'm trying to read all the XML files in the folder together. While it works just fine for files without accented characters in their filename, I start getting error as soon as one is mixed in the lot.

Even if I try to read a single file with the accented character, as in the comment above, I get an error.
{code:java}
spark.conf.set(""spark.sql.caseSensitive"", ""true"")
df = (
  spark.read.format('xml')
   .option(""rowTag"", ""ClinicalDocument"")
  .load('/dbfs/user/hive/warehouse/hls_cms_source.db/raw_files/synthea_mass/ccda/José_Emilio366_Macías944_1e740307-8780-4542-abeb-7037a2557a0e.xml')
){code}
 
Error:

 
{code:java}
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: dbfs:/dbfs/user/hive/warehouse/hls_cms_source.db/raw_files/synthea_mass/ccda/José_Emilio366_Macías944_1e740307-8780-4542-abeb-7037a2557a0e.xml{code}
 

 ;;;","10/Mar/23 17:19;srowen;You would not add /dbfs on Databricks in this case, that's not relevant or the issue.
What if you escape the path as if in a URL?;;;",,,,,,,,,,,,
Typo in StreamingQuery.scala,SPARK-42196,13521585,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ganeshchand,ganeshchand,ganeshchand,26/Jan/23 05:40,30/Jan/23 05:43,30/Oct/23 17:26,30/Jan/23 05:43,3.2.3,,,,,,,,,,,,,,,,,,,3.4.0,,,,Structured Streaming,,,,,0,,,,,"{color:#172b4d}/**{color}
{color:#172b4d} * Returns the unique id of this run of the query. That is, every start/restart of a query *will*{color}
{color:#172b4d} ** *generated* a unique runId. Therefore, every time a query is restarted from{color}
{color:#172b4d} * checkpoint, it will have the same [[id]] but different [[runId]]s.{color}
{color:#172b4d} */{color}
{color:#172b4d}def runId: UUID{color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,,,,Mon Jan 30 05:43:43 UTC 2023,,,,,,,,,,"0|z1feeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/23 05:55;ganeshchand;I have created a PR with a fix - https://github.com/apache/spark/pull/39750;;;","26/Jan/23 05:56;apachespark;User 'ganeshchand' has created a pull request for this issue:
https://github.com/apache/spark/pull/39750;;;","26/Jan/23 05:57;apachespark;User 'ganeshchand' has created a pull request for this issue:
https://github.com/apache/spark/pull/39750;;;","30/Jan/23 05:43;gurwls223;Issue resolved by pull request 39750
[https://github.com/apache/spark/pull/39750];;;",,,,,,,,,,,
dataframe API filter criteria throwing ParseException when reading a JDBC column name with special characters,SPARK-42193,13521576,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Cannot Reproduce,,kc.shanmugavel,kc.shanmugavel,26/Jan/23 02:29,13/Feb/23 07:18,30/Oct/23 17:26,13/Feb/23 06:58,3.3.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"*On Spark 3.3.0,* when reading from a JDBC table(used SQLite to repro) using spark.read.jdbc command with sqlite-jdbc:3.34.0.jar on a table and column name containing special characters. Dataframe API filter criteria fails with parse Exception 

*[#Script:]*
{code:java}
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName(""Databricks Support"") \
    .config(""spark.jars.packages"", ""org.xerial:sqlite-jdbc:3.34.0"") \
    .getOrCreate()

columns = [""id"", ""/abc/column"", ""value""]
data = [(1, 'A', 100), (2, 'B', 200), (3, 'B', 300)]

rdd = spark.sparkContext.parallelize(data)
df = spark.createDataFrame(rdd).toDF(*columns)

options = {""url"": ""jdbc:sqlite:/<local-path>/spark-3.3.1-bin-hadoop3/jars/test.db"", ""dbtable"": '""/abc/table""', ""driver"": ""org.sqlite.JDBC""}

df.coalesce(1).write.format(""jdbc"").options(**options).mode(""append"").save()

df_1 = spark.read.format(""jdbc"") \
    .option(""url"", ""jdbc:sqlite:/<local-path>/spark-3.3.1-bin-hadoop3/jars/test.db"") \
    .option(""dbtable"", '""/abc/table""') \
    .option(""driver"", ""org.sqlite.JDBC"") \
    .load()

df_2 = df_1.filter(""`/abc/column` = 'B'"")

df_2.show() {code}
Error:
{code:java}
``` Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
 File ""/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/pyspark/sql/dataframe.py"", line 606, in show
  print(self._jdf.showString(n, 20, vertical))
 File ""/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py"", line 1321, in __call__
 File ""/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/pyspark/sql/utils.py"", line 196, in deco
  raise converted from None
pyspark.sql.utils.ParseException: 
Syntax error at or near '/': extra input '/'(line 1, pos 0)

== SQL ==
/abc/column
^^^```  {code}
However, when using Spark 3.2.1, we are able to successfully apply dataframe.filter option
{code:java}
>>> df_2.show()
+---+-----------+-----+
| id|/abc/column|value|
+---+-----------+-----+
|  2|          B|  200|
|  3|          B|  300|
+---+-----------+-----+ {code}
*Repro steps:*
 # Download [Spark 3.2.1 in local |https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz]
 # Download and Copy the sqlite-jdbc:3.34.0.jar into the jar folder present in the local spark download folder
 # Run the above [#script] by providing the jar path 
 # This will create a */abc/table* with column */abc/column*  and returns result when applying filter criteria
 # Download spark ** [3.3.0 in local|https://www.apache.org/dyn/closer.lua/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz]
 # Repeat #2, #3 
 # Fails with parse exception. 

could you please let us know how we can filter on the special characters column or escape them on spark version 3.3.0?",,,,,,,,,,,,,,,,,,,SPARK-41990,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Feb 13 07:18:14 UTC 2023,,,,,,,,,,"0|z1feco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/23 06:45;maxgekk;I haven't reproduced the issue on the recent master. Seems like it has been already fixed by [~huaxingao] in https://issues.apache.org/jira/browse/SPARK-41990 also cc [~dongjoon];;;","13/Feb/23 07:18;dongjoon;+1 for [~maxgekk]'s assessment. ;;;",,,,,,,,,,,,,
Force SBT protobuf version to match Maven on branch 3.2 and 3.3,SPARK-42188,13521497,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,svaughan,svaughan,svaughan,25/Jan/23 14:41,26/Jan/23 02:31,30/Oct/23 17:26,26/Jan/23 02:31,3.2.3,3.3.1,,,,,,,,,,,,,,,,,,3.2.4,3.3.2,,,Build,,,,,0,,,,,"Update SparkBuild.scala to force SBT use of protobuf-java to match the Maven version.  The Maven dependencyManagement section forces protobuf-java to use 2.5.0, but SBT is using 3.14.0.

Snippet from Maven dependency tree

 
{noformat}
[INFO] +- com.google.crypto.tink:tink:jar:1.6.0:compile
[INFO] |  +- com.google.protobuf:protobuf-java:jar:2.5.0:compile    <--- 2.x
[INFO] |  \- com.google.code.gson:gson:jar:2.8.6:compile{noformat}
  Snippet from SBT dependency tree
{noformat}
[info]   +-com.google.crypto.tink:tink:1.6.0
[info]   | +-com.google.code.gson:gson:2.8.6
[info]   | +-com.google.protobuf:protobuf-java:3.14.0               <--- 3.x{noformat}
The fix is updating SparkBuild.scala just like SPARK-11538 did with guava.  In addition we should comment on the need to keep the top-level pom.xml and SparkBuild.scala in sync as was done in SPARK-41247

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 25 17:41:04 UTC 2023,,,,,,,,,,"0|z1fdvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 17:14;apachespark;User 'snmvaughan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39745;;;","25/Jan/23 17:14;apachespark;User 'snmvaughan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39745;;;","25/Jan/23 17:40;apachespark;User 'snmvaughan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39746;;;","25/Jan/23 17:41;apachespark;User 'snmvaughan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39746;;;",,,,,,,,,,,
Make SparkR able to stop properly when the connection is timed-out,SPARK-42186,13521442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,25/Jan/23 12:25,25/Jan/23 17:13,30/Oct/23 17:26,25/Jan/23 17:13,3.5.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SparkR,,,,,0,,,,,"{code}
./bin/sparkR --conf spark.r.backendConnectionTimeout=10
{code}

wait 10 secs.

{code}

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.4.0-SNAPSHOT
      /_/


SparkSession Web UI available at http://192.168.35.219:4040
SparkSession available as 'spark'(master = local[*], app id = local-1674649482367).
> 23/01/25 21:24:53 WARN RBackendHandler: Ignoring read timeout in RBackendHandler
spark.session()
Error in spark.session() : could not find function ""spark.session""
> spark.session.stop()
Error in spark.session.stop() :
  could not find function ""spark.session.stop""
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 25 17:13:34 UTC 2023,,,,,,,,,,"0|z1fdj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 12:42;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39742;;;","25/Jan/23 17:13;dongjoon;Issue resolved by pull request 39742
[https://github.com/apache/spark/pull/39742];;;",,,,,,,,,,,,,
Upgrade ORC to 1.7.8,SPARK-42179,13521224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,25/Jan/23 07:31,25/Jan/23 11:24,30/Oct/23 17:26,25/Jan/23 11:24,3.3.2,,,,,,,,,,,,,,,,,,,3.3.2,,,,Build,SQL,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 25 11:24:47 UTC 2023,,,,,,,,,,"0|z1fc6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 07:38;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39735;;;","25/Jan/23 11:24;dongjoon;Issue resolved by pull request 39735
[https://github.com/apache/spark/pull/39735];;;",,,,,,,,,,,,,
Change master to brach-3.4 in GitHub Actions,SPARK-42177,13521201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,25/Jan/23 03:21,25/Jan/23 03:38,30/Oct/23 17:26,25/Jan/23 03:36,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Project Infra,,,,,0,,,,,See https://github.com/apache/spark/actions/runs/4002380215/jobs/6869886029,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 25 03:38:52 UTC 2023,,,,,,,,,,"0|z1fc1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 03:36;gurwls223;Issue resolved by pull request 39731
[https://github.com/apache/spark/pull/39731];;;","25/Jan/23 03:37;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39730;;;","25/Jan/23 03:38;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39731;;;","25/Jan/23 03:38;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39731;;;",,,,,,,,,,,
Cast boolean to timestamp fails with ClassCastException,SPARK-42176,13521193,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,25/Jan/23 00:49,25/Jan/23 03:34,30/Oct/23 17:26,25/Jan/23 03:33,3.3.1,3.4.0,3.5.0,,,,,,,,,,,,,,,,,3.3.2,3.4.0,3.5.0,,SQL,,,,,0,,,,,"When casting a boolean value to timestamp, the following error is thrown:
{code:java}
[info]   java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
[info]   at scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:107)
[info]   at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getWriter$5(InternalRow.scala:178)
[info]   at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getWriter$5$adapted(InternalRow.scala:178) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 25 03:33:47 UTC 2023,,,,,,,,,,"0|z1fbzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 02:05;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39729;;;","25/Jan/23 03:33;gurwls223;Issue resolved by pull request 39729
[https://github.com/apache/spark/pull/39729];;;",,,,,,,,,,,,,
Use scikit-learn instead of sklearn,SPARK-42174,13521185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,24/Jan/23 22:49,25/Jan/23 00:29,30/Oct/23 17:26,25/Jan/23 00:29,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Project Infra,PySpark,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 25 00:29:36 UTC 2023,,,,,,,,,,"0|z1fby0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 22:51;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39727;;;","25/Jan/23 00:29;gurwls223;Issue resolved by pull request 39727
[https://github.com/apache/spark/pull/39727];;;",,,,,,,,,,,,,
Fix `pyspark-errors` module and enable it in GitHub Action,SPARK-42171,13521153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,24/Jan/23 18:15,24/Jan/23 21:08,30/Oct/23 17:26,24/Jan/23 21:08,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,PySpark,Tests,,,,0,3.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 24 21:08:24 UTC 2023,,,,,,,,,,"0|z1fbqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 18:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39721;;;","24/Jan/23 21:08;dongjoon;Issue resolved by pull request 39721
[https://github.com/apache/spark/pull/39721];;;",,,,,,,,,,,,,
"Files added to the spark-submit command with master K8s and deploy mode cluster, end up in a non deterministic location inside the driver.",SPARK-42170,13521128,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,santosh.pingale,santosh.pingale,24/Jan/23 14:24,24/Jan/23 14:25,30/Oct/23 17:26,,3.2.2,3.3.0,,,,,,,,,,,,,,,,,,,,,,Kubernetes,Spark Submit,,,,0,,,,,"Files added to the spark-submit command with master K8s and deploy mode cluster, end up in a non deterministic location inside the driver.

eg:

{{spark-submit --files myfile --master k8s.. --deploy-mode cluster` will upload the files to /tmp/spark-uuid/myfile}}

The issue happens because [Utils.createTempDir()|https://github.com/apache/spark/blob/v3.3.1/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L344] creates a directory with a uuid in the directory name. This issue does not affect the --archives option, because we `unarchive` the archives into the destination directory which is relative to the working dir. This bug affects file access pre & post app creation. For example if we distribute python dependencies with pex, we need to use --files to attach the pex file and change the spark.pyspark.python to point to this file. But the file location can not be determined before submitting the app. On the other hand, after the app is created, referencing the files without using `SparkFiles.get` also does not work",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-01-24 14:24:42.0,,,,,,,,,,"0|z1fblk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoGroup with window function returns incorrect result when partition keys differ in order,SPARK-42168,13521114,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EnricoMi,EnricoMi,EnricoMi,24/Jan/23 12:12,30/Jan/23 10:55,30/Oct/23 17:26,26/Jan/23 01:43,3.0.3,3.1.3,3.2.3,,,,,,,,,,,,,,,,,3.2.4,,,,PySpark,SQL,,,,0,correctness,,,,"The following example returns an incorrect result:
{code:java}
import pandas as pd

from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, lit, sum

spark = SparkSession \
    .builder \
    .getOrCreate()

ids = 1000
days = 1000
parts = 10

id_df = spark.range(ids)
day_df = spark.range(days).withColumnRenamed(""id"", ""day"")
id_day_df = id_df.join(day_df)
left_df = id_day_df.select(col(""id"").alias(""id""), col(""day"").alias(""day""), lit(""left"").alias(""side"")).repartition(parts).cache()
right_df = id_day_df.select(col(""id"").alias(""id""), col(""day"").alias(""day""), lit(""right"").alias(""side"")).repartition(parts).cache()  #.withColumnRenamed(""id"", ""id2"")

# note the column order is different to the groupBy(""id"", ""day"") column order below
window = Window.partitionBy(""day"", ""id"")

left_grouped_df = left_df.groupBy(""id"", ""day"")
right_grouped_df = right_df.withColumn(""day_sum"", sum(col(""day"")).over(window)).groupBy(""id"", ""day"")

def cogroup(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:
    return pd.DataFrame([{
        ""id"": left[""id""][0] if not left.empty else (right[""id""][0] if not right.empty else None),
        ""day"": left[""day""][0] if not left.empty else (right[""day""][0] if not right.empty else None),
        ""lefts"": len(left.index),
        ""rights"": len(right.index)
    }])

df = left_grouped_df.cogroup(right_grouped_df) \
         .applyInPandas(cogroup, schema=""id long, day long, lefts integer, rights integer"")

df.explain()
df.show(5)
{code}
Output is
{code}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- FlatMapCoGroupsInPandas [id#8L, day#9L], [id#29L, day#30L], cogroup(id#8L, day#9L, side#10, id#29L, day#30L, side#31, day_sum#54L), [id#64L, day#65L, lefts#66, rights#67]
   :- Sort [id#8L ASC NULLS FIRST, day#9L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(id#8L, day#9L, 200), ENSURE_REQUIREMENTS, [plan_id=117]
   :     +- ...
   +- Sort [id#29L ASC NULLS FIRST, day#30L ASC NULLS FIRST], false, 0
      +- Project [id#29L, day#30L, id#29L, day#30L, side#31, day_sum#54L]
         +- Window [sum(day#30L) windowspecdefinition(day#30L, id#29L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS day_sum#54L], [day#30L, id#29L]
            +- Sort [day#30L ASC NULLS FIRST, id#29L ASC NULLS FIRST], false, 0
               +- Exchange hashpartitioning(day#30L, id#29L, 200), ENSURE_REQUIREMENTS, [plan_id=112]
                  +- ...


+---+---+-----+------+
| id|day|lefts|rights|
+---+---+-----+------+
|  0|  3|    0|     1|
|  0|  4|    0|     1|
|  0| 13|    1|     0|
|  0| 27|    0|     1|
|  0| 31|    0|     1|
+---+---+-----+------+
only showing top 5 rows
{code}
The first child is hash-partitioned by {{id}} and {{{}day{}}}, while the second child is hash-partitioned by {{day}} and {{id}} (required by the window function). Therefore, rows end up in different partitions.

This has been fixed in Spark 3.3 by [#32875|https://github.com/apache/spark/pull/32875/files#diff-e938569a4ca4eba8f7e10fe473d4f9c306ea253df151405bcaba880a601f075fR75-R76]:
{code}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- FlatMapCoGroupsInPandas [id#8L, day#9L], [id#29L, day#30L], cogroup(id#8L, day#9L, side#10, id#29L, day#30L, side#31, day_sum#54L)#63, [id#64L, day#65L, lefts#66, rights#67]
   :- Sort [id#8L ASC NULLS FIRST, day#9L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(id#8L, day#9L, 200), ENSURE_REQUIREMENTS, [plan_id=117]
   :     +- ...
   +- Sort [id#29L ASC NULLS FIRST, day#30L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(id#29L, day#30L, 200), ENSURE_REQUIREMENTS, [plan_id=118]
         +- Project [id#29L, day#30L, id#29L, day#30L, side#31, day_sum#54L]
            +- Window [sum(day#30L) windowspecdefinition(day#30L, id#29L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS day_sum#54L], [day#30L, id#29L]
               +- Sort [day#30L ASC NULLS FIRST, id#29L ASC NULLS FIRST], false, 0
                  +- Exchange hashpartitioning(day#30L, id#29L, 200), ENSURE_REQUIREMENTS, [plan_id=112]
                     +- ...

+---+---+-----+------+
| id|day|lefts|rights|
+---+---+-----+------+
|  0| 13|    1|     1|
|  0| 63|    1|     1|
|  0| 89|    1|     1|
|  0| 95|    1|     1|
|  0| 96|    1|     1|
+---+---+-----+------+
only showing top 5 rows
{code}

Only PySpark is to be affected ({{FlatMapCoGroupsInPandas }}), as Scala API uses {{CoGroup}}. {{FlatMapCoGroupsInPandas}} reports required child distribution {{ClusteredDistribution}}, while {{CoGroup}} reports {{HashClusteredDistribution}}. The {{EnsureRequirements}} rule correctly recognizes a {{HashClusteredDistribution(id, day)}} as not compatible with {{hashpartitioning(day, id)}}, while {{ClusteredDistribution(id, day)}} is compatible with {{hashpartitioning(day, id)}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jan 30 10:55:11 UTC 2023,,,,,,,,,,"0|z1fbig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 14:06;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39717;;;","26/Jan/23 01:43;gurwls223;Issue resolved by pull request 39717
[https://github.com/apache/spark/pull/39717];;;","26/Jan/23 07:24;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39752;;;","28/Jan/23 10:49;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39781;;;","30/Jan/23 10:55;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39803;;;",,,,,,,,,,
Schema pruning fails on non-foldable array index or map key,SPARK-42163,13521048,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,David Cashman,David Cashman,David Cashman,23/Jan/23 23:08,31/Jan/23 02:17,30/Oct/23 17:26,31/Jan/23 02:17,3.2.3,,,,,,,,,,,,,,,,,,,3.4.0,,,,Optimizer,,,,,0,,,,,"Schema pruning tries to extract selected fields from struct extractors. It looks through GetArrayItem/GetMapItem, but when doing so, it ignores the index/key, which may itself be a struct field. If it is a struct field that is not otherwise selected, and some other field of the same attribute is selected, then pruning will drop the field, resulting in an optimizer error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 31 02:17:02 UTC 2023,,,,,,,,,,"0|z1fb40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 15:05;apachespark;User 'cashmand' has created a pull request for this issue:
https://github.com/apache/spark/pull/39718;;;","31/Jan/23 02:17;cloud_fan;Issue resolved by pull request 39718
[https://github.com/apache/spark/pull/39718];;;",,,,,,,,,,,,,
Memory usage on executors increased drastically for a complex query with large number of addition operations,SPARK-42162,13521035,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scnakandala,scnakandala,scnakandala,23/Jan/23 20:43,10/Feb/23 15:57,30/Oct/23 17:26,10/Feb/23 15:57,3.3.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"With the [recent changes|https://github.com/apache/spark/pull/37851]  in the expression canonicalization, a complex query with a large number of Add operations ends up consuming 10x more memory on the executors.

The reason for this issue is that with the new changes the canonicalization process ends up generating lot of intermediate objects, especially for complex queries with a large number of commutative operators. In this specific case, a heap histogram analysis shows that a large number of Add objects use the extra memory.
This issue does not happen before PR [#37851.|https://github.com/apache/spark/pull/37851]

The high memory usage causes the executors to lose heartbeat signals and results in task failures.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Feb 10 15:57:16 UTC 2023,,,,,,,,,,"0|z1fb1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 18:49;apachespark;User 'db-scnakandala' has created a pull request for this issue:
https://github.com/apache/spark/pull/39722;;;","24/Jan/23 18:50;apachespark;User 'db-scnakandala' has created a pull request for this issue:
https://github.com/apache/spark/pull/39722;;;","10/Feb/23 15:57;cloud_fan;Issue resolved by pull request 39722
[https://github.com/apache/spark/pull/39722];;;",,,,,,,,,,,,
`spark.scheduler.mode=FAIR` should provide FAIR scheduler,SPARK-42157,13520943,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,22/Jan/23 22:39,24/Jan/23 07:48,30/Oct/23 17:26,24/Jan/23 07:48,2.2.3,2.3.4,2.4.8,3.0.3,3.1.3,3.2.3,3.3.1,3.4.0,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,Spark Core,,,,,0,,,,, !Screenshot 2023-01-22 at 2.39.34 PM.png! ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/23 22:39;dongjoon;Screenshot 2023-01-22 at 2.39.34 PM.png;https://issues.apache.org/jira/secure/attachment/13054765/Screenshot+2023-01-22+at+2.39.34+PM.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 24 07:48:52 UTC 2023,,,,,,,,,,"0|z1fagw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/23 22:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39703;;;","24/Jan/23 07:48;dongjoon;Issue resolved by pull request 39703
[https://github.com/apache/spark/pull/39703];;;",,,,,,,,,,,,,
Support client-side retries in Spark Connect Python client,SPARK-42156,13520940,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,grundprinzip-db,grundprinzip-db,grundprinzip-db,22/Jan/23 17:57,31/Jan/23 07:05,30/Oct/23 17:26,31/Jan/23 07:05,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Connect,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-39375,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 31 07:05:43 UTC 2023,,,,,,,,,,"0|z1fag8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/23 18:29;apachespark;User 'grundprinzip' has created a pull request for this issue:
https://github.com/apache/spark/pull/39695;;;","31/Jan/23 07:05;gurwls223;Issue resolved by pull request 39695
[https://github.com/apache/spark/pull/39695];;;",,,,,,,,,,,,,
Fix getPartitionFiltersAndDataFilters() to handle filters without referenced attributes,SPARK-42134,13520766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,20/Jan/23 13:46,21/Jan/23 02:38,30/Oct/23 17:26,21/Jan/23 02:38,3.4.0,,,,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jan 20 14:00:42 UTC 2023,,,,,,,,,,"0|z1f9e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/23 14:00;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/39676;;;",,,,,,,,,,,,,,
DeduplicateRelations rule breaks plan when co-grouping the same DataFrame,SPARK-42132,13520424,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,EnricoMi,EnricoMi,20/Jan/23 10:53,09/Oct/23 13:34,30/Oct/23 17:26,11/Aug/23 02:28,3.0.3,3.1.3,3.2.3,3.3.0,3.3.1,3.4.0,3.5.0,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,correctness,pull-request-available,,,"Co-grouping two DataFrames that share references breaks on the DeduplicateRelations rule:
{code:java}
val df = spark.range(3)

val left_grouped_df = df.groupBy(""id"").as[Long, Long]
val right_grouped_df = df.groupBy(""id"").as[Long, Long]

val cogroup_df = left_grouped_df.cogroup(right_grouped_df) {
  case (key, left, right) => left
}

cogroup_df.explain()
{code}
{code:java}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- SerializeFromObject [input[0, bigint, false] AS value#12L]
   +- CoGroup, id#0: bigint, id#0: bigint, id#0: bigint, [id#13L], [id#13L], [id#13L], [id#13L], obj#11: bigint
      :- !Sort [id#13L ASC NULLS FIRST], false, 0
      :  +- !Exchange hashpartitioning(id#13L, 200), ENSURE_REQUIREMENTS, [plan_id=16]
      :     +- Range (0, 3, step=1, splits=16)
      +- Sort [id#13L ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(id#13L, 200), ENSURE_REQUIREMENTS, [plan_id=17]
            +- Range (0, 3, step=1, splits=16)
{code}

The DataFrame cannot be computed:
{code:java}
cogroup_df.show()
{code}
{code:java}
java.lang.IllegalStateException: Couldn't find id#13L in [id#0L]
{code}

The rule replaces `id#0L` on the right side with `id#13L` while replacing all occurrences in `CoGroup`. Some occurrences of `id#0L` in `CoGroup`refer to the left side and should not be replaced. Further, `id#0L` of the right deserializer is not replaced.",,,,,,,,,,,,,,,,SPARK-43781,,SPARK-43781,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jan 20 11:22:36 UTC 2023,,,,,,,,,,"0|z1f7a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/23 11:21;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39673;;;","20/Jan/23 11:22;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39673;;;",,,,,,,,,,,,,
"Spark 3.3.0, Error with java.io.IOException: Mkdirs failed to create file",SPARK-42127,13520272,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,shamim_er123,shamim_er123,20/Jan/23 04:16,28/Mar/23 11:40,30/Oct/23 17:26,10/Mar/23 17:19,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"23/01/18 20:23:24 WARN TaskSetManager: Lost task 4.0 in stage 0.0 (TID 4) (10.64.109.72 executor 0): java.io.IOException: Mkdirs failed to create file:/var/backup/_temporary/0/_temporary/attempt_202301182023173234741341853025716_0005_m_000004_0 (exists=false, cwd=file:/opt/spark-3.3.0/work/app-20230118202317-0001/0)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)
        at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)
        at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)
        at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)
        at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750) ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 17:19:40 UTC 2023,,,,,,,,,,"0|z1f6c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 00:54;gurwls223;[~shamim_er123]how did you face this error? would be great if there are steps to reproduce this.;;;","10/Mar/23 17:19;srowen;No detail here, not obvious that this isn't just a permissions issue;;;",,,,,,,,,,,,,
Push down limit through Python UDFs,SPARK-42115,13520190,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,19/Jan/23 11:52,02/Feb/23 01:01,30/Oct/23 17:26,02/Feb/23 01:01,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,PySpark,SQL,,,,0,,,,,"{code}
from pyspark.sql.functions import udf

spark.range(10).write.mode(""overwrite"").parquet(""/tmp/abc"")

@udf(returnType='string')
def my_udf(arg):
    return arg


df = spark.read.parquet(""/tmp/abc"")
df.limit(10).withColumn(""prediction"", my_udf(df[""id""])).explain()
{code}

As an example. since Python UDFs are executed asynchronously, so pushing limits benefit the performance.

{code}
== Physical Plan ==
CollectLimit 10
+- *(2) Project [id#3L, pythonUDF0#10 AS prediction#6]
   +- BatchEvalPython [my_udf(id#3L)#5], [pythonUDF0#10]
      +- *(1) ColumnarToRow
         +- FileScan parquet [id#3L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/abc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>
{code}

This is a regression from Spark 3.3.1:

{code}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [id#3L, pythonUDF0#10 AS prediction#6]
   +- BatchEvalPython [my_udf(id#3L)#5], [pythonUDF0#10]
      +- GlobalLimit 10
         +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=30]
            +- LocalLimit 10
               +- FileScan parquet [id#3L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/abc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Feb 02 01:01:34 UTC 2023,,,,,,,,,,"0|z1f5u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/23 13:17;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39653;;;","01/Feb/23 08:17;apachespark;User 'kelvinjian-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/39842;;;","02/Feb/23 01:01;cloud_fan;Issue resolved by pull request 39842
[https://github.com/apache/spark/pull/39842];;;",,,,,,,,,,,,
Upgrade pandas to 1.5.3,SPARK-42113,13520157,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,19/Jan/23 09:21,19/Jan/23 16:54,30/Oct/23 17:26,19/Jan/23 16:54,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Pandas API on Spark,,,,,0,,,,,To support latest pandas.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jan 19 16:54:58 UTC 2023,,,,,,,,,,"0|z1f5mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/23 09:26;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/39651;;;","19/Jan/23 16:54;dongjoon;Issue resolved by pull request 39651
[https://github.com/apache/spark/pull/39651];;;",,,,,,,,,,,,,
Add null check before `ContinuousWriteRDD#compute` method close dataWriter,SPARK-42112,13520153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,19/Jan/23 08:17,20/Jan/23 03:24,30/Oct/23 17:26,20/Jan/23 03:24,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"Run 

 

mvn clean test -pl sql/core -Dtest=none -DwildcardSuites=org.apache.spark.sql.streaming.continuous.ContinuousSuite -am

 

NPE exists in the test log

 
{code:java}
- repeatedly restart
...
16:07:39.891 ERROR org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD: Writer for partition 1 is aborting.
16:07:39.891 ERROR org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD: Writer for partition 1 aborted.
16:07:39.892 WARN org.apache.spark.util.Utils: Suppressing exception in finally: null
java.lang.NullPointerException
    at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.$anonfun$compute$7(ContinuousWriteRDD.scala:91)
    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1558)
    at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.compute(ContinuousWriteRDD.scala:91)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
    at org.apache.spark.scheduler.Task.run(Task.scala:139)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1502)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750) {code}
 

 

The test did not fail because Utils.tryWithSafeFinallyAndFailureCallbacks function suppressed Exception in finally block

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jan 20 03:24:40 UTC 2023,,,,,,,,,,"0|z1f5m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/23 08:30;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39650;;;","20/Jan/23 03:24;dongjoon;This is resolved via https://github.com/apache/spark/pull/39650;;;",,,,,,,,,,,,,
Upgrade Kafka to 3.3.2,SPARK-42109,13520107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,18/Jan/23 20:06,18/Jan/23 22:38,30/Oct/23 17:26,18/Jan/23 22:38,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Structured Streaming,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 18 22:38:08 UTC 2023,,,,,,,,,,"0|z1f5cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/23 20:15;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39646;;;","18/Jan/23 20:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39646;;;","18/Jan/23 22:38;dongjoon;Issue resolved by pull request 39646
[https://github.com/apache/spark/pull/39646];;;",,,,,,,,,,,,
Spark 3.3.0 binary breaking change missing from release notes,SPARK-42107,13520022,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,rozza,rozza,18/Jan/23 10:28,25/Jan/23 01:01,30/Oct/23 17:26,,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Java API,,,,,0,,,,,"SPARK-37929 contains a binary breaking change in the SupportsNamespaces API


See: [https://github.com/apache/spark/pull/35246/files#r792289685]

 

There is no mention in the [release notes|https://spark.apache.org/releases/spark-release-3-3-0.html]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 25 01:01:05 UTC 2023,,,,,,,,,,"0|z1f4tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 01:01;gurwls223;cc [~cloud_fan] and [~dchvn]. I think we should at least add them into release notes.;;;",,,,,,,,,,,,,,
Throw ExecutorDeadException in fetchBlocks when executor dead,SPARK-42104,13519943,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,warrenzhu25,warrenzhu25,17/Jan/23 19:12,17/Jan/23 19:12,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"When fetchBlocks failed due to IOException, ExecutorDeadException will be thrown when executor is dead.

There're other cases that executor dead will cause TimeoutException or other Exceptions.
{code:java}
Caused by: java.lang.RuntimeException: java.util.concurrent.TimeoutException: Waited 30000 milliseconds (plus 143334 nanoseconds delay) for SettableFuture@624de392[status=PENDING]
    at org.sparkproject.guava.base.Throwables.propagate(Throwables.java:243)
    at org.apache.spark.network.client.TransportClient.sendRpcSync(TransportClient.java:293)
    at org.apache.spark.network.crypto.AuthClientBootstrap.doSparkAuth(AuthClientBootstrap.java:113)
    at org.apache.spark.network.crypto.AuthClientBootstrap.doBootstrap(AuthClientBootstrap.java:80)
    at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:300)
    at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)
    at org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)
    at org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)
    at org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-01-17 19:12:44.0,,,,,,,,,,"0|z1f4cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResolveInlineTables should handle RuntimeReplaceable,SPARK-42098,13519852,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,cloud_fan,cloud_fan,17/Jan/23 06:11,28/Jul/23 04:47,30/Oct/23 17:26,28/Jul/23 04:46,3.3.1,,,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,,,,,"spark-sql> VALUES (try_divide(5, 0));
cannot evaluate expression try_divide(5, 0) in inline table definition; line 1 pos 8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jul 28 04:46:41 UTC 2023,,,,,,,,,,"0|z1f3s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/23 17:26;dtenedor;[~cloud_fan]  [~srielau]  I can help with this if you guys need help;;;","28/Jul/23 04:46;cloud_fan;Issue resolved by pull request 42110
[https://github.com/apache/spark/pull/42110];;;",,,,,,,,,,,,,
Support `fill_value` for `ps.Series.add`,SPARK-42094,13519830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,17/Jan/23 01:43,08/Feb/23 00:17,30/Oct/23 17:26,08/Feb/23 00:17,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,Pandas API on Spark,,,,,0,,,,,For pandas function parity: https://pandas.pydata.org/docs/reference/api/pandas.Series.add.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Feb 08 00:17:08 UTC 2023,,,,,,,,,,"0|z1f3nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/23 08:50;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/39790;;;","08/Feb/23 00:17;gurwls223;Issue resolved by pull request 39790
[https://github.com/apache/spark/pull/39790];;;",,,,,,,,,,,,,
Introduce sasl retry count in RetryingBlockTransferor,SPARK-42090,13519791,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuzhihong@gmail.com,yuzhihong@gmail.com,yuzhihong@gmail.com,16/Jan/23 14:00,24/Jan/23 18:24,30/Oct/23 17:26,17/Jan/23 06:49,3.4.0,,,,,,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,Spark Core,,,,,0,,,,,"Previously a boolean variable, saslTimeoutSeen, was used in RetryingBlockTransferor. However, the boolean variable wouldn't cover the following scenario:

1. SaslTimeoutException
2. IOException
3. SaslTimeoutException
4. IOException

Even though IOException at #2 is retried (resulting in increment of retryCount), the retryCount would be cleared at step #4.
Since the intention of saslTimeoutSeen is to undo the increment due to retrying SaslTimeoutException, we should keep a counter for SaslTimeoutException retries and subtract the value of this counter from retryCount.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jan 23 18:59:51 UTC 2023,,,,,,,,,,"0|z1f3ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/23 14:05;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/39611;;;","17/Jan/23 06:49;mridulm80;Issue resolved by pull request 39611
[https://github.com/apache/spark/pull/39611];;;","18/Jan/23 00:13;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39634;;;","18/Jan/23 00:13;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39632;;;","18/Jan/23 00:14;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39634;;;","23/Jan/23 18:55;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39709;;;","23/Jan/23 18:56;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39709;;;","23/Jan/23 18:59;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39710;;;",,,,,,,
Running python3 setup.py sdist on windows reports a permission error,SPARK-42088,13519711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zheju_he,zheju_he,zheju_he,16/Jan/23 08:02,17/Jan/23 00:38,30/Oct/23 17:26,17/Jan/23 00:38,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Build,,,,,0,,,,,"My system version is windows 10, and I can run setup.py with administrator permissions, so there will be no error. However, it may be troublesome for us to upgrade permissions with Windows Server, so we need to modify the code of setup.py to ensure no error. To avoid the hassle of compiling for the user, I suggest modifying the following code to enable the out-of-the-box effect
{code:python}
def _supports_symlinks():
    """"""Check if the system supports symlinks (e.g. *nix) or not.""""""
    return getattr(os, ""symlink"", None) is not None and ctypes.windll.shell32.IsUserAnAdmin() != 0 if sys.platform == ""win32"" else True
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 17 00:38:58 UTC 2023,,,,,,,,,,"0|z1f2x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/23 08:09;zheju_he;This is my pr address https://github.com/apache/spark/pull/39603;;;","16/Jan/23 08:09;apachespark;User 'zekai-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/39603;;;","17/Jan/23 00:38;gurwls223;Issue resolved by pull request 39603
[https://github.com/apache/spark/pull/39603];;;",,,,,,,,,,,,
Avoid leaking the qualified-access-only restriction,SPARK-42084,13519694,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,16/Jan/23 05:38,18/Jan/23 10:59,30/Oct/23 17:26,18/Jan/23 10:44,3.3.1,,,,,,,,,,,,,,,,,,,3.3.2,3.4.0,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 18 10:44:22 UTC 2023,,,,,,,,,,"0|z1f2tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/23 06:00;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39596;;;","16/Jan/23 06:00;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39596;;;","18/Jan/23 10:44;cloud_fan;Issue resolved by pull request 39596
[https://github.com/apache/spark/pull/39596];;;",,,,,,,,,,,,
`core` module requires `javax.servlet-api`,SPARK-42072,13519657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Cannot Reproduce,,dongjoon,dongjoon,15/Jan/23 11:49,17/Jan/23 04:20,30/Oct/23 17:26,17/Jan/23 04:20,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 17 04:20:23 UTC 2023,,,,,,,,,,"0|z1f2l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/23 11:52;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39579;;;","15/Jan/23 11:53;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39579;;;","17/Jan/23 04:20;dongjoon;I verified on a clean Apple Silicon machine and master branch works fine. I'm close this issue for now.;;;",,,,,,,,,,,,
Data duplicate or data lost with non-deterministic function,SPARK-42069,13519636,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,leejianwei,leejianwei,15/Jan/23 03:44,15/Jan/23 03:57,30/Oct/23 17:26,,3.0.0,3.1.0,3.2.3,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"When write table with shuffle data and non-deterministic function, data may be duplicate or lost due to retry task attempt.

For example:
{quote}
insert overwrite table target_table partition(ds)
select ... from a join b join c...
ditributed by ds, cast(rand()*10 as int)
{quote}

As rand() is non deterministic, the order of input to shuffle data may change in retry task. a row that is already present in another shuffe output might get distributed again to a new shuffle output (causing data duplication) or some row might not get any shuffle out as the designated shuffle output might have already finished (causing data loss).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-01-15 03:44:31.0,,,,,,,,,,"0|z1f2gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The DATATYPE_MISMATCH error class contains inappropriate and duplicating subclasses,SPARK-42066,13519167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,srielau,srielau,14/Jan/23 21:56,30/Jan/23 10:56,30/Oct/23 17:26,30/Jan/23 10:56,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,,,0,,,,,"subclass WRONG_NUM_ARGS (with suggestions) semantically does not belong into DATATYPE_MISMATCH and there is an error class with that same name.
We should rea the subclasses for this errorclass, which seems to have become a bit of a dumping ground...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jan 30 10:56:11 UTC 2023,,,,,,,,,,"0|z1ezk8:",9223372036854775807,,,,,maxgekk,,,,,,,,,,,,,,,,,,"17/Jan/23 05:35;itholic;Let me take a look;;;","17/Jan/23 11:16;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/39625;;;","30/Jan/23 10:56;cloud_fan;Issue resolved by pull request 39625
[https://github.com/apache/spark/pull/39625];;;",,,,,,,,,,,,
Mark Expressions that have state has stateful,SPARK-42061,13518549,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,cloud_fan,cloud_fan,14/Jan/23 03:03,18/Jan/23 01:54,30/Oct/23 17:26,18/Jan/23 01:54,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 18 01:54:41 UTC 2023,,,,,,,,,,"0|z1evqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/23 22:11;apachespark;User 'lzlfred' has created a pull request for this issue:
https://github.com/apache/spark/pull/39630;;;","18/Jan/23 01:54;cloud_fan;Issue resolved by pull request 39630
[https://github.com/apache/spark/pull/39630];;;",,,,,,,,,,,,,
Update ORC to 1.8.2,SPARK-42059,13518458,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,14/Jan/23 00:02,14/Jan/23 02:57,30/Oct/23 17:26,14/Jan/23 02:57,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Build,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Jan 14 02:57:16 UTC 2023,,,,,,,,,,"0|z1ev6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/23 00:05;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39561;;;","14/Jan/23 00:06;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39561;;;","14/Jan/23 02:57;dongjoon;Issue resolved by pull request 39561
[https://github.com/apache/spark/pull/39561];;;",,,,,,,,,,,,
Avoid losing exception info in Protobuf errors,SPARK-42057,13518357,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rangadi,rangadi,rangadi,13/Jan/23 18:03,14/Jan/23 04:30,30/Oct/23 17:26,14/Jan/23 04:30,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Protobuf,,,,,0,,,,,"Protobuf connector related error handlers incorrectly report the exception. This is makes it hard for users to see actual issue. E.g. if there is a {{FileNotFoundException}} these error handlers use pass {{exception.getCause()}} rather than passing {{{}exception{}}}. As result, we lose the information that it was a {{FileNotFoundException}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-40653,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sat Jan 14 04:30:17 UTC 2023,,,,,,,,,,"0|z1euk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/23 18:08;apachespark;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39536;;;","14/Jan/23 04:30;Gengliang.Wang;Issue resolved by pull request 39536
[https://github.com/apache/spark/pull/39536];;;",,,,,,,,,,,,,
Add `connect-client-jvm` to connect module,SPARK-42046,13518038,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,13/Jan/23 06:12,27/Jan/23 03:52,30/Oct/23 17:26,13/Jan/23 11:13,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Tests,,,,,0,,,,,,,,,,,,,,,,,,,,,,,SPARK-42200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jan 13 11:13:21 UTC 2023,,,,,,,,,,"0|z1esls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/23 06:15;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39549;;;","13/Jan/23 11:13;gurwls223;Issue resolved by pull request 39549
[https://github.com/apache/spark/pull/39549];;;",,,,,,,,,,,,,
Kryo ClassCastException getting task result when JDK versions mismatch,SPARK-42036,13517916,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Problem,,jzhuge,jzhuge,12/Jan/23 19:29,10/Mar/23 17:20,30/Oct/23 17:26,10/Mar/23 17:20,3.3.0,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"{noformat}
22/12/21 01:27:12 ERROR TaskResultGetter: Exception while getting task result
com.esotericsoftware.kryo.KryoException: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.nio.ByteBuffer
Serialization trace:
lowerBounds (org.apache.iceberg.GenericDataFile)
taskFiles (org.apache.iceberg.spark.source.SparkWrite$TaskCommit)
writerCommitMessage (org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTaskResult)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144)
{noformat}
Iceberg 1.1 `BaseFile.lowerBounds` is defined as
{code:java}
Map<Integer, ByteBuffer> {code}
Driver JDK version: 1.8.0_352 (Azul Systems, Inc.)
Executor JDK version: openjdk version ""17.0.5"" 2022-10-18 LTS

Kryo version: 4.0.2

 

Same Spark job works when both driver and executors run the same JDK 8 or JDK 17.",,,,,,,,,,,,,,,,,,,,,,SPARK-34023,,,SPARK-42035,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Mar 10 17:20:11 UTC 2023,,,,,,,,,,"0|z1eruo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 17:20;srowen;Mismatching java versions would never be supported per se;;;",,,,,,,,,,,,,,
"QueryExecutionListener and Observation API, df.observe do not work with `foreach` action.",SPARK-42034,13517904,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Zing,hryhoriev.nick,hryhoriev.nick,12/Jan/23 17:28,10/Oct/23 03:23,30/Oct/23 17:26,13/Feb/23 05:10,3.1.3,3.2.2,3.3.1,,,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,,,0,pull-request-available,sql-api,,,"Observation API, {{observe}} dataframe transformation, and custom QueryExecutionListener.
Do not work with {{foreach}} or {{foreachPartition actions.}}
{{This is due to }}QueryExecutionListener functions do not trigger on queries whose action is {{foreach}} or {{{}foreachPartition{}}}.
But the Spark GUI SQL tab sees this query as SQL query and shows its query plans and etc.


here is the code to reproduce it:
https://gist.github.com/GrigorievNick/e7cf9ec5584b417d9719e2812722e6d3","I test it locally and on YARN in cluster mode.
Spark 3.3.1 and 3.2.2 and 3.1.1.
Yarn 2.9.2 and 3.2.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,,,,Mon Feb 13 05:10:11 UTC 2023,,,,,,,,,,"0|z1ers0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 00:53;gurwls223;I think the action has to be triggered by using `withAction` in the codes.;;;","25/Jan/23 00:53;gurwls223;please go ahead a PR if you're interested in doing that!;;;","09/Feb/23 15:47;Zing;I am interested in this pr, if no one else is in development I will fix this issue;;;","09/Feb/23 23:57;gurwls223;please go ahead;;;","10/Feb/23 07:36;hryhoriev.nick;I can do PR, but I need someone to target me to some doc or piece of code to understand how this part of spark works.
Do there any documentation or at least a diagram to show spark-sql architecture?
I really do not understand how rdd execution is linked to SQL query listeners.;;;","11/Feb/23 17:33;apachespark;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/39976;;;","13/Feb/23 05:10;gurwls223;Issue resolved by pull request 39976
[https://github.com/apache/spark/pull/39976];;;",,,,,,,,
CreateDataframe from Pandas with Struct and Timestamp,SPARK-42027,13517787,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,grundprinzip-db,grundprinzip-db,12/Jan/23 09:39,16/Jul/23 03:57,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,"The following should be supported and correctly truncate the nanosecond timestamps.

{code:python}
from datetime import datetime, timezone, timedelta
from pandas import Timestamp

ts=Timestamp(year=2019, month=1, day=1, nanosecond=500, tz=timezone(timedelta(hours=-8)))

d = pd.DataFrame({""col1"": [1], ""col2"": [{""a"":1, ""b"":2.32, ""c"":ts}]})
spark.createDataFrame(d).collect()

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Sun Jul 16 03:57:35 UTC 2023,,,,,,,,,,"0|z1er20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/23 03:28;gurwls223;Converted to a general issue in PySpark.;;;","15/Jul/23 23:42;gdhuper;[~grundprinzip-db] [~gurwls223] Is this issue up for grabs? I would like to take this on. Thanks ;;;","16/Jul/23 03:33;gurwls223;I think it will require a pretty large fix to support this correctly from my cursory look (might be wrong). Please go ahead for a PR [~gdhuper] ;;;","16/Jul/23 03:57;gdhuper;[~gurwls223] Got it. Any pointers on where to get started? I am pretty new to this codebase and wanted to get involved. Thanks 

 ;;;",,,,,,,,,,,
SparkR cannot collect dataframe with NA in a date column along with another timestamp column,SPARK-42005,13517728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Resolved,,atalvivek,atalvivek,12/Jan/23 03:56,31/Jan/23 06:26,30/Oct/23 17:26,31/Jan/23 05:05,3.3.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,SparkR,,,,,0,,,,,"This issue seems to be related with https://issues.apache.org/jira/browse/SPARK-17811, which was resolved by [https://github.com/apache/spark/pull/15421] .

If there exists a column of data type `date` which is completely NA, and another column of data type `timestamp`, then SparkR cannot collect that Spark dataframe into R dataframe.

The reproducible code snippet is below. 
{code:java}
df <- data.frame(x = as.Date(NA), y = as.POSIXct(""2022-01-01""))
SparkR::collect(SparkR::createDataFrame(df))

#> Error in handleErrors(returnStatus, conn): org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 25) (ip-10-172-210-194.us-west-2.compute.internal executor driver): java.lang.IllegalArgumentException: Invalid type N
#> at org.apache.spark.api.r.SerDe$.readTypedObject(SerDe.scala:94)
#> at org.apache.spark.api.r.SerDe$.readObject(SerDe.scala:68)
#> at #> org.apache.spark.sql.api.r.SQLUtils$.$anonfun$bytesToRow$1(SQLUtils.scala:129)
#> at org.apache.spark.sql.api.r.SQLUtils$.$anonfun$bytesToRow$1$adapted(SQLUtils.scala:128)
#> at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
#> at scala.collection.immutable.Range.foreach(Range.scala:158)
#> ...{code}
This issue does not appear If the column of `date` data type is {_}not missing{_}. Or if there _does not exist_ any other column with data type as `timestamp`.
{code:java}
df <- data.frame(x = as.Date(""2022-01-01""), y = as.POSIXct(""2022-01-01""))
SparkR::collect(SparkR::createDataFrame(df))

#>            x             y                                                         
#> 1     2022-01-01    2022-01-01{code}
or
{code:java}
df <- data.frame(x = as.Date(NA), y = as.character(""2022-01-01""))
SparkR::collect(SparkR::createDataFrame(df))

#>            x             y
#> 1        <NA>       2022-01-01{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,SPARK-18011,SPARK-17811,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,R,scala,,,Tue Jan 31 06:26:46 UTC 2023,,,,,,,,,,"0|z1eqp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/23 17:38;atalvivek;[~gurwls223], could you please check this issue? I don't have any knowledge of Scala, so I can't identify the root cause here.;;;","31/Jan/23 05:05;atalvivek;This issue is fixed by [https://github.com/apache/spark/pull/39681.|https://github.com/apache/spark/pull/39681];;;","31/Jan/23 06:26;gurwls223;Thanks for checking it [~atalvivek];;;",,,,,,,,,,,,
schema_of_json only accepts foldable expressions,SPARK-41995,13517708,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,emaynard,emaynard,11/Jan/23 23:08,11/Jan/23 23:26,30/Oct/23 17:26,,3.3.1,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"Right now schema_of_json only accepts foldable expressions, or literals. But it could be extended to accept any arbitrary expression.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 11 23:26:35 UTC 2023,,,,,,,,,,"0|z1eqlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 23:25;apachespark;User 'eric-maynard' has created a pull request for this issue:
https://github.com/apache/spark/pull/39519;;;","11/Jan/23 23:26;apachespark;User 'eric-maynard' has created a pull request for this issue:
https://github.com/apache/spark/pull/39519;;;",,,,,,,,,,,,,
Interpreted mode subexpression elimination can throw exception during insert,SPARK-41991,13517671,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,11/Jan/23 19:16,23/Feb/23 19:13,30/Oct/23 17:26,13/Jan/23 00:42,3.3.1,3.4.0,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"Example:
{noformat}
drop table if exists tbl1;
create table tbl1 (a int, b int) using parquet;

set spark.sql.codegen.wholeStage=false;
set spark.sql.codegen.factoryMode=NO_CODEGEN;

insert into tbl1
select id as a, id as b
from range(1, 5);
{noformat}
This results in the following exception:
{noformat}
java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.ExpressionProxy cannot be cast to org.apache.spark.sql.catalyst.expressions.Cast
	at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2514)
	at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2512)
{noformat}
The query produces 2 bigint values, but the table's schema expects 2 int values, so Spark wraps each output field with a {{Cast}}.

Later, in {{InterpretedUnsafeProjection}}, {{prepareExpressions}} tries to wrap the two {{Cast}} expressions with an {{ExpressionProxy}}. However, the parent expression of each {{Cast}} is a {{CheckOverflowInTableInsert}} expression, which does not accept {{ExpressionProxy}} as a child.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Feb 23 19:13:50 UTC 2023,,,,,,,,,,"0|z1eqdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 20:41;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39518;;;","11/Jan/23 20:42;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39518;;;","13/Jan/23 00:42;gurwls223;Fixed in https://github.com/apache/spark/pull/39518;;;","23/Feb/23 19:13;apachespark;User 'RunyaoChen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40140;;;",,,,,,,,,,,
Filtering by composite field name like `field name` doesn't work with pushDownPredicate = true,SPARK-41990,13517668,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,mkrasilnikova,mkrasilnikova,11/Jan/23 19:08,13/Feb/23 06:46,30/Oct/23 17:26,15/Jan/23 06:53,3.3.0,3.3.1,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"Suppose we have some table in postgresql with field `Last Name` The following code results in error

Dataset<Row> dataset = sparkSession.read()
.format(""jdbc"")
.option(""url"", myUrl)
.option(""dbtable"", ""myTable"")
.option(""user"", ""myUser"")
.option(""password"", ""muPassword"")
.load();

dataset.where(""`Last Name`='Tessel'"").show();    //error

 

 

Exception in thread ""main"" org.apache.spark.sql.catalyst.parser.ParseException: 
Syntax error at or near 'Name': extra input 'Name'(line 1, pos 5)

== SQL ==
Last Name
-----^^^

    at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseMultipartIdentifier(ParseDriver.scala:67)
    at org.apache.spark.sql.connector.expressions.LogicalExpressions$.parseReference(expressions.scala:40)
    at org.apache.spark.sql.connector.expressions.FieldReference$.apply(expressions.scala:368)
    at org.apache.spark.sql.sources.IsNotNull.toV2(filters.scala:262)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.$anonfun$unhandledFilters$1(JDBCRelation.scala:278)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.$anonfun$unhandledFilters$1$adapted(JDBCRelation.scala:278)

 

But if we set pushDownPredicate to false everything works fine.",,,,,,,,,,,,,,,,,,SPARK-42193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jan 16 06:02:03 UTC 2023,,,,,,,,,,"0|z1eqco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/23 03:24;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39524;;;","12/Jan/23 03:25;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39524;;;","14/Jan/23 01:22;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/39564;;;","14/Jan/23 01:23;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/39564;;;","15/Jan/23 06:53;dongjoon;Issue resolved by pull request 39564
[https://github.com/apache/spark/pull/39564];;;","16/Jan/23 06:02;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/39597;;;",,,,,,,,,
PYARROW_IGNORE_TIMEZONE warning can break application logging setup,SPARK-41989,13517649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,soxofaan,soxofaan,soxofaan,11/Jan/23 15:53,12/Jan/23 09:26,30/Oct/23 17:26,12/Jan/23 09:24,3.2.3,,,,,,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,PySpark,,,,,0,,,,,"in {code}python/pyspark/pandas/__init__.py{code}  there is currently a warning when {{PYARROW_IGNORE_TIMEZONE}} env var is not set (https://github.com/apache/spark/blob/187c4a9c66758e973633c5c309b551b1d9094e6e/python/pyspark/pandas/__init__.py#L44-L59):

{code:python}
    import logging

    logging.warning(
        ""'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to ""...
{code}

The {{logging.warning()}} call  will silently do a {{logging.basicConfig()}} call (at least in python 3.9, which I tried).
(FYI: Something like {{logging.getLogger(...).warning()}} would not do this silent call)


This has the following very hard to figure out side-effect:
importing `pyspark.pandas` (directly or indirectly somewhere)  might break your logging setup (if PYARROW_IGNORE_TIMEZONE is not set).

Very basic  example (assuming PYARROW_IGNORE_TIMEZONE is not set):

{code:python}
import logging
import pyspark.pandas

logging.basicConfig(level=logging.DEBUG)

logger = logging.getLogger(""test"")
logger.warning(""I warn you"")
logger.debug(""I debug you"")
{code}

Will only produce the warning, not the debug line.
By removing the {{import pyspark.pandas}}, the debug line is produced",python 3.9 env with pyspark installed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu Jan 12 09:24:40 UTC 2023,,,,,,,,,,"0|z1eq8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 15:59;soxofaan;I would propose to change 
{code:python}
    logging.warning(
        ""'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to ""...
{code}

to 
{code:python}
    logger = logging.getLogger(__name__)
    logger.warning(
        ""'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to ""...
{code}

which has the added benefit that users can trace back this warning to the module that triggered it ;;;","11/Jan/23 16:08;soxofaan;I just tried and could reproduce this problem as well with pyspark 3.3.1 on python 3.11;;;","11/Jan/23 16:28;apachespark;User 'soxofaan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39516;;;","12/Jan/23 09:24;gurwls223;Issue resolved by pull request 39516
[https://github.com/apache/spark/pull/39516];;;",,,,,,,,,,,
Centralize more column resolution rules,SPARK-41985,13517600,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Jan/23 11:31,03/Feb/23 05:07,30/Oct/23 17:26,01/Feb/23 16:25,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Feb 03 05:07:26 UTC 2023,,,,,,,,,,"0|z1epxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 11:43;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39508;;;","01/Feb/23 16:25;cloud_fan;Issue resolved by pull request 39508
[https://github.com/apache/spark/pull/39508];;;","03/Feb/23 05:07;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39867;;;",,,,,,,,,,,,
"When the inserted partition type is of string type, similar `dt=01` will be converted to `dt=1`",SPARK-41982,13517574,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zhongjingxiong,zhongjingxiong,zhongjingxiong,11/Jan/23 09:12,17/Jan/23 04:47,30/Oct/23 17:26,17/Jan/23 04:47,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,,,0,,,,,"At present, during the process of upgrading Spark2.4 to Spark3.2, we carefully read the migration documentwe and found a kind of situation not involved:
{code:java}
create table if not exists test_90(a string, b string) partitioned by (dt string);
desc formatted test_90;
// case1
insert into table test_90 partition (dt=05) values(""1"",""2"");
// case2
insert into table test_90 partition (dt='05') values(""1"",""2"");
drop table test_90;{code}
in spark2.4.3, it will generate such a path:
{code:java}
// the path
hdfs://test5/user/hive/db1/test_90/dt=05 

//result
spark-sql> select * from test_90;
1       2       05
1       2       05
Time taken: 1.316 seconds, Fetched 2 row(s)

spark-sql> show partitions test_90; 
dt=05 
Time taken: 0.201 seconds, Fetched 1 row(s)

spark-sql> select * from test_90 where dt='05';
1       2       05
1       2       05
Time taken: 0.212 seconds, Fetched 2 row(s)

spark-sql> explain insert into table test_90 partition (dt=05) values(""1"",""2"");
== Physical Plan ==
Execute InsertIntoHiveTable InsertIntoHiveTable `db1`.`test_90`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, Map(dt -> Some(05)), false, false, [a, b]
+- LocalTableScan [a#116, b#117]
Time taken: 1.145 seconds, Fetched 1 row(s){code}
in spark3.2.0, it will generate two path:
{code:java}
// the path
hdfs://test5/user/hive/db1/test_90/dt=05 
hdfs://test5/user/hive/db1/test_90/dt=5 

// result
spark-sql> select * from test_90;
1       2       05
1       2       5
Time taken: 2.119 seconds, Fetched 2 row(s)

spark-sql> show partitions test_90;
dt=05
dt=5
Time taken: 0.161 seconds, Fetched 2 row(s)

spark-sql> select * from test_90 where dt='05';
1       2       05
Time taken: 0.252 seconds, Fetched 1 row(s)

spark-sql> explain insert into table test_90 partition (dt=05) values(""1"",""2"");
plan
== Physical Plan ==
Execute InsertIntoHiveTable `db1`.`test_90`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, [dt=Some(5)], false, false, [a, b]
+- LocalTableScan [a#109, b#110]{code}
This will cause problems in reading data after the user switches to spark3. The root cause is that in the process of partition field resolution, Spark3 has a process of strongly converting this string type, which will cause partition `05` to lose the previous `0`

So I think we have two solutions:

one is to record the risk clearly in the migration document, and the other is to repair this case, because we internally keep the partition of string type as string type, regardless of whether single or double quotation marks are added.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 17 04:47:16 UTC 2023,,,,,,,,,,"0|z1eprs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 11:16;zhongjingxiong;cc [~cloud_fan] [~gurwls223] I want to know your opinion.;;;","13/Jan/23 19:11;apachespark;User 'smallzhongfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/39558;;;","13/Jan/23 19:12;apachespark;User 'smallzhongfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/39558;;;","17/Jan/23 04:47;cloud_fan;Issue resolved by pull request 39558
[https://github.com/apache/spark/pull/39558];;;",,,,,,,,,,,
`toPandas` should support duplicate filed names when arrow-optimization is on,SPARK-41971,13517524,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,podongfeng,podongfeng,11/Jan/23 03:12,04/May/23 18:07,30/Oct/23 17:26,04/May/23 18:04,3.4.0,,,,,,,,,,,,,,,,,,,3.5.0,,,,PySpark,,,,,0,,,,,"toPandas support duplicate columns name, but for a struct column, it doesnot support duplicate field names.

{code:java}
In [27]: spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", False)

In [28]: spark.sql(""select 1 v, 1 v"").toPandas()
Out[28]: 
   v  v
0  1  1

In [29]: spark.sql(""select struct(1 v, 1 v)"").toPandas()
Out[29]: 
  struct(1 AS v, 1 AS v)
0                 (1, 1)

In [30]: spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", True)

In [31]: spark.sql(""select 1 v, 1 v"").toPandas()
Out[31]: 
   v  v
0  1  1

In [32]: spark.sql(""select struct(1 v, 1 v)"").toPandas()
/Users/ruifeng.zheng/Dev/spark/python/pyspark/sql/pandas/conversion.py:204: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.
  Ran out of field metadata, likely malformed
  warn(msg)
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
Cell In[32], line 1
----> 1 spark.sql(""select struct(1 v, 1 v)"").toPandas()

File ~/Dev/spark/python/pyspark/sql/pandas/conversion.py:143, in PandasConversionMixin.toPandas(self)
    141 tmp_column_names = [""col_{}"".format(i) for i in range(len(self.columns))]
    142 self_destruct = jconf.arrowPySparkSelfDestructEnabled()
--> 143 batches = self.toDF(*tmp_column_names)._collect_as_arrow(
    144     split_batches=self_destruct
    145 )
    146 if len(batches) > 0:
    147     table = pyarrow.Table.from_batches(batches)

File ~/Dev/spark/python/pyspark/sql/pandas/conversion.py:358, in PandasConversionMixin._collect_as_arrow(self, split_batches)
    356             results.append(batch_or_indices)
    357     else:
--> 358         results = list(batch_stream)
    359 finally:
    360     # Join serving thread and raise any exceptions from collectAsArrowToPython
    361     jsocket_auth_server.getResult()

File ~/Dev/spark/python/pyspark/sql/pandas/serializers.py:55, in ArrowCollectSerializer.load_stream(self, stream)
     50 """"""
     51 Load a stream of un-ordered Arrow RecordBatches, where the last iteration yields
     52 a list of indices that can be used to put the RecordBatches in the correct order.
     53 """"""
     54 # load the batches
---> 55 for batch in self.serializer.load_stream(stream):
     56     yield batch
     58 # load the batch order indices or propagate any error that occurred in the JVM

File ~/Dev/spark/python/pyspark/sql/pandas/serializers.py:98, in ArrowStreamSerializer.load_stream(self, stream)
     95 import pyarrow as pa
     97 reader = pa.ipc.open_stream(stream)
---> 98 for batch in reader:
     99     yield batch

File ~/.dev/miniconda3/envs/spark_dev/lib/python3.9/site-packages/pyarrow/ipc.pxi:638, in __iter__()

File ~/.dev/miniconda3/envs/spark_dev/lib/python3.9/site-packages/pyarrow/ipc.pxi:674, in pyarrow.lib.RecordBatchReader.read_next_batch()

File ~/.dev/miniconda3/envs/spark_dev/lib/python3.9/site-packages/pyarrow/error.pxi:100, in pyarrow.lib.check_status()

ArrowInvalid: Ran out of field metadata, likely malformed

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Thu May 04 18:07:43 UTC 2023,,,,,,,,,,"0|z1epgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 03:14;podongfeng;I think that is due to something is wrong in `ArrowConverter`.

In Spark, a schema is just a StructType, but in arrow that is not the case, a schema is a class other than datatype. This difference maybe the cause.;;;","20/Mar/23 18:57;nikj;Can I work on this issue?;;;","04/May/23 18:04;XinrongM;Issue resolved by pull request 40988
[https://github.com/apache/spark/pull/40988];;;","04/May/23 18:07;XinrongM;Hi [~nikj] , the issue has been resolved. Feel free to pick other issues that you are interested in. Normally we comment on the ticket and file the pull request afterward directly.;;;",,,,,,,,,,,
Flaky Test: StreamingQueryStatusListenerSuite.test small retained queries,SPARK-41969,13517498,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dongjoon,dongjoon,10/Jan/23 20:51,11/Jan/23 10:01,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,"I saw this failures on master branch frequently.

https://github.com/apache/spark/runs/10560025461
https://github.com/apache/spark/runs/10556299549
https://github.com/apache/spark/runs/10551101022

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 11 09:43:50 UTC 2023,,,,,,,,,,"0|z1epaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 09:13;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39504;;;","11/Jan/23 09:13;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39503;;;","11/Jan/23 09:43;panbingkun;Seems like it's duplicated to https://issues.apache.org/jira/browse/SPARK-41972?;;;",,,,,,,,,,,,
SBT unable to resolve particular packages from the imported maven build,SPARK-41967,13517454,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,vicennial,vicennial,10/Jan/23 13:41,10/Jan/23 13:41,30/Oct/23 17:26,,3.4.0,,,,,,,,,,,,,,,,,,,,,,,Connect,,,,,0,,,,,"An SBT issue causes the resolution from the imported maven build for particular packages to not work for an unknown reason. This affects Spark-Connect-related projects (see [here|https://github.com/apache/spark/blob/6cae6aa5156655c79eb3f20292ccec6c479c3b1b/project/SparkBuild.scala#L667-L668] and [here|https://github.com/apache/spark/blob/6cae6aa5156655c79eb3f20292ccec6c479c3b1b/project/SparkBuild.scala#L902-L904] for example) by forcing duplicate deps.

The pom build works fine when removing the affected dep (like guava for example) but the sbt build then fails. Thus, we are forced to explicitly mention the versions of the affected packages so that SBT can then parse the version(s) to manually include them (and they're also added as a dep in maven to ensure version consistency with sbt)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-01-10 13:41:00.0,,,,,,,,,,"0|z1ep1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow arbitrary custom classpath with proxy user in cluster mode,SPARK-41958,13517367,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,10/Jan/23 03:20,06/Jun/23 02:31,30/Oct/23 17:26,10/Jan/23 09:06,2.4.8,3.0.3,3.1.3,3.2.3,3.3.1,,,,,,,,,,,,,,,3.3.3,3.4.0,,,Spark Core,,,,,0,,,,,To avoid arbitrary classpath in spark cluster.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jun 06 00:15:49 UTC 2023,,,,,,,,,,"0|z1eok0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/23 03:26;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/39474;;;","10/Jan/23 03:27;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/39474;;;","10/Jan/23 09:06;gurwls223;Issue resolved by pull request 39474
[https://github.com/apache/spark/pull/39474];;;","01/Jun/23 18:14;degant7;[~gurwls223] [~Ngone51] is it possible to backport this fix to Spark 3.3? Since it is Severity 9.9?;;;","06/Jun/23 00:15;dongjoon;This is backported to branch-3.3 via https://github.com/apache/spark/pull/41428;;;",,,,,,,,,,
Upgrade Parquet to fix off-heap memory leaks in Zstd codec,SPARK-41952,13517298,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,chengpan,alexey.kudinkin,alexey.kudinkin,09/Jan/23 23:48,11/Oct/23 06:07,30/Oct/23 17:26,20/Feb/23 17:45,3.1.3,3.2.3,3.3.1,,,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,Input/Output,,,,,0,pull-request-available,,,,"Recently, native memory leak have been discovered in Parquet in conjunction of it using Zstd decompressor from luben/zstd-jni library (PARQUET-2160).

This is very problematic to a point where we can't use Parquet w/ Zstd due to pervasive OOMs taking down our executors and disrupting our jobs.

Luckily fix addressing this had already landed in Parquet:
[https://github.com/apache/parquet-mr/pull/982]

 

Now, we just need to
 # Updated version of Parquet is released in a timely manner
 # Spark is upgraded onto this new version in the upcoming release

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Feb 20 06:00:37 UTC 2023,,,,,,,,,,"0|z1eo4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/23 08:53;LuciferYang;For the old Spark versions, is it possible to introduce other costs by upgrading parquet?  Should we directly introduce parquet.hadoop.CodecFactory to old Spark version and fix them accordingly? 

After that, we can also revert the changes of the Spark version(for example, master and Spark 3.4) that can be solved by upgrading parquet;;;","20/Feb/23 05:38;chengpan;Fix on Spark side is feasible, I'm working on this.;;;","20/Feb/23 05:51;dongjoon;May I ask why you put me `Shepherd` field, [~alexey.kudinkin] ? Let me first remove me from there.;;;","20/Feb/23 06:00;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/40091;;;",,,,,,,,,,,
Fix NPE for error classes: CANNOT_PARSE_JSON_FIELD,SPARK-41948,13517202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,09/Jan/23 08:34,23/Jan/23 12:16,30/Oct/23 17:26,23/Jan/23 12:16,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jan 23 12:16:16 UTC 2023,,,,,,,,,,"0|z1enjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/23 08:42;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39466;;;","09/Jan/23 08:43;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39466;;;","23/Jan/23 12:16;maxgekk;Issue resolved by pull request 39466
[https://github.com/apache/spark/pull/39466];;;",,,,,,,,,,,,
Update the contents of error class guidelines,SPARK-41947,13517191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,09/Jan/23 07:23,09/Jan/23 20:24,30/Oct/23 17:26,09/Jan/23 20:24,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"The error class guidelines for `core/src/main/resources/error/README.md` is out of date, we should update the guidelines to match the current behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jan 09 20:24:21 UTC 2023,,,,,,,,,,"0|z1enhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/23 07:29;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/39464;;;","09/Jan/23 07:30;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/39464;;;","09/Jan/23 20:24;maxgekk;Issue resolved by pull request 39464
[https://github.com/apache/spark/pull/39464];;;",,,,,,,,,,,,
DiskBlockManager can't clear all of the tmp dirs in k8s,SPARK-41946,13517187,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,lfspace,lfspace,09/Jan/23 06:17,11/Jan/23 11:43,30/Oct/23 17:26,,3.2.0,3.3.1,,,,,,,,,,,,,,,,,,,,,,Block Manager,Spark Core,,,,0,,,,,"I have a test for spark on k8s  that my job had completed, but some tmp dirs are still remained. Those dirs are start with ""blockmgr"". My ""spark.local.dirs"" is mount on local NVMe SSD.

The DiskBlockManager may be clear the dirs  just one iteration ， then the process be killed.

The shuffle data will be getting larger and   fill the disk space.

!image-2023-01-09-14-03-01-840.png!","Spark 3.2.2

kubernetes v1.20.8 

jdk1.8.0_162",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-01-09 06:17:31.0,,,,,,,,,,"0|z1engg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR datetime column compare with Sys.time() throws error in R (>= 4.2.0),SPARK-41937,13517141,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,atalvivek,atalvivek,atalvivek,08/Jan/23 02:37,09/Jan/23 00:41,30/Oct/23 17:26,09/Jan/23 00:41,3.3.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,R,SparkR,,,,0,newbie,,,,"Base R 4.2.0 introduced a change ([[Rd] R 4.2.0 is released|https://stat.ethz.ch/pipermail/r-announce/2022/000683.html]), ""{{{}Calling if() or while() with a condition of length greater than one gives an error rather than a warning.{}}}""

The below code is a reproducible example of the issue. If it is executed in R >=4.2.0 then it will generate an error, or else just a warning message. `{{{}Sys.time()`{}}} is a multi-class object in R, and throughout the Spark R repository '{{{}if{}}}' statement is used as: `{{{}if(class(x) == ""Column""){}}}` - this causes error in the latest R version >= 4.2.0. Note that R allows an object to have multiple '{{{}class{}}}' names as a character vector ([R: Object Classes|https://stat.ethz.ch/R-manual/R-devel/library/base/html/class.html]); hence this type of check itself was not a good idea in the first place.

The below chunks are executed on R version 4.1.3.
{code:java}
{
 SparkR::sparkR.session()
 t <- Sys.time()
 sdf <- SparkR::createDataFrame(data.frame(x = t + c(-1, 1, -1, 1, -1)))
 SparkR::collect(SparkR::filter(sdf, SparkR::column('x') > t))
}
#> Warning in if (class(e2) == 'Column') {: the condition has length > 1 
#> and only the first element will be used
#> x
#> 1 2023-01-07 20:40:20
#> 2 2023-01-07 20:40:20 

{code}
 

 
{code:java}
{
 Sys.setenv(`_R_CHECK_LENGTH_1_CONDITION_` = ""true"")
 SparkR::sparkR.session()
 t <- Sys.time()
 sdf <- SparkR::createDataFrame(data.frame(x = t + c(-1, 1, -1, 1, -1)))
 SparkR::collect(SparkR::filter(sdf, SparkR::column('x') > t))
}
#> Error in h(simpleError(msg, call)): error in evaluating the argument 'x' 
#> in selecting a method for function 'collect': error in evaluating the 
#> argument 'condition' in selecting a method for function 'filter': the
#> condition has length > 1 {code}
 

Similar issue is noted for these SparkR functions where {{Sys.time()}} type of multi-class data might be used: {{lit, fillna, when, otherwise, contains, ifelse }}

The suggested change is to add the `{{{}all{}}}` function (or `{{{}any{}}}`, as appropriate) while doing the check of whether `{{{}class(.){}}}` is `{{{}Column{}}}` or not: `{{{}if(all(class(.) == ""Column"")){}}}`. Or, better to use `{{{}base::inherits{}}}` for this check as `{{{}if(inherits(., ""Column"")){}}}`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,R,,,,Mon Jan 09 00:41:17 UTC 2023,,,,,,,,,,"0|z1en68:",9223372036854775807,,,,,atalvivek,,,,,,,,,,,,,,,,,,"08/Jan/23 03:39;apachespark;User 'atalv' has created a pull request for this issue:
https://github.com/apache/spark/pull/39454;;;","08/Jan/23 03:40;apachespark;User 'atalv' has created a pull request for this issue:
https://github.com/apache/spark/pull/39454;;;","09/Jan/23 00:41;gurwls223;Issue resolved by pull request 39454
[https://github.com/apache/spark/pull/39454];;;",,,,,,,,,,,,
Task throw Exception call cleanUpAllAllocatedMemory cause throw NPE ,SPARK-41920,13516925,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,angerszhuuuu,angerszhuuuu,06/Jan/23 03:32,06/Jan/23 03:32,30/Oct/23 17:26,,3.2.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"{code:java}
23/01/03 21:41:18 INFO SortBasedPusher: Pushdata is not empty , do push.
Traceback (most recent call last):
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/rcmd_feature/appcache/application_1671694574014_2488441/container_e260_1671694574014_2488441_01_000107/pyspark.zip/pyspark/daemon.py"", line 186, in manager
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/rcmd_feature/appcache/application_1671694574014_2488441/container_e260_1671694574014_2488441_01_000107/pyspark.zip/pyspark/daemon.py"", line 74, in worker
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/rcmd_feature/appcache/application_1671694574014_2488441/container_e260_1671694574014_2488441_01_000107/pyspark.zip/pyspark/worker.py"", line 643, in main
    if read_int(infile) == SpecialLengths.END_OF_STREAM:
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/rcmd_feature/appcache/application_1671694574014_2488441/container_e260_1671694574014_2488441_01_000107/pyspark.zip/pyspark/serializers.py"", line 564, in read_int
    raise EOFError
EOFError
23/01/03 21:41:29 ERROR Executor: Exception in task 605.1 in stage 94.0 (TID 58026)
java.lang.NullPointerException
	at org.apache.spark.memory.TaskMemoryManager.getPage(TaskMemoryManager.java:399)
	at org.apache.spark.shuffle.rss.SortBasedPusher.pushData(SortBasedPusher.java:155)
	at org.apache.spark.shuffle.rss.SortBasedPusher.spill(SortBasedPusher.java:317)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:177)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:289)
	at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:116)
	at org.apache.spark.sql.execution.python.HybridRowQueue.createNewQueue(RowQueue.scala:227)
	at org.apache.spark.sql.execution.python.HybridRowQueue.add(RowQueue.scala:250)
	at org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$10(EvalPythonExec.scala:125)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1159)
	at scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-01-06 03:32:01.0,,,,,,,,,,"0|z1elug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sorting issue with partitioned-writing and planned write optimization disabled,SPARK-41914,13516724,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EnricoMi,EnricoMi,EnricoMi,05/Jan/23 21:05,13/Jan/23 07:07,30/Oct/23 17:26,10/Jan/23 05:10,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,correctness,,,,"Spark 3.4.0 introduced option {{{}spark.sql.optimizer.plannedWrite.enabled{}}}, which is enabled by default. When disabled, partitioned writing loses in-partition order when spilling occurs.

This is related to SPARK-40885 where setting option {{spark.sql.optimizer.plannedWrite.enabled}} to {{true}} will remove the existing sort (for {{day}} and {{{}id{}}}) entirely.

Run this with 512m memory and one executor, e.g.:
{code}
spark-shell --driver-memory 512m --master ""local[1]""
{code}
{code:scala}
import org.apache.spark.sql.SaveMode

spark.conf.set(""spark.sql.optimizer.plannedWrite.enabled"", false)

val ids = 2000000
val days = 2
val parts = 2

val ds = spark.range(0, days, 1, parts).withColumnRenamed(""id"", ""day"").join(spark.range(0, ids, 1, parts))

ds.repartition($""day"")
  .sortWithinPartitions($""day"", $""id"")
  .write
  .partitionBy(""day"")
  .mode(SaveMode.Overwrite)
  .csv(""interleaved.csv"")
{code}
Check the written files are sorted (states OK when file is sorted):
{code:bash}
for file in interleaved.csv/day\=*/part-*
do
  echo ""$(sort -n ""$file"" | md5sum | cut -d "" "" -f 1)  $file""
done | md5sum -c
{code}
Files should look like this
{code}
0
1
2
...
1048576
1048577
1048578
...
{code}
But they look like
{code}
0
1048576
1
1048577
2
1048578
...
{code}
The cause issue is the same as in SPARK-40588. A sort (for {{{}day{}}}) is added on top of the existing sort (for {{day}} and {{{}id{}}}). Spilling interleaves the sorted spill files.

{code}
Sort [input[0, bigint, false] ASC NULLS FIRST], false, 0
+- AdaptiveSparkPlan isFinalPlan=false
   +- Sort [day#2L ASC NULLS FIRST, id#4L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(day#2L, 200), REPARTITION_BY_COL, [plan_id=30]
         +- BroadcastNestedLoopJoin BuildLeft, Inner
            :- BroadcastExchange IdentityBroadcastMode, [plan_id=28]
            :  +- Project [id#0L AS day#2L]
            :     +- Range (0, 2, step=1, splits=2)
            +- Range (0, 2000000, step=1, splits=2)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 10 05:10:26 UTC 2023,,,,,,,,,,"0|z1ekls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/23 12:33;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39431;;;","10/Jan/23 05:10;cloud_fan;Issue resolved by pull request 39431
[https://github.com/apache/spark/pull/39431];;;",,,,,,,,,,,,,
Subquery should not validate CTE,SPARK-41912,13516714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,amaliujia,amaliujia,amaliujia,05/Jan/23 19:04,06/Jan/23 03:31,30/Oct/23 17:26,06/Jan/23 03:31,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Fri Jan 06 03:31:02 UTC 2023,,,,,,,,,,"0|z1ekjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/23 19:08;apachespark;User 'amaliujia' has created a pull request for this issue:
https://github.com/apache/spark/pull/39414;;;","06/Jan/23 03:31;cloud_fan;Issue resolved by pull request 39414
[https://github.com/apache/spark/pull/39414];;;",,,,,,,,,,,,,
Filtering by row_index always returns empty results,SPARK-41896,13516619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,olaky,olaky,olaky,05/Jan/23 09:17,16/Jan/23 12:18,30/Oct/23 17:26,14/Jan/23 04:29,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,,,0,,,,,"Queries that include a filter with row_index currently always return an empty result. This is because we consider all metadata attributes constant per file [here|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala#L76] and the filter then always evaluates to false.

This should be fixed as a follow up to SPARK-41791",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jan 16 12:18:38 UTC 2023,,,,,,,,,,"0|z1ejyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/23 11:51;apachespark;User 'olaky' has created a pull request for this issue:
https://github.com/apache/spark/pull/39408;;;","14/Jan/23 04:29;cloud_fan;Issue resolved by pull request 39408
[https://github.com/apache/spark/pull/39408];;;","16/Jan/23 12:18;apachespark;User 'olaky' has created a pull request for this issue:
https://github.com/apache/spark/pull/39608;;;",,,,,,,,,,,,
sql/core module mvn clean failed,SPARK-41894,13516596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,05/Jan/23 07:28,09/Jan/23 02:22,30/Oct/23 17:26,09/Jan/23 02:22,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Structured Streaming,Tests,,,,0,,,,,"run the following commands:
 # mvn clean install -pl sql/core -am -DskipTests
 # mvn test -pl sql/core 
 # mvn clean

 

then following error:

 
{code:java}
[INFO] Spark Project Parent POM ........................... SUCCESS [  0.133 s]
[INFO] Spark Project Tags ................................. SUCCESS [  0.008 s]
[INFO] Spark Project Sketch ............................... SUCCESS [  0.007 s]
[INFO] Spark Project Local DB ............................. SUCCESS [  0.008 s]
[INFO] Spark Project Networking ........................... SUCCESS [  0.015 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  0.020 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [  0.007 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  0.008 s]
[INFO] Spark Project Core ................................. SUCCESS [  0.279 s]
[INFO] Spark Project ML Local Library ..................... SUCCESS [  0.010 s]
[INFO] Spark Project GraphX ............................... SUCCESS [  0.016 s]
[INFO] Spark Project Streaming ............................ SUCCESS [  0.039 s]
[INFO] Spark Project Catalyst ............................. SUCCESS [  0.262 s]
[INFO] Spark Project SQL .................................. FAILURE [  1.305 s]
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] Spark Project YARN ................................. SKIPPED
[INFO] Spark Project Mesos ................................ SKIPPED
[INFO] Spark Project Kubernetes ........................... SKIPPED
[INFO] Spark Project Hive Thrift Server ................... SKIPPED
[INFO] Spark Ganglia Integration .......................... SKIPPED
[INFO] Spark Project Hadoop Cloud Integration ............. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED
[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED
[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED
[INFO] Spark Kinesis Integration .......................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED
[INFO] Spark Avro ......................................... SKIPPED
[INFO] Spark Project Connect Common ....................... SKIPPED
[INFO] Spark Project Connect Server ....................... SKIPPED
[INFO] Spark Project Connect Client ....................... SKIPPED
[INFO] Spark Protobuf ..................................... SKIPPED
[INFO] Spark Project Kinesis Assembly ..................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  2.896 s
[INFO] Finished at: 2023-01-05T15:15:57+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:3.1.0:clean (default-clean) on project spark-sql_2.13: Failed to clean project: Failed to delete /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc -> [Help 1]
 {code}
 

 

run :
 * ll /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc

 

 
{code:java}
-rw-r--r-- 1 work work 12 Dec 28 16:06 /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc{code}
 

 

and current user(work) can't rm this file:
 * rm  /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc

 
{code:java}
rm: cannot remove `/${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc': Permission denied {code}
need to use root to clean this file

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jan 09 02:22:39 UTC 2023,,,,,,,,,,"0|z1ejtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/23 07:30;LuciferYang;The running environment is linux,  I haven't found the specific case that generated this file now, need more investigation

 

 

 ;;;","05/Jan/23 08:31;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39406;;;","05/Jan/23 08:32;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39406;;;","09/Jan/23 02:22;kabhwan;Issue resolved by pull request 39406
[https://github.com/apache/spark/pull/39406];;;",,,,,,,,,,,
--packages may not work on Windows 11,SPARK-41885,13516501,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,zsxwing,zsxwing,04/Jan/23 17:11,04/Jan/23 17:11,30/Oct/23 17:26,,3.2.1,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,"Gastón Ortiz reported an issue when using spark 3.2.1 and hadoop 2.7 in windows 11. See [https://github.com/delta-io/delta/issues/1059]

Looks like executor cannot fetch the jar files. See the critical stack trace below (the full stack trace is in [https://github.com/delta-io/delta/issues/1059] ):
{code:java}
org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:366) at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:762) at org.apache.spark.util.Utils$.fetchFile(Utils.scala:549) at org.apache.spark.executor.Executor.$anonfun$updateDependencies$13(Executor.scala:962) at org.apache.spark.executor.Executor.$anonfun$updateDependencies$13$adapted(Executor.scala:954) at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985) at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149) at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237) at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44) at scala.collection.mutable.HashMap.foreach(HashMap.scala:149) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984) at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:954) at org.apache.spark.executor.Executor.<init>(Executor.scala:247) at  {code}
This is not a Delta Lake issue, as this can be reproduced by running `pyspark --packages org.apache.kafka:kafka-clients:2.8.1` as well.

I don't have a Windows 11 environment to debug. Hence I help Gastón Ortiz create this ticket and it would be great if anyone who has a Windows 11 environment can help this.",Hadoop 2.7 in windows 11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,2023-01-04 17:11:38.0,,,,,,,,,,"0|z1ej88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CreateHiveTableAsSelectCommand should set the overwrite flag correctly,SPARK-41859,13516322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,03/Jan/23 12:33,04/Jan/23 05:09,30/Oct/23 17:26,04/Jan/23 05:09,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Wed Jan 04 05:09:45 UTC 2023,,,,,,,,,,"0|z1ei4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/23 13:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39365;;;","03/Jan/23 13:03;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39365;;;","04/Jan/23 05:09;cloud_fan;Issue resolved by pull request 39365
[https://github.com/apache/spark/pull/39365];;;",,,,,,,,,,,,
Fix ORC reader perf regression due to DEFAULT value feature,SPARK-41858,13516291,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,03/Jan/23 09:48,03/Jan/23 18:45,30/Oct/23 17:26,03/Jan/23 18:40,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,A huge ORC reader perf regression is detected by SPARK-41782. The root cause was SPARK-39862.,,,,,,,,,,,,,,,,,,,,SPARK-39862,,SPARK-41862,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Tue Jan 03 18:40:57 UTC 2023,,,,,,,,,,"0|z1ehxk:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,,"03/Jan/23 10:43;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39362;;;","03/Jan/23 18:40;dongjoon;Issue resolved by pull request 39362
[https://github.com/apache/spark/pull/39362];;;",,,,,,,,,,,,,
Tasks are over-scheduled with TaskResourceProfile,SPARK-41848,13516236,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ivoson,Ngone51,Ngone51,03/Jan/23 02:44,09/Jan/23 20:03,30/Oct/23 17:26,09/Jan/23 20:03,3.4.0,,,,,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,,,0,,,,,"{code:java}
test(""SPARK-XXX"") {
  val conf = new SparkConf().setAppName(""test"").setMaster(""local-cluster[1,4,1024]"")
  sc = new SparkContext(conf)
  val req = new TaskResourceRequests().cpus(3)
  val rp = new ResourceProfileBuilder().require(req).build()

  val res = sc.parallelize(Seq(0, 1), 2).withResources(rp).map { x =>
    Thread.sleep(5000)
    x * 2
  }.collect()
  assert(res === Array(0, 2))
} {code}
In this test, tasks are supposed to be scheduled in order since each task requires 3 cores but the executor only has 4 cores. However, we noticed 2 tasks are launched concurrently from the logs.

It turns out that we used the TaskResourceProfile (taskCpus=3) of the taskset for task scheduling:
{code:java}
val rpId = taskSet.taskSet.resourceProfileId
val taskSetProf = sc.resourceProfileManager.resourceProfileFromId(rpId)
val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(taskSetProf, conf) {code}
but the ResourceProfile (taskCpus=1) of the executor for updating the free cores in ExecutorData:
{code:java}
val rpId = executorData.resourceProfileId
val prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)
val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(prof, conf)
executorData.freeCores -= taskCpus {code}
which results in the inconsistency of the available cores.",,,,,,,,,,,,,,,,,,,,SPARK-39853,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,,Mon Jan 09 20:03:37 UTC 2023,,,,,,,,,,"0|z1ehlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/23 02:51;Ngone51;cc [~ivoson] ;;;","05/Jan/23 14:31;apachespark;User 'ivoson' has created a pull request for this issue:
https://github.com/apache/spark/pull/39410;;;","09/Jan/23 20:03;dongjoon;Issue resolved by pull request 39410
[https://github.com/apache/spark/pull/39410];;;",,,,,,,,,,,,
